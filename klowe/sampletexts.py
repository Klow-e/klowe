

# klowe/sampletexts.py


###############################################################################################


def KSampleTexts():
    """
    Prints every variable that starts with 'ksampletext_', that's to say, every sample text in KlowE.
    """
    for i in globals(): print(i) if i.startswith("ksampletext_") else None


###############################################################################################


ksampletext_wikipedia_math_matematicas: str = "Matemáticas. Las matemáticas o, también, la matemática (del latín mathematĭca, y a la vez del griego, transliterado como, derivado de μάθημα, tr. máthēma ‘lo que se comprende’) es una ciencia formal que estudia los patrones, propiedades, estructuras y relaciones presentes en sistemas lógicos y abstractos creados por los humanos, conceptos tales como cantidad, forma, espacio y número se podrían considerar como el objeto de estudio de la matemática.[5]​[6]​[7]​[8]​[9]​ Descripción Las ciencias naturales han hecho un uso extensivo de la matemática para explicar diversos fenómenos observables, tal como lo expresó Eugene Paul Wigner (Premio Nobel de Física en 1963): «El primer punto es que la enorme utilidad de las matemáticas en las ciencias naturales es algo que roza lo misterioso y que no tiene una explicación racional. En segundo lugar, es precisamente esta extraña utilidad de los conceptos matemáticos lo que plantea la cuestión de la unicidad de nuestras teorías físicas.» [10]​ «El milagro de la adecuación del lenguaje de las matemáticas para la formulación de las leyes de la física es un don maravilloso que no comprendemos ni merecemos.» [11]​ Galileo Galilei, en la misma línea, lo había expresado así: «La filosofía está escrita en este enorme libro, que está continuamente abierto ante nuestros ojos (digo en el nuevo idioma), pero uno no puede entenderlo primero, uno no aprende a entender el idioma y a conocer los caracteres en que está escrito. Está escrito en lenguaje matemático, y los caracteres son triángulos, círculos y otras figuras geométricas, sin las cuales es imposible entender una palabra; sin éstos es un vano vagar por un oscuro laberinto.» [12]​ Mediante la abstracción y el uso de la lógica en el razonamiento, la matemática ha evolucionado basándose en el cálculo y las mediciones, junto con el estudio sistemático de la forma y el movimiento de los objetos físicos.[13] Las matemáticas, desde sus comienzos, han tenido un fin práctico. Las explicaciones que se apoyaban en la lógica aparecieron por primera vez con la matemática helénica, especialmente con los Elementos de Euclides.[14] La matemática siguió desarrollándose, con continuas interrupciones, hasta que en el Renacimiento las innovaciones matemáticas interactuaron con los nuevos descubrimientos científicos. Como consecuencia, hubo una aceleración en la investigación que continúa hasta la actualidad. Hoy día, la matemática se usa en todo el mundo como una herramienta esencial en muchos campos, entre los que se encuentran las ciencias naturales,[15] las ciencias aplicadas, las humanidades,[16]​[17]​[18] la medicina[19] y las ciencias sociales,[20]​[21]​[22] e incluso disciplinas que, aparentemente, no están vinculadas con ella, como la música[23] (por ejemplo, en cuestiones de resonancia armónica, Cuerda vibrante,[24]​[25] etc.) y la literatura.[26]​[27] Las matemáticas aplicadas, rama de la matemática destinada a la aplicación del conocimiento matemático a otros ámbitos, inspiran y hacen uso de los nuevos descubrimientos matemáticos y, en ocasiones, conducen al desarrollo de nuevas disciplinas. Los matemáticos[28] también participan en la matemática pura, sin tener en cuenta sus aplicaciones, aunque estas suelen ser descubiertas con el paso del tiempo. Historia Artículo principal: Historia de las matemáticas Las matemáticas son una de las ciencias más antiguas. Floreció primero antes de la antigüedad en Mesopotamia,[29] en cuanto a la geometría[30] India y China, y más tarde en la antigüedad en Grecia y el helenismo. De ahí data la orientación hacia la tarea de «demostración puramente lógica» y la primera axiomatización, a saber, la geometría euclidiana.[30] En la Edad Media sobrevivió de forma independiente en el primer humanismo de las universidades y en el mundo árabe. A principios de la era moderna, François Viète introdujo variables y René Descartes inauguró un enfoque computacional de la geometría[31]​[32]​[33] mediante el uso de coordenadas. La consideración de las tasas de cambio (fluxión)[34] así como la descripción de las tangentes y la determinación de los contenidos de las superficies (cuadratura)[35] condujeron al cálculo infinitesimal[13] de Gottfried Wilhelm Leibniz e Isaac Newton.[36] La mecánica de Newton y su ley de la gravitación fueron también una fuente de orientación de problemas matemáticos como el problema de los tres cuerpos[37]​[38]​[39] en los siglos siguientes. Otro de los principales problemas de la primera época moderna fue la solución de ecuaciones algebraicas cada vez más complicadas. Para hacer frente a esto, Niels Henrik Abel y Évariste Galois desarrollaron el concepto de grupo, que describe las relaciones entre las simetrías de un objeto.[40]​[41] El álgebra más reciente y, en particular, la geometría algebraica pueden considerarse como una profundización de estas investigaciones. Una idea entonces nueva en el intercambio de cartas entre Blaise Pascal y Pierre de Fermat en 1654 acerca del problema de los juegos de azar,[42]​[43]​[44] aunque existían otras soluciones discutibles como las de Cardano, quien intentó matematizarlas. Pierre-Simon Laplace hace un recuento de los diferentes logros hasta 1812 cuando publica su Ensayo filosófico sobre las posibilidades.[45] Las nuevas ideas y métodos conquistaron muchos campos. Pero durante siglos, la teoría clásica de la probabilidad se dividió en escuelas separadas. Los intentos de definir explícitamente el término «probabilidad» solo tuvieron éxito para casos especiales. Solo la publicación del libro de texto de Andrei Kolmogorov en 1933 Los fundamentos de la Teoría de la Probabilidad [46] completó el desarrollo de los fundamentos de la teoría moderna de la probabilidad. En el transcurso del siglo XIX, el cálculo infinitesimal[13] encontró su forma actual de rigor gracias a los trabajos de Augustin-Louis Cauchy y Karl Weierstrass. La teoría de conjuntos[47] desarrollada por Georg Cantor hacia finales del siglo XIX es también indispensable en la matemática actual, aunque las paradojas del concepto ingenuo de conjuntos dejaron claro, en un primer momento, la incierta base sobre la que se asentaban las matemáticas.[48]​ El desarrollo de la primera mitad del siglo XX estuvo influenciado por la publicación de los problemas de Hilbert. Uno de los problemas intentaba axiomatizar completamente las matemáticas; al mismo tiempo, se hicieron grandes esfuerzos de abstracción, es decir, el intento de reducir los objetos a sus propiedades esenciales. Así, Emmy Noether desarrolló los fundamentos del álgebra moderna,[49] Felix Hausdorff desarrolló la topología general como el estudio de los espacios topológicos, Stefan Banach desarrolló probablemente el concepto más importante del análisis funcional, el espacio de Banach que lleva su nombre. Un nivel de abstracción aún mayor, un marco común para la consideración de construcciones similares de diferentes áreas de las matemáticas, fue finalmente creado por la introducción de la teoría de categorías por Samuel Eilenberg y Saunders Mac Lane. Introducción Etimología La palabra «matemática» (del griego μαθηματικά mathēmatiká, «cosas que se aprenden») viene del griego antiguo μάθημα (máthēma), que quiere decir «campo de estudio o instrucción». Las matemáticas requieren un esfuerzo de instrucción o aprendizaje, refiriéndose a áreas del conocimiento que solo pueden entenderse tras haber sido instruido en las mismas, como la astronomía. «El arte matemática» (μαθηματική τέχνη, mathēmatikḗ tékhnē) se contrapondría en esto a la música, «el arte de las musas» (μουσική τέχνη, mousikē téchnē), que sería un arte, como la poesía, retórica[50]​[51] y similares, que se puede apreciar directamente, «que se puede entender sin haber sido instruido».[52] Aunque el término ya era usado por los pitagóricos (matematikoi) en el siglo VI a. C., alcanzó su significado más técnico y reducido de «estudio matemático» en los tiempos de Aristóteles (siglo IV a. C.). Su adjetivo es μαθηματικός (mathēmatikós), «relacionado con el aprendizaje», lo cual, de manera similar, vino a significar «matemático». En particular, μαθηματική τέχνη (mathēmatikḗ tékhnē; en latín ars mathematica), significa «el arte matemática». La forma más usada es el plural matemáticas (cuyo acortamiento, en algunos países, es «mates»[53]​[54]​), que tiene el mismo significado que el singular[2] y viene de la forma latina mathematica (Cicerón), basada en el plural en griego τα μαθηματικά (ta mathēmatiká), usada por Aristóteles y que significa, a grandes rasgos, «todas las cosas matemáticas». Algunos autores, sin embargo, hacen uso de la forma singular del término; tal es el caso de Bourbaki, en el tratado Elementos de matemática (Élements de mathématique, 1940), destaca la uniformidad de este campo aportada por la visión axiomática moderna, aunque también hace uso de la forma plural como en Éléments d'histoire des mathématiques (1969),[55] posiblemente sugiriendo que es Bourbaki quien finalmente realiza la unificación de las matemáticas.[56] Así mismo, en el escrito L'Architecture des mathématiques (1948) plantea el tema en la sección «¿Matemáticas, singular o plural?» donde defiende la unicidad conceptual de la matemática aunque hace uso de la forma plural en dicho escrito.[57]​[58]​[3]​[4]​ Algunas definiciones de matemática Establecer definiciones claras y precisas es el fundamento de la matemática, aunque encontrar una definición única para ella es improbable.[59] Se muestran algunas reflexiones de reconocidos autores: René Descartes: «Y considerando esto más atentamente al cabo se nota que solamente aquellas en las que se estudia cierto orden y medida hacen referencia a la Mathesis,[60] y que no importa si tal medida ha de buscarse en los números, en las figuras, en los astros, en los sonidos o en cualquier otro objeto;» [61]​ Carl Friedrich Gauss: «El matemático se abstrae totalmente de la naturaleza de los objetos y el contenido de sus relaciones; se preocupa únicamente por la enumeración y la comparación de las relaciones entre ellos [...]» [62]​[63]​ David Hilbert: «[...] nos lleva a una concepción de las matemáticas que considera a éstas como un inventario de fórmulas a las que corresponden, en primer lugar, expresiones concretas de enunciados finitistas y a las que se añaden, en segundo lugar, otras fórmulas que carecen de todo significado y que constituyen los objetos ideales de nuestra teoría.» [64]​[63]​ Benjamin Peirce: «La matemática es la ciencia que extrae conclusiones necesarias.» [65]​ Bertrand Russell: Trató de probar[66]​[67]​ «que toda la Matemática pura trabaja exclusivamente con conceptos definibles en función de un número muy pequeño de conceptos lógicos fundamentales, y de que todas las proposiciones se pueden deducir de un número muy pequeño de principios lógicos fundamentales.» John David Barrow: «En el fondo, matemáticas es el nombre que le damos al conjunto de todos los patrones e interrelaciones posibles. Algunos de esos patrones están entre formas, otros están en secuencias de números, mientras que otros son relaciones más abstractas entre estructuras. La esencia de las matemáticas radica en las relaciones entre cantidades y cualidades. Por lo tanto, son las relaciones entre los números, no los números en sí mismos, las que constituyen el foco de interés de los matemáticos modernos.» [5]​ Epistemología y controversia sobre la matemática como ciencia El carácter epistemológico y científico de la matemática ha sido ampliamente discutido. En la práctica, la matemática se emplea para estudiar relaciones cuantitativas, estructuras, relaciones geométricas y las magnitudes variables. Los matemáticos buscan patrones, formulan nuevas conjeturas e intentan alcanzar la verdad matemática mediante deducciones rigurosas. Estas les permiten establecer los axiomas y las definiciones apropiados para dicho fin.[59]​[70] Algunas definiciones clásicas restringen las matemáticas al razonamiento sobre cantidades,[71] aunque solo una parte de la matemática actual usa números,[72] predominando el análisis lógico de construcciones abstractas no cuantitativas. Existe cierta discusión acerca de si los objetos matemáticos, como los números[73] y puntos, realmente existen o simplemente provienen de la imaginación humana. El matemático Benjamin Peirce definió las matemáticas como «la ciencia que señala las conclusiones necesarias».[65] Por otro lado: «cuando las leyes de la matemática se refieren a la realidad, no son exactas; cuando son exactas, no se refieren a la realidad».[74]​ Albert Einstein Se ha discutido el carácter científico de las matemáticas debido a que sus procedimientos y resultados poseen una firmeza e inevitabilidad inexistentes en otras disciplinas como pueden ser la física, la química o la biología. Así, la matemática sería tautológica, infalible y a priori, mientras que otras, como la geología o la fisiología, serían falibles y a posteriori. Son estas características lo que hace dudar de colocarse en el mismo rango que las disciplinas antes citadas pese a las afirmaciones como las de John Stuart Mill quien sostenía en 1843: «En realidad, las leyes de los números son verdades físicas provenientes de la observación.»​ Así, los matemáticos pueden descubrir nuevos procedimientos para resolver integrales o teoremas, pero se muestran incapaces de descubrir un suceso que ponga en duda el Teorema de Pitágoras[76]​[77] o cualquier otro, como sí sucede constantemente con las ciencias de la naturaleza.[78]​ El teorema de Pitágoras es uno de los enunciados más conocidos y antiguos de las matemáticas.[76]​ Un ábaco, instrumento para efectuar operaciones aritméticas sencillas (sumas, restas y también multiplicaciones), fue muy utilizado en otros tiempos. La matemática puede ser entendida como ciencia; si es así debiera señalarse su objeto y su método. Sin embargo, algunos plantean que la matemática es un lenguaje formal, seguro, eficiente, aplicable al entendimiento de la naturaleza, tal como indicó Galileo; además muchos fenómenos de carácter social, otros de carácter biológico[79] o geológico, pueden ser estudiados mediante la aplicación de ecuaciones diferenciales,[80]​[81] cálculo de probabilidades o teoría de conjunto.[47] Precisamente, el avance de la física y de la química ha exigido la invención de nuevos conceptos, instrumentos y métodos en la matemática, sobre todo en el análisis real, análisis complejo y el análisis matricial.[82]​ Aspectos formales, metodológicos y estéticos La inspiración, las matemáticas puras, aplicadas y la estética Isaac Newton (1643-1727), comparte con Leibniz la autoría del desarrollo del cálculo integral y diferencial.[36]​ Es muy posible que el arte de calcular[83]​[84]​[85] haya sido desarrollado antes incluso que la escritura,[86]​[87] relacionado fundamentalmente con la contabilidad y la administración de bienes, el comercio, en la agrimensura y, posteriormente, en la astronomía. Actualmente, todas las ciencias aportan problemas que son estudiados por matemáticos, al mismo tiempo que aparecen nuevos problemas dentro de las propias matemáticas. Por ejemplo, el físico Richard Feynman propuso la integral de caminos como fundamento de la mecánica cuántica, combinando el razonamiento matemático y el enfoque de la física, pero todavía, no se ha logrado una definición plenamente satisfactoria en términos matemáticos. Igualmente, la teoría de cuerdas, una teoría científica en desarrollo que trata de unificar las cuatro fuerzas fundamentales de la física, sigue inspirando a las más modernas matemáticas.[88]​ Algunas matemáticas solo son relevantes en el área en la que estaban inspiradas y son aplicadas para otros problemas en ese campo. Sin embargo, a menudo las matemáticas inspiradas en un área concreta resultan útiles en muchos ámbitos, y se incluyen dentro de los conceptos matemáticos generales aceptados. El notable hecho de que incluso la matemática más pura habitualmente tiene aplicaciones prácticas es lo que Eugene Paul Wigner ha definido como «la irrazonable eficacia de las matemáticas en las Ciencias Naturales».[89]​[15]​ Como en la mayoría de las áreas de estudio, la explosión de los conocimientos en la era científica ha llevado a la especialización de las matemáticas. Hay una importante distinción entre las matemáticas puras y las matemáticas aplicadas. La mayoría de los matemáticos que se dedican a la investigación se centran únicamente en una de estas áreas y, a veces, la elección se realiza cuando comienzan su licenciatura. Varias áreas de las matemáticas aplicadas se han fusionado con otras áreas tradicionalmente fuera de las matemáticas y se han convertido en disciplinas independientes, como pueden ser la estadística, la investigación de operaciones o la informática. Aquellos que sienten predilección por las matemáticas, consideran que prevalece un aspecto estético que define a la mayoría de las matemáticas. Muchos matemáticos hablan de la elegancia de la matemática, su intrínseca estética y su belleza interna. En general, uno de sus aspectos más valorados es la simplicidad. Hay belleza en una simple y contundente demostración, como la demostración de Euclides de la existencia de infinitos números primos, y en un elegante análisis numérico que acelera el cálculo, así como en la transformada rápida de Fourier. Godfrey Harold Hardy en A Mathematician's Apology [90] (Apología de un matemático) expresó la convicción de que estas consideraciones estéticas son, en sí mismas, suficientes para justificar el estudio de las matemáticas puras. Los matemáticos con frecuencia se esfuerzan por encontrar demostraciones de los teoremas que son especialmente elegantes, el excéntrico matemático Paul Erdős se refiere a este hecho como la búsqueda de pruebas de El Libro en el que Dios ha escrito sus demostraciones favoritas. La popularidad de la matemática recreativa es otra señal que nos indica el placer que produce resolver las preguntas matemáticas. Notación, lenguaje y rigor Artículo principal: Notación matemática Leonhard Euler. Probablemente el más prolífico matemático de todos los tiempos. La mayor parte de la notación[97] matemática que se utiliza hoy en día no se inventó hasta el siglo XVIII.[98]​[99] Antes de eso, las matemáticas eran escritas con palabras, un minucioso proceso que limitaba el avance matemático. En el siglo XVIII, Euler, fue responsable de muchas de las notaciones empleadas en la actualidad. La notación[97] moderna hace que las matemáticas sean mucho más fácil para los profesionales, pero para los principiantes resulta complicada. La notación reduce las matemáticas al máximo, hace que algunos símbolos[99] contengan una gran cantidad de información. Al igual que la notación musical, la notación matemática moderna tiene una sintaxis estricta y codifica la información que sería difícil de escribir de otra manera. El símbolo de infinito en diferentes tipografías. El lenguaje matemático también puede ser difícil para los principiantes. Palabras tales como o y solo si tienen significados más precisos que en lenguaje cotidiano. Además, palabras como abierto y cuerpo tienen significados matemáticos muy concretos. La jerga matemática, o lenguaje matemático, incluye términos técnicos como homeomorfismo o integrabilidad. La razón que explica la necesidad de utilizar la notación y la jerga es que el lenguaje matemático requiere más precisión que el lenguaje cotidiano. Los matemáticos se refieren a esta precisión en el lenguaje y en la lógica como el «rigor». El rigor es una condición indispensable que debe tener una demostración matemática. Los matemáticos quieren que sus teoremas a partir de los axiomas sigan un razonamiento sistemático. Esto sirve para evitar teoremas erróneos, basados en intuiciones falibles, que se han dado varias veces en la historia de esta ciencia.[100] El nivel de rigor previsto en las matemáticas ha variado con el tiempo: los griegos buscaban argumentos detallados, pero en tiempos de Isaac Newton los métodos empleados eran menos rigurosos. Los problemas inherentes de las definiciones que Newton utilizaba dieron lugar a un resurgimiento de un análisis cuidadoso y a las demostraciones oficiales del siglo XIX. Ahora, los matemáticos continúan apoyándose entre ellos mediante demostraciones asistidas por ordenador.[101]​ Un axioma se interpreta tradicionalmente como una «verdad evidente», pero esta concepción es problemática. En el ámbito formal, un axioma no es más que una cadena de símbolos, que tiene un significado intrínseco solo en el contexto de todas las fórmulas derivadas de un sistema axiomático. La matemática como ciencia Carl Friedrich Gauss, apodado el «príncipe de los matemáticos», se refería a la matemática como «la reina de las ciencias». Carl Friedrich Gauss se refería a la matemática como «la reina de las ciencias». Tanto en el latín original Scientiārum Regīna, así como en alemán Königin der Wissenschaften, la palabra ciencia debe ser interpretada como (campo de) conocimiento. Si se considera que la ciencia es el estudio del mundo físico, entonces las matemáticas, o por lo menos las matemáticas puras, no son una ciencia. Muchos filósofos creen que las matemáticas no son experimentalmente falsables y, por ende, no son una ciencia según la definición de Karl Popper.[103] No obstante, en la década de 1930 una importante labor en la lógica matemática demuestra que las matemáticas no pueden reducirse a la lógica[104] y Karl Popper llegó a la conclusión de que «la mayoría de las teorías matemáticas son, como las de física y biología, hipotético-deductivas. Por lo tanto, las matemáticas puras se han vuelto más cercanas a las ciencias naturales[15] cuyas hipótesis son conjeturas, así ha sido hasta ahora».[105] Otros pensadores, en particular Imre Lakatos, han solicitado una versión de Falsacionismo[106]​[107] para las propias matemáticas.[108]​ Una visión alternativa es que determinados campos científicos (como la física teórica) son matemáticas con axiomas que pretenden corresponder a la realidad. De hecho, el físico teórico, John Michael Ziman, propone que la ciencia es «conocimiento público» y, por tanto, incluye a las matemáticas.[109] En cualquier caso, las matemáticas tienen mucho en común con distintos campos de las ciencias físicas, especialmente la exploración de las consecuencias lógicas de las hipótesis. La intuición[110] y la experimentación también desempeñan un papel importante en la formulación de conjeturas tanto en las matemáticas como en las otras ciencias. Las matemáticas experimentales siguen ganando representación dentro de las matemáticas. El cálculo[13] y simulación[111] están jugando un papel cada vez mayor tanto en las ciencias como en las matemáticas, atenuando la objeción de que las matemáticas no se sirven del método científico. En 2002 Stephen Wolfram propuso, en su libro[112] Un nuevo tipo de ciencia, que la matemática computacional merece ser explorada empíricamente como un campo científico. Las opiniones de los matemáticos sobre este asunto son muy variadas. Muchos matemáticos consideran que llamar a su campo ciencia es minimizar la importancia de su perfil estético, además supone negar su historia dentro de las siete artes liberales. Otros consideran que hacer caso omiso de su conexión con las ciencias supone ignorar la evidente conexión entre las matemáticas y sus aplicaciones en la ciencia y la ingeniería, que ha impulsado considerablemente el desarrollo de las matemáticas. Otro asunto de debate, que guarda cierta relación con el anterior, es si la matemática fue creada (como el arte) o descubierta (como la ciencia). Este es uno de los muchos temas de incumbencia de la filosofía de las matemáticas. Los premios matemáticos se mantienen generalmente separados de sus equivalentes en la ciencia. El más prestigioso premio dentro de las matemáticas es la Medalla Fields,[113] fue instaurado en 1936 y se concede cada cuatro años. A menudo se le considera el equivalente del Premio Nobel para la ciencia. Otros premios son el Premio Wolf en matemática, creado en 1978, que reconoce los logros en vida de los matemáticos, y el Premio Abel, otro gran premio internacional, que se introdujo en 2003. Estos dos últimos se conceden por un excelente trabajo, que puede ser una investigación innovadora o la solución de un problema pendiente en un campo determinado. Una famosa lista de esos 23 problemas sin resolver,[114] denominada los «Problemas de Hilbert», fue recopilada en 1900 por el matemático alemán David Hilbert. Esta lista ha alcanzado gran popularidad entre los matemáticos y, al menos, nueve de los problemas ya han sido resueltos. Una nueva lista de siete problemas fundamentales, titulada «Problemas del milenio», se publicó en 2000. La solución de cada uno de los problemas será recompensada con 1 millón de dólares. Curiosamente, tan solo uno (la hipótesis de Riemann) aparece en ambas listas. Ramas de estudio de las matemáticas Artículo principal: Áreas de las matemáticas La Sociedad Matemática Americana distingue unas 5.000 ramas distintas de matemática.[115] En una subdivisión escolarizada de la matemática se distinguen cinco áreas de estudio básicas: la cantidad, la estructura, el espacio, el cambio y la variabilidad que se corresponden con la aritmética, el álgebra, la geometría, el cálculo, la probabilidad y estadística. Como señalaba Richard Courant[116] «Es posible seguir una ruta directa a partir de los elementos fundamentales hasta puntos avanzados» para que puedan divisarse las directrices de la matemática como ciencia. Además, hay ramas de las matemáticas conectadas a otros campos, por ejemplo la lógica, teoría de conjuntos y las matemáticas aplicadas entre muchas otras tal como indica la Sociedad Matemática Americana.[115]​ Véase también: Categoría:Áreas de las matemáticas Matemática pura Artículo principal: Matemáticas puras Cantidad Números naturales Enteros Números racionales Números reales Números complejos Estructura Combinatoria Teoría de números Teoría de grupos Teoría de grafos Teoría del orden Álgebra Espacio Geometría Trigonometría Geometría diferencial Topología Geometría fractal Teoría de la medida Cambio Cálculo Cálculo vectorial Ecuaciones diferenciales Sistemas dinámicos Teoría del caos Análisis complejo Matemática aplicada Artículo principal: Matemáticas aplicadas El concepto «matemática aplicada» se refiere a aquellos métodos y herramientas matemáticas que pueden ser utilizados en el análisis o resolución de problemas pertenecientes al área de las ciencias básicas o aplicadas. Muchos métodos matemáticos han resultado efectivos en el estudio de problemas en física, química, biología,[15] medicina,[117] ciencias sociales,[22] ingeniería, economía,[118] finanzas, ecología entre otras. Sin embargo, una posible diferencia es que en matemática aplicada se procura el desarrollo de la matemática «hacia afuera», es decir su aplicación o transferencia hacia el resto de las áreas. Y en menor grado «hacia dentro» o sea, hacia el desarrollo de la matemática misma. Este último sería el caso de la matemática pura o matemática elemental. La matemática aplicada se usa con frecuencia en distintas áreas tecnológicas para modelado,[119]​[120] simulación[111] y optimización de procesos o fenómenos,[121] como el túnel de viento o el diseño de experimentos. Estadística y ciencias de la decisión La estadística es la rama de la matemática que estudia la variabilidad, así como el proceso aleatorio que la genera siguiendo leyes de probabilidad.[122] Es un conocimiento fundamental para la investigación científica en algunos campos de la tecnología, como informática e ingeniería, y de las ciencias fácticas,[123] como economía,[118] genética, sociología,[124] psicología,[125] medicina,[117] contabilidad, etc. En ocasiones, estas áreas de conocimiento necesitan aplicar técnicas estadísticas durante su proceso de investigación factual, con el fin de obtener nuevos conocimientos basados en la experimentación y en la observación, precisando para ello recolectar, organizar, presentar y analizar un conjunto de datos numéricos y, a partir de ellos y de un marco teórico, hacer las inferencias apropiadas.[117]​[125]​[126]​[127]​[128]​ Se consagra en forma directa al gran problema universal de cómo tomar decisiones inteligentes y acertadas en condiciones de incertidumbre. La estadística descriptiva sirve como fuente de instrucción en los niveles básicos de estadística aplicada a las ciencias fácticas[123] y, por tanto, los conceptos manejados y las técnicas empleadas suelen ser presentadas de la forma más simple y clara posibles. Matemática computacional"
ksampletext_wikipedia_math_calculo: str = "Cálculo. En general el término cálculo (del latín calculus, piedrecita, usado para contar o como ayuda al calcular) hace referencia al resultado correspondiente a la acción de calcular. Calcular, por su parte, consiste en realizar las operaciones necesarias para prever el resultado de una acción previamente concebida, o conocer las consecuencias que se pueden derivar de unos datos previamente conocidos. No obstante, el uso más común del término «cálculo» es el lógico-matemático. Desde esta perspectiva, el cálculo consiste en un procedimiento mecánico o algoritmo, mediante el cual podemos conocer las consecuencias que se derivan de las variables previamente conocidas debidamente formalizadas y simbolizadas. Cálculo como razonamiento y cálculo lógico-matemático Ejemplo de aplicación de un cálculo algebraico a la resolución de un problema, según la interpretación de una teoría física. La expresión del cálculo algebraico  Pero si interpretamos  Al mismo tiempo, según dicha teoría, sirve para resolver el problema de calcular cuántos kilómetros ha recorrido un coche que circula de Madrid a Barcelona a una velocidad constante de 60 km/h durante 4 horas de recorrido. 240 kilómetros recorridos = 60 km/h x 4 h Las dos acepciones del cálculo (la general y la restringida) arriba definidas están íntimamente ligadas. El cálculo es una actividad natural y primordial en el hombre, que comienza en el mismo momento en que empieza a relacionar unas cosas con otras en un pensamiento o discurso. El cálculo lógico natural como razonamiento es el primer cálculo elemental del ser humano. El cálculo en sentido lógico-matemático aparece cuando se toma conciencia de esta capacidad de razonar y trata de formalizarse. Por lo tanto, podemos distinguir dos tipos de operaciones: Operaciones orientadas hacia la consecución de un fin, como prever, programar, conjeturar, estimar, precaver, prevenir, proyectar, configurar, etc. que incluyen en cada caso una serie de complejas actividades y habilidades tanto de pensamiento como de conducta. En su conjunto dichas actividades adquieren la forma de argumento o razones que justifican una finalidad práctica o cognoscitiva. Operaciones formales como algoritmo que se aplica bien directamente a los datos conocidos o a los esquemas simbólicos de la interpretación lógico-matemática de dichos datos; las posibles conclusiones, inferencias o deducciones de dicho algoritmo son el resultado de la aplicación de reglas estrictamente establecidas de antemano. Resultado que es: Conclusión de un proceso de razonamiento. Resultado aplicable directamente a los datos iniciales (resolución de problemas). Modelo de relaciones previamente establecido como teoría científica y significativo respecto a determinadas realidades (Creación de modelos científicos). Mero juego formal simbólico de fundamentación, creación y aplicación de las reglas que constituyen el sistema formal del algoritmo (Cálculo lógico-matemático, propiamente dicho). Dada la importancia que históricamente ha adquirido la actividad lógico-matemática en la cultura humana el presente artículo se refiere a este último sentido. De hecho la palabra, en su uso habitual, casi queda restringida a este ámbito de aplicación; para algunos, incluso, queda reducida a un solo tipo de cálculo matemático, pues en algunas universidades se llamaba «Cálculo» a una asignatura específica de cálculo matemático (como puede ser el cálculo infinitesimal, análisis matemático, cálculo diferencial e integral, etc.). En un artículo general sobre el tema no puede desarrollarse el contenido de lo que supone el cálculo lógico-matemático en la actualidad. Aquí se expone solamente el fundamento de sus elementos más simples, teniendo en cuenta que sobre estas estructuras simples se construyen los cálculos más complejos tanto en el aspecto lógico como en el matemático. Historia del cálculo Artículo principal: Historia del cálculo De la Antigüedad Reconstrucción de un ábaco romano. Un ábaco moderno. El término «cálculo» procede del latín calculus, piedrecita que se mete en el calzado y que produce molestia. Precisamente, tales piedrecitas ensartadas en tiras constituían el ábaco romano que, junto con el suanpan chino, constituyen las primeras máquinas de calcular en el sentido de contar. Los antecedentes de procedimiento de cálculo, como algoritmo, se encuentran en los que utilizaron los geómetras griegos, Eudoxo en particular, en el sentido de llegar por aproximación de restos cada vez más pequeños, a una medida de figuras curvas; así como Diofanto precursor del álgebra. Se considera que Arquímedes fue uno de los matemáticos más grandes de la antigüedad y, en general, de toda la historia.[2]​[3] Usó el método exhaustivo para calcular el área bajo el arco de una parábola con el sumatorio de una serie infinita, y dio una aproximación extremadamente precisa del número Pi.[4] También definió la espiral que lleva su nombre, fórmulas para los volúmenes de las superficies de revolución y un ingenioso sistema para expresar números muy largos. La consideración del cálculo como una forma de razonamiento abstracto aplicado en todos los ámbitos del conocimiento se debe a Aristóteles, quien en sus escritos lógicos fue el primero en formalizar y simbolizar los tipos de razonamientos categóricos (silogismos). Este trabajo sería completado más tarde por los estoicos, los megáricos, la Escolástica. Los algoritmos actuales del cálculo aritmético, utilizados universalmente, son fruto de un largo proceso histórico. De vital importancia son las aportaciones de Muhammad ibn al-Juarismi en el siglo IX;[5]​ En el siglo XIII, Fibonacci introduce en Europa la representación de los números arábigos del sistema decimal. Se introdujo el 0, ya de antiguo conocido en la India y se construye definitivamente el sistema decimal de diez cifras con valor posicional. La escritura antigua de números en Babilonia, en Egipto, en Grecia o en Roma, hacía muy difícil un procedimiento mecánico de cálculo.[6]​ El sistema decimal fue muy importante para el desarrollo de la contabilidad de los comerciantes de la Baja Edad Media, en los inicios del capitalismo. El concepto de función por tablas ya era practicado de antiguo pero adquirió especial importancia en la Universidad de Oxford en el siglo XIV.[7] La idea de un lenguaje o algoritmo capaz de determinar todas las verdades, incluidas las de la fe, aparecen en el intento de Raimundo Lulio en su Ars Magna A fin de lograr una operatividad mecánica se confeccionaban unas tablas a partir de las cuales se podía generar un algoritmo prácticamente mecánico. Este sistema de tablas ha perdurado en algunas operaciones durante siglos, como las tablas de logaritmos, o las funciones trigonométricas; las tablas venían a ser como la calculadora de hoy día; un instrumento imprescindible de cálculo. Las amortizaciones de los créditos en los bancos, por ejemplo, se calculaban a partir de tablas elementales hasta que se produjo la aplicación de la informática en el tercer tercio del siglo XX. A finales de la Edad Media la discusión entre los partidarios del ábaco y los partidarios del algoritmo se decantó claramente por estos últimos.[8] De especial importancia es la creación del sistema contable por partida doble recomendado por Luca Pacioli fundamental para el progreso del capitalismo en el Renacimiento.[9]​ Renacimiento El sistema que usamos actualmente fue introducido por Luca Pacioli en 1494, el cual fue creado y desarrollado para responder a la necesidad de la contabilidad en los negocios de la burguesía renacentista. El desarrollo del álgebra (con la introducción de un sistema de símbolos por un lado, y la resolución de problemas por medio de las ecuaciones) vino de la mano de los grandes matemáticos de la época renacentista como Tartaglia, Stevin, Cardano o Vieta y fue esencial para el planteamiento y solución de los más diversos problemas que surgieron en la época, que dieron como consecuencia los grandes descubrimientos que hicieron posible el progreso científico que surgiría en el siglo XVII.[10]​ Siglos XVII y XVIII Página del artículo de Leibniz Explication de l'Arithmétique Binaire, 1703/1705 En el siglo XVII el cálculo conoció un enorme desarrollo siendo los autores más destacados Descartes,[11] Pascal[12] y, finalmente, Leibniz y Newton[13] con el cálculo infinitesimal que en muchas ocasiones ha recibido simplemente, por absorción, el nombre de cálculo. El concepto de cálculo formal en el sentido de algoritmo reglado para el desarrollo de un razonamiento y su aplicación al mundo de lo real,[14] adquiere una importancia y desarrollo enorme respondiendo a una necesidad de establecer relaciones matemáticas entre diversas medidas, esencial para el progreso de la ciencia física que, debido a esto, es tomada como nuevo modelo de Ciencia frente a la especulación tradicional filosófica, por el rigor y seguridad que ofrece el cálculo matemático. Cambia así el sentido tradicional de la Física como filosofía de la naturaleza y toma el sentido de ciencia que estudia los cuerpos materiales, en cuanto materiales. A partir de entonces el propio sistema de cálculo permite establecer modelos sobre la realidad física, cuya comprobación experimental[15] supone la confirmación de la teoría como sistema. Es el momento de la consolidación del llamado método científico cuyo mejor exponente es en aquel momento la Teoría de la Gravitación Universal y las leyes de la Mecánica de Newton.[16]​ Siglos XIX y XX George Boole Durante el siglo XIX y XX el desarrollo científico y la creación de modelos teóricos fundados en sistemas de cálculo aplicables tanto en mecánica como en electromagnetismo y radioactividad, etc., así como en astronomía fue impresionante. Las geometrías no euclidianas encuentran aplicación en modelos teóricos de astronomía y física. El mundo deja de ser un conjunto de infinitas partículas que se mueven en un espacio-tiempo absoluto y se convierte en un espacio de configuración o espacio de fases de  La lógica asimismo sufrió una transformación radical.[17] La formalización simbólica fue capaz de integrar las leyes lógicas en un cálculo matemático, hasta el punto que la distinción entre razonamiento lógico-formal y cálculo matemático viene a considerarse como meramente utilitaria. En la segunda mitad del siglo XIX y primer tercio del XX, a partir del intento de formalización de todo el sistema matemático, Frege, y de matematización de la lógica, (Bolzano, Boole, Whitehead, Russell) fue posible la generalización del concepto como cálculo lógico. Se lograron métodos muy potentes de cálculo, sobre todo a partir de la posibilidad de tratar como «objeto» conjuntos de infinitos elementos, dando lugar a los números transfinitos de Cantor. Mediante el cálculo la lógica encuentra nuevos desarrollos como lógicas modales y lógicas polivalentes. Los intentos de axiomatizar el cálculo como cálculo perfecto por parte de Hilbert y Poincaré, llevaron, como consecuencia de diversas paradojas (Cantor, Russell, etc.) a nuevos intentos de axiomatización, Axiomas de Zermelo-Fraenkel y a la demostración de Gödel de la imposibilidad de un sistema de cálculo perfecto: consistente, decidible y completo en 1931, de grandes implicaciones lógicas, matemáticas y científicas. Actualidad En la actualidad, el cálculo en su sentido más general, en tanto que cálculo lógico interpretado matemáticamente como sistema binario, y físicamente hecho material mediante la lógica de circuitos electrónicos, ha adquirido una dimensión y desarrollo impresionante por la potencia de cálculo conseguida por los ordenadores, propiamente máquinas computadoras. La capacidad y velocidad de cálculo de estas máquinas hace lo que humanamente sería imposible: millones de operaciones por segundo. El cálculo así utilizado se convierte en un instrumento fundamental de la investigación científica por las posibilidades que ofrece para la modelización de las teorías científicas, adquiriendo especial relevancia en ello el cálculo numérico. Cálculo infinitesimal: breve reseña Artículo principal: Cálculo infinitesimal El cálculo infinitesimal, llamado por brevedad «cálculo», tiene su origen en la antigua geometría griega. Demócrito calculó el volumen de pirámides y conos considerándolos formados por un número infinito de secciones de grosor infinitesimal (infinitamente pequeño). Eudoxo y Arquímedes utilizaron el «método de agotamiento» o exhaución para encontrar el área de un círculo con la exactitud finita requerida mediante el uso de polígonos regulares inscritos de cada vez mayor número de lados. En el periodo tardío de Grecia, el neoplatónico Pappus de Alejandría hizo contribuciones sobresalientes en este ámbito. Sin embargo, las dificultades para trabajar con números irracionales y las paradojas de Zenón de Elea impidieron formular una teoría sistemática del cálculo en el periodo antiguo. En el siglo XVII, Cavalieri y Torricelli ampliaron el uso de los infinitesimales, Descartes y Fermat utilizaron el álgebra para encontrar el área y las tangentes (integración y derivación en términos modernos). Fermat e Isaac Barrow tenían la certeza de que ambos cálculos estaban relacionados, aunque fueron Newton (hacia 1660), en Inglaterra y Leibniz en Alemania (hacia 1670) quienes demostraron que los problemas del área y la tangente son inversos, lo que se conoce como teorema fundamental del cálculo. Leibniz es el creador del simbolismo de la derivada, diferencial y la ∫ estilizada para la integración, en vez de la I de Bernoulli. Usó el nombre de cálculo diferencial y el nombre de cálculo integral propuso Juan Bernoulli, que sustituyó al nombre de 'cálculo sumatorio' de Leibniz. La simbología de Leibniz impulsó el avance del cálculo en Europa continental.[18]​ El descubrimiento de Newton, a partir de su teoría de la gravitación universal, fue anterior al de Leibniz, pero el retraso en su publicación aún provoca controversias sobre quién de los dos fue el primero. Newton utilizó el cálculo en mecánica en el marco de su tratado «Principios matemáticos de filosofía natural», obra científica por excelencia, llamando a su método de «fluxiones». Leibniz utilizó el cálculo en el problema de la tangente a una curva en un punto, como límite de aproximaciones sucesivas, dando un carácter más filosófico a su discurso. Sin embargo, terminó por adoptarse la notación de Leibniz por su versatilidad. En el siglo XVIII aumentó considerablemente el número de aplicaciones del cálculo, pero el uso impreciso de las cantidades infinitas e infinitesimales, así como la intuición geométrica, causaban todavía confusión y duda sobre sus fundamentos. De hecho, la noción de límite, central en el estudio del cálculo, era aún vaga e imprecisa en ese entonces. Uno de sus críticos más notables fue el filósofo George Berkeley. En el siglo XIX el trabajo de los analistas matemáticos sustituyeron esas vaguedades por fundamentos sólidos basados en cantidades finitas: Bolzano y Cauchy definieron con precisión los conceptos de límite en términos de épsilon-delta y de derivada, Cauchy y Riemann hicieron lo propio con las integrales, y Dedekind y Weierstrass con los números reales. Fue el periodo de la fundamentación del cálculo. Por ejemplo, se supo que las funciones diferenciables son continuas y que las funciones continuas son integrables, aunque los recíprocos son falsos. En el siglo XX, el análisis no convencional, legitimó el uso de los infinitesimales, al mismo tiempo que la aparición de las computadoras ha incrementado las aplicaciones y velocidad del cálculo. Actualmente, el cálculo infinitesimal tiene un doble aspecto: por un lado, se ha consolidado su carácter disciplinario en la formación de la sociedad culta del conocimiento, destacando en este ámbito textos propios de la disciplina como el de Louis Leithold, el de Earl W. Swokowski, el de Denis G. Zill o el de James Stewart, entre muchos otros; por otro su desarrollo como disciplina científica que ha desembocado en ámbitos tan especializados como el cálculo fraccional, la teoría de funciones analíticas de variable compleja o el análisis matemático. El éxito del cálculo ha sido extendido con el tiempo a las ecuaciones diferenciales, al cálculo de vectores, al cálculo de variaciones, al análisis complejo y a las topología algebraica y topología diferencial entre muchas otras ramas. El desarrollo y uso del cálculo ha tenido efectos muy importantes en casi todas las áreas de la vida moderna: es fundamento para el cálculo numérico aplicado en casi todos los campos técnicos y/o científicos cuya principal característica es la continuidad de sus elementos, en especial en la física. Prácticamente todos los desarrollos técnicos modernos como la construcción, aviación, transporte, meteorología, etc., hacen uso del cálculo. Muchas fórmulas algebraicas se usan hoy en día en balística, calefacción, refrigeración, etc. Como complemento del cálculo, en relación con sistemas teóricos o físicos cuyos elementos carecen de continuidad, se ha desarrollado una rama especial conocida como Matemática discreta. Recientemente, se ha desarrollado el Cálculo Fraccional de Conjuntos (en inglés, Fractional Calculus of Sets o FCS) como una metodología derivada del Cálculo Fraccional. Esta metodología, mencionada por primera vez en el artículo Sets of Fractional Operators and Numerical Estimation of the Order of Convergence of a Family of Fractional Fixed-Point Methods,[19] tiene como objetivo caracterizar y organizar los elementos del cálculo fraccional mediante el uso de conjuntos, aprovechando la variedad de operadores fraccionales disponibles en la literatura.[20]​[21]​[22]​[23]​[24]​[25]​ Actualmente, el cálculo fraccional carece de una definición unificada de lo que constituye una derivada fraccional. En consecuencia, cuando no es necesario especificar explícitamente la forma de una derivada fraccional, típicamente se denota de la siguiente manera: Los operadores fraccionales tienen varias representaciones, pero una de sus propiedades fundamentales es que recuperan los resultados del cálculo tradicional a medida que  Denotando   y  lim Cálculo lógico Artículo principal: Cálculo lógico El cálculo lógico es un sistema de reglas de inferencia o deducción de un enunciado a partir de otro u otros. El cálculo lógico requiere un conjunto consistente de axiomas y unas reglas de inferencia; su propósito es poder deducir algorítmicamente proposiciones lógicas verdaderas a partir de dichos axiomas. La inferencia es una operación lógica que consiste en obtener una proposición lógica como conclusión a partir de otra(s) (premisas) mediante la aplicación de reglas de inferencia.[27]​ Informalmente interpretamos que alguien infiere —o deduce— T de R si acepta que si R tiene valor de verdad V, entonces, necesariamente, T tiene valor de verdad V. Sin embargo, en el enfoque moderno del cálculo lógico no es necesario acudir al concepto de verdad, para construir el cálculo lógico. Los hombres en nuestra tarea diaria, utilizamos constantemente el razonamiento deductivo. Partimos de enunciados empíricos —supuestamente verdaderos y válidos— para concluir en otro enunciado que se deriva de aquellos, según las leyes de la lógica natural.[28]​ La lógica, como ciencia formal, se ocupa de analizar y sistematizar dichas leyes, fundamentarlas y convertirlas en las reglas que permiten la transformación de unos enunciados —premisas- en otros -conclusiones— con objeto de convertir las operaciones en un algoritmo riguroso y eficaz, que garantiza que dada la verdad de las premisas, la conclusión es necesariamente verdadera. Al aplicar las reglas de un cálculo lógico a los enunciados de un argumento mediante la simbolización adecuada como fórmulas o expresiones bien formadas (EBF) del cálculo, construimos un modelo o sistema deductivo. En ese contexto, las reglas de formación de fórmulas definen la sintaxis de un lenguaje formal de símbolos no interpretados, es decir, sin significado alguno; y las reglas de transformación del sistema permiten transformar dichas expresiones en otras equivalentes; entendiendo por equivalentes que ambas tienen siempre y de forma necesaria el mismo valor de verdad. Dichas transformaciones son meramente tautologías. Un lenguaje formal que sirve de base para el cálculo lógico está formado por varias clases de entidades: Un conjunto de elementos primitivos. Dichos elementos pueden establecerse por enumeración, o definidos por una propiedad tal que permita discernir sin duda alguna cuándo un elemento pertenece o no pertenece al sistema. Un conjunto de reglas de formación de «expresiones bien formadas» (EBF) que permitan en todo momento establecer, sin forma de duda, cuándo una expresión pertenece al sistema y cuándo no. Un conjunto de reglas de transformación de expresiones, mediante las cuales partiendo de una expresión bien formada del cálculo podremos obtener una nueva expresión equivalente y bien formada que pertenece al cálculo. Cuando en un cálculo así definido se establecen algunas expresiones determinadas como verdades primitivas o axiomas, decimos que es un sistema formal axiomático. Un cálculo así definido si cumple al mismo tiempo estas tres condiciones decimos que es un Cálculo Perfecto: Es consistente: No es posible que dada una expresión bien formada del sistema, ƒ, y su negación, no – ƒ, sean ambas teoremas del sistema. No puede haber contradicción entre las expresiones del sistema. Decidible: Dada cualquier expresión bien formada del sistema podemos encontrar un método que nos permita decidir mediante una serie finita de operaciones si dicha expresión es o no es un teorema del sistema. Completo: Cuando dada cualquier expresión bien formada del sistema, podemos establecer la demostración matemática o prueba de que es un teorema del sistema. La misma lógica-matemática ha demostrado que tal sistema de cálculo perfecto «no es posible» (véase el Teorema de Gödel). Sistematización de un cálculo de deducción natural Reglas de formación de fórmulas I. Una letra enunciativa (con o sin subíndice) es una EBF. II. Si A es una EBF, ¬ A también lo es. III. Si A es una EBF y B también, entonces A ∧ B; A ∨ B; A → B; A ↔ B, también lo son. IV. Ninguna expresión es una fórmula del Cálculo sino en virtud de I, II, III. Notas: A, B, … con mayúsculas están utilizadas como metalenguaje en el que cada variable expresa cualquier proposición, atómica (p,q,r,s, …) o molecular (p ∧ q), (p ∨ q), …309>100 A, B, … son símbolos que significan variables; ¬, ∧, ∨, →, ↔, son símbolos constantes. Existen diversas formas de simbolización. Utilizamos aquí la de uso más frecuente en España.[29]​ Reglas de transformación de fórmulas 1) Regla de sustitución (R.T.1): Dada una tesis EBF del cálculo, en la que aparecen variables de enunciados, el resultado de sustituir una, algunas o todas esas variables por expresiones bien formadas (EBF) del cálculo, será también una tesis EBF del cálculo. Y ello con una única restricción, si bien muy importante: cada variable ha de ser sustituida siempre que aparece y siempre por el mismo sustituto. Veamos el ejemplo: 1 [(p ∧ q) ∨ r] → t ∨ s Transformación 2 A ∨ r → B Donde A = (p ∧ q); y donde B = (t ∨ s) 3 C → B Donde C = A ∨ r O viceversa 1 C → B Transformación 2 A ∨ r → B Donde A ∨ r = C 3 [(p ∧ q) ∨ r] → t ∨ s Donde (p ∧ q) = A; y donde (t ∨ s) = B 2) Regla de separación (R.T.2): Si X es una tesis EBF del sistema y lo es también X → Y, entonces Y es una tesis EBF del sistema. Esquemas de inferencia Sobre la base de estas dos reglas, siempre podremos reducir un argumento cualquiera a la forma: [A ∧ B ∧ C … ∧ N] → Y lo que constituye un esquema de inferencia en el que una vez conocida la verdad de cada una de las premisas A, B, … N y, por tanto, de su producto, podemos obtener la conclusión Y con valor de verdad V, siempre y cuando dicho esquema de inferencia sea una ley lógica, es decir su tabla de verdad nos muestre que es una tautología. Por la regla de separación podremos concluir Y, de forma independiente como verdad. Dada la poca operatividad de las tablas de verdad, el cálculo se construye como una cadena deductiva aplicando a las premisas o a los teoremas deducidos las leyes lógicas utilizadas como reglas de transformación, como se expone en cálculo lógico. El lenguaje natural como modelo de un cálculo lógico Naturalmente el cálculo lógico es útil porque puede tener aplicaciones, pero ¿en qué consisten o cómo se hacen tales aplicaciones? Podemos considerar que el lenguaje natural es un modelo de C si podemos someterlo, es decir, aplicarle una correspondencia en C.[30]​ Para ello es necesario someter al lenguaje natural a un proceso de formalización de tal forma que podamos reducir las expresiones lingüísticas del lenguaje natural a EBF de un cálculo mediante reglas estrictas manteniendo el sentido de verdad lógica de dichas expresiones del lenguaje natural. Esto es lo que se expone en cálculo lógico. Las diversas formas en que tratemos las expresiones lingüísticas formalizadas como proposiciones lógicas dan lugar a sistemas diversos de formalización y cálculo: Cálculo proposicional o cálculo de enunciados Cuando se toma la oración simple significativa del lenguaje natural con posible valor de verdad o falsedad como una proposición atómica, como un todo sin analizar. Cálculo como lógica de clases Cuando se toma la oración simple significativa del lenguaje natural con posible valor de verdad o falsedad como resultado del análisis de la oración como una relación de individuos o posibles individuos que poseen o no poseen una propiedad común determinada como pertenecientes o no pertenecientes a una clase natural o a un conjunto como individuos. Cálculo de predicados o cuantificacional Cuando se toma la oración simple significativa del lenguaje natural con posible valor de verdad o falsedad como resultado del análisis de la misma de forma que una posible función predicativa (P), se predica de unos posibles sujetos variables (x) [tomados en toda su posible extensión: (Todos los x); o referente a algunos indeterminados: (algunos x)], o de una constante individual existente (a). Cálculo como lógica de relaciones Cuando se toma la oración simple significativa con posible valor de verdad propio, verdadero o falso, como resultado del análisis de la oración como una relación R que se establece entre un sujeto y un predicado. La simbolización y formación de EBFs en cada uno de esos cálculos, así como las reglas de cálculo se trata en cálculo lógico."
ksampletext_wikipedia_math_algebra: str = "Álgebra. El álgebra (del árabe​) es la rama de la matemática que estudia la combinación de elementos de estructuras abstractas acorde a ciertas reglas.[3] Originalmente esos elementos podían ser interpretados como números o cantidades, por lo que el álgebra en cierto modo fue originalmente una generalización y extensión de la aritmética.[4]​[5] En el álgebra moderna existen áreas del álgebra que en modo alguno pueden considerarse extensiones de la aritmética (álgebra abstracta, álgebra homológica, álgebra exterior, etc.). El álgebra elemental difiere de la aritmética en el uso de abstracciones, como el empleo de letras para representar números que son desconocidos o que pueden tomar muchos valores. Por ejemplo, en  La palabra álgebra también se utiliza en ciertas formas especializadas. Un tipo especial de objeto matemático en el álgebra abstracta se llama álgebra, y la palabra se usa, por ejemplo, en las frases álgebra lineal y topología algebraica. Etimología La palabra álgebra proviene del  y cálculo de datos[2] del título del libro de principios del siglo, La ciencia del restablecimiento y el equilibrio por el matemático y astrónomo persa Muḥammad ibn Mūsā al-Khwārizmī. En su obra, el término al-jabr se refería a la operación de mover un término de un lado de una ecuación al otro, المقابلة al-muqābala equilibrar se refería a añadir términos iguales a ambos lados. Acortada a simplemente algeber o álgebra en latín, la palabra acabó entrando en la lengua inglesa durante el siglo XV, ya sea desde el español, el italiano o el latín medieval. Originalmente se refería al procedimiento quirúrgico de fijar huesos rotos o dislocados. El significado matemático se registró por primera vez (en inglés) en el siglo XVI.[6]​ Introducción A diferencia de la aritmética elemental, que trata de los números y las operaciones fundamentales, en álgebra -para lograr la generalización- se introducen además símbolos (usualmente letras) para representar parámetros (variables o coeficientes), o cantidades desconocidas (incógnitas); las expresiones así formadas son llamadas «fórmulas algebraicas» y expresan una regla o un principio general.[7] El álgebra conforma una de las grandes áreas de las matemáticas, junto a la teoría de números, la geometría y el análisis. Página del libro Kitāb al-mukhtaṣar fī ḥisāb al-ŷabr wa-l-muqābala, de Al-Juarismi La palabra «álgebra» proviene del vocablo árabe الجبر al-ŷabar (en árabe dialectal por asimilación progresiva se pronunciaba [alŷɛbɾ], de donde derivan los términos de las lenguas europeas), que se traduce como 'restauración' o 'reposición, reintegración'. Deriva del tratado escrito alrededor del año 820 e. c. por el matemático y astrónomo persa Muhammad ibn Musa al-Jwarizmi (conocido como Al Juarismi), titulado Al-kitāb al-mukhtaṣar fī ḥisāb al-ŷarabi waˀl-muqābala (Compendio de cálculo por reintegración y comparación), el cual proporcionaba operaciones simbólicas para la solución sistemática de ecuaciones lineales y cuadráticas. Muchos de sus métodos derivan del desarrollo de la matemática en el islam medieval, destacando la independencia del álgebra como una disciplina matemática independiente de la geometría y de la aritmética.[8] Puede considerarse al álgebra como el arte de hacer cálculos del mismo modo que en aritmética, pero con objetos matemáticos no-numéricos.[9]​ El adjetivo «algebraico» denota usualmente una relación con el álgebra, como por ejemplo en estructura algebraica. Por razones históricas, también puede indicar una relación con las soluciones de ecuaciones polinomiales, números algebraicos, extensión algebraica o expresión algebraica. Conviene distinguir entre: Álgebra elemental es la parte del álgebra que se enseña generalmente en los cursos de matemáticas. Álgebra abstracta es el nombre dado al estudio de las «estructuras algebraicas» propiamente. El álgebra usualmente se basa en estudiar las combinaciones de cadenas finitas de signos y, mientras que análisis matemático requiere estudiar límites y sucesiones de una cantidad infinita de elementos. Historia del álgebra Véase también: Historia de la matemática El álgebra en la antigüedad Las raíces del álgebra pueden rastrearse hasta la antigua matemática babilónica,[10] que había desarrollado un avanzado sistema aritmético con el que fueron capaces de hacer cálculos en una forma algorítmica. Con el uso de este sistema lograron encontrar fórmulas y soluciones para resolver problemas que, en la actualidad, suelen resolverse mediante ecuaciones lineales, ecuaciones de segundo grado y ecuaciones indeterminadas. En contraste, la mayoría de los egipcios de esta época, y la mayoría de los matemáticos griegos y chinos del primer milenio antes de Cristo, normalmente resolvían tales ecuaciones por métodos geométricos, tales como los descritos en el Papiro de Rhind, Los Elementos de Euclides y Los nueve capítulos sobre el arte matemático. Papiro de Ahmes; datado entre 2000 al 1800 a. e. c. Papiro de Ahmes; datado entre 2000 al 1800 a. e. c. Elementos de Euclides, ca. Elementos de Euclides, ca. 300 a. e. c. Las nueve lecciones del arte matemático; compilado durante siglos II y III a. e. c. Las nueve lecciones del arte matemático; compilado durante siglos II y III a. e. c. Arithmetica; escrito por Diofanto alrededor de 280 de nuestra era Véase también: Matemática helénica Los matemáticos de la Antigua Grecia introdujeron una importante transformación al crear un álgebra de tipo geométrico, en donde los «términos» eran representados mediante los «lados de objetos geométricos», usualmente líneas a las cuales asociaban letras.[9] Los matemáticos helénicos Herón de Alejandría y Diofanto[11] así como también los matemáticos indios como Brahmagupta, siguieron las tradiciones de Egipto y Babilonia, si bien la Arithmetica de Diofanto y el Brahmasphutasiddhanta de Brahmagupta se hallan a un nivel de desarrollo mucho más alto.[12] Por ejemplo, la primera solución aritmética completa (incluyendo al cero y soluciones negativas) para las ecuaciones cuadráticas fue descrita por Brahmagupta en su libro Brahmasphutasiddhanta. Más tarde, los matemáticos árabes y musulmanes desarrollarían métodos algebraicos a un grado mucho mayor de sofisticación. Diofanto (siglo III), algunas veces llamado «el pádre del álgebra», fue un matemático alejandrino, autor de una serie de libros intitulados Arithmetica. Estos textos tratan de las soluciones a las ecuaciones algebraicas.[13]​ Influencia árabe Véase también: Matemática en el islam medieval Los babilonios y Diofanto utilizaron sobre todo métodos especiales ad hoc para resolver ecuaciones, la contribución de Al-Khwarizmi fue fundamental; resuelve ecuaciones lineales y cuadráticas sin el simbolismo algebraico, números negativos o el cero, por lo que debe distinguir varios tipos de >jab.[14]​ El matemático persa Omar Khayyam desarrolló la geometría algebraica y encontró la solución geométrica de la ecuación cúbica. Otro matemático persa, Sharaf Al-Din al-Tusi, encontró la solución numérica y algebraica a diversos casos de ecuaciones cúbicas; también desarrolló el concepto de función. Los matemáticos indios Mahavirá y Bhaskara II, el matemático persa Al-Karaji, y el matemático chino Zhu Shijie, resolvieron varios casos de ecuaciones de grado tres, cuatro y cinco, así como ecuaciones polinómicas de orden superior mediante métodos numéricos. Edad Moderna Durante la Edad Moderna europea tienen lugar numerosas innovaciones, y se alcanzan resultados que claramente superan los resultados obtenidos por los matemáticos árabes, persas, indios o griegos. Parte de este estímulo viene del estudio de las ecuaciones polinómicas de tercer y cuarto grado. Las soluciones para ecuaciones polinómicas de segundo grado ya era conocida por los matemáticos babilónicos cuyos resultados se difundieron por todo el mundo antiguo. El descrubrimiento del procedimiento para encontrar soluciones algebraicas de tercer y cuarto orden se dieron en la Italia del siglo XVI. También es notable que la noción de determinante fue descubierta por el matemático japonés Kowa Seki en el siglo XVII, seguido por Gottfried Leibniz diez años más tarde, con el fin de resolver sistemas de ecuaciones lineales simultáneas utilizando matrices. Entre los siglos XVI y XVII se consolidó la noción de número complejo, con lo cual la noción de álgebra empezaba a apartarse de cantidades medibles. Gabriel Cramer también hizo un trabajo sobre matrices y determinantes en el siglo XVIII. También Leonhard Euler, Joseph-Louis Lagrange, Adrien-Marie Legendre y numerosos matemáticos del siglo XVIII hicieron avances notables en álgebra. Siglo XIX El álgebra abstracta se desarrolló en el siglo XIX, inicialmente centrada en lo que hoy se conoce como teoría de Galois y en temas de la constructibilidad.[15] Los trabajos de Gauss generalizaron numerosas estructuras algebraicas. La búsqueda de una fundamentación matemática rigurosa y una clasificación de los diferentes tipos de construcciones matemáticas llevó a crear áreas del álgebra abstracta durante el siglo XIX absolutamente independientes de nociones aritméticas o geométricas (algo que no había sucedido con el álgebra de los siglos anteriores). Áreas de matemáticas con la palabra álgebra en su nombre Algunas áreas de las matemáticas que entran en la clasificación de álgebra abstracta tienen la palabra álgebra en su nombre; el álgebra lineal es un ejemplo. Otras no: La teoría de grupos, la teoría de anillos y la teoría de campos son ejemplos. En esta sección, enumeramos algunas áreas de las matemáticas con la palabra álgebra en el nombre. Álgebra elemental, la parte del álgebra que se suele enseñar en los cursos elementales de matemáticas. Álgebra abstracta, en la que se definen e investigan las estructuras algebraicas como grupos, anillos y campos. Álgebra lineal, en la que se estudian las propiedades específicas de las ecuaciones lineales, los espacios vectoriales y las matrices. Álgebra de Boole, una rama del álgebra que abstrae el cálculo con los valor de verdades falso y verdadero. Álgebra conmutativa, el estudio de los anillos conmutativos. Álgebra computacional, la implementación de métodos algebraicos como algoritmos y programas de ordenador. Álgebra homológica, el estudio de las estructuras algebraicas fundamentales para el estudio de los espacios topológicos. Álgebra universal, en la que se estudian propiedades comunes a todas las estructuras algebraicas. Teoría de números algebraicos, en la que se estudian las propiedades de los números desde un punto de vista algebraico. Geometría algebraica, una rama de la geometría, que en su forma primitiva especifica las curvas y superficies como soluciones de ecuaciones polinómicas. Combinatoria algebraica, en la que se utilizan métodos algebraicos para estudiar cuestiones combinatorias. Álgebra relacional: conjunto de relaciones finitas que son cerradas bajo ciertos operadores. Muchas estructuras matemáticas se llaman álgebras: Álgebra sobre un cuerpo o más generalmente álgebra sobre un anillo. Muchas clases de álgebras sobre un campo o sobre un anillo tienen un nombre específico: Álgebra asociativa Álgebra no asociativa Álgebra de Lie Álgebra de Hopf C*-álgebra Álgebra simétrica Álgebra exterior Álgebra tensorial En teoría de la medida, σ-álgebra Álgebra sobre un conjunto En teoría de categorías Álgebra F y co-álgebra F Álgebra T En lógica, Álgebra de relación, un álgebra booleana residuada expandida con una involución llamada conversa. Álgebra booleana, un complementado del Retículo distributivo. Álgebra de Heyting Notación algebraica Notación matemática de raíz cuadrada de x Consiste en que los números se emplean para representar cantidades conocidas y determinadas. Las letras se emplean para representar toda clase de cantidades, ya sean conocidas o desconocidas. Las cantidades conocidas se expresan por las primeras letras del alfabeto: a, b, c, d, … Las cantidades desconocidas se representan por las últimas letras del alfabeto: u, v, w, x, y, z.[16]​ Los signos empleados en álgebra son tres clases: Signos de operación, signos de relación y signos de agrupación.[16]​ Signos de operación En álgebra se verifican con las cantidades las mismas operaciones que en aritmética: suma, resta, multiplicación, elevación a potencias y extracción de raíces, que se indican con los principales signos de aritmética excepto el signo de multiplicación. En lugar del signo × suele emplearse un punto entre los factores y también se indica a la multiplicación colocando los factores entre paréntesis. Así a⋅b y (a)(b) equivale a a × b. Signos de relación Se emplean estos signos para indicar la relación que existe entre dos cantidades. Los principales son: =, que se lee igual a. Así, a=b se lee “a igual a b”. >, que se lee mayor que. Así, x + y > m se lee “x + y mayor que m”. <, que se lee menor que. Así, a < b + c se lee “a menor que b + c”. Signos de agrupación Signos y símbolos más comunes Los signos y símbolos son utilizados en el álgebra —y en general en teoría de conjuntos y álgebra de conjuntos— con los que se constituyen ecuaciones, matrices, series, etc. Sus letras son llamadas variables, ya que se usa esa misma letra en otros problemas y su valor va variando. Aquí algunos ejemplos: Signos y símbolos Expresión Uso + Además de expresar adición, también es usada para expresar operaciones binarias c o k Expresan términos constantes Primeras letras del abecedario a, b, c, … Se utilizan para expresar cantidades conocidas Últimas letras del abecedario …, x, y, z Se utilizan para expresar incógnitas n Expresa cualquier número (1, 2, 3, 4, …, n) Exponentes y subíndices Expresar cantidades de la misma especie, de diferente magnitud. Simbología de Conjuntos[17]​ Símbolo Descripción ∈ Es un elemento del conjunto o pertenece al conjunto. ∉ No es un elemento del conjunto o no pertenece al conjunto. ⎜ Tal que n (C) Cardinalidad del conjunto C U Conjunto Universo Φ Conjunto vacío ⊆ Subconjunto de ⊂ Subconjunto propio de ⊄ No es subconjunto propio de > Mayor que < Menor que ≥ Mayor o igual que ≤ Menor o igual que ∩ Intersección de conjuntos ∪ Unión de Conjuntos A' Complemento del conjunto A = Símbolo de igualdad ≠ No es igual a … El conjunto continúa ⇔ Si y solo si ¬ (en algunos ocasiones ∼) No, negación lógica (es falso que) ∧ Y ∨ O Lenguaje algebraico Lenguaje algebraico[17]​ Lenguaje común Lenguaje algebraico Un número cualquiera m Un número cualquiera aumentado en siete m + 7 La diferencia de dos números cualesquiera f - q El doble de un número excedido en cinco 2x + 5 La división de un número entero entre su antecesor x/(x-1) La mitad de un número d/2 El cuadrado de un número y^2 La media de la suma de dos números (b+c)/2 Las dos terceras partes de un número disminuidos en cinco es igual a 12. 2/3 (x-5) = 12 Tres números naturales consecutivos. x, x + 1, x + 2. La parte mayor de 1200, si la menor es w 1200 - w El cuadrado de un número aumentado en siete b2 + 7 Las tres quintas partes de un número más la mitad de su consecutivo equivalen a tres. 3/5 p + 1/2 (p+1) = 3 El producto de un número con su antecesor equivalen a 30. x(x-1) = 30 El cubo de un número más el triple del cuadrado de dicho número x3 + 3x2 Estructura algebraica Artículo principal: Estructura algebraica En matemáticas, una estructura algebraica es un conjunto de elementos con unas propiedades operacionales determinadas; es decir, lo que define a la estructura del conjunto son las operaciones que se pueden realizar con los elementos de dicho conjunto y las propiedades matemáticas que dichas operaciones poseen. Un objeto matemático constituido por un conjunto no vacío y algunas leyes de composición interna definida en él es una estructura algebraica. Las estructuras algebraicas más importantes son"
ksampletext_wikipedia_math_numero: str = "Número. Un número es un objeto matemático utilizado para contar (cantidades), medir (magnitudes) y etiquetar. Los números más sencillos, que utilizamos todos en la vida cotidiana, son los números naturales: 1, 2, 3, etc. Se denotan mediante  Los números desempeñan un papel esencial en las ciencias empíricas, ya que permiten cuantificar y describir fenómenos observables. No solo los números naturales son utilizados, sino también diversos conjuntos numéricos desarrollados a lo largo de la historia de las matemáticas. El conjunto de los números enteros (representado por  Desde la antigüedad, se conoce la existencia de números que no pueden expresarse como fracciones de enteros. Por ejemplo, la longitud de la diagonal de un cuadrado de lado unidad es  Con el tiempo, se introdujeron otros tipos de números para ampliar el alcance del análisis matemático, como los imaginarios, complejos y trascendentes, que permiten describir y resolver fenómenos de creciente complejidad en diversas ramas de la ciencia y la ingeniería. Nótese que la teoría de números es una rama de las matemáticas que se ocupa de los enteros (no de números en general). El origen de los números es que los primeros números que el hombre inventó fueron los números naturales, los cuales se utilizaban y se utilizan para contar elementos de un conjunto finito, ya que se procede a enumerar dichos elementos de una manera ordenada seleccionándolos uno tras otro a la vez que se le atribuye a cada uno un número. Tipos de números Clasificación de los números. Los números más conocidos son los números naturales. Denotados mediante  Otro tipo de números ampliamente usados son números fraccionarios, los cuales representan tanto cantidades inferiores a una unidad, como números mixtos (un conjunto de unidades más una parte inferior a la unidad). Los números fraccionarios pueden ser expresados siempre como cocientes de enteros. El conjunto de todos los números fraccionarios es el conjunto de los números racionales (que usualmente se define para que incluya tanto a los racionales positivos, como a los racionales negativos y el cero). Este conjunto de números se designa como  Los números racionales permiten resolver gran cantidad de problemas prácticos, pero desde los antiguos griegos se conoce que ciertas relaciones geométricas (la diagonal de un cuadrado de lado unidad) son números no enteros que tampoco son racionales. Igualmente, la solución numérica de una ecuación polinómica cuyos coeficientes son números racionales, usualmente es un número no racional. Puede demostrarse que cualquier número irracional puede representarse como una sucesión de Cauchy de números racionales que se aproximan a un límite numérico. El conjunto de todos los números racionales y los irracionales (obtenidos como límites de sucesiones de Cauchy de números racionales) es el conjunto de los números reales  Uno de los problemas de los números reales es que no forman un cuerpo algebraicamente cerrado, por lo que ciertos problemas no tienen solución planteados en términos de números reales. Esa es una de las razones por las cuales se introdujeron los números complejos  Al margen de los números reales y complejos, claramente conectados con problemas de las ciencias naturales, existen otros tipos de números que generalizan aún más y extienden el concepto de número de una manera más abstracta y responden más a creaciones deliberadas de matemáticos. La mayoría de estas generalizaciones del concepto de número se usan solo en matemáticas, aunque algunos de ellos han encontrado aplicaciones para resolver ciertos problemas físicos. Entre ellos están los números hipercomplejos, que incluyen a los cuaterniones, útiles para representar rotaciones en un espacio de tres dimensiones, y generalizaciones de estos, como octoniones y los sedeniones. A un nivel un poco más abstracto también se han ideado conjuntos de números capaces de tratar con cantidades infinitas e infinitesimales, como los hiperreales  Lista de los tipos de números existentes La teoría de los números trata básicamente de las propiedades de los números naturales y los enteros, mientras que las operaciones del álgebra y el cálculo permiten definir la mayor parte de los sistemas numéricos, entre los cuales están: Números naturales Número primo Números compuestos Números perfectos Números enteros Números negativos Números pares Números impares Números racionales Números reales Números irracionales Números algebraicos Números trascendentes Extensiones de los números reales Números complejos Números hipercomplejos Cuaterniones Octoniones Números hiperreales Números superreales Números surreales Números usados en teoría de conjuntos Números ordinales Números cardinales Números transfinitos Estructura de los sistemas numéricos En álgebra abstracta y análisis matemático un sistema numérico se caracteriza por una: Estructura algebraica, usualmente un anillo conmutativo o cuerpo matemático (en el caso no conmutativo son un álgebra sobre un cuerpo y en el caso de los números naturales solo un monoide conmutativo). Estructura de orden, usualmente un conjunto ordenado, en el caso de los números naturales, enteros, racionales y reales se trata de conjuntos totalmente ordenados, aunque los números complejos e hipercomplejos solo son conjuntos parcialmente ordenados. Los reales además son un conjunto bien ordenado y con un orden denso.[1]​ Estructura topológica, los conjuntos numéricos numerables usualmente son conjuntos disconexos, sobre los que se considera la topología discreta, mientras que sobre los conjuntos no numerables se considera una topología que los hace adecuados para el análisis matemático. Otra propiedad interesante de muchos conjuntos numéricos es que son representables mediante diagramas de Hasse, diagramas de Euler y diagramas de Venn, pudiéndose tomar una combinación de ambos en un diagrama de Euler-Venn con la forma característica de cuadrilátero y además pudiéndose representar internamente un diagrama de Hasse (es una recta). Tanto históricamente como conceptualmente, los diversos conjuntos numéricos, desde el más simple de los números naturales, hasta extensiones transcendentes de los números reales y complejos, elaboradas mediante la teoría de modelos durante el siglo XX, se construyen desde una estructura más simple hasta otra más compleja.[2]​ Números naturales especiales El estudio de ciertas propiedades que cumplen los números ha producido una enorme cantidad de tipos de números, la mayoría sin un interés matemático específico. Se pueden encuadrar dentro de la matemática recreativa. A continuación se indican algunos: Perfecto: número igual a la suma de sus divisores (incluyendo el 1). Ejemplo: 6 = 1 + 2 + 3. Sheldon: el número 73, es el 21° número primo, que al multiplicar 7 × 3 = 21; Y al dar la vuelta a sus dígitos da 37 que es el 12° número primo. Narcisista: número de n dígitos que resulta ser igual a la suma de las potencias de orden n de sus dígitos. Ejemplo: 153 = 1³ + 5³ + 3³. Omirp: número primo que al invertir sus dígitos da otro número primo. Ejemplo: 1597 y 7951 son primos. Vampiro: número que es el producto de dos números obtenidos a partir de sus dígitos. Ejemplo: 2187 = 27 × 81. Hamsteriano: Su estructura aritmética N= (a×b)2-1, donde a y b son primos los dos, la suma de sus divisores sobrepasa N, y la cantidad de sus divisores es > a×b/2; va como ejemplo: 1224 = (5×7)2-1 Pitagórico: una terna pitagórica son tres números que cumplen las siguientes condiciones: el cuadrado de uno de ellos, más el cuadrado de otro, es igual al cuadrado del tercero, por ejemplo: (3, 4, 5) ya que 32 + 42 = 9 + 16 = 25 = 52 Una vez entendido el problema de la naturaleza y la clasificación de los números, surge otro, más práctico, pero que condiciona todo lo que se va a hacer con ellos: la manera de escribirlos. El sistema que se ha impuesto universalmente es la numeración posicional, gracias al invento del cero, con una base constante. Más formalmente, en Los fundamentos de la aritmética, Gottlob Frege (1848-1925) realiza una definición de «número», la cual fue tomada como referencia por muchos matemáticos (entre ellos Bertrand Russell [1872-1870], cocreador de Principia mathematica): «n» es un número, es entonces la definición de «que existe un concepto “F” para el cual “n” aplica», que a su vez se ve explicado como que «n» es la extensión del concepto «equinumerable con» para «F», y dos conceptos son equinumerables si existe una relación «uno a uno» (véase que no se utiliza el símbolo «1» porque no está definido aún) entre los elementos que lo componen (es decir, una biyección en otros términos). Véase también que Frege, tanto como cualquier otro matemático, se ve inhabilitado para definir al número como la expresión de una cantidad, porque la simbología matemática no hace referencia necesaria a la numerabilidad, y el hecho de «cantidad» referiría a algo numerable, mientras que números se adoptan para definir la cardinalidad de, por ejemplo, los elementos que se encuentran en el intervalo abierto (0, 1), que contiene innumerables elementos (el continuo). Peano, antes de establecer sus cinco proposiciones sobre los números naturales, explícita que supone sabida una definición (quizás debido a su «obviedad») de las palabras o conceptos cero, sucesor y número. De esta manera postula: 0 es un número natural el sucesor de todo número es un número dos números diferentes no tienen el mismo sucesor 0 no es el sucesor de ningún número y la propiedad inductiva Sin embargo, si uno define el concepto cero como el número 100, y el concepto número como los números mayores a 100, entonces las cinco proposiciones mencionadas anteriormente aplican, no a la idea que Peano habría querido comunicar, sino a su formalización. La definición de número se encuentra por ende no totalmente formalizada, aunque se encuentre un acuerdo mayoritario en adoptar la definición enunciada por Frege. Historia del concepto de número Hueso de Ishango. Cognitivamente el concepto de número está asociado a la habilidad de contar y comparar cuál de dos conjuntos de entidades similares tiene mayor cantidad de elementos. Las primeras sociedades humanas se encontraron muy pronto con el problema de determinar cuál de dos conjuntos era «mayor» que otro, o de conocer con precisión cuántos elementos formaban una colección de cosas. Esos problemas podían ser resueltos simplemente contando. La habilidad de contar del ser humano, no es un fenómeno simple, aunque la mayoría de culturas tienen sistemas de cuenta que llegan como mínimo a centenares, algunos pueblos con una cultura material simple, solo disponen de términos para los números 1, 2 y 3 y usualmente usan el término «muchos» para cantidades mayores, aunque cuando es necesario usan recursivamente expresiones traducibles como «3 más 3 y otros 3». El conteo se debió iniciar mediante el uso de objetos físicos (tales como montones de piedras) y de marcas de cuenta, como las encontradas en huesos tallados: el de Lebombo, con 29 muescas grabadas en un hueso de babuino, tiene unos 37 000 años de antigüedad y otro hueso de lobo encontrado en la antigua Checoslovaquia, con 57 marcas dispuestas en cinco grupos de 11 y dos sueltas, se ha estimado en unos 30 000 años de antigüedad. Ambos casos constituyen una de las más antiguas marcas de cuenta conocidas habiéndose sugerido que pudieran estar relacionadas con registros de fases lunares.[3] En cuanto al origen ordinal algunas teorías lo sitúan en rituales religiosos. Los sistemas numerales de la mayoría de familias lingüísticas reflejan que la operación de contar estuvo asociado al conteo de dedos (razón por la cual los sistemas de base decimal y vigesimal son los más abundantes), aunque está testimoniado el empleo de otras bases numéricas. El paso hacia los símbolos numerales, al igual que la escritura, se ha asociado a la aparición de sociedades complejas con instituciones centralizadas constituyendo artificios burocráticos de contabilidad en registros impositivos y de propiedades. Su origen estaría en primitivos símbolos con diferentes formas para el recuento de diferentes tipos de bienes como los que se han encontrado en Mesopotamia inscritos en tablillas de arcilla que a su vez habían venido a sustituir progresivamente el conteo de diferentes bienes mediante fichas de arcilla (constatadas al menos desde el 8000 a. C.) Los símbolos numerales más antiguos encontrados se sitúan en las civilizaciones mesopotámicas usándose como sistema de numeración ya no solo para la contabilidad o el comercio sino también para la agrimensura o la astronomía como, por ejemplo, registros de movimientos planetarios.[4]​ En conjunto, desde hace 5000 años la mayoría de las civilizaciones han contado como lo hacemos hoy aunque la forma de escribir los números (si bien todos representan con exactitud los naturales) ha sido muy diversa. Básicamente la podemos clasificar en tres categorías: Sistemas de notación aditiva. Acumulan los símbolos de todas las unidades, decenas, centenas, …, necesarios hasta completar el número. Aunque los símbolos pueden ir en cualquier orden, adoptaron siempre una determinada posición (de más a menos). De este tipo son los sistemas de numeración: egipcio, hitita, cretense, romano, griego, armenio y judío. Sistemas de notación híbrida. Combinan el principio aditivo con el multiplicativo. En los anteriores 500 se representa con 5 símbolos de 100, en estos se utiliza la combinación del 5 y el 100. El orden de las cifras es ahora fundamental (estamos a un paso del sistema posicional). De este tipo son los sistemas de numeración: chino clásico, asirio, armenio, etíope y maya. Este último utilizaba símbolos para el 1, el 5 y el 0. Siendo este el primer uso documentado del cero tal como lo conocemos hoy (año 36 a. C.) ya que el de los babilonios solo se utilizaba entre otros dígitos. Sistemas de notación posicional. La posición de las cifras nos indica si son unidades, decenas, centenas, …, o en general la potencia de la base. Solo tres culturas además de la india lograron desarrollar un sistema de este tipo: el sistema chino (300 a. C.) que no disponía de 0, el sistema babilónico (2000 a. C.) con dos símbolos, de base 10 aditivo hasta el 60 y posicional (de base 60) en adelante, sin 0 hasta el 300 a. C. Las fracciones unitarias egipcias (Papiro de Ahmes/Rhind) Artículo principal: Fracción egipcia En este papiro adquirido por Alexander Henry Rhind (1833-1863) en 1858, cuyo contenido data del 2000 al 1800 a. C. además del sistema de numeración antes descrito nos encontramos con su tratamiento de las fracciones. No consideran las fracciones en general, solo las fracciones unitarias (inversas de los naturales 1/20) que se representan con un signo oval encima del número, la fracción 2/3 que se representa con un signo especial y en algunos casos fracciones del tipo  Al ser un sistema sumativo la notación es: 1+1/2+1/4. La operación fundamental es la suma y nuestras multiplicaciones y divisiones se hacían por «duplicaciones» y «mediaciones», por ejemplo 69×19=69×(16+2+1), donde 16 representa 4 duplicaciones y 2 una duplicación. Fracciones sexagesimales babilónicas (documentos cuneiformes) En las tablillas cuneiformes de la dinastía Hammurabi (1800-1600 a. C.) aparece el sistema posicional, antes referido, extendido a las fracciones, pero XXX vale para  Para calcular recurrían, como nosotros antes de disponer de máquinas, a las numerosas tablas que disponían: De multiplicar, de inversos, de cuadrados y cubos, de raíces cuadradas y cúbicas, de potencias sucesivas de un número dado no fijo, etc. Por ejemplo, para calcular  Realizaban las operaciones de forma parecida a hoy, la división multiplicando por el inverso (para lo que utilizan sus tablas de inversos). En la tabla de inversos faltan los de 7 y 11 que tienen una expresión sexagesimal infinitamente larga. Sí están, pero no se percataron del desarrollo periódico. Descubrimiento de los inconmensurables Las circunstancias y la fecha de este descubrimiento son inciertas, aunque se atribuye a la escuela pitagórica (se utiliza el teorema de Pitágoras). Aristóteles (384-322 a. C.) menciona una demostración de la inconmensurabilidad de la diagonal de un cuadrado con respecto a su lado basada en la distinción entre lo par y lo impar. La reconstrucción que realiza C. Boyer es: Sean d:diagonal, s:lado y d/s racional, que podremos escribirlo como  La teoría pitagórica de todo es número quedó seriamente dañada. El problema lo resolvería Eudoxo de Cnido (408-355 a. C.) tal como nos indica Euclides en el libro V de Los elementos. Para ello estableció el axioma de Arquímedes: «Dos magnitudes tienen una razón si se puede encontrar un múltiplo de una de ellas que supere a la otra» (excluye el 0). Después, en la definición 5, da la famosa formulación de Eudoxo: «Dos magnitudes están en la misma razón  En el libro Historia de la matemática (1985), de J. P. Colette, se hace la observación de que esta definición está muy próxima a la de número real que dará Dedekind (1831-1916), divide las fracciones en las  Creación del cero Artículo principal: Cero En cualquier sistema de numeración posicional surge el problema de la falta de unidades de determinado orden. Por ejemplo, en el sistema babilónico el número  Hacia el siglo III a. C., en Grecia, se comenzó a representar la nada mediante una o que significa oudos 'vacío', y que no dio origen al concepto de cero como existe hoy en día. La idea del cero como concepto matemático parece haber surgido en la India antes que en ningún otro lugar. La única notación ordinal del Viejo Mundo fue la sumeria, donde el cero se representaba por un vacío. En América, la primera expresión conocida del sistema de numeración vigesimal prehispánico data del siglo III a. C. Se trata de una estela olmeca tardía, la cual ya contaba tanto con el concepto de orden como el de cero. Los mayas inventaron cuatro signos para el cero; los principales eran: el corte de un caracol para el cero matemático, y una flor para el cero calendárico (que implicaba no la ausencia de cantidad, sino el cumplimiento de un ciclo). Números negativos Brahmagupta, en el 628 de nuestra era, considera las dos raíces de las ecuaciones cuadráticas, aunque una de ellas sea negativa o irracional. De hecho en su obra es la primera vez que aparece sistematizada la aritmética (+, -, *, /, potencias y raíces) de los números positivos, negativos y el cero, que él llamaba los bienes, las deudas y la nada. Así, por ejemplo, para el cociente, establece: Positivo dividido por positivo, o negativo dividido por negativo, es afirmativo. Cifra dividido por cifra es nada (0/0=0). Positivo dividido por negativo es negativo. Negativo dividido por afirmativo es negativo. Positivo o negativo dividido por cifra es una fracción que la tiene por denominador (a/0=¿?) No solo utilizó los negativos en los cálculos, sino que los consideró como entidades aisladas, sin hacer referencia a la geometría. Todo esto se consiguió gracias a su despreocupación por el rigor y la fundamentación lógica, y su mezcla de lo práctico con lo formal. Sin embargo, el tratamiento que hicieron de los negativos cayó en el vacío, y fue necesario que transcurrieran varios siglos (hasta el Renacimiento) para que fuese recuperado. Al parecer, los chinos también poseían la idea de número negativo, y estaban acostumbrados a calcular con ellos utilizando varillas negras para los negativos y rojas para los positivos. Transmisión del sistema indo-arábigo a Occidente Varios autores del siglo XIII contribuyeron a esta difusión, destacan Alexandre de Villedieu (1225), Sacrobosco (circa 1195, o 1200-1256) y sobre todo Leonardo de Pisa (1180-1250). Este último, conocido como Fibonacci, viajó por Oriente y aprendió de los árabes el sistema posicional hindú. Escribió un libro, El Liber abaci, que trata en el capítulo I la numeración posicional, en los cuatro siguientes las operaciones elementales, en los capítulos VI y VII las fracciones: comunes, sexagesimales y unitarias (¡no usa los decimales, principal ventaja del sistema!), y en el capítulo XIV los radicales cuadrados y cúbicos. También contiene el problema de los conejos que da la serie:  No aparecen los números negativos, que tampoco consideraron los árabes, debido a la identificación de número con magnitud (¡obstáculo que duraría siglos!). A pesar de la ventaja de sus algoritmos de cálculo, se desataría por diversas causas una lucha encarnizada entre abacistas y algoristas, hasta el triunfo final de estos últimos. Las fracciones continuas Pietro Antonio Cataldi (1548-1626), aunque con ejemplos numéricos, desarrolla una raíz cuadrada en fracciones continuas como hoy: Queremos calcular  Siendo así los números irracionales aceptados con toda normalidad, pues se les podía aproximar fácilmente mediante números racionales. Primera formulación de los números complejos Los números complejos eran en pocos casos aceptados como raíces o soluciones de ecuaciones (M. Stifel (1487-1567), S. Stevin (1548-1620)) y por casi ninguno como coeficientes). Estos números se llamaron inicialmente ficticii 'ficticios' (el término imaginario usado actualmente es reminiscente de estas reticencias a considerarlos números respetables). A pesar de esto G. Cardano (1501-1576) conoce la regla de los signos y R. Bombelli (1526-1573) las reglas aditivas a través de haberes y débitos, pero se consideran manipulaciones formales para resolver ecuaciones, sin entidad al no provenir de la medida o el conteo. Cardano en la resolución del problema dividir 10 en dos partes tales que su producto valga 40 obtiene como soluciones  En la resolución de ecuaciones cúbicas con la fórmula de Cardano-Tartaglia, aunque las raíces sean reales, aparecen en los pasos intermedios raíces de números negativos. En esta situación Bombelli dice en su Álgebra que tuvo lo que llamó una idea loca, esta era que los radicales podían tener la misma relación que los radicandos y operar con ellos, tratando de eliminarlos después. En un texto posterior en 20 años utiliza p.d.m.  Generalización de las fracciones decimales Aunque se encuentra un uso más que casual de las fracciones decimales en la Arabia medieval y en la Europa renacentista, y ya en 1579 Vieta (1540-1603) proclamaba su apoyo a éstas frente a las sexagesimales, y las aceptaban los matemáticos que se dedicaban a la investigación, su uso se generalizó con la obra que Simon Stevin publicó en 1585 De Thiende (La Disme). En su definición primera dice que la Disme es una especie de aritmética que permite efectuar todas las cuentas y medidas utilizando únicamente números naturales. En las siguientes define nuestra parte entera: cualquier número que vaya el primero se dice comienzo y su signo es (0), (primera posición decimal 1/10). El siguiente se dice primera y su signo es (1) (segunda posición decimal 1/100). El siguiente se dice segunda (2). Es decir, los números decimales que escribe: 0,375 como 3(1)7(2)5(3), o 372,43 como 372(0)4(1)3(2). Añade que no se utiliza ningún número roto (fracciones), y el número de los signos, exceptuando el 0, no excede nunca a 9. Esta notación la simplificó Joost Bürgi (1552-1632) eliminando la mención al orden de las cifras y sustituyéndolo por un «.» en la parte superior de las unidades 372·43, poco después Magini (1555-1617) usó el «.» entre las unidades y las décimas: 372.43, uso que se generalizaría al aparecer en la Constructio de Napier (1550-1617) de 1619. La «,» también fue usada a comienzos del siglo XVII por el holandés Willebrord Snellius: 372,43. El principio de inducción matemática Artículo principal: Inducción matemática Su antecedente es un método de demostración, llamado inducción completa, por aplicación reiterada de un mismo silogismo que se extiende indefinidamente y que usó Maurolyco (1494-1575) para demostrar que la suma de los primeros  Si  El hecho de que  entonces  De manera intuitiva se entiende la inducción como un efecto dominó. Suponiendo que se tiene una fila infinita de fichas de dominó, el paso base equivale a tirar la primera ficha; por otro lado, el paso inductivo equivale a demostrar que si alguna ficha se cae, entonces la ficha siguiente también se caerá. La conclusión es que se pueden tirar todas las fichas de esa fila. La interpretación geométrica de los números complejos Esta interpretación suele ser atribuida a Gauss (1777-1855) que hizo su tesis doctoral sobre el teorema fundamental del álgebra, enunciado por primera vez por Harriot y Girard en 1631, con intentos de demostración realizados por D’Alembert, Euler y Lagrange, demostrando que las pruebas anteriores eran falsas y dando una demostración correcta primero para el caso de coeficientes, y después de complejos. También trabajó con los números enteros complejos que adoptan la forma  La representación gráfica de los números complejos había sido descubierta ya por Caspar Wessel (1745-1818) pero pasó desapercibida, y así el plano de los números complejos se llama «plano de Gauss» a pesar de no publicar sus ideas hasta 30 años después. Desde la época de Girard (mitad siglo XVII) se conocía que los números reales se pueden representar en correspondencia con los puntos de una recta. Al identificar ahora los complejos con los puntos del plano los matemáticos se sentirán cómodos con estos números, ver es creer. Descubrimiento de los números trascendentes La distinción entre números irracionales algebraicos y trascendentes data del siglo XVIII, en la época en que Euler demostró que  Liouville (1809-1882) demostró en 1844 que todos los números de la forma  Hermite (1822-1901) en una memoria Sobre la función exponencial de 1873 demostró la trascendencia de  Lindeman (1852-1939) en la memoria Sobre el número  El problema 7 de Hilbert (1862-1943) que plantea si  Teorías de los irracionales Hasta mediados del siglo XIX los matemáticos se contentaban con una comprensión intuitiva de los números y sus sencillas propiedades no son establecidas lógicamente hasta el siglo XIX. La introducción del rigor en el análisis puso de manifiesto la falta de claridad y la imprecisión del sistema de los números reales, y exigía su estructuración lógica sobre bases aritméticas. Bolzano había hecho un intento de construir los números reales basándose en sucesiones de números racionales, pero su teoría pasó desapercibida y no se publicó hasta 1962. Hamilton hizo un intento, haciendo referencia a la magnitud tiempo, a partir de particiones de números racionales: si  cuando  y si  cuando  pero no desarrolló más su teoría. Pero en el mismo año 1872 cinco matemáticos, un francés y cuatro alemanes, publicaron sus trabajos sobre la aritmetización de los números reales: Charles Meray (1835-1911) en su obra Nouveau précis d’analyse infinitesimale define el número irracional como un límite de sucesiones de números racionales,[5]​[6] sin tener en cuenta que la existencia misma del límite presupone una definición del número real. Hermann Heine (1821-1881) publicó, en el Journal de Crelle en 1872, su artículo «Los elementos de la teoría de funciones», donde proponía ideas similares a las de Cantor, teoría que en conjunto se llama actualmente «teorema de Heine-Cantor». Richard Dedekind (1831-1916) publica su Stetigkeit und irrationale zahlen. Su idea se basa en la continuidad de la recta real y en los agujeros que hay si solo consideramos los números racionales. En la sección dedicada al «dominio R» enuncia un axioma por el que se establece la continuidad de la recta: «cada punto de la recta divide los puntos de ésta en dos clases tales que cada punto de la primera se encuentra a la izquierda de cada punto de la segunda clase, entonces existe un único punto que produce esta división». Esta misma idea la utiliza en la sección «creación de los números irracionales» para introducir su concepto de «cortadura». Bertrand Russell apuntaría después que es suficiente con una clase, pues esta define a la otra. Georg Cantor (1845-1918). Define los conceptos de: sucesión fundamental, sucesión elemental, y límite de una sucesión fundamental, y partiendo de ellos define el número real. Karl Weierstrass (1815-1897). No llegó a publicar su trabajo, continuación de los de Bolzano, Abel y Cauchy, pero fue conocido por sus enseñanzas en la Universidad de Berlín. Su caracterización basada en los «intervalos encajados», que pueden contraerse a un número racional pero no necesariamente lo hacen, no es tan generalizable como las anteriores, pero proporciona fácil acceso a la representación decimal de los números reales. Álgebras hipercomplejas La construcción de obtención de los números complejos a partir de los números reales, y su conexión con el grupo de transformaciones afines en el plano sugirió a algunos matemáticos otras generalizaciones similares conocidas como números hipercomplejos. En todas estas generalizaciones los números complejos son un subconjunto de estos nuevos sistemas numéricos, aunque estas generalizaciones tienen la estructura matemática de álgebra sobre un cuerpo, pero en ellos la operación de multiplicación no es conmutativa. Teoría de conjuntos Artículo principal: Teoría de conjuntos La teoría de conjuntos sugirió muchas y variadas formas de extender los números naturales y los números reales de formas diferentes a como los números complejos extendían al conjunto de los números reales. El intento de capturar la idea de conjunto con un número no finito de elementos llevó a la aritmética de números transfinitos que generalizan a los naturales, pero no a los números enteros. Los números transfinitos fueron introducidos por Georg Cantor hacia 1873. Los números hiperreales usados en el análisis no estándar generalizan a los reales pero no a los números complejos (aunque admiten una complejificación que generalizaría también a los números complejos). Aunque parece los números hiperreales no proporcionan resultados matemáticos interesantes que vayan más allá de los obtenibles en el análisis real, algunas demostraciones y pruebas matemáticas parecen más simples en el formalismo de los números hiperreales, por lo que no están exentos de importancia práctica. Socialmente Los números naturales por la necesidad de contar. Los números fraccionarios por la necesidad de medir partes de un todo, y compartir. Los enteros negativos por fenómenos de doble sentido: izquierda-derecha, arriba-abajo, pérdida-ganancia. Los números reales por la necesidad de medir segmentos. Los números complejos por exigencias de resolver ecuaciones algebraicas, como el caso de la cúbicas o de x2 + 1 = 0.[7]​ Sistemas de representación de los números Cifra, dígito y numeral Artículo principal: Cifra (matemática) Una de las formas más frecuentes de representar números por escrito consiste en un «conjunto finito de símbolos» o dígitos que, adecuadamente combinados, permiten formar cifras que funcionan como representaciones de números (cuando una secuencia específica de signos se emplea para representar un número se la llama numeral, aunque una cifra también puede representar simplemente un código identificativo). Base numérica Artículo principal: Base (aritmética) Tanto las lenguas naturales como la mayor parte de sistemas de representación de números mediante cifras, usan un inventario finito de unidades para expresar una cantidad mucho mayor de números. Una manera importante de lograr eso es el uso de una base aritmética en esos sistemas un número se expresa en general mediante suma o multiplicación de números. Los sistemas puramente aritméticos recurren a bases donde cada signo recibe una interpretación diferente según su posición. Así en el siguiente numeral arábigo (base 10): 13568 El <8> por estar en última posición representa unidades, el <6> representa decenas, el <5> centenas, el <3> millares y el <1> decenas de millares. Es decir, ese numeral representara el número: 13568 13568 Muchas lenguas del mundo usan una base decimal, igual que el sistema arábigo, aunque también es frecuente que las lenguas usen sistemas vigesimales (base 20). De hecho la idea de usar un número finito de dígitos o signos para representar números arbitrariamente grandes funciona para cualquier base b, donde b es un número entero mayor o igual que 2. Los ordenadores frecuentemente usan para sus operaciones la base binaria (b = 2), y para ciertos usos también se emplea la base octal (b = 8 ) o hexadecimal (b = 16). La base coincide con el número de signos primarios, si un sistema posicional tiene b símbolos primarios que designaremos por  Designará al número: Números en las lenguas naturales Artículo principal: Numeral (lingüística) Las lenguas naturales usan nombres o numerales para los números frecuentemente basados en el contaje mediante dedos, razón por la cual la mayoría de las lenguas usan sistemas de numeración en base 10 (dedos de las manos) o base 20 (dedos de manos y pies), aunque también existen algunos sistemas exóticos que emplean otras bases."

ksampletext_wikipedia_phys_mecanicacuantica: str = "Mecánica cuántica. La mecánica cuántica es la rama de la física que estudia la naturaleza a escalas espaciales pequeñas, los sistemas atómicos, subatómicos, sus interacciones con la radiación electromagnética y otras fuerzas, en términos de cantidades observables. Se basa en la observación de que todas las formas de energía se liberan en unidades discretas o paquetes llamados cuantos. Las partículas con esta propiedad pueden pertenecer a dos tipos distintos: fermiones o bosones. Algunos de estos últimos están ligados a una -interacción fundamental (por ejemplo, el fotón pertenece a la electromagnética). Sorprendentemente, la teoría cuántica solo permite normalmente cálculos probabilísticos o estadísticos de las características observadas de las partículas elementales, entendidos en términos de funciones de onda. La ecuación de Schrödinger desempeña, en la mecánica cuántica, el papel que las leyes de Newton y la conservación de la energía desempeñan en la mecánica clásica. Es decir, la predicción del comportamiento futuro de un sistema dinámico y es una ecuación de onda en términos de una función de onda la que predice analíticamente la probabilidad precisa de los eventos o resultados. En teorías anteriores de la física clásica, la energía era tratada únicamente como un fenómeno continuo, en tanto que la materia se supone que ocupa una región muy concreta del espacio y que se mueve de manera continua. Según la teoría cuántica, la energía se emite y se absorbe en cantidades discretas y minúsculas. Un paquete individual de energía, llamado cuanto, en algunas situaciones se comporta como una partícula de materia. Por otro lado, se encontró que las partículas exponen algunas propiedades ondulatorias cuando están en movimiento y ya no son vistas como localizadas en una región determinada, sino más bien extendidas en cierta medida. La luz u otra radiación emitida o absorbida por un átomo solo tiene ciertas frecuencias (o longitudes de onda), como puede verse en la línea del espectro asociado al elemento químico representado por tal átomo. La teoría cuántica demuestra que tales frecuencias corresponden a niveles definidos de los cuantos de luz, o fotones, y es el resultado del hecho de que los electrones del átomo solo pueden tener ciertos valores de energía permitidos. Cuando un electrón pasa de un nivel permitido a otro, una cantidad de energía es emitida o absorbida, cuya frecuencia es directamente proporcional a la diferencia de energía entre los dos niveles. La mecánica cuántica surge tímidamente en los inicios del siglo XX dentro de las tradiciones más profundas de la física para dar una solución a problemas para los que las teorías conocidas hasta el momento habían agotado su capacidad de explicar, como la llamada catástrofe ultravioleta en la radiación de cuerpo negro predicha por la física estadística clásica y la inestabilidad de los átomos en el modelo atómico de Rutherford. La primera propuesta de un principio propiamente cuántico se debe a Max Planck en 1900, para resolver el problema de la radiación de cuerpo negro, que fue duramente cuestionado, hasta que Albert Einstein lo convierte en el principio que exitosamente pueda explicar el efecto fotoeléctrico. Las primeras formulaciones matemáticas completas de la mecánica cuántica no se alcanzan hasta mediados de la década de 1920, sin que hasta el día de hoy se tenga una interpretación coherente de la teoría, en particular del problema de la medición. El formalismo de la mecánica cuántica se desarrolló durante la década de 1920. En 1924, Louis de Broglie propuso que, al igual que las ondas de luz presentan propiedades de partículas, como ocurre en el efecto fotoeléctrico, las partículas, también presentan propiedades ondulatorias. Dos formulaciones diferentes de la mecánica cuántica se presentaron después de la sugerencia de Broglie. En 1926, la mecánica ondulatoria de Erwin Schrödinger implica la utilización de una entidad matemática, la función de onda, que está relacionada con la probabilidad de encontrar una partícula en un punto dado en el espacio. En 1925, la mecánica matricial de Werner Heisenberg no hace mención alguna de las funciones de onda o conceptos similares, pero ha demostrado ser matemáticamente equivalente a la teoría de Schrödinger. Un descubrimiento importante de la teoría cuántica es el principio de incertidumbre, enunciado por Heisenberg en 1927, que pone un límite teórico absoluto en la precisión de ciertas mediciones. Como resultado de ello, la asunción clásica de los científicos de que el estado físico de un sistema podría medirse exactamente y utilizarse para predecir los estados futuros tuvo que ser abandonada. Esto supuso una revolución filosófica y dio pie a numerosas discusiones entre los más grandes físicos de la época. La mecánica cuántica propiamente dicha no incorpora a la relatividad en su formulación matemática. La parte de la mecánica cuántica que incorpora elementos relativistas de manera formal para abordar diversos problemas se conoce como mecánica cuántica relativista o ya, en forma más correcta y acabada, teoría cuántica de campos (que incluye a su vez a la electrodinámica cuántica, cromodinámica cuántica y teoría electrodébil dentro del modelo estándar)[1] y más generalmente, la teoría cuántica de campos en espacio-tiempo curvo. La única interacción elemental que no se ha podido cuantizar hasta el momento ha sido la interacción gravitatoria. Este problema constituye entonces uno de los mayores desafíos de la física del siglo XXI. La mecánica cuántica se combinó con la teoría de la relatividad en la formulación de Paul Dirac de 1928, lo que, además, predijo la existencia de antipartículas. Otros desarrollos de la teoría incluyen la estadística cuántica, presentada en una forma por Einstein y Bose (la estadística de Bose-Einstein) y en otra forma por Dirac y Enrico Fermi (la estadística de Fermi-Dirac); la electrodinámica cuántica, interesada en la interacción entre partículas cargadas y los campos electromagnéticos, su generalización, la teoría cuántica de campos y la electrónica cuántica. La mecánica cuántica proporciona el fundamento de la fenomenología del átomo, de su núcleo y de las partículas elementales (lo cual requiere necesariamente el enfoque relativista). También su impacto en teoría de la información, criptografía y química ha sido decisivo entre esta misma. Contexto histórico La mecánica cuántica es, cronológicamente hablando, la última de las grandes ramas de la física. Se formuló a principios del siglo XX, casi al mismo tiempo que la teoría de la relatividad, aunque el grueso de la mecánica cuántica se desarrolló a partir de 1920 (siendo la teoría de la relatividad especial de 1905 y la teoría general de la relatividad de 1915). Además al advenimiento de la mecánica cuántica existían diversos problemas no resueltos en la electrodinámica clásica. El primero de estos problemas era la emisión de radiación de cualquier objeto en equilibrio, llamada radiación térmica, que es la que proviene de la vibración microscópica de las partículas que lo componen. Usando las ecuaciones de la electrodinámica clásica, la energía que emitía esta radiación térmica tendía al infinito, si se suman todas las frecuencias que emitía el objeto, con ilógico resultado para los físicos. También la estabilidad de los átomos no podía ser explicada por el electromagnetismo clásico, y la noción de que el electrón fuera o bien una partícula clásica puntual o bien una cáscara esférica de dimensiones finitas resultaban igualmente problemáticas para esto. Radiación electromagnética El problema de la radiación electromagnética de un cuerpo negro fue uno de los primeros problemas resueltos en el seno de la mecánica cuántica. Es en el seno de la mecánica estadística donde surgen por primera vez las ideas cuánticas en 1900. Al físico alemán Max Planck se le ocurrió un artificio matemático: si en el proceso aritmético se sustituía la integral de esas frecuencias por una suma no continua (discreta), se dejaba de obtener infinito como resultado, con lo que se eliminaba el problema; además, el resultado obtenido concordaba con lo que después era medido. Fue Max Planck quien entonces enunció la hipótesis de que la radiación electromagnética es absorbida y emitida por la materia en forma de «cuantos» de luz o fotones de energía cuantizados introduciendo una constante estadística, que se denominó constante de Planck. Su historia es inherente al siglo XX, ya que la primera formulación cuántica de un fenómeno fue dada a conocer por el mismo Planck el 14 de diciembre de 1900 en una sesión de la Sociedad Física de la Academia de Ciencias de Berlín.[2]​ La idea de Planck habría permanecido muchos años solo como hipótesis sin verificar por completo si Albert Einstein no la hubiera retomado, proponiendo que la luz, en ciertas circunstancias, se comporta como partículas de energía (los cuantos de luz o fotones) en su explicación del efecto fotoeléctrico. Fue Albert Einstein quien completó en 1905 las correspondientes leyes del movimiento su teoría especial de la relatividad, demostrando que el electromagnetismo era una teoría esencialmente no mecánica. Culminaba así lo que se ha dado en llamar física clásica, es decir, la física no-cuántica. Usó este punto de vista llamado por él «heurístico», para desarrollar su teoría del efecto fotoeléctrico, publicando esta hipótesis en 1905, lo que le valió el Premio Nobel de Física de 1921. Esta hipótesis fue aplicada también para proponer una teoría sobre el calor específico, es decir, la que resuelve cuál es la cantidad de calor necesaria para aumentar en una unidad la temperatura de la unidad de masa de un cuerpo. El siguiente paso importante se dio hacia 1925, cuando Louis De Broglie propuso que cada partícula material tiene una longitud de onda asociada, inversamente proporcional a su masa, y a su velocidad. Así quedaba establecida la dualidad onda/materia. Poco tiempo después Erwin Schrödinger formuló una ecuación de movimiento para las «ondas de materia», cuya existencia había propuesto De Broglie y varios experimentos sugerían que eran reales. La mecánica cuántica introduce una serie de hechos contraintuitivos que no aparecían en los paradigmas físicos anteriores; con ella se descubre que el mundo atómico no se comporta como esperaríamos. Los conceptos de incertidumbre o cuantización son introducidos por primera vez aquí. Además la mecánica cuántica es la teoría científica que ha proporcionado las predicciones experimentales más exactas hasta el momento, a pesar de estar sujeta a las probabilidades. Inestabilidad de los átomos clásicos El segundo problema importante que la mecánica cuántica resolvió a través del modelo de Bohr, fue el de la estabilidad de los átomos. De acuerdo con la teoría clásica un electrón orbitando alrededor de un núcleo cargado positivamente debería emitir energía electromagnética perdiendo así velocidad hasta caer sobre el núcleo. La evidencia empírica era que esto no sucedía, y sería la mecánica cuántica la que resolvería este hecho primero mediante postulados ad hoc formulados por Bohr y más tarde mediante modelos como el modelo atómico de Schrödinger basados en supuestos más generales. A continuación se explica el fracaso del modelo clásico. En mecánica clásica, un átomo de hidrógeno es un tipo de problema de los dos cuerpos en que el protón sería el primer cuerpo que tiene más del 99% de la masa del sistema y el electrón es el segundo cuerpo que es mucho más ligero. Para resolver el problema de los dos cuerpos es conveniente hacer la descripción del sistema, colocando el origen del sistema de referencia en el centro de masa de la partícula de mayor masa, esta descripción es correcta considerando como masa de la otra partícula la masa reducida que viene dada por 999 Siendo  watt Ese proceso acabaría con el colapso del átomo sobre el núcleo en un tiempo muy corto dadas las grandes aceleraciones existentes. A partir de los datos de la ecuación anterior el tiempo de colapso sería de 10-8 s, es decir, de acuerdo con la física clásica los átomos de hidrógeno no serían estables y no podrían existir más de una cienmillonésima de segundo. Esa incompatibilidad entre las predicciones del modelo clásico y la realidad observada llevó a buscar un modelo que explicara fenomenológicamente el átomo. El modelo atómico de Bohr era un modelo fenomenológico y provisorio que explicaba satisfactoriamente aunque de manera heurística algunos datos, como el orden de magnitud del radio atómico y los espectros de absorción del átomo, pero no explicaba cómo era posible que el electrón no emitiera radiación perdiendo energía. La búsqueda de un modelo más adecuado llevó a la formulación del modelo atómico de Schrödinger en el cual puede probarse que el valor esperado de la aceleración es nulo, y sobre esa base puede decirse que la energía electromagnética emitida debería ser también nula. Sin embargo, al contrario del modelo de Bohr, la representación cuántica de Schrödinger es difícil de entender en términos intuitivos. Desarrollo histórico Artículo principal: Historia de la mecánica cuántica La teoría cuántica fue desarrollada en su forma básica a lo largo de la primera mitad del siglo XX. El hecho de que la energía se intercambie de forma discreta se puso de relieve por hechos experimentales como los siguientes, inexplicables con las herramientas teóricas anteriores de la mecánica clásica o la electrodinámica: Fig. 1: La función de onda del electrón de un átomo de hidrógeno posee niveles de energía definidos y discretos denotados por un número cuántico n=1, 2, 3,... y valores definidos de momento angular caracterizados por la notación: s, p, d,... Las áreas brillantes en la figura corresponden a densidades elevadas de probabilidad de encontrar el electrón en dicha posición. Espectro de la radiación del cuerpo negro, resuelto por Max Planck con la cuantización de la energía. La energía total del cuerpo negro resultó que tomaba valores discretos más que continuos. Este fenómeno se llamó cuantización, y los intervalos posibles más pequeños entre los valores discretos son llamados quanta (singular: quantum, de la palabra latina para «cantidad», de ahí el nombre de mecánica cuántica). La magnitud de un cuanto es un valor fijo llamado constante de Planck, y que vale: 6,626 ×10-34 J·s. Bajo ciertas condiciones experimentales, los objetos microscópicos como los átomos o los electrones exhiben un comportamiento ondulatorio, como en la interferencia. Bajo otras condiciones, las mismas especies de objetos exhiben un comportamiento corpuscular, de partícula, («partícula» quiere decir un objeto que puede ser localizado en una región concreta del espacio), como en la dispersión de partículas. Este fenómeno se conoce como dualidad onda-partícula. Las propiedades físicas de objetos con historias asociadas pueden ser correlacionadas, en una amplitud prohibida para cualquier teoría clásica, solo pueden ser descritos con precisión si se hace referencia a ambos a la vez. Este fenómeno es llamado entrelazamiento cuántico y la desigualdad de Bell describe su diferencia con la correlación ordinaria. Las medidas de las violaciones de la desigualdad de Bell fueron algunas de las mayores comprobaciones de la mecánica cuántica. Explicación del efecto fotoeléctrico, dada por Albert Einstein, en que volvió a aparecer esa misteriosa necesidad de cuantizar la energía. Efecto Compton. El desarrollo formal de la teoría fue obra de los esfuerzos conjuntos de varios físicos y matemáticos de la época como Schrödinger, Heisenberg, Einstein, Dirac, Bohr y Von Neumann entre otros (la lista es larga). Algunos de los aspectos fundamentales de la teoría están siendo aún estudiados activamente. La mecánica cuántica ha sido también adoptada como la teoría subyacente a muchos campos de la física y la química, incluyendo la física de la materia condensada, la química cuántica y la física de partículas. La región de origen de la mecánica cuántica puede localizarse en la Europa central, en Alemania y Austria, y en el contexto histórico del primer tercio del siglo XX. Suposiciones más importantes Artículo principal: Interpretaciones de la mecánica cuántica Las suposiciones más importantes de esta teoría son las siguientes: Al ser imposible fijar a la vez la posición y el momento de una partícula, se renuncia al concepto de trayectoria, vital en mecánica clásica. En vez de eso, el movimiento de una partícula puede ser explicado por una función matemática que asigna, a cada punto del espacio y a cada instante, la probabilidad de que la partícula descrita se halle en tal posición en ese instante (al menos, en la interpretación de la Mecánica cuántica más usual, la probabilista o interpretación de Copenhague). A partir de esa función, o función de ondas, se extraen teóricamente todas las magnitudes del movimiento necesarias. Existen dos tipos de evolución temporal, si no ocurre ninguna medida el estado del sistema o función de onda evolucionan de acuerdo con la ecuación de Schrödinger, sin embargo, si se realiza una medida sobre el sistema, este sufre un «salto cuántico» hacia un estado compatible con los valores de la medida obtenida (formalmente el nuevo estado será una proyección ortogonal del estado original). Existen diferencias notorias entre los estados ligados y los que no lo están. La energía no se intercambia de forma continua en un estado ligado, sino en forma discreta lo cual implica la existencia de paquetes mínimos de energía llamados cuantos, mientras en los estados no ligados la energía se comporta como un continuo. Descripción de la teoría Interpretación de Copenhague Artículo principal: Interpretación de Copenhague Para describir la teoría de forma general es necesario un tratamiento matemático riguroso, pero aceptando una de las tres interpretaciones de la mecánica cuántica (a partir de ahora la Interpretación de Copenhague), el marco se relaja. La mecánica cuántica describe el estado instantáneo de un sistema (estado cuántico) con una función de onda que codifica la distribución de probabilidad de todas las propiedades medibles, u observables. Algunos observables posibles sobre un sistema dado son la energía, posición, momento y momento angular. La mecánica cuántica no asigna valores definidos a los observables, sino que hace predicciones sobre sus distribuciones de probabilidad. Las propiedades ondulatorias de la materia son explicadas por la interferencia de las funciones de onda. Estas funciones de onda pueden variar con el transcurso del tiempo. Esta evolución es determinista si sobre el sistema no se realiza ninguna medida aunque esta evolución es estocástica y se produce mediante colapso de la función de onda cuando se realiza una medida sobre el sistema (Postulado IV de la MC). Por ejemplo, una partícula moviéndose sin interferencia en el espacio vacío puede ser descrita mediante una función de onda que es un paquete de ondas centrado alrededor de alguna posición media. Según pasa el tiempo, el centro del paquete puede trasladarse, cambiar, de modo que la partícula parece estar localizada más precisamente en otro lugar. La evolución temporal determinista de las funciones de onda es descrita por la ecuación de Schrödinger. Algunas funciones de onda describen estados físicos con distribuciones de probabilidad que son constantes en el tiempo, estos estados se llaman estacionarios, son estados propios del operador hamiltoniano y tienen energía bien definida. Muchos sistemas que eran tratados dinámicamente en mecánica clásica son descritos mediante tales funciones de onda estáticas. Por ejemplo, un electrón en un átomo sin excitar se dibuja clásicamente como una partícula que rodea el núcleo, mientras que en mecánica cuántica es descrito por una nube de probabilidad estática que rodea al núcleo. Cuando se realiza una medición en un observable del sistema, la función de ondas se convierte en una del conjunto de las funciones llamadas funciones propias o estados propios del observable en cuestión. Este proceso es conocido como colapso de la función de onda. Las probabilidades relativas de ese colapso sobre alguno de los estados propios posibles son descritas por la función de onda instantánea justo antes de la reducción. Considerando el ejemplo anterior sobre la partícula en el vacío, si se mide la posición de la misma, se obtendrá un valor impredecible x. En general, es imposible predecir con precisión qué valor de x se obtendrá, aunque es probable que se obtenga uno cercano al centro del paquete de ondas, donde la amplitud de la función de onda es grande. Después de que se ha hecho la medida, la función de onda de la partícula colapsa y se reduce a una que esté muy concentrada en torno a la posición observada x. La ecuación de Schrödinger es determinista en el sentido de que, dada una función de onda a un tiempo inicial dado, la ecuación suministra una predicción concreta de qué función tendremos en cualquier tiempo posterior. Durante una medida, el eigen-estado al cual colapsa la función es probabilista y en este aspecto la mecánica cuántica es no determinista. Así que la naturaleza probabilista de la mecánica cuántica nace del acto de la medida. Esto conduce al problema de definir objetivamente en qué momento se produce la medida y la evolución pasa de lineal y determinista, a no-lineal y estocástica/aleatoria, cuestión que se conoce como problema de la medida y que, además de la interpretación de Copenhague, ha dado lugar a un número elevado de propuestas de resolución, conocidas como interpretaciones de la mecánica cuántica. Formulación matemática Artículos principales: Postulados de la mecánica cuántica y Notación braket. En la formulación matemática rigurosa, desarrollada por Dirac y von Neumann, los estados posibles de un sistema cuántico están representados por vectores unitarios (llamados estados) que pertenecen a un Espacio de Hilbert complejo separable (llamado el espacio de estados). Qué tipo de espacio de Hilbert es necesario en cada caso depende del sistema; por ejemplo, el espacio de estados para los estados de posición y momento es el espacio de funciones de cuadrado integrable  Cada magnitud observable queda representada por un operador lineal hermítico definido sobre un dominio denso del espacio de estados. Cada estado propio de un observable corresponde a un eigenvector del operador, y el valor propio o eigenvalor asociado corresponde al valor del observable en aquel estado propio. El espectro de un operador puede ser continuo o discreto. La medida de un observable representado por un operador con espectro discreto solo puede tomar un conjunto numerable de posibles valores, mientras que los operadores con espectro continuo presentan medidas posibles en intervalos reales completos. Durante una medida, la probabilidad de que un sistema colapse a uno de los eigenestados viene dada por el cuadrado del valor absoluto del producto interno entre el estado propio o auto-estado (que podemos conocer teóricamente antes de medir) y el vector estado del sistema antes de la medida. Podemos así encontrar la distribución de probabilidad de un observable en un estado dado computando la descomposición espectral del operador correspondiente. El principio de incertidumbre de Heisenberg se representa por la aseveración de que los operadores correspondientes a ciertos observables no conmutan. Principio de Incertidumbre Una de las consecuencias del formalismo cuántico es el principio de incertidumbre. En su forma más familiar, establece que ninguna medición de una partícula cuántica puede implicar simultáneamente predicciones precisas para la medición de su posición y la medición de su momento.[3]​[4] Tanto posición como momento son observables, esto significa que son representados por operadores hermíticos. El operador posición  Dado un estado cuántico, la regla de Born nos permite encontrar valores para  y de la misma manera para el momento: El principio de incertidumbre establece que En principio, cualquiera de las desviaciones estándar puede hacerse arbitrariamente pequeña, pero no ambas simultáneamente .[5] Esta desigualdad se generaliza a pares arbitrarios de operadores autoadjuntos  y proporciona el límite inferior en el producto de las desviaciones estándar: Otra consecuencia de la relación de conmutación canónica es que los operadores posición y momento son la transformada de Fourier del otro, de modo que una descripción de un objeto según su momento es la transformada de Fourier de su descripción según su posición. El hecho de que la dependencia en cantidad de movimiento sea la transformada de Fourier de la dependencia en posición significa que el operador de cantidad de movimiento es equivalente (hasta un factor de  Aplicaciones En muchos aspectos, la tecnología moderna opera a una escala en la que los efectos cuánticos son significativos. Las aplicaciones importantes de la teoría cuántica incluyen la química cuántica, la óptica cuántica, la computación cuántica, los imanes superconductores, los diodos emisores de luz, el amplificador óptico y el láser, el transistor y semiconductores como el microprocesador, imágenes médicas y de investigación como la resonancia magnética y el microscopio electrónico.[6] Las explicaciones de muchos fenómenos biológicos y físicos tienen su origen en la naturaleza del enlace químico, sobre todo la macromolécula del ADN. Relatividad y la mecánica cuántica Artículos principales: Teoría cuántica de campos y Segunda cuantización. El mundo moderno de la física se funda notablemente en dos teorías principales, la relatividad general y la mecánica cuántica, aunque ambas teorías usan principios aparentemente incompatibles. Los postulados que definen la teoría de la relatividad de Einstein y la teoría del quántum están apoyados por rigurosa y repetida evidencia empírica. Sin embargo, ambas se resisten a ser incorporadas dentro de un mismo modelo coherente. Desde mediados del siglo XX, aparecieron teorías cuánticas relativistas del campo electromagnético (electrodinámica cuántica) y las fuerzas nucleares (modelo electrodébil, cromodinámica cuántica), pero no se tiene una teoría cuántica relativista del campo gravitatorio que sea plenamente consistente y válida para campos gravitatorios intensos (existen aproximaciones en espacios asintóticamente planos). Todas las teorías cuánticas relativistas consistentes usan los métodos de la teoría cuántica de campos. En su forma ordinaria, la teoría cuántica abandona algunos de los supuestos básicos de la teoría de la relatividad, como por ejemplo el principio de localidad usado en la descripción relativista de la causalidad. El mismo Einstein había considerado absurda la violación del principio de localidad a la que parecía abocar la mecánica cuántica. La postura de Einstein fue postular que la mecánica cuántica si bien era consistente era incompleta. Para justificar su argumento y su rechazo a la falta de localidad y la falta de determinismo, Einstein y varios de sus colaboradores postularon la llamada paradoja de Einstein-Podolsky-Rosen (EPR), la cual demuestra que medir el estado de una partícula puede instantáneamente cambiar el estado de su socio enlazado, aunque las dos partículas pueden estar a una distancia arbitrariamente grande. Modernamente el paradójico resultado de la paradoja EPR se sabe es una consecuencia perfectamente consistente del llamado entrelazamiento cuántico. Es un hecho conocido que si bien la existencia del entrelazamiento cuántico efectivamente viola el principio de localidad, en cambio no viola la causalidad definido en términos de información, puesto que no hay transferencia posible de información. Si bien en su tiempo, parecía que la paradoja EPR suponía una dificultad empírica para la mecánica cuántica, y Einstein consideró que la mecánica cuántica en la interpretación de Copenhague podría ser descartada por experimento, décadas más tarde los experimentos de Alain Aspect (1981) revelaron que efectivamente la evidencia experimental parece apuntar en contra del principio de localidad.[7] Y por tanto, el resultado paradójico que Einstein rechazaba como «sin sentido» parece ser lo que sucede precisamente en el mundo real."
ksampletext_wikipedia_phys_teoriadecuerdas: str = "Teoría de cuerdas. Las teorías de cuerdas son una serie de hipótesis científicas y modelos fundamentales de física teórica que asumen que las partículas subatómicas, aparentemente puntuales, son en realidad estados vibracionales de un objeto extendido más básico llamado cuerda o filamento.[1]​ De acuerdo con estas teorías, un electrón no sería un punto sin estructura interna y de dimensión cero, sino una cuerda minúscula en forma de lazo vibrando en un espacio-tiempo de más de cuatro dimensiones; de hecho, el planteamiento matemático de esta teoría no funciona a menos que el universo tenga diez dimensiones. Mientras que un punto simplemente se movería por el espacio, una cuerda podría hacer algo más: vibrar de diferentes maneras. Si vibrase de cierto modo, veríamos un electrón; pero si lo hiciese de otro, veríamos un fotón, un cuark o cualquier otra partícula del modelo estándar dependiendo de la forma concreta en que estuviese vibrando. Estas teorías, ampliadas con otras como la de las supercuerdas o la teoría M, pretende alejarse de la concepción del punto-partícula. La siguiente formulación de una teoría de cuerdas se debe a Jöel Scherk y John Henry Schwarz, que en 1974 publicaron un artículo en el que mostraban que una teoría basada en objetos unidimensionales o cuerdas en lugar de partículas puntuales podía describir la fuerza gravitatoria, aunque estas ideas no recibieron en ese momento mucha atención hasta la primera revolución de supercuerdas de 1984. De acuerdo con la formulación de la teoría de cuerdas surgida de esta revolución, las teorías de cuerdas pueden considerarse de hecho un caso general de teoría de Kaluza-Klein cuantizada. Las ideas fundamentales son dos: Los objetos básicos de la teoría no serían partículas puntuales, sino objetos unidimensionales extendidos (en las cinco teorías de supercuerdas convencionales estos objetos eran unidimensionales o cuerdas; actualmente en la teoría-M se admiten también de dimensión superior o p-branas). Esto renormaliza algunos infinitos de los cálculos perturbativos. El espacio-tiempo en el que se mueven las cuerdas y p-branas de la teoría no sería el espacio-tiempo ordinario de cuatro dimensiones, sino un espacio de tipo Kaluza-Klein, en el que a las cuatro dimensiones convencionales se añaden seis dimensiones compactadas en forma de variedad de Calabi-Yau. Por tanto convencionalmente en la teoría de cuerdas existe una dimensión temporal, tres dimensiones espaciales ordinarias y seis dimensiones compactadas e inobservables en la práctica. La inobservabilidad de las dimensiones adicionales está relacionada con el hecho de que estas estarían compactadas, y solo serían relevantes a escalas pequeñas comparables con la longitud de Planck. Igualmente, con la precisión de medida convencional las cuerdas cerradas con una longitud similar a la longitud de Planck se asemejarían a partículas puntuales. Desarrollos posteriores Tras la introducción de la teoría de cuerdas, se consideró la conveniencia de introducir el principio de que la teoría fuera supersimétrica; es decir, que admitiera una simetría abstracta que relacionara fermiones y bosones. Actualmente la mayoría de teóricos de cuerdas trabajan en teorías supersimétricas; de ahí que la teoría de cuerdas actualmente se llame teoría de supercuerdas. Esta última teoría es básicamente una teoría de cuerdas supersimétrica; es decir, que es invariante bajo transformaciones de supersimetría. Actualmente existen cinco teorías de supercuerdas relacionadas con los cinco modos que se conocen de implementar la supersimetría en el modelo de cuerdas. Aunque dicha multiplicidad de teorías desconcertó a los especialistas durante más de una década, el saber convencional actual sugiere que las cinco teorías son casos límites de una teoría única sobre un espacio de 10 dimensiones (las tres del espacio y una temporal serían las 4 que ya conocemos más otras seis adicionales resabiadas o compactadas) y una que las engloba formando membranas de las cuales se podría escapar parte de la gravedad de ellas en forma de gravitones. Esta teoría única, llamada teoría M, de la que solo se conocerían algunos aspectos, fue conjeturada en 1995. Variantes de la teoría La teoría de supercuerdas es algo actual. En sus principios (mediados de los años 1980) aparecieron unas cinco teorías de cuerdas, las cuales después fueron identificadas como límites particulares de una sola teoría: la teoría M. Las cinco versiones de la teoría actualmente existentes, entre las que pueden establecerse varias relaciones de dualidad, son: La Teoría de cuerdas de Tipo I, donde aparecen tanto cuerdas y D-branas abiertas como cerradas, que se mueven sobre un espacio-tiempo de diez dimensiones. Las D-branas tienen una, cinco y nueve dimensiones espaciales. La Teoría de cuerdas de Tipo IIA. Es también una teoría de diez dimensiones, pero que emplea solo cuerdas y D-branas cerradas. Incorpora los gravitinos (partículas teóricas asociadas al gravitón mediante relaciones de supersimetría). Usa D-branas de dimensión 0, 2, 4, 6 y 8. La Teoría de cuerdas de Tipo IIB. Difiere de la teoría de tipo IIA principalmente en el hecho de que esta última es no quiral (conservando la paridad). La Teoría de cuerda heterótica SO(32) (Heterótica-O), basada en el grupo de simetría O(32). La Teoría de cuerda heterótica E8xE8 (Heterótica-E), basada en el grupo de Lie excepcional E8. Fue propuesta en 1987 por Gross, Harvey, Martinec y Rohm. El término teoría de cuerdas se refiere en realidad a las teorías de cuerdas bosónicas de 26 dimensiones y la teoría de supercuerdas de diez dimensiones, esta última descubierta al añadir supersimetría a la teoría de cuerdas bosónica. Hoy en día la teoría de cuerdas se suele referir a la variante supersimétrica, mientras que la antigua se conoce por el nombre completo de teoría de cuerdas bosónicas. En 1995, Edward Witten conjeturó que las cinco diferentes teorías de supercuerdas son casos límite de una desconocida teoría de once dimensiones llamada teoría-M. La conferencia donde Witten mostró algunos de sus resultados inició la llamada segunda revolución de supercuerdas. En esta teoría M intervienen como objetos animados físicos fundamentales no solo cuerdas unidimensionales, sino toda una variedad de objetos no perturbativos, extendidos en varias dimensiones, que se llaman colectivamente p-branas (este nombre es una aféresis de membrana). Controversia sobre la teoría Aunque la teoría de cuerdas, según sus defensores, pudiera llegar a convertirse en una de las teorías físicas más predictivas, capaz de explicar algunas de las propiedades más fundamentales de la naturaleza en términos geométricos, los físicos que han trabajado en ese campo hasta la fecha no han podido hacer predicciones concretas con la precisión necesaria para confrontarlas con datos experimentales, al parecer se necesita de una tecnología más avanzada para visualizar las partículas, lo cual llevara muchos años. Dichos problemas de predicción se deberían, según el autor, a que el modelo no es falsable, y por tanto, no es científico,[2] o bien a que «la teoría de las supercuerdas es tan ambiciosa que solo puede ser del todo correcta o del todo equivocada. El único problema es que sus matemáticas son tan nuevas y tan difíciles que durante varias décadas no sabremos cuáles son»,[3] dicho esto en 1990. D. Gross, premio Nobel de física por su trabajo en el modelo estándar, se convirtió en un formidable luchador de la teoría de cuerdas, pero recientemente ha dicho: «No sabemos de qué estamos hablando».[4]​ Si los teóricos de cuerdas se equivocan, no pueden equivocarse solo un poco. Si las nuevas dimensiones y las simetrías no existen, consideraremos a los teóricos de cuerdas unos de los mayores fracasados de la ciencia (...). Su historia constituirá una leyenda moral de cómo no hacer ciencia, de cómo no permitir que se sobrepasen tanto los límites, hasta el punto de convertir la conjetura teórica en fantasía. Lee Smolin[5]​ Otras teorías En 1997, el físico teórico argentino Juan Maldacena propuso un sorprendente modelo del universo según el cual la gravedad surge de cuerdas infinitesimales, delgadas y vibrantes y puede ser reinterpretada en términos físicos. Así, este mundo de cuerdas matemáticamente intrincado, que existe en diez dimensiones espaciales, no sería más que un holograma: la acción real se desarrollaría en un cosmos plano, más simple y en el que no hay gravedad. La idea de Maldacena entusiasmó a los físicos, entre otras razones porque resolvía aparentes inconsistencias entre la física cuántica y la teoría de la gravedad de Einstein. Así, el argentino proporcionó a los científicos una 'piedra Rosetta matemática', una 'dualidad', que les permitía resolver los problemas de un modelo que parecían no tener respuesta en el otro, y viceversa. Pero a pesar de la validez de sus ideas aún no se había logrado hallar ninguna prueba rigurosa de su teoría. Según un artículo publicado en la revista científica Nature, Yoshifumi Hyakutake, de la Universidad de Ibaraki (Japón), y sus colegas, proporcionaron en dos de sus estudios, sino una prueba real, al menos una muestra convincente de que la conjetura de Maldacena es cierta. En uno de los estudios, Hyakutake calculó la energía interna de un agujero negro, la posición de su horizonte de sucesos (el límite entre el agujero negro y el resto del universo), su entropía y otras propiedades a partir de las predicciones de la teoría de cuerdas y de los efectos asociados a las 'partículas virtuales', que aparecen continuamente dentro y fuera de la existencia. En el otro, él y sus colaboradores calcularon la energía interna del correspondiente universo de dimensión inferior sin gravedad. Los dos cálculos informáticos coinciden. Parece que es un cálculo correcto, dice Maldacena, al tiempo que subraya que los hallazgos son una forma interesante de demostrar muchas ideas de la gravedad cuántica y la teoría de cuerdas. Numéricamente han confirmado, tal vez por primera vez, algo de lo que estábamos bastante seguros pero era todavía una conjetura: que la termodinámica de ciertos agujeros negros puede ser reproducida desde un universo dimensional inferior, explica Leonard Susskind, físico teórico de la Universidad de Stanford, en California, quien fue uno de los primeros teóricos en explorar la idea de universos holográficos. Falsacionismo y teoría de cuerdas Artículo principal: Criterio de demarcación La teoría de cuerdas o la teoría M podrían no ser falsables, según sus críticos.[6]​[7]​[8]​[9]​[10] Diversos autores han declarado su preocupación de que la teoría de cuerdas no sea falsable y como tal, siguiendo las tesis del filósofo de la ciencia Karl Popper, la teoría de cuerdas sería equivalente a una pseudociencia.[11]​[12]​[13]​[14]​[15]​[16]​ El filósofo de la ciencia Mario Bunge ha manifestado lo siguiente: La consistencia, la sofisticación y la belleza nunca son suficientes en la investigación científica. La teoría de cuerdas es sospechosa (de pseudociencia). Parece científica porque aborda un problema abierto que es a la vez importante y difícil, el de construir una teoría cuántica de la gravitación. Pero la teoría postula que el espacio físico tiene seis o siete dimensiones, en lugar de tres, simplemente para asegurarse consistencia matemática. Puesto que estas dimensiones extra son inobservables, y puesto que la teoría se ha resistido a la confirmación experimental durante más de tres décadas, parece ciencia ficción, o al menos, ciencia fallida. La física de partículas está inflada con sofisticadas teorías matemáticas que postulan la existencia de entidades extrañas que no interactúan de forma apreciable, o para nada en absoluto, con la materia ordinaria, y como consecuencia, quedan a salvo al ser indetectables. Puesto que estas teorías se encuentran en discrepancia con el conjunto de la Física, y violan el requerimiento de falsacionismo, pueden calificarse de pseudocientíficas, incluso aunque lleven pululando un cuarto de siglo y se sigan publicando en las revistas científicas más prestigiosas. Mario Bunge, 2006.[10]​ Impacto de la promoción de la teoría en el mundo académico Smolin indica que la teoría de cuerdas se ha convertido en el principal camino de exploración de «las grandes cuestiones de la física» debido a una agresiva promoción, considerando que resulta prácticamente un «suicidio profesional» para cualquier joven físico teórico no ingresar en sus filas. Expone además que «a pesar de la escasa inversión en [...] otros campos de investigación, algunos de ellos han avanzado más que el de la teoría de cuerdas» e identifica los siguientes rasgos en las «comunidades de supercuerdas»:[5]​ Tremenda autosuficiencia y conciencia de pertenecer a una élite. Comunidades monolíticas con gran uniformidad de opiniones sobre cuestiones abiertas, generalmente impuestas por los que constituyen la jerarquía de la comunidad. Sentido de identificación con el grupo parecido a la pertenencia a una comunidad religiosa o partido político. Sentido de frontera entre el grupo y otros expertos. Gran desinterés por las ideas y personas que no son del grupo. Una confianza excesiva en interpretar positivamente los resultados e incluso aceptarlos exclusivamente porque son creídos por la mayoría. Una falta de percepción del riesgo que conlleva una nueva teoría."
ksampletext_wikipedia_phys_electromagnetismo: str = "Electromagnetismo. El electromagnetismo es la rama de la física que estudia y unifica los fenómenos eléctricos y magnéticos en una sola teoría. El electromagnetismo describe la interacción de partículas cargadas con campos eléctricos y magnéticos. La interacción electromagnética es una de las cuatro fuerzas fundamentales del universo conocido. El electromagnetismo es una rama de la física que estudia los efectos producidos por el magnetismo, lo cual surge a partir de la corriente eléctrica. Por su parte, el magnetismo es la disciplina que examina los fenómenos asociados a los imanes. Su nombre proviene de Magnesia, un distrito en Asia Menor (actual Turquía), donde se descubrieron por primera vez las piedras llamadas magnetitas, que tienen la capacidad de atraer ciertos metales.[1]​ El electromagnetismo abarca diversos fenómenos del mundo real, como por ejemplo la luz. La luz es un campo electromagnético oscilante que se irradia desde partículas cargadas aceleradas. Aparte de la gravedad, la mayoría de las fuerzas en la experiencia cotidiana son consecuencia del electromagnetismo. Los principios del electromagnetismo encuentran aplicaciones en diversas disciplinas afines, tales como las microondas, antenas, máquinas eléctricas, comunicaciones por satélite, bioelectromagnetismo, plasmas, investigación nuclear, la fibra óptica, la interferencia y la compatibilidad electromagnéticas, la conversión de energía electromecánica, la meteorología por radar, y la observación remota. Los dispositivos electromagnéticos incluyen transformadores, relés, radio/TV, teléfonos, motores eléctricos, líneas de transmisión, guías de onda y láseres. Los fundamentos de la teoría electromagnética fueron presentados por Michael Faraday y formulados por primera vez de modo completo por James Clerk Maxwell en 1865. La formulación consiste en cuatro ecuaciones diferenciales vectoriales que relacionan el campo eléctrico, el campo magnético y sus respectivas fuentes materiales (corriente eléctrica, polarización eléctrica y polarización magnética), conocidas como ecuaciones de Maxwell, lo que ha sido considerada como la «segunda gran unificación de la física», siendo la primera realizada por Isaac Newton. El estudio de los campos electromagnéticos se puede dividir en electrostática —el estudio de las interacciones entre cargas en reposo— y la electrodinámica —el estudio de las interacciones entre cargas en movimiento y la radiación—. La teoría clásica del electromagnetismo se basa en la fuerza de Lorentz y en las ecuaciones de Maxwell. Muchas propiedades ópticas y físicas de la materia también son explicados por la teoría electromagnética. El electromagnetismo es una teoría de campos; es decir, las explicaciones y predicciones que provee se basan en magnitudes físicas vectoriales o tensoriales dependientes de la posición en el espacio y del tiempo. El electromagnetismo describe los fenómenos físicos macroscópicos en los cuales intervienen cargas eléctricas en reposo y en movimiento, usando para ello campos eléctricos y magnéticos y sus efectos sobre las sustancias sólidas, líquidas y gaseosas. Por ser una teoría macroscópica, es decir, aplicable a un número muy grande de partículas y a distancias grandes respecto de las dimensiones de estas, el electromagnetismo no describe los fenómenos atómicos y moleculares. La electrodinámica cuántica proporciona la descripción cuántica de esta interacción, que puede ser unificada con la interacción nuclear débil según el modelo electrodébil. Espectro electromagnético. Historia Esta sección es un extracto de Historia del electromagnetismo.[editar] El físico danés Hans Christian Ørsted, realizando el experimento que le permitió descubrir la relación entre la electricidad y el magnetismo en 1820. La historia del electromagnetismo, considerada como el conocimiento y el uso registrado de las fuerzas electromagnéticas, data de hace más de dos mil años. En la antigüedad ya estaban familiarizados con los efectos de la electricidad atmosférica, en particular del rayo[2] ya que las tormentas son comunes en las latitudes más meridionales, ya que también se conocía el fuego de San Telmo. Sin embargo, se comprendía poco la electricidad y no eran capaces de producir estos fenómenos.[3]​[4]​ Durante los siglos XVII y XVIII, William Gilbert, Otto von Guericke, Stephen Gray, Benjamin Franklin, Alessandro Volta entre otros investigaron estos dos fenómenos de manera separada y llegaron a conclusiones coherentes con sus experimentos. A principios del siglo XIX, Hans Christian Ørsted encontró evidencia empírica de que los fenómenos magnéticos y eléctricos estaban relacionados. De ahí es que los trabajos de físicos como André-Marie Ampère, William Sturgeon, Joseph Henry, Georg Simon Ohm, Michael Faraday en ese siglo, son unificados por James Clerk Maxwell en 1861 con un conjunto de ecuaciones que describían ambos fenómenos como uno solo, como un fenómeno electromagnético.[4]​ Las ahora llamadas ecuaciones de Maxwell demostraban que los campos eléctricos y los campos magnéticos eran manifestaciones de un solo campo electromagnético. Además, describía la naturaleza ondulatoria de la luz, mostrándola como una onda electromagnética.[5] Con una sola teoría consistente que describía estos dos fenómenos antes separados, los físicos pudieron realizar varios experimentos prodigiosos e inventos muy útiles como la bombilla eléctrica por Thomas Alva Edison o el generador de corriente alterna por Nikola Tesla.[6] El éxito predictivo de la teoría de Maxwell y la búsqueda de una interpretación coherente de sus implicaciones, fue lo que llevó a Albert Einstein a formular su teoría de la relatividad que se apoyaba en algunos resultados previos de Hendrik Antoon Lorentz y Henri Poincaré. En la primera mitad del siglo XX, con el advenimiento de la mecánica cuántica, el electromagnetismo tuvo que mejorar su formulación para que fuera coherente con la nueva teoría. Esto se logró en la década de 1940 cuando se completó una teoría cuántica electromagnética conocida como electrodinámica cuántica Historia de la teoría Hans Christian Oersted Originalmente, la electricidad y el magnetismo se consideraban dos fenómenos independientes entre sí. Este punto de vista cambió, sin embargo, con la publicación en 1873 del Tratado de electricidad y magnetismo de James Maxwell , que mostró que la interacción de cargas positivas y negativas está gobernada por una sola fuerza. Hay cuatro efectos principales, resultantes de estas interacciones, que han sido claramente demostrados por experimentos: Las cargas eléctricas son atraídas o repelidas entre sí con una fuerza inversamente proporcional al cuadrado de la distancia entre ellas: las cargas diferentes se atraen, las cargas iguales se repelen. Los polos magnéticos (o estados de polarización en puntos separados) se atraen o repelen entre sí de manera similar y siempre van en pares: cada polo norte no existe por separado del polo sur. La corriente eléctrica en un cable crea un campo magnético circular alrededor del cable, dirigido (en sentido horario o antihorario) según el flujo de corriente. Se induce una corriente en el bucle del cable cuando se acerca o aleja con relación al campo magnético, o cuando el imán se acerca o aleja del bucle del cable; la dirección de la corriente depende de la dirección de estos movimientos. André-Marie Ampere En preparación para la conferencia, la noche del 21 de abril de 1820, Hans Christian Oersted hizo una observación asombrosa. Cuando estaba compilando el material, notó que la aguja de la brújula se desviaba del polo norte magnético cuando se encendía y apagaba la corriente eléctrica de la batería que estaba usando. Esta desviación lo llevó a creer que los campos magnéticos emanan de todos los lados de un cable a través del cual fluye una corriente eléctrica, al igual que la luz y el calor se propagan en el espacio, y esa experiencia indica una conexión directa entre la electricidad y el magnetismo. Michael Faraday En el momento del descubrimiento, Oersted no ofreció una explicación satisfactoria de este fenómeno y no intentó presentar el fenómeno en cálculos matemáticos. Sin embargo, tres meses después, comenzó a realizar investigaciones más intensivas. Poco después, publicó los resultados de su investigación, demostrando que una corriente eléctrica crea un campo magnético cuando fluye a través de cables. En el sistema CGS , la unidad de inducción electromagnética, Oe, recibió su nombre de su contribución al campo del electromagnetismo. James Clerk Maxwell Las conclusiones de Oersted llevaron a un estudio intensivo de electrodinámica por parte de la comunidad científica mundial. Las obras de Dominique François Arago también se remontan a 1820 , quien advirtió que un cable por el que fluye una corriente eléctrica atrae limaduras de hierro . También magnetizó por primera vez alambres de hierro y acero, colocándolos dentro de una bobina de alambres de cobre por donde pasaba la corriente. También logró magnetizar la aguja colocándola en una bobina y descargando la Botella de Leyden a través de la bobina. Independientemente de Arago, Davy descubrió la magnetización del acero y el hierro por la corriente . Las primeras definiciones cuantitativas de la acción de una corriente sobre un imán de la misma forma se remontan a 1820 y pertenecen a científicos franceses Jean-Baptiste Biot y Félix Savart.[7] Los experimentos de Oersted también influyeron en el físico francés André-Marie Ampere , quien presentó la ley electromagnética entre un conductor y una corriente en forma matemática. El descubrimiento de Oersted también representa un paso importante hacia un concepto de campo unificado. Esta unidad, que fue descubierta por Michael Faraday , completada por James Clerk Maxwell , y también refinada por Oliver Heaviside y Heinrich Hertz, es uno de los logros clave del siglo XIX en física matemática . Este descubrimiento tuvo implicaciones de gran alcance, una de las cuales fue comprender la naturaleza de la luz. La luz y otras ondas electromagnéticas toman la forma de fenómenos oscilatorios autopropagantes cuantificados del campo electromagnético llamados fotones. Diferentes frecuencias de vibración conducen a diferentes formas de radiación electromagnética: desde ondas de radio a bajas frecuencias, a luz visible a frecuencias medias, a rayos gamma a altas frecuencias. Oersted no fue la única persona que descubrió la conexión entre la electricidad y el magnetismo. En 1802, Giovanni Domenico Romagnosi , un jurista italiano, desvió una aguja magnética con descargas electrostáticas. Pero, de hecho, la investigación de Romagnosi no utilizó una celda galvánica y no había corriente continua como tal. El informe del descubrimiento se publicó en 1802 en un periódico italiano, pero la comunidad científica apenas lo notó en ese momento. Ramas Electrostática Artículo principal: Electrostática La electrostática es el estudio de los fenómenos asociados a los cuerpos cargados en reposo. Como describe la ley de Coulomb, estos cuerpos ejercen fuerzas entre sí. Su comportamiento se puede analizar en términos de la idea de un campo eléctrico que rodea cualquier cuerpo cargado, de manera que otro cuerpo cargado colocado dentro del campo estará sujeto a una fuerza proporcional a la magnitud de su carga y de la magnitud del campo en su ubicación. El que la fuerza sea atractiva o repulsiva depende de la polaridad de la carga. La electrostática tiene muchas aplicaciones, que van desde el análisis de fenómenos como tormentas eléctricas hasta el estudio del comportamiento de los tubos electrónicos. Un electroscopio usado para medir la carga eléctrica de un objeto. Cuando hablamos de electrostática nos referimos a los fenómenos que ocurren debido a una propiedad intrínseca y discreta de la materia, la carga, cuando es estacionaria o no depende del tiempo. La unidad de carga elemental, es decir, la más pequeña observable, es la carga que tiene el electrón.[9] Se dice que un cuerpo está cargado eléctricamente cuando tiene exceso o falta de electrones en los átomos que lo componen. Por definición el defecto de electrones se la denomina carga positiva y al exceso carga negativa.[10] La relación entre los dos tipos de carga es de atracción cuando son diferentes y de repulsión cuando son iguales. La carga elemental es una unidad muy pequeña para cálculos prácticos, por eso en el Sistema Internacional la unidad de carga eléctrica, el culombio, se define como la cantidad de carga transportada en un segundo por una corriente de un amperio de intensidad de corriente eléctrica. que equivale a la carga de 6,25 x 1018 electrones.[9] El movimiento de electrones por un conductor se denomina corriente eléctrica y la cantidad de carga eléctrica que pasa por unidad de tiempo se define como la intensidad de corriente. Se pueden introducir más conceptos como el de diferencia de potencial o el de resistencia, que nos conducirían ineludiblemente al área de circuitos eléctricos, y todo eso se puede ver con más detalle en el artículo principal. El nombre de la unidad de carga se debe a Coulomb, quien en 1785 llegó a una relación matemática de la fuerza eléctrica entre cargas puntuales, que ahora se la conoce como ley de Coulomb: Entre dos cargas puntuales  Las cargas elementales al no encontrarse solas se las debe tratar como una distribución de ellas. Por eso debe implementarse el concepto de campo, definido como una región del espacio donde existe una magnitud escalar o vectorial dependiente o independiente del tiempo. Así el campo eléctrico  Campo eléctrico de cargas puntuales. lim Y así finalmente llegamos a la expresión matemática que define el campo eléctrico: Es importante conocer el alcance de este concepto de campo eléctrico: nos brinda la oportunidad de conocer cuál es su intensidad y qué ocurre con una carga en cualquier parte de dicho campo sin importar el conocimiento de qué lo provoca.[11]​ Una forma de obtener qué cantidad de fuerza eléctrica pasa por cierto punto o superficie del campo eléctrico es usar el concepto de flujo eléctrico. Este flujo eléctrico  El matemático y físico, Carl Friedrich Gauss, demostró que la cantidad de flujo eléctrico en un campo es igual al cociente entre la carga encerrada por la superficie en la que se calcula el flujo,  (1) Véanse también: Carga eléctrica, Ley de Coulomb, Campo eléctrico, Potencial eléctrico y Ley de Gauss. Magnetostática Artículo principal: Magnetostática Líneas de fuerza de una barra magnética. La magnetósfera de la Tierra, empujada por el viento solar. No fue sino hasta el año de 1820, cuando Hans Christian Ørsted descubrió que el fenómeno magnético estaba ligado al eléctrico, que se obtuvo una teoría científica para el magnetismo.[12] La presencia de una corriente eléctrica, o sea, de un flujo de carga debido a una diferencia de potencial, genera una fuerza magnética que no varía en el tiempo. Si tenemos una carga q a una velocidad  Para determinar el valor de ese campo magnético, Jean Baptiste Biot en 1820,[13] dedujo una relación para corrientes estacionarias, ahora conocida como ley de Biot-Savart: Donde  (2) Además en la magnetostática existe una ley comparable a la de Gauss en la electrostática, la ley de Ampère. Esta ley nos dice que la circulación en un campo magnético es igual a la densidad de corriente que exista en una superficie cerrada: Cabe indicar que esta ley de Gauss es una generalización de la ley de Biot-Savart. Además que las fórmulas expresadas aquí son para cargas en el vacío, para más información consúltese los artículos principales. Véanse también: Ley de Ampère, Corriente eléctrica, Campo magnético, Ley de Biot-Savart y Momento magnético dipolar. Electrodinámica clásica Artículo principal: Electrodinámica La electrodinámica es el estudio de los fenómenos asociados a los cuerpos cargados en movimiento y a los campos eléctricos y magnéticos variables. Dado que una carga en movimiento produce un campo magnético, la electrodinámica se refiere a efectos tales como el magnetismo, la radiación electromagnética, y la inducción electromagnética, incluyendo las aplicaciones prácticas, tales como el generador eléctrico y el motor eléctrico. Esta área de la electrodinámica, conocida como electrodinámica clásica, fue sistemáticamente explicada por James Clerk Maxwell, y las ecuaciones de Maxwell describen los fenómenos de esta área con gran generalidad. Una novedad desarrollada más reciente es la electrodinámica cuántica, que incorpora las leyes de la teoría cuántica a fin de explicar la interacción de la radiación electromagnética con la materia. Paul Dirac, Heisenberg y Wolfgang Pauli fueron pioneros en la formulación de la electrodinámica cuántica. La electrodinámica es inherentemente relativista y da unas correcciones que se introducen en la descripción de los movimientos de las partículas cargadas cuando sus velocidades se acercan a la velocidad de la luz. Se aplica a los fenómenos involucrados con aceleradores de partículas y con tubos electrónicos funcionando a altas tensiones y corrientes. En las secciones anteriores se han descrito campos eléctricos y magnéticos que no variaban con el tiempo. Pero los físicos a finales del siglo XIX descubrieron que ambos campos estaban ligados y así un campo eléctrico en movimiento, una corriente eléctrica que varíe, genera un campo magnético y un campo magnético de por sí implica la presencia de un campo eléctrico. Entonces, lo primero que debemos definir es la fuerza que tendría una partícula cargada que se mueva en un campo magnético y así llegamos a la unión de las dos fuerzas anteriores, lo que hoy conocemos como la fuerza de Lorentz: (3) Entre 1890 y 1900 Liénard y Wiechert calcularon el campo electromagnético asociado a cargas en movimiento arbitrario, resultado que se conoce hoy como potenciales de Liénard-Wiechert. Por otro lado, para generar una corriente eléctrica en un circuito cerrado debe existir una diferencia de potencial entre dos puntos del circuito, a esta diferencia de potencial se la conoce como fuerza electromotriz o «fem». Esta fuerza electromotriz es proporcional a la rapidez con que el flujo magnético varía en el tiempo, esta ley fue encontrada por Michael Faraday y es la interpretación de la inducción electromagnética, así un campo magnético que varía en el tiempo induce a un campo eléctrico, a una fuerza electromotriz. Matemáticamente se representa como: (4) El físico James Clerk Maxwell de 1861 relacionó las anteriormente citadas ecuaciones para la ley de Gauss ((1)), ley de Gauss para el campo magnético ((2)), ley de Faraday ((4)) e introdujo el concepto de una corriente de desplazamiento como una densidad de corriente efectiva para llegar a la ley de Ampère generalizada (5): (5) Las cuatro ecuaciones, tanto en su forma diferencial como en la integral aquí descritas, son fruto de la reformulación del trabajo de Maxwell realizada por Oliver Heaviside y Heinrich Rudolf Hertz. Pero el verdadero poder de estas ecuaciones, más la fuerza de Lorentz (3), se centra en que juntas son capaces de describir cualquier fenómeno electromagnético, además de las consecuencias físicas que posteriormente se describirán.[14]​ Esquema de una onda electromagnética. La genialidad del trabajo de Maxwell es que sus ecuaciones describen un campo eléctrico que va ligado inequívocamente a un campo magnético perpendicular a este y a la dirección de su propagación, este campo es ahora llamado campo electromagnético. Dichos campos podían ser derivados de un potencial escalar. La solución de las ecuaciones de Maxwell implicaba la existencia de una onda que se propagaba a la velocidad de la luz, con lo que además de unificar los fenómenos eléctricos y magnéticos la teoría formulada por Maxwell predecía con absoluta certeza los fenómenos ópticos.[15]​ Así la teoría predecía a una onda que, contraria a las ideas de la época, no necesitaba un medio de propagación; la onda electromagnética se podía propagar en el vacío debido a la generación mutua de los campos magnéticos y eléctricos. Esta onda a pesar de tener una velocidad constante, la velocidad de la luz c, puede tener diferente longitud de onda y consecuentemente dicha onda transporta energía. La radiación electromagnética recibe diferentes nombres al variar su longitud de onda, como rayos gamma, rayos X, espectro visible, etc.; pero en su conjunto recibe el nombre de espectro electromagnético. Espectro electromagnético. Véanse también: Fuerza de Lorentz, Fuerza electromotriz, Ley de Ampère, Ecuaciones de Maxwell y Campo electromagnético. Electrodinámica relativista Artículo principal: Tensor de campo electromagnético Clásicamente, al fijar un sistema de referencia, se puede descomponer los campos eléctricos y magnéticos del campo electromagnético. Pero, en la teoría de la relatividad especial, al tener a un observador con movimiento relativo respecto al sistema de referencia, este medirá efectos eléctricos y magnéticos diferentes de un mismo fenómeno electromagnético. El campo eléctrico y la inducción magnética a pesar de ser elementos vectoriales no se comportan como magnitudes físicas vectoriales, por el contrario la unión de ambos constituye otro ente físico llamado tensor y en este caso el tensor de campo electromagnético.[16]​ Así, la expresión para el campo electromagnético es: Esta representación se conoce como formulación covariante tetradimensional del electromagnetismo. Las expresiones covariantes para las ecuaciones de Maxwell (7) y la fuerza de Lorentz (6) se reducen a: (6) (7) u }}  u }=0} Dada la forma de las ecuaciones anteriores, si el dominio sobre el que se extiende el campo electromagnético es simplemente conexo el campo electromagnético puede expresarse como la derivada exterior de un cuadrivector llamado potencial vector, relacionado con los potenciales del electromagnetismo clásico de la siguiente manera: Donde: La relación entre el cuadrivector potencial y el tensor de campo electromanético resulta ser: El hecho de que la interacción electromagnética pueda representarse por un (cuadri)vector que define completamente el campo electromagnético es la razón por la que se afirma en el tratamiento moderno que la interacción electromagnética es un campo vectorial. En relatividad general el tratamiento del campo electromagnético en un espacio-tiempo curvo es similar al presentado aquí para el espacio-tiempo de Minkowski, solo que las derivadas parciales respecto a las coordenadas deben substituirse por derivadas covariantes. Electrodinámica cuántica Diagrama de Feynman mostrando la fuerza electromagnética entre dos electrones por medio del intercambio de un fotón virtual. Artículo principal: Electrodinámica cuántica Posteriormente a la revolución cuántica de inicios del siglo XX, los físicos se vieron forzados a buscar una teoría cuántica de la interacción electromagnética. El trabajo de Einstein con el efecto fotoeléctrico y la posterior formulación de la mecánica cuántica sugerían que la interacción electromagnética se producía mediante el intercambio de partículas elementales llamadas fotones. La nueva formulación cuántica lograda en la década de 1940 describe la interacción entre los bosones, o partículas portadoras de la interacción, y las otras partículas portadoras de materia (los fermiones).[17]​ La electrodinámica cuántica es principalmente una teoría cuántica de campos renormalizada. Su desarrollo fue obra de Sinitiro Tomonaga, Julian Schwinger, Richard Feynman y Freeman Dyson alrededor de los años 1947 a 1949.[18] En la electrodinámica cuántica, la interacción entre partículas viene descrita por un lagrangiano que posee simetría local, concretamente simetría de gauge. Para la electrodinámica cuántica, el campo de gauge donde los fermiones interactúan es el campo electromagnético, descrito en esta teoría como los estados de bosones (fotones, en este caso) portadores de la interacción.[18]​ Matemáticamente, el lagrangiano para la interacción entre fermiones mediante intercambio de fotones viene dado por: u }\,} Donde el significado de los términos son: Véanse también: Teoría cuántica de campos, Ecuación de Dirac y Modelo estándar."
ksampletext_wikipedia_phys_teoriadelarelatividad: str = "Teoría de la relatividad. La teoría de la relatividad incluye tanto a la teoría de la relatividad especial como la de la relatividad general, formuladas principalmente por Albert Einstein a principios del siglo XX, que pretendían resolver la incompatibilidad existente entre la mecánica newtoniana y el electromagnetismo.[3]​ La teoría de la relatividad especial, publicada en 1905, trata de la física del movimiento de los cuerpos en ausencia de fuerzas gravitatorias, en el que se hacían compatibles las ecuaciones de Maxwell del electromagnetismo con una reformulación de las leyes del movimiento. En la teoría de la relatividad especial, Einstein, Lorentz y Minkowski, entre otros, unificaron los conceptos de espacio y tiempo, en un tramado tetradimensional al que se le denominó espacio-tiempo. La relatividad especial fue una teoría revolucionaria para su época, con la que el tiempo absoluto de Newton quedó relegado y conceptos como la invariabilidad en la velocidad de la luz, la dilatación del tiempo, la contracción de la longitud y la equivalencia entre masa y energía fueron introducidos. Además, con las formulaciones de la relatividad especial, las leyes de la Física son invariantes en todos los sistemas de referencia inerciales; como consecuencia matemática, se encuentra como límite superior de velocidad a la de la luz y se elimina la causalidad determinista que tenía la física hasta entonces. Hay que indicar que las leyes del movimiento de Newton son un caso particular de esta teoría donde la masa, al viajar a velocidades muy pequeñas, no experimenta variación alguna en longitud ni se transforma en energía y al tiempo se le puede considerar absoluto. La teoría de la relatividad general, publicada en 1915, es una teoría de la gravedad que reemplaza a la gravedad newtoniana, aunque coincide numéricamente con ella para campos gravitatorios débiles y velocidades «pequeñas». La teoría general se reduce a la teoría especial en presencia de campos gravitatorios. La relatividad general estudia la interacción gravitatoria como una deformación en la geometría del espacio-tiempo. En esta teoría se introducen los conceptos de la curvatura del espacio-tiempo como la causa de la interacción gravitatoria, el principio de equivalencia que dice que para todos los observadores locales inerciales las leyes de la relatividad especial son invariantes y la introducción del movimiento de una partícula por líneas geodésicas. La relatividad general no es la única teoría que describe la atracción gravitatoria, pero es la que más datos relevantes comprobables ha encontrado. Anteriormente, a la interacción gravitatoria se la describía matemáticamente por medio de una distribución de masas, pero en esta teoría no solo la masa percibe esta interacción, sino también la energía, mediante la curvatura del espacio-tiempo y por eso se necesita otro lenguaje matemático para poder describirla, el cálculo tensorial. Muchos fenómenos, como la curvatura de la luz por acción de la gravedad y la desviación en la órbita de Mercurio, son perfectamente predichos por esta formulación. La relatividad general también abrió otro campo de investigación en la física, conocido como cosmología y es ampliamente utilizado en la astrofísica.[4]​ El 7 de marzo de 2010, la Academia Israelí de Ciencias exhibió públicamente los manuscritos originales de Einstein (redactados en 1905). El documento, que contiene 46 páginas de textos y fórmulas matemáticas escritas a mano, fue donado por Einstein a la Universidad Hebrea de Jerusalén en 1925 con motivo de su inauguración.[5]​[6]​[7]​ Conceptos principales Artículo principal: Anexo:Glosario de relatividad El supuesto básico de la teoría de la relatividad es que la localización de los sucesos físicos, tanto en el tiempo como en el espacio, son relativos al estado de movimiento del observador: así, la longitud de un objeto en movimiento o el instante en que algo sucede, a diferencia de lo que sucede en mecánica newtoniana, no son invariantes absolutos, y diferentes observadores en movimiento relativo entre sí diferirán respecto a ellos (las longitudes y los intervalos temporales, en relatividad son relativos y no absolutos). Relatividad especial Artículo principal: Teoría de la relatividad especial La teoría de la relatividad especial, también llamada teoría de la relatividad restringida, fue publicada por Albert Einstein en 1905 y describe la física del movimiento en el marco de un espacio-tiempo plano. Esta teoría describe correctamente el movimiento de los cuerpos incluso a grandes velocidades y sus interacciones electromagnéticas, se usa básicamente para estudiar sistemas de referencia inerciales (no es aplicable para problemas astrofísicos donde el campo gravitatorio desempeña un papel importante). Estos conceptos fueron presentados anteriormente por Poincaré y Lorentz, que son considerados como precursores de la teoría. Si bien la teoría resolvía un buen número de problemas del electromagnetismo y daba una explicación del experimento de Michelson y Morley, no proporciona una descripción relativista adecuada del campo gravitatorio. Tras la publicación del artículo de Einstein, la nueva teoría de la relatividad especial fue aceptada en unos pocos años por prácticamente la totalidad de los físicos y los matemáticos. De hecho, Poincaré o Lorentz habían estado muy cerca de llegar al mismo resultado que Einstein. La forma geométrica definitiva de la teoría se debe a Hermann Minkowski, antiguo profesor de Einstein en la Politécnica de Zúrich; acuñó el término «espacio-tiempo» (Raumzeit) y le dio la forma matemática adecuada.[nota 1] El espacio-tiempo de Minkowski es una variedad tetradimensional en la que se entrelazaban de una manera indisoluble las tres dimensiones espaciales y el tiempo. En este espacio-tiempo de Minkowski, el movimiento de una partícula se representa mediante su línea de universo (Weltlinie), una curva cuyos puntos vienen determinados por cuatro variables distintas: las tres dimensiones espaciales ( Relatividad general Esta sección es un extracto de Relatividad general.[editar] Representación artística de la explosión de la supernova SN 2006gy, situada a 238 millones de años luz. De ser válido el principio de acción a distancia, las perturbaciones de origen gravitatorio de este estallido nos afectarían inmediatamente y más tarde nos llegarían las de origen electromagnético, que se transmiten a la velocidad de la luz. Esquema bidimensional de la curvatura del espacio-tiempo (cuatro dimensiones) generada por una masa esférica. La teoría general de la relatividad o relatividad general es una teoría del campo gravitatorio y de los sistemas de referencia generales, publicada por Albert Einstein en 1915 y 1916. El nombre de la teoría se debe a que generaliza la llamada teoría especial de la relatividad y el principio de relatividad para un observador arbitrario. Los principios fundamentales introducidos en esta generalización son el principio de equivalencia, que describe la aceleración y la gravedad como aspectos distintos de la misma realidad, la noción de la curvatura del espacio-tiempo y el principio de covariancia generalizado. La teoría de la relatividad general propone que la propia geometría del espacio-tiempo se ve afectada por la presencia de materia, de lo cual resulta una teoría relativista del campo gravitatorio. De hecho la teoría de la relatividad general predice que el espacio-tiempo no será plano en presencia de materia y que la curvatura del espacio-tiempo será percibida como un campo gravitatorio. La intuición básica de Einstein fue postular que en un punto concreto no se puede distinguir experimentalmente entre un cuerpo acelerado uniformemente y un campo gravitatorio uniforme. La teoría general de la relatividad permitió también reformular el campo de la cosmología. Einstein expresó el propósito de la teoría de la relatividad general para aplicar plenamente el programa de Ernst Mach de la relativización de todos los efectos de inercia, incluso añadiendo la llamada constante cosmológica a sus ecuaciones de campo[8] para este propósito. Este punto de contacto real de la influencia de Ernst Mach fue claramente identificado en 1918, cuando Einstein distingue lo que él bautizó como el principio de Mach (los efectos inerciales se derivan de la interacción de los cuerpos) del principio de la relatividad general, que se interpreta ahora como el principio de covariancia general.[9]​ El matemático alemán David Hilbert escribió e hizo públicas las ecuaciones de la covariancia antes que Einstein, ello resultó en no pocas acusaciones de plagio contra Einstein, pero probablemente sea más porque es una teoría (o perspectiva) geométrica. La misma postula que la presencia de masa o energía «curva» el espacio-tiempo, y esta curvatura afecta la trayectoria de los cuerpos móviles e incluso la trayectoria de la luz. Formalismo de la teoría de la relatividad Representación de la línea de universo de una partícula. Como no es posible reproducir un espacio-tiempo de cuatro dimensiones, en la figura se representa solo la proyección sobre 2 dimensiones espaciales y una temporal. Partículas En la teoría de la relatividad una partícula puntual queda representada por un par  Campos Cuando se consideran campos o distribuciones continuas de masa, se necesita algún tipo de generalización para la noción de partícula. Un campo físico posee momentum y energía distribuidos en el espacio-tiempo, el concepto de cuadrimomento se generaliza mediante el llamado tensor de energía-impulso que representa la distribución en el espacio-tiempo tanto de energía como de momento lineal. A su vez un campo dependiendo de su naturaleza puede representarse por un escalar, un vector o un tensor. Por ejemplo el campo electromagnético se representa por un tensor de segundo orden totalmente antisimétrico o 2-forma. Si se conoce la variación de un campo o una distribución de materia, en el espacio y en el tiempo entonces existen procedimientos para construir su tensor de energía-impulso. Magnitudes físicas En relatividad, estas magnitudes físicas son representadas por vectores 4-dimensionales o bien por objetos matemáticos llamados tensores, que generalizan los vectores, definidos sobre un espacio de cuatro dimensiones. Matemáticamente estos 4-vectores y 4-tensores son elementos definidos del espacio vectorial tangente al espacio-tiempo (y los tensores se definen y se construyen a partir del fibrado tangente o cotangente de la variedad que representa el espacio-tiempo). Correspondencia entre E3[nota 2] y M4[nota 3]​ Espacio tridimensional euclídeo Espacio-tiempo de Minkowski Punto Suceso Longitud Intervalo Velocidad Cuadrivelocidad Momentum Cuadrimomentum Igualmente además de cuadrivectores, se definen cuadritensores (tensores ordinarios definidos sobre el fibrado tangente del espacio-tiempo concebido como variedad lorentziana). La curvatura del espacio-tiempo se representa por un 4-tensor (tensor de cuarto orden), mientras que la energía y el momento de un medio continuo o el campo electromagnético se representan mediante 2-tensores (simétrico el tensor de energía-impulso, antisimétrico el de campo electromagnético). Los cuadrivectores son, de hecho, 1-tensores, en esta terminología. En este contexto se dice que una magnitud es un invariante relativista si tiene el mismo valor para todos los observadores, obviamente todos los invariantes relativistas son escalares (0-tensores), frecuentemente formados por la contracción de magnitudes tensoriales. El intervalo relativista El intervalo relativista puede definirse en cualquier espacio-tiempo, sea este plano como en la relatividad especial, o curvo como en relatividad general. Sin embargo, por simplicidad, discutiremos inicialmente el concepto de intervalo para el caso de un espacio-tiempo plano. El tensor métrico del espacio-tiempo plano de Minkowski se designa con la letra  El intervalo, la distancia tetradimensional, se representa mediante la expresión  Reproducción de un cono de luz, en el que se representan dos dimensiones espaciales y una temporal (eje de ordenadas). El observador se sitúa en el origen, mientras que el futuro y el pasado absolutos vienen representados por las partes inferior y superior del eje temporal. El plano correspondiente a t = 0 se denomina plano de simultaneidad o hipersuperficie de presente (también llamado «diagrama de Minkowski»). Los sucesos situados dentro de los conos están vinculados al observador por intervalos temporales. Los que se sitúan fuera, por intervalos espaciales. Los intervalos pueden ser clasificados en tres categorías: Intervalos espaciales (cuando  Los intervalos nulos pueden ser representados en forma de cono de luz, popularizados por el celebérrimo libro de Stephen Hawking, Breve Historia del Tiempo. Sea un observador situado en el origen, el futuro absoluto (los sucesos que serán percibidos por el individuo) se despliega en la parte superior del eje de ordenadas, el pasado absoluto (los sucesos que ya han sido percibidos por el individuo) en la parte inferior, y el presente percibido por el observador en el punto 0. Los sucesos que están fuera del cono de luz no nos afectan, y por lo tanto se dice de ellos que están situados en zonas del espacio-tiempo que no tienen relación de causalidad con la nuestra. Imaginemos, por un momento, que en la galaxia Andrómeda, situada a 2.5 millones de años luz de nosotros, sucedió un cataclismo cósmico hace 100 000 años. Dado que, primero: la luz de Andrómeda tarda 2 millones de años en llegar hasta nosotros y segundo: nada puede viajar a una velocidad superior a la de los fotones, es evidente, que no tenemos manera de enterarnos de lo que sucedió en dicha Galaxia hace tan solo 100 000 años. Se dice, por lo tanto, que el intervalo existente entre dicha hipotética catástrofe cósmica y nosotros, observadores del presente, es un intervalo espacial ( Imagen de la galaxia Andrómeda, tomada por el telescopio Spitzer, tal como era hace 2.5 millones de años (por estar situada a 2.5 millones de años luz). Los sucesos acaecidos 1 000 000 de años atrás se observarán desde la Tierra dentro de un millón y medio de años. Se dice, por tanto, que entre tales eventos y nosotros existe un intervalo espacial. Análisis El único problema con esta hipótesis, es que al entrar en un agujero negro, se anula el espacio-tiempo, y como ya sabemos, algo que contenga algún volumen o masa, debe tener como mínimo un espacio donde ubicarse, el tiempo en ese caso, no tiene mayor importancia, pero el espacio juega un rol muy importante en la ubicación de volúmenes, por lo que esto resulta muy improbable, pero no imposible para la tecnología. Podemos escoger otro episodio histórico todavía más ilustrativo: El de la estrella de Belén, tal y como fue interpretada por Johannes Kepler. Este astrónomo alemán consideraba que dicha estrella se identificaba con una supernova que tuvo lugar el año 5 a. C., cuya luz fue observada por los astrónomos chinos contemporáneos, y que vino precedida en los años anteriores por varias conjunciones planetarias en la constelación de Piscis. Esa supernova probablemente estalló miles de años atrás, pero su luz no llegó a la Tierra sino hasta el año 5 a. C. De ahí que el intervalo existente entre dicho evento y las observaciones de los astrónomos egipcios y megalíticos (que tuvieron lugar varios siglos antes de Cristo) sea un intervalo espacial, pues la radiación de la supernova nunca pudo llegarles. Por el contrario, la explosión de la supernova por un lado, y las observaciones realizadas por los tres magos en Babilonia y por los astrónomos chinos en el año 5 a. C. por el otro, están unidas entre sí por un intervalo temporal, ya que la luz sí pudo alcanzar a dichos observadores. El tiempo propio y el intervalo se relacionan mediante la siguiente equivalencia: Esta invarianza se expresa a través de la llamada geometría hiperbólica: La ecuación del intervalo  Cuadrivelocidad, aceleración y cuadrimomentum Artículos principales: Cuadrivelocidad y Cuadrimomento. En el espacio-tiempo de Minkowski, las propiedades cinemáticas de las partículas se representan fundamentalmente por tres magnitudes: La cuadrivelocidad (o tetravelocidad), la cuadriaceleración y el cuadrimomentum (o tetramomentum). La cuadrivelocidad es un cuadrivector tangente a la línea de universo de la partícula, relacionada con la velocidad coordenada de un cuerpo medida por un observador en reposo cualquiera, esta velocidad coordenada se define con la expresión newtoniana  La velocidad coordenada de un cuerpo con masa depende caprichosamente del sistema de referencia que escojamos, mientras que la cuadrivelocidad propia es una magnitud que se transforma de acuerdo con el principio de covariancia y tiene un valor siempre constante equivalente al intervalo dividido entre el tiempo propio ( La cuadriaceleración puede ser definida como la derivada temporal de la cuadrivelocidad ( Junto con los principios de invarianza del intervalo y la cuadrivelocidad, juega un papel fundamental la ley de conservación del cuadrimomentum. Es aplicable aquí la definición newtoniana del momentum  Como tanto la velocidad de la luz como el cuadrimomentum son magnitudes conservadas, también lo es su producto, al que se le da el nombre de energía conservada  Componentes  Magnitud del cuadrimomentum  Magnitud en cuerpos con masa  Magnitud en fotones (masa = 0)  Energía  Energía en cuerpos con masa (cuerpos en reposo, p=0)  Energía en fotones (masa en reposo = 0)  La aparición de la Relatividad Especial puso fin a la secular disputa que mantenían en el seno de la mecánica clásica las escuelas de los mecanicistas y los energetistas. Los primeros sostenían, siguiendo a Descartes y Huygens, que la magnitud conservada en todo movimiento venía constituida por el momentum total del sistema, mientras que los energetistas —que tomaban por base los estudios de Leibniz— consideraban que la magnitud conservada venía conformada por la suma de dos cantidades: La fuerza viva, equivalente a la mitad de la masa multiplicada por la velocidad al cuadrado ( La mecánica newtoniana dio la razón a ambos postulados, afirmando que tanto el momentum como la energía son magnitudes conservadas en todo movimiento sometido a fuerzas conservativas. Sin embargo, la Relatividad Especial dio un paso más allá, por cuanto a partir de los trabajos de Einstein y Minkowski el momentum y la energía dejaron de ser considerados como entidades independientes y se les pasó a considerar como dos aspectos, dos facetas de una única magnitud conservada: el cuadrimomentum. Componentes y magnitud de los diferentes conceptos cinemáticos Concepto Componentes Expresión algebraica Partículas con masa Fotones Intervalo  ot =0}  Cuadrivelocidad  no definida Aceleración  (sistemas inerciales) ot =0} (sistemas no inerciales) Aceleración no definida Cuadrimomentum  El tensor de energía-impulso (Tab) Artículo principal: Tensor de energía-impulso Tensor de tensión-energía Tres son las ecuaciones fundamentales que en física newtoniana describen el fenómeno de la gravitación universal: la primera, afirma que la fuerza gravitatoria entre dos cuerpos es proporcional al producto de sus masas e inversamente proporcional al cuadrado de su distancia (1); la segunda, que el potencial gravitatorio ( Sin embargo, estas ecuaciones no son compatibles con la Relatividad Especial por dos razones: En primer lugar la masa no es una magnitud absoluta, sino que su medición deriva en resultados diferentes dependiendo de la velocidad relativa del observador. De ahí que la densidad de masa  En segundo lugar, si el concepto de espacio es relativo, también lo es la noción de densidad. Es evidente que la contracción del espacio producida por el incremento de la velocidad de un observador, impide la existencia de densidades que permanezcan invariables ante las transformaciones de Lorentz. Por todo ello, resulta necesario prescindir del término  O lo que es lo mismo: El componente  donde  Además, si los componentes del tensor se miden por un observador en reposo relativo respecto al fluido, entonces, el tensor métrico viene constituido simplemente por la métrica de Minkowski: diag diag Puesto que además la tetravelocidad del fluido respecto al observador en reposo es: como consecuencia de ello, los coeficientes del tensor de tensión-energía son los siguientes: Parte de la materia que cae en el disco de acreción de un agujero negro es expulsada a gran velocidad en forma de chorros. En supuestos como este, los efectos gravitomagnéticos pueden llegar a alcanzar cierta importancia. Donde  Podemos, a partir del tensor de tensión-energía, calcular cuánta masa contiene un determinado volumen del fluido: Retomando la definición de este tensor expuesta unas líneas más arriba, se puede definir al coeficiente  Del mismo modo, es posible deducir matemáticamente a partir del tensor de tensión-energía la definición newtoniana de presión, introduciendo en la mentada ecuación cualquier par de índices que sean diferentes de cero: La hipersuperficie  Finalmente, derivamos parcialmente ambos miembros de la ecuación respecto al tiempo, y teniendo en cuenta que la fuerza no es más que la tasa de incremento temporal del momentum obtenemos el resultado siguiente: Que contiene la definición newtoniana de la presión como fuerza ejercida por unidad de superficie. El tensor electromagnético (Fab) Artículo principal: Tensor de campo electromagnético Las ecuaciones deducidas por el físico escocés James Clerk Maxwell demostraron que electricidad y magnetismo no son más que dos manifestaciones de un mismo fenómeno físico: el campo electromagnético. Ahora bien, para describir las propiedades de este campo los físicos de finales del siglo XIX debían utilizar dos vectores diferentes, los correspondientes los campos eléctrico y magnético. Fue la llegada de la relatividad especial la que permitió describir las propiedades del electromagnetismo con un solo objeto geométrico, el vector cuadripotencial, cuyo componente temporal se correspondía con el potencial eléctrico, mientras que sus componentes espaciales eran los mismos que los del potencial magnético. De este modo, el campo eléctrico puede ser entendido como la suma del gradiente del potencial eléctrico más la derivada temporal del potencial magnético: y el campo magnético, como el rotacional del potencial magnético: abla  imes A} Las propiedades del campo electromagnético pueden también expresarse utilizando un tensor de segundo orden denominado tensor de Faraday y que se obtiene diferenciando exteriormente al vector cuadripotencial  La fuerza de Lorentz puede deducirse a partir de la siguiente expresión: Donde "

ksampletext_wikipedia_chem_valenciaquimica: str = "Valencia (química). La valencia es el número de electrones que le faltan o debe ceder un elemento químico para completar su último nivel de energía. Estos electrones son los que pone en juego durante una reacción química o para establecer un enlace químico con otro elemento. Hay elementos con más de una valencia, por ello fue reemplazado este concepto con el de números de oxidación que finalmente representa lo mismo. A través del siglo XX, el concepto de valencia ha evolucionado en una amplia gama de aproximaciones para describir el enlace químico, incluyendo la estructura de Lewis (1916), la teoría del enlace de valencia (1927), la teoría de los orbitales moleculares (1928), la teoría de repulsión de pares electrónicos de la capa de valencia (1958) y todos los métodos avanzados de química cuántica. En química, históricamente se ha considerado la valencia de un elemento como su capacidad de combinarse con otros elementos, para formar compuestos moleculares. Diferentes autores utilizan distintas definiciones de valencia, lo que crea un debate sobre cuál es la correcta. Por ejemplo, algunos autores confunden número de coordinación con valencia o estado de oxidación con valencia, a pesar de que esos tres términos son diferentes entre sí. Historia La etimología de la palabra «valencia» proviene de 1543, significando molde, del latín valentía poder, capacidad, y el significado químico refiriéndose al «poder combinante de un elemento» está registrado desde 1884, del alemán Valenz.[1]​ En 1890, William Higgins publicó bocetos sobre lo que él llamó combinaciones de partículas últimas, que esbozaban el concepto de enlaces de valencia.[2] Si, por ejemplo, de acuerdo a Higgins, la fuerza entre la partícula última de oxígeno y la partícula última de nitrógeno era 6, luego la fuerza del enlace debería ser dividida acordemente, y de modo similar para las otras combinaciones de partículas últimas: estas son las de la tabla periódica. Combinaciones de partículas últimas de William Higgins (1789). Sin embargo, el origen no exacto de la teoría de las valencias químicas puede ser rastreado a una publicación de Edward Frankland, en la que combinó las viejas teorías de los radicales libres y «teoría de tipos» con conceptos sobre afinidad química para mostrar que ciertos elementos tienen la tendencia a combinarse con otros elementos para formar compuestos conteniendo tres equivalentes del átomo unido, por ejemplo, en los grupos de tres átomos (vg. NO3, NH3, NI3, etc.) o cinco, por ejemplo en los grupos de cinco átomos (vg. N2O5, NH4O, P2O5, etc.) Es en este modo, según Franklin, que sus afinidades están mejor satisfechas. Siguiendo estos ejemplos y postulados, Franklin declaró cuán obvio esto es que:[3]​ Una tendencia o ley prevalece (aquí), y que, no importa qué puedan ser los caracteres de los átomos que se unen, el poder combinante de los elementos atrayentes, si me puedo permitir el término, se satisface siempre por el mismo número de estos átomos. Descripción La capacidad combinatoria o afinidad de un átomo de un elemento dado viene determinada por el número de átomos de hidrógeno con los que se combina. En el metano, el carbono tiene una valencia de 4; en el amoníaco, el nitrógeno tiene una valencia de 3; en el agua, el oxígeno tiene una valencia de 2; y en el cloruro de hidrógeno, el cloro tiene una valencia de 1. El cloro, al tener una valencia de uno, puede sustituir al hidrógeno. El fósforo tiene una valencia de 5 en el pentacloruro de fósforo, PCl 5. Los diagramas de valencia de un compuesto representan la conectividad de los elementos, con líneas dibujadas entre dos elementos, a veces llamadas enlaces, que representan una valencia saturada para cada elemento.[2] Las dos tablas siguientes muestran algunos ejemplos de diferentes compuestos, sus diagramas de valencia y las valencias para cada elemento del compuesto. Compuesto H Hidrógeno CH Metano C Propano C Propileno C Acetileno Diagrama      Valencias  Hidrógeno: 1 Carbono: 4 Hidrógeno: 1 Carbono: 4 Hidrogeno: 1 Carbono: 4 Hidrógeno: 1 Carbono: 4 Hidrógeno: 1 Compuesto NH Amoníaco NaCN Cianuro de sodio PSCl Thiophosphoryl chloride H Ácido sulfhídrico H 2SO Ácido sulfúrico H Dithionic acid Cl Óxido perclórico XeO Tetraóxido de xenón Diagrama         Valencias  Nitrógeno: 3 Hidrógeno: 1 Sodio: 1 Carbono: 4 Nitrógeno: 3 Fósforo: 5 Azufre: 2 Cloro: 1 Azufre: 2 Hidrógeno: 1 Azufre: 6 Oxígeno: 2 Hidrógeno: 1 Azufre: 6 Oxígeno: 2 Hidrógeno: 1 Cloro: 7 Oxígeno: 2 Xenón: 8 Oxígeno: 2 Definiciones modernas La valencia es definida por la IUPAC como:[4]​ El número máximo de átomos univalentes (originalmente átomos de hidrógeno o de cloro) que pueden combinarse con un átomo del elemento considerado, o con un fragmento, o por el que puede sustituirse un átomo de este elemento. Una descripción moderna alternativa es: El número de átomos de hidrógeno que pueden combinarse con un elemento en un hidruro binario o el doble del número de átomos de oxígeno que se combinan con un elemento en su óxido u óxidos. Esta definición difiere de la definición de la IUPAC, ya que se puede decir que un elemento tiene más de una valencia. Una definición moderna muy similar dada en un artículo reciente define la valencia de un átomo particular en una molécula como el número de electrones que un átomo utiliza en la unión, con dos fórmulas equivalentes para calcular la valencia:[5]​ valencia = número de electrones en la capa de valencia del átomo libre - número de electrones no enlazantes del átomo en la molécula valencia = número de enlaces + carga formal. Sin embargo esta definición de valencia es incorrecta. Por esta definición, el átomo de nitrógeno en el ion de amonio [NH4]+ es pentavalente, y en el ion de amida [NH2]- es monovalente, que obviamente es falso, porque el átomo de nitrógeno en los iones de amonio y amida es trivalente. Por lo tanto, esta definición es engañosa porque puede dar resultados falsos. Desarrollo histórico La etimología de la palabra valencia se remonta a 1425, con el significado de extracto, preparado, del latín valentia fuerza, capacidad, del anterior valor valía, valor, y el significado químico referido al poder combinatorio de un elemento se registra a partir de 1884, del alemán Valenz.[6]​ William Higgins' combinaciones de partículas últimas (1789) El concepto de valencia se desarrolló en la segunda mitad del siglo XIX y ayudó a explicar con éxito la estructura molecular de los compuestos inorgánicos y orgánicos.[7]​La búsqueda de las causas subyacentes de la valencia condujo a las teorías modernas del enlace químico, incluyendo el átomo cúbico (1902), estructura de Lewiss (1916), teoría del enlace de valencia (1927), orbitales molecularess (1928), teoría de repulsión de pares de electrones de la corteza de valencia (1958), y todos los métodos avanzados de la química cuántica. En 1789, William Higgins publicó opiniones sobre lo que denominó combinaciones de partículas últimas, que prefiguraron el concepto de enlaces de valencia.[8] Si, por ejemplo, según Higgins, la fuerza entre la partícula última de oxígeno y la partícula última de nitrógeno fuera 6, entonces la intensidad de la fuerza se dividiría en consecuencia, y lo mismo para las demás combinaciones de partículas últimas (véase la ilustración). Sin embargo, el origen exacto de la teoría de las valencias químicas se remonta a un trabajo de Edward Frankland de 1852, en el que combinó la antigua teoría de los radicales con ideas sobre la afinidad química para demostrar que ciertos elementos tienen tendencia a combinarse con otros elementos para formar compuestos que contienen 3, es decir, en los grupos de 3 átomos (por ejemplo, NO 3, NH 3, NI 3, etc.) o 5, es decir, en los grupos de 5 átomos (por ejemplo, NO 5, NH 4O, PO 5, etc.), equivalentes de los elementos unidos. Según él, ésta es la manera en que mejor se satisfacen sus afinidades, y siguiendo estos ejemplos y postulados, declara lo obvio que es que[9]​ Una tendencia o ley prevalece (aquí), y es que, cualesquiera que sean los caracteres de los átomos que se unen, el poder combinatorio del elemento que atrae, si se me permite el término, se satisface siempre con el mismo número de estos átomos. En 1857 August Kekulé propuso valencias fijas para muchos elementos, como 4 para el carbono, y las utilizó para proponer fórmulas estructurales para muchas moléculas de orgánica, que todavía se aceptan hoy en día. Lothar Meyer en su libro de 1864, Die modernen Theorien der Chemie, que contenía una primera versión de la tabla periódica con 28 elementos, clasificó por primera vez los elementos en seis familias según su valencia. Los trabajos sobre la organización de los elementos por peso atómico, hasta entonces se habían visto obstaculizados por el uso generalizado de peso equivalentes para los elementos, en lugar de pesos atómicos.[10]​ La mayoría de los químicos del siglo XIX definían la valencia de un elemento como el número de sus enlaces sin distinguir diferentes tipos de valencia o de enlace. Sin embargo, en 1893 Alfred Werner describió metal de transición complejos de coordinaciónes como [Co(NH 6]Cl 3, en los que distinguió valencias principales y subsidiarias (en alemán: 'Hauptvalenz' y 'Nebenvalenz'), correspondientes a los conceptos modernos de estado de oxidación y número de coordinación respectivamente. Para los elementos del grupo principal, en 1904 Richard Abegg consideró valencias positivas y negativas (estados de oxidación máximo y mínimo), y propuso la regla de Abegg según la cual su diferencia es a menudo 8. Electrones y valencia El modelo de Rutherford del átomo nuclear (1911) demostró que el exterior de un átomo está ocupado por electrones, lo que sugiere que los electrones son responsables de la interacción de los átomos y de la formación de enlaces químicos. En 1916, Gilbert N. Lewis explicó la valencia y el enlace químico en términos de una tendencia de los átomos (del grupo principal) a alcanzar una octeto estable de 8 electrones de valencia. Según Lewis, el enlace covalente conduce a octetos por la compartición de electrones, y el enlace iónico conduce a octetos por la transferencia de electrones de un átomo a otro. El término covalencia se atribuye a Irving Langmuir, quien afirmó en 1919 que el número de pares de electrones que un átomo dado comparte con los átomos adyacentes se denomina covalencia de ese átomo.[11] El prefijo co- significa juntos, de modo que un enlace covalente significa que los átomos comparten una valencia. Posteriormente, ahora es más común hablar de enlaces covalentes en lugar de valencia, que ha caído en desuso en trabajos de nivel superior a partir de los avances en la teoría del enlace químico, pero sigue siendo ampliamente utilizado en estudios elementales, donde proporciona una introducción heurística al tema. En la década de 1930, Linus Pauling propuso que también existen enlaces covalentes polares, que son intermedios entre los covalentes y los iónicos, y que el grado de carácter iónico depende de la diferencia de electronegatividad de los dos átomos enlazados. Pauling también consideró las moléculas hipervalentes, en las que los elementos del grupo principal tienen valencias aparentes superiores a la máxima de 4 permitida por la regla del octeto. Por ejemplo, en la molécula de hexafluoruro de azufre (SF 6), Pauling consideró que el azufre forma 6 enlaces verdaderos de dos electrones utilizando orbitales atómicos híbridos sp3d2, que combinan un orbital s, tres orbitales p y dos orbitales d. Sin embargo, más recientemente, cálculos cuántico-mecánicos sobre esta molécula y otras similares han demostrado que el papel de los orbitales d en el enlace es mínimo, y que la molécula SF 6 debería describirse como una molécula con 6 enlaces covalentes polares (en parte iónicos) formados por sólo cuatro orbitales en el azufre (un s y tres p) de acuerdo con la regla del octeto, junto con seis orbitales en los fluorinos.[12] Cálculos similares en moléculas de metales de transición muestran que el papel de los orbitales p es menor, de modo que un orbital s y cinco orbitales d en el metal son suficientes para describir el enlace.[13]​ Tipos de valencia Valencia positiva máxima: es el número positivo que refleja la máxima capacidad de combinación de un átomo. Este número coincide con el grupo de la tabla periódica de los elementos al cual pertenece. Por ejemplo, el cloro (Cl) pertenece al grupo 7, por lo que su valencia positiva máxima es 7. Valencia negativa solo para el grupo A no para el grupo B: es el número negativo que refleja la capacidad que tiene un átomo de combinarse con otro pero que esté actuando con valencia positiva. Este número negativo se puede determinar contando lo que le falta a la valencia positiva máxima para llegar a 8, pero con signo -. Por ejemplo: a la valencia máxima positiva del átomo de cloro es 7, por lo que le falta un electrón para cumplir el octeto, entonces su valencia negativa será -1. Vista general El concepto fue desarrollado a mediados del siglo XIX, en un intento por racionalizar la fórmula química de compuestos químicos diferentes. En 1919, Irving Langmuir, tomó prestado el término para explicar el modelo del átomo cúbico de Gilbert N. Lewis al enunciar que el número de pares de electrones que cualquier átomo dado comparte con el átomo adyacente es denominado la covalencia del átomo. El prefijo co- significa «junto», así que un enlace covalente significa que los átomos comparten valencia. De ahí, si un átomo, por ejemplo, tiene una valencia +1, significa que perdió un electrón, y otro con una valencia de -1, significa que tiene un electrón adicional. Luego, un enlace entre estos dos átomos resultaría porque se complementarían o compartirían sus tendencias en el balance de la valencia. Subsecuentemente, actualmente es más común hablar de enlace covalente en vez de valencia, que ha caído en desuso del nivel más alto de trabajo, con los avances en la teoría del enlace químico, pero aún es usado ampliamente en estudios elementales donde provee una introducción heurística a la materia. Definición del número de enlaces Se creía originalmente que el número de enlaces formados por un elemento dado era una propiedad química fija y, en efecto, en muchos casos, es una buena aproximación. Por ejemplo, en muchos de sus compuestos, el carbono forma cuatro enlaces, el oxígeno dos y el hidrógeno uno. Sin embargo, pronto se hizo evidente que, para muchos elementos, la valencia podría variar entre compuestos diferentes. Uno de los primeros ejemplos en ser identificado era el fósforo, que algunas veces se comporta como si tuviera una valencia de tres, y otras como si tuviera una valencia de cinco. Un método para resolver este problema consiste en especificar la valencia para cada compuesto individual: aunque elimina mucho de la generalidad del concepto, esto ha dado origen a la idea de número de oxidación (usado en la nomenclatura Stock y a la notación lambda en la nomenclatura IUPAC de química inorgánica). Definición de IUPAC La Unión Internacional de Química Pura y Aplicada (IUPAC) ha hecho algunos intentos de llegar a una definición desambigua de valencia. La versión actual, adoptada en 1994, es la siguiente:[14]​ La valencia es el máximo número de átomos univalentes (originalmente átomos de hidrógeno o cloro) que pueden combinarse con un átomo del elemento en consideración, o con un fragmento, o para el cual un átomo de este elemento puede ser sustituido. Esta definición reimpone una valencia única para cada elemento a expensas de despreciar, en muchos casos, una gran parte de su química. La mención del hidrógeno y el cloro es por razones históricas, aunque ambos en la práctica forman compuestos principalmente en los que sus átomos forman un enlace simple. Las excepciones en el caso del hidrógeno incluyen el ion bifluoruro, [HF2]−, y los diversos hidruros de boro tales como el diborano: estos son ejemplos de enlace de tres centros. El cloro forma un número de fluoruro—ClF, ClF3 y ClF5—y su valencia, de acuerdo a la definición de la IUPAC, es cinco. El flúor es el elemento para el que el mayor número de átomos se combinan con átomos de otros elementos: es univalente en todos sus compuestos, excepto en el ion [H2F]+. En efecto, la definición IUPAC sólo puede ser resuelta al fijar las valencias del hidrógeno y el flúor como uno, convención que ha sido seguida acá. Valencias de los elementos Artículo principal: Anexo:Estados de oxidación de los elementos Las valencias de la mayoría de los elementos se basan en el fluoruro más alto conocido.["
ksampletext_wikipedia_chem_quimicaorganica: str = "Química orgánica. La química orgánica es la rama de la química que estudia una clase numerosa de moléculas, que, en su mayoría contienen carbono formando enlaces covalentes: carbono-carbono o carbono-hidrógeno y otros heteroátomos, también conocidos como compuestos orgánicos. Debido a la omnipresencia del carbono en los compuestos que esta rama de la química estudia, esta disciplina también es llamada química del carbono.[1]​ Historia El trabajo de Friedrich Wöhler sobre la síntesis de la urea es considerado por muchos como el inicio de la química orgánica, y en particular de la síntesis orgánica. La química orgánica constituyó o se instituyó como disciplina en los años treinta. El desarrollo de nuevos métodos de análisis de las sustancias de origen animal y vegetal, basados en el empleo de disolventes, como el éter o el alcohol, permitió el aislamiento de un gran número de sustancias orgánicas que recibieron el nombre de principios inmediatos. La aparición de la química orgánica se asocia a menudo al descubrimiento, en 1828, por el químico alemán Friedrich Wöhler, de que la sustancia inorgánica cianato de amonio podía convertirse en urea, una sustancia orgánica que se encuentra en la orina de muchos animales. Antes de este descubrimiento, los químicos creían que para sintetizar sustancias orgánicas, era necesaria la intervención de lo que llamaban ‘la fuerza vital’, es decir, los organismos vivos. El experimento de Wöhler[2] rompió la barrera entre sustancias orgánicas e inorgánicas. De esta manera, los químicos modernos consideran compuestos orgánicos a aquellos que contienen carbono e hidrógeno, y otros elementos (que pueden ser uno o más), siendo los más comunes: oxígeno, nitrógeno, azufre y los halógenos. En 1856, sir William Henry Perkin, mientras trataba de estudiar la quinina, accidentalmente fabricó el primer colorante orgánico ahora conocido como malva de Perkin.[3]​ La diferencia entre la química orgánica y la química biológica,es que en la segunda las moléculas de ADN tienen una historia y, por ende, en su estructura nos hablan de su historia, del pasado en el que se han constituido, mientras que una molécula orgánica, creada hoy, es solo testigo de su presente, sin pasado y sin evolución histórica.[4]​ Cronología Artículo principal: Cronología de la Química orgánica 1675: Lémery clasifica los productos químicos naturales, según su origen en minerales, vegetales y animales 1784: Antoine Lavoisier demuestra que todos los productos vegetales y animales están formados básicamente por carbono e hidrógeno y, en menor proporción, nitrógeno, oxígeno y azufre 1807: Jöns Jacob Berzelius clasifica los productos químicos en: Orgánicos: los que proceden de organismos vivos. Inorgánicos: los que proceden de la materia inanimada. 1816: Michel Eugène Chevreul prepara distintos jabones a partir de diferentes fuentes de ácidos grasos y diversas bases, produciendo así distintas sales de ácidos grasos (o jabones), que no resultaron ser más que productos orgánicos nuevos derivados de productos naturales (grasas animales y vegetales). 1828: Friedrich Wöhler, a partir de sustancias inorgánicas y con técnicas normales de laboratorio, sintetizó la sustancia urea, la segunda sustancia orgánica obtenida artificialmente, luego del oxalato de amonio. Fórmula desarrollada urea 1856: Sir William Perkin sintetiza el primer colorante orgánico por accidente. 1865: August Kekulé propuso que los átomos de carbono que forman el benceno se unen formando cadenas cerradas o anillos. Primeros compendios La tarea de presentar la química orgánica de manera sistemática y global se realizó mediante una publicación surgida en Alemania, fundada por el químico Friedrich Konrad Beilstein (1838-1906). Su Handbuch der organischen Chemie (Manual de la química orgánica) comenzó a publicarse en Hamburgo en 1880 y consistió en dos volúmenes que recogían información de unos quince mil compuestos orgánicos conocidos. Cuando la Deutsche chemische Gesellschaft (Sociedad Alemana de Química) trató de elaborar la cuarta reedición, en la segunda década del siglo XX, la cifra de compuestos orgánicos se había multiplicado por diez. Treinta y siete volúmenes fueron necesarios para la edición básica, que aparecieron entre 1916 y 1937. Un suplemento de 27 volúmenes se publicó en 1938, recogiendo información aparecida entre 1910 y 1919. En la actualidad, se está editando el Fünftes Ergänzungswerk (quinta serie complementaria), que recoge la documentación publicada entre 1960 y 1979. Para ofrecer con más prontitud sus últimos trabajos, el Beilstein Institut ha creado el servicio Beilstein On line, que funciona desde 1988. Recientemente, se ha comenzado a editar periódicamente un CD-ROM, Beilstein Current Facts in Chemistry, que selecciona la información química procedente de importantes revistas. Actualmente, la citada información está disponible a través de internet. El alma de la química orgánica: el carbono Estructura tetraédrica del metano. La gran cantidad de compuestos orgánicos que existen tiene su explicación en las características del átomo de carbono, que tiene cuatro electrones en su capa de valencia: según la regla del octeto necesita ocho para completarla, por lo que forma cuatro enlaces (valencia = 4) con otros átomos. Esta especial configuración electrónica da lugar a una variedad de posibilidades de hibridación orbital del átomo de carbono (hibridación química). La molécula orgánica más sencilla que existe es el metano. En esta molécula, el carbono presenta hibridación sp3, con los átomos de hidrógeno formando un tetraedro. El carbono forma enlaces covalentes con facilidad para alcanzar una configuración estable, estos enlaces los forma con facilidad con otros carbonos, lo que permite formar frecuentemente cadenas abiertas (lineales o ramificadas) y cerradas (anillos). Clasificación de compuestos orgánicos La clasificación de los compuestos orgánicos puede realizarse de diversas maneras: atendiendo a su origen (natural o sintético), a su estructura (p. ej.: alifático o aromático), a su funcionalidad (p. ej.: alcoholes o cetonas), o a su peso molecular (p. ej.: monómeros o polímeros). Clasificación según su origen La clasificación de los compuestos orgánicos según el origen es de dos tipos: naturales o sintéticos. A menudo, los de origen natural se entiende que son los presentes en los seres vivos, pero no siempre es así, ya que algunas moléculas orgánicas también se sintetizan ex-vivo, es decir en ambientes inertes, como por ejemplo el ácido fórmico en el cometa Halle-Bopp. Natural In-vivo Los compuestos orgánicos presentes en los seres vivos o biosintetizados constituyen una gran familia de compuestos orgánicos. Su estudio tiene interés en medicina, farmacia, perfumería, cocina y muchos otros campos más. Carbohidratos Los carbohidratos están compuestos fundamentalmente de carbono (C), oxígeno (O) e hidrógeno (H). Son a menudo llamados azúcares, pero esta nomenclatura no es del todo correcta. Tienen una gran presencia en el reino vegetal (fructosa, celulosa, almidón, alginatos), pero también en el animal (glucógeno, glucosa). Se suelen clasificar según su grado de polimerización en: Monosacáridos (glucosa, fructosa, ribosa y desoxirribosa) Disacáridos (sacarosa, lactosa, maltosa) Trisacáridos (maltotriosa, rafinosa) Polisacáridos (alginatos, ácido algínico, celulosa, almidón, etc.) Lípidos Los lípidos son un conjunto de moléculas orgánicas, la mayoría biomoléculas, compuestas principalmente por carbono e hidrógeno y en menor medida oxígeno, aunque también pueden contener fósforo, azufre y nitrógeno. Tienen como característica principal el ser hidrófobas (insolubles en agua) y solubles en disolventes orgánicos como la bencina, el benceno y el cloroformo. En el uso coloquial, a los lípidos se les llama incorrectamente grasas, ya que las grasas son solo un tipo de lípidos procedentes de animales. Los lípidos cumplen funciones diversas en los organismos vivientes, entre ellas la de reserva energética (como los triglicéridos), la estructural (como los fosfolípidos de las bicapas) y la reguladora (como las hormonas esteroides). Proteínas fórmula química de un aminoácido. Las proteínas son polipéptidos, es decir están formados por la polimerización de péptidos, y estos por la unión de aminoácidos. Pueden considerarse así poliamidas naturales, ya que el enlace peptídico es análogo al enlace amida. Comprenden una familia muy importante de moléculas en los seres vivos, pero en especial en el reino animal. Por otra parte, son producto de la expresión de genes contenidos en el ADN. Algunos ejemplos de proteínas son el colágeno, las fibroínas, o la seda de araña. Ácidos nucleicos Los ácidos nucleicos son polímeros formados por la repetición de monómeros denominados nucleótidos, unidos mediante enlaces fosfodiéster. Se forman, así, largas cadenas; algunas moléculas de ácidos nucleicos llegan a alcanzar pesos moleculares gigantescos, con millones de nucleótidos encadenados. Están formados por la moléculas de carbono, hidrógeno, oxígeno, nitrógeno y fosfato. Los ácidos nucleicos almacenan la información genética de los organismos vivos y son los responsables de la transmisión hereditaria. Existen dos tipos básicos, el ADN y el ARN. Moléculas pequeñas Estructura de la testosterona. Una hormona, que se puede clasificar como molécula pequeña en el argot-químico-orgánico. Las moléculas pequeñas son compuestos orgánicos de peso molecular moderado (generalmente se consideran pequeñas aquellas con peso molecular menor a 1000 g/mol) y que aparecen en pequeñas cantidades en los seres vivos, pero no por ello su importancia es menor. A ellas pertenecen distintos grupos de hormonas como la testosterona, el estrógeno u otros grupos como los alcaloides. Las moléculas pequeñas tienen gran interés en la industria farmacéutica por su relevancia en el campo de la medicina. Ex-vivo Son compuestos orgánicos que han sido sintetizados sin la intervención de ningún ser vivo, en ambientes extracelulares y extravirales. Procesos geológicos Sello alemán de 1964 conmemorativo de la descripción de la estructura del benceno por Friedrich August Kekulé en 1865. El petróleo es una sustancia clasificada como mineral en la cual se presentan una gran cantidad de compuestos orgánicos. Muchos de ellos, como el benceno, son empleados por el hombre tal cual, pero muchos otros son tratados o derivados para conseguir una gran cantidad de compuestos orgánicos, como por ejemplo los monómeros para la síntesis de materiales poliméricos o plásticos. Procesos atmosféricos El sistema climático está constituido por la atmósfera, la hidrósfera, la biosfera, la geosfera y sus interacciones. Las variaciones en el equilibrio climático pueden generar diversos procesos como el calentamiento global, el efecto invernadero o la disminución de la capa de ozono. Procesos de síntesis planetaria En el año 2000 el ácido fórmico, un compuesto orgánico sencillo, también fue hallado en la cola del cometa Hale-Bopp.[5]​[6] Puesto que la síntesis orgánica de estas moléculas es inviable bajo las condiciones espaciales, este hallazgo parece sugerir que a la formación del sistema solar debió anteceder un periodo de calentamiento durante su colapso final.[6]​ Sintético Desde la síntesis de Wöhler de la urea un altísimo número de compuestos orgánicos han sido sintetizados químicamente para beneficio humano. Estos incluyen fármacos, desodorantes, perfumes, detergentes, jabones, fibras textiles sintéticas, materiales plásticos, polímeros en general, o colorantes orgánicos. Cadenas hidrocarbonadas sencillas Hidrocarburos El compuesto más simple es el metano, un átomo de carbono con cuatro de hidrógeno (valencia = 1), pero también puede darse la unión carbono-carbono, formando cadenas de distintos tipos, ya que pueden darse enlaces simples, dobles o triples. Cuando el resto de enlaces de estas cadenas son con hidrógeno, se habla de hidrocarburos, que pueden ser: Saturados: con enlaces covalentes simples, alcanos. Insaturados: con dobles enlaces covalentes (alquenos) o triples (alquinos). Hidrocarburos cíclicos: Hidrocarburos saturados con cadena cerrada, como el ciclohexano. Aromáticos: estructura cíclica. Radicales y ramificaciones de cadena Estructura de un hidrocarburo ramificado nombrado 5-butil-3,9-dimetil-undecano. Los radicales[7] son fragmentos de cadenas de carbonos que cuelgan de la cadena principal. Su nomenclatura se hace con la raíz correspondiente (en el caso de un carbono met-, dos carbonos et-, tres carbonos prop-, cuatro carbonos but-, cinco carbonos pent-, seis carbonos hex-, y así sucesivamente) y el sufijo -il. Además, se indica con un número, colocado delante, la posición que ocupan. El compuesto más simple que se puede hacer con radicales es el 2-metilpropano. En caso de que haya más de un radical, se nombrarán por orden alfabético de las raíces. Por ejemplo, el 2-etil, 5-metil, 8-butil, 10-docoseno. Clasificación según los grupos funcionales Los compuestos orgánicos también pueden contener otros elementos, también otros grupos de átomos además del carbono e hidrógeno, llamados grupos funcionales. Un ejemplo es el grupo hidroxilo, que forma los alcoholes: un átomo de oxígeno enlazado a uno de hidrógeno (-OH), al que le queda una valencia libre. Asimismo también existen funciones alqueno (dobles enlaces), éteres, ésteres, aldehídos, cetonas, carboxílicos, carbamoilos,[8] azo, nitro o sulfóxido, entre otros.[9]​ Alquino Alquino Hidroxilo Hidroxilo Éter Éter Amina Amina Aldehído Aldehído Cetona Cetona Carboxilo Carboxilo Éster Éster Amida Amida Azo Azo Nitro Nitro Sulfóxido Sulfóxido Monómero de la celulosa. Oxigenados Son cadenas de carbonos con uno o varios átomos de oxígeno. Pueden ser: Alcoholes: Las propiedades físicas de un alcohol se basan principalmente en su estructura. El alcohol está compuesto por un alcano y agua. Contiene un grupo hidrofóbico (sin afinidad por el agua) del tipo de un alcano, y un grupo hidroxilo que es hidrófilo (con afinidad por el agua), similar al agua. De estas dos unidades estructurales, el grupo –OH da a los alcoholes sus propiedades físicas características, y el alquilo es el que las modifica, dependiendo de su tamaño y forma. El grupo –OH es muy polar y, lo que es más importante, es capaz de establecer puentes de hidrógeno: con sus moléculas compañeras o con otras moléculas neutras. Dependiendo de la cantidad de grupos -OH que forman parte del alcohol, el mismo puede ser clasificado como monohidroxilado (presencia de un hidroxilo) o polihidroxilado (dos o más grupos hidroxilos en la molécula). Aldehídos: Los aldehídos son compuestos orgánicos caracterizados por poseer el grupo funcional -CHO. Se denominan como los alcoholes correspondientes, cambiando la terminación -ol por -al: Es decir, el grupo carbonilo H-C=O está unido a un solo radical orgánico. Cetonas: Una cetona es un compuesto orgánico caracterizado por poseer un grupo funcional carbonilo unido a dos átomos de carbono, a diferencia de un aldehído, en donde el grupo carbonilo se encuentra unido al menos a un átomo de hidrógeno. Cuando el grupo funcional carbonilo es el de mayor relevancia en dicho compuesto orgánico, las cetonas se nombran agregando el sufijo -ona al hidrocarburo del cual provienen (hexano, hexanona; heptano, heptanona; etc). También se puede nombrar posponiendo cetona a los radicales a los cuales está unido (por ejemplo: metilfenil cetona). Cuando el grupo carbonilo no es el grupo prioritario, se utiliza el prefijo oxo- (ejemplo: 2-oxopropanal). El grupo funcional carbonilo consiste en un átomo de carbono unido con un doble enlace covalente a un átomo de oxígeno. El tener dos átomos de carbono unidos al grupo carbonilo, es lo que lo diferencia de los ácidos carboxílicos, aldehídos, ésteres. El doble enlace con el oxígeno, es lo que lo diferencia de los alcoholes y éteres. Las cetonas suelen ser menos reactivas que los aldehídos dado que los grupos alquílicos actúan como dadores de electrones por efecto inductivo. Ácidos carboxílicos: Los ácidos carboxílicos constituyen un grupo de compuestos que se caracterizan porque poseen un grupo funcional llamado grupo carboxilo o grupo carboxi (–COOH); se produce cuando coinciden sobre el mismo carbono un grupo hidroxilo (-OH) y carbonilo (C=O). Se puede representar como COOH o CO2H... Ésteres: Los ésteres presentan el grupo éster (-O-CO-) en su estructura. Algunos ejemplos de sustancias con este grupo incluyen el ácido acetil salicílico, componente de la aspirina, o algunos compuestos aromáticos como el acetato de isoamilo, con característico olor a plátano. Los aceites también son ésteres de ácidos grasos con glicerol. Éteres: Los éteres presentan el grupo éter(-O-) en su estructura. Suelen tener bajo punto de ebullición y son fácilmente descomponibles. Por ambos motivos, los éteres de baja masa molecular suelen ser peligrosos ya que sus vapores pueden ser explosivos. Nitrogenados Aminas: Las aminas son compuestos orgánicos caracterizados por la presencia del grupo amina (-N<). Las aminas pueden ser primarias (R-NH2), secundarias (R-NH-R) o terciarias (R-NR´-R). Las aminas suelen dar compuestos ligeramente amarillentos y con olores que recuerdan a pescado u orina. Amidas: Las amidas son compuestos orgánicos caracterizados por la presencia del grupo amida (-NH-CO-) en su estructura. Las proteínas o polipéptidos son poliamidas naturales formadas por enlaces peptídicos entre distintos aminoácidos. Isocianatos: Los isocianatos tienen el grupo isocianato (-N=C=O). Este grupo es muy electrófilo, reaccionando fácilmente con el agua para descomponerse mediante la transposición de Hofmann dar una amina y anhídrico carbónico, con los hidroxilos para dar uretanos, y con las aminas primarias o secundarias para dar ureas. Cíclicos Son compuestos que contienen un ciclo saturado. Un ejemplo de estos son los norbornanos, que en realidad son compuestos bicíclicos, los terpenos, u hormonas como el estrógeno, progesterona, testosterona u otras biomoléculas como el colesterol. Aromáticos El furano (C4H4O) es un ejemplo de compuesto aromático. Estructura tridimensional del furano mostrando la nube electrónica de electrones π. Los compuestos aromáticos tienen estructuras cíclicas insaturadas. El benceno es el claro ejemplo de un compuesto aromático, entre cuyos derivados están el tolueno, el fenol o el ácido benzoico. En general se define un compuesto aromático aquel que tiene anillos que cumplen la regla de Hückel, es decir que tienen 4n+2 electrones en orbitales π (n=0,1,2,...). A los compuestos orgánicos que tienen otro grupo distinto al carbono en sus cilos (normalmente N, O u S) se denominan compuestos aromáticos heterocíclicos. Así los compuestos aromáticos se suelen dividir en: Derivados del benceno: Policíclicos (antraceno, naftaleno, fenantreno, etc.), fenoles, aminas aromáticas, fulerenos, etc. Compuestos heterocíclicos: Piridina, furano, tiofeno, pirrol, porfirina, etc. Isómeros Isómeros del C6H12. Ya que el carbono puede enlazarse de diferentes maneras, una cadena puede tener diferentes configuraciones de enlace dando lugar a los llamados isómeros, moléculas tienen la misma fórmula química, pero distintas estructuras y propiedades. Existen distintos tipos de isomería: isomería de cadena, isomería de función, tautomería, estereoisomería, y estereoisomería configuracional. El ejemplo mostrado a la izquierda es un caso de isometría de cadena en la que el compuesto con fórmula C6H12 puede ser un ciclo (ciclohexano) o un alqueno lineal, el 1-hexeno. Un ejemplo de isomería de función sería el caso del propanal y la acetona, ambos con fórmula C3H6O. Compuestos orgánicos Artículo principal: Compuesto orgánico Los compuestos orgánicos pueden dividirse de manera muy general en: Compuestos alifáticos Compuestos aromáticos Compuestos heterocíclicos Compuestos organometálicos Polímeros Relación con la biología Una de las principales relaciones entre la química orgánica y la biología es el estudio de la síntesis y estructura de moléculas orgánicas de importancia en los procesos moleculares realizados por los organismos vivos, es decir en el metabolismo.[10] La bioquímica es el campo interdisciplinar científico que estudia los seres vivos, y ya que estos usan compuestos que contienen carbono, la química orgánica es imprescindible para comprender los procesos metabólicos. En términos biológicos la química orgánica es de gran importancia sobre todo en un contexto celular y esto lo podemos ejemplificar con moléculas como los carbohidratos, presentes desde la membrana plasmática así como en la estructura química del ADN, los lípidos quienes son la base principal de la membrana plasmática, las proteínas que ayudan a dar sostén a un organismo o sus funciones como enzimas y el ADN, molécula encargada de resguardar la información genética de los organismos vivos."
ksampletext_wikipedia_chem_molecula: str = "Molécula. En química, una molécula (del nuevo latín molecula, que es un diminutivo de la palabra moles, 'masa') es un grupo eléctricamente neutro y suficientemente estable de al menos dos átomos en una configuración definida, unidos por enlaces químicos fuertes covalentes.[4]​[5]​[6]​[7]​[8]​[9]​ En este estricto sentido, las moléculas se diferencian de los iones poliatómicos. En la química orgánica y la bioquímica, el término molécula se utiliza de manera menos estricta y se aplica también a los compuestos orgánicos (moléculas orgánicas) y en las biomoléculas. Antes, se definía la molécula de forma menos general y precisa, como la más pequeña parte de una sustancia que podía tener existencia independiente y estable conservando aún sus propiedades fisicoquímicas. De acuerdo con esta definición, podían existir moléculas monoatómicas. En la teoría cinética de los gases, el término molécula se aplica a cualquier partícula gaseosa con independencia de su composición. De acuerdo con esta definición, los átomos de un gas noble se considerarían moléculas aunque se componen de átomos no enlazados.[10]​ Una molécula puede consistir en varios átomos de un único elemento químico, como en el caso del oxígeno diatómico (O2),[11] o de diferentes elementos, como en el caso del agua (H2O).[12] Los átomos y complejos unidos por enlaces no covalentes como los enlaces de hidrógeno o los enlaces iónicos no se suelen considerar como moléculas individuales. Las moléculas como componentes de la materia son comunes en las sustancias orgánicas (y por tanto en la bioquímica). También conforman la mayor parte de los océanos y de la atmósfera. Sin embargo, un gran número de sustancias sólidas familiares, que incluyen la mayor parte de los minerales que componen la corteza, el manto y el núcleo de la Tierra, contienen muchos enlaces químicos, pero no están formados por moléculas. Además, ninguna molécula típica puede ser definida en los cristales iónicos (sales) o en cristales covalentes, aunque estén compuestos por celdas unitarias que se repiten, ya sea en un plano (como en el grafito) o en tres dimensiones (como en el diamante o el cloruro de sodio). Este sistema de repetir una estructura unitaria varias veces también es válida para la mayoría de las fases condensadas de la materia con enlaces metálicos, lo que significa que los metales sólidos tampoco están compuestos por moléculas. En el vidrio (sólidos que presentan un estado vítreo desordenado), los átomos también pueden estar unidos por enlaces químicos sin que se pueda identificar ningún tipo de molécula, pero tampoco existe la regularidad de la repetición de unidades que caracteriza a los cristales. Casi toda la química orgánica y buena parte de la química inorgánica se ocupan de la síntesis y reactividad de moléculas y compuestos moleculares. La química física y, especialmente, la química cuántica también estudian, cuantitativamente, en su caso, las propiedades y reactividad de las moléculas. La bioquímica está íntimamente relacionada con la biología molecular, ya que ambas estudian a los seres vivos a nivel molecular. El estudio de las interacciones específicas entre moléculas, incluyendo el reconocimiento molecular es el campo de estudio de la química supramolecular. Estas fuerzas explican las propiedades físicas como la solubilidad o el punto de ebullición de un compuesto molecular.[13]​ Las moléculas rara vez se encuentran sin interacción entre ellas, salvo en gases enrarecidos y en los gases nobles. Así, pueden encontrarse en redes cristalinas, como el caso de las moléculas de H2O en el hielo o con interacciones intensas, pero que cambian rápidamente de direccionalidad, como en el agua líquida. En orden creciente de intensidad, las fuerzas intermoleculares más relevantes son: las fuerzas de Van der Waals y los puentes de hidrógeno. La dinámica molecular es un método de simulación por computadora que utiliza estas fuerzas para tratar de explicar las propiedades de las moléculas. No se puede definir una molécula típica para sales ni para cristales covalentes, aunque estos a menudo se componen de células unitarias repetidas que se extienden en un plano, por ejemplo, el grafeno ; o tridimensionalmente, por ejemplo, el diamante, el cuarzo, o el cloruro de sodio. El tema de la estructura celular unitaria repetida también se aplica a la mayoría de los metales que son fases condensadas con enlaces metálicos. Por tanto, los metales sólidos no están hechos de moléculas. En los vidrios, que son sólidos que existen en un estado vítreo desordenado, los átomos se mantienen unidos por enlaces químicos sin presencia de ninguna molécula definible, ni ninguna de la regularidad de la estructura celular unitaria repetida que caracteriza a las sales, cristales covalentes y rieles. Ciencia molecular La ciencia de las moléculas se denomina química molecular o física molecular, dependiendo de si se centra en la química o en la física. La química molecular se ocupa de las leyes que rigen la interacción entre las moléculas que da lugar a la formación y ruptura de enlaces químicos, mientras que la física molecular se ocupa de las leyes que rigen su estructura y propiedades. En la práctica, sin embargo, esta distinción es imprecisa. En las ciencias moleculares, una molécula consiste en un sistema estable (estado ligado) compuesto por dos o más átomos. Los iones poliatómicos pueden considerarse a veces como moléculas cargadas eléctricamente. El término molécula inestable se utiliza para especies muy reactivas, es decir, conjuntos de corta duración (resonancias) de electrones y núcleos, como radicales, iones moleculares, moléculas de Rydberg, estados de transición, complejos de van der Waals, o sistemas de átomos en colisión como en el condensado de Bose-Einstein. Historia y etimología Artículo principal: Historia de la teoría molecular Según la Real Academia Española el vocablo «molécula» deriva del latín moles 'mole' o 'masa' y el sufijo diminutivo -ula 'masa pequeña'.[14]​ Molécula (1794) - «partícula extremadamente diminuta», del francés molécule (1678), del Nuevo Latín molecula, diminutivo del latín moles masa, barrera. Un significado vago al principio; la moda de la palabra (utilizada hasta finales del siglo XVIII solo en forma latina) se remonta a la filosofía de Descartes.[15]​[16]​ La definición de molécula ha ido evolucionando a medida que ha aumentado el conocimiento de la estructura de las moléculas. Las definiciones anteriores eran menos precisas, y definían las moléculas como las partículas más pequeñas de sustancia químicas puras que aún conservan su composición y sus propiedades químicas.[17] Esta definición a menudo se rompe ya que muchas sustancias en la experiencia ordinaria, como rocas, sales, y metales, se componen de grandes redes cristalinas de átomos de enlace químico o iones, pero no están hechas de moléculas discretas. Definición y sus límites De manera menos general y precisa, se ha definido molécula como la parte más pequeña de una sustancia química que conserva sus propiedades químicas, y a partir de la cual se puede reconstituir la sustancia sin reacciones químicas. De acuerdo con esta definición, que resulta razonablemente útil para aquellas sustancias puras constituidas por moléculas, podrían existir las moléculas monoatómicas de gases nobles, mientras que las redes cristalinas, sales, metales y la mayoría de vidrios quedarían en una situación confusa. Las moléculas lábiles pueden perder su consistencia en tiempos relativamente cortos, pero si el tiempo de vida medio es del orden de unas pocas vibraciones moleculares, estamos ante un estado de transición que no se puede considerar molécula. Actualmente, es posible el uso de láser pulsado para el estudio de la química de estos sistemas. Las entidades que comparten la definición de las moléculas, pero tienen carga eléctrica se denominan iones poliatómicos, iones moleculares o moléculas ion. Las sales compuestas por iones poliatómicos se clasifican habitualmente dentro de los materiales de base molecular o materiales moleculares. Ejemplo de molécula poliatómica: el agua Las moléculas están formadas por partículas. Una molécula viene a ser la porción de materia más pequeña que aún conserva las propiedades de la materia original. Las moléculas se encuentran fuertemente enlazadas con la finalidad de formar materia. Las moléculas están formadas por átomos unidos por medio de enlaces químicos. Una molécula es una unidad de sustancia que puede ser monoatómica o poliatómica. La unidad de todas las sustancias gaseosas es la molécula.[18]​ Tipos de moléculas Las moléculas se pueden clasificar en: Moléculas discretas: constituidas por un número bien definido de átomos, sean estos del mismo elemento (moléculas homonucleares, como el dinitrógeno o el fullereno) o de elementos distintos (moléculas heteronucleares, como el agua). Molécula de dinitrógeno, el gas que es el componente mayoritario del aire Molécula de dinitrógeno, el gas que es el componente mayoritario del aire Molécula de fullereno, tercera forma estable del carbono tras el diamante y el grafito Molécula de fullereno, tercera forma estable del carbono tras el diamante y el grafito Molécula de agua, «disolvente universal», de importancia fundamental en innumerables procesos bioquímicos e industriales Molécula de agua, «disolvente universal», de importancia fundamental en innumerables procesos bioquímicos e industriales Representación poliédrica del anión de Keggin, un polianión molecular Representación poliédrica del anión de Keggin, un polianión molecular Macromoléculas o polímeros: constituidas por la repetición de una unidad comparativamente simple —o un conjunto limitado de dichas unidades— y que alcanzan pesos moleculares relativamente altos. Representación de un fragmento de ADN, un polímero de importancia fundamental en la genética Representación de un fragmento de ADN, un polímero de importancia fundamental en la genética Enlace peptídico que une los péptidos para formar proteínas Enlace peptídico que une los péptidos para formar proteínas Representación de un fragmento lineal de polietileno, el plástico más usado Representación de un fragmento lineal de polietileno, el plástico más usado Primera generación de un dendrímero, un tipo especial de polímero que crece de forma fractal Primera generación de un dendrímero, un tipo especial de polímero que crece de forma fractal Enlaces Los átomos que forman las moléculas se mantienen juntos mediante enlaces covalentes o enlaces iónicos. Varios tipos de elementos no metálicos existen solo como moléculas en el medio ambiente. Por ejemplo, el hidrógeno solo existe como molécula de hidrógeno. Una molécula de un compuesto está formada por dos o más elementos.[19] Una molécula homonuclear está formada por dos o más átomos de un solo elemento. Mientras que algunas personas dicen que un cristal metálico puede considerarse una sola molécula gigante unida por enlaces metálicos,[20] otros señalan que los metales actúan de manera muy diferente a las moléculas.[21]​ Covalente Artículo principal: Enlace covalente Un enlace covalente que forma H2 (derecha) donde dos átomos de hidrógeno comparten los dos electrones. Un enlace covalente es un enlace químico que implica el intercambio de pares de electrones entre átomos. Estos pares de electrones se denominan pares compartidos o pares de enlace, y el equilibrio estable de fuerzas atractivas y repulsivas entre átomos, cuando comparten electrones, se denomina enlace covalente.[22]​ Iónico Artículo principal: Enlace iónico El sodio y el flúor experimentan una reacción redox para formar fluoruro de sodio. El sodio pierde su electrón externo para adoptar una configuración electrónica estable, y este electrón entra en el átomo de flúor en forma exotérmica. El enlace iónico es un tipo de enlace químico que implica la atracción electrostática entre iones con carga eléctrica opuesta y es la interacción principal que se produce en los compuestos iónicos. Los iones son átomos que han perdido uno o más electrones (denominados cationes) y átomos que han ganado uno o más electrones (denominados aniones).[23] Esta transferencia de electrones se denomina electrovalencia en contraste con la covalencia. En el caso más simple, el catión es un átomo de metal y el anión es un átomo no metálico, pero estos iones pueden ser de naturaleza más complicada, por ejemplo, iones moleculares como NH4+ o SO4 2−. A temperaturas y presiones normales, la unión iónica crea principalmente sólidos (u ocasionalmente líquidos) sin moléculas identificables separadas, pero la vaporización/sublimación de tales materiales produce pequeñas moléculas separadas donde los electrones aún se transfieren lo suficiente como para que los enlaces se consideren iónicos en lugar de covalentes. Descripción La estructura molecular puede ser descrita de diferentes formas. La fórmula molecular es útil para moléculas sencillas, como H2O para el agua o NH3 para el amoniaco. Contiene los símbolos de los elementos presentes en la molécula, así como su proporción indicada por los subíndices. Para moléculas más complejas, como las que se encuentran comúnmente en química orgánica, la fórmula química no es suficiente, y vale la pena usar una fórmula estructural o una fórmula esqueletal, las que indican gráficamente la disposición espacial de los distintos grupos funcionales. Cuando se quieren mostrar variadas propiedades moleculares, o se trata de sistemas muy complejos como proteínas, ADN o polímeros, se utilizan representaciones especiales, como los modelos tridimensionales (físicos o representados por ordenador). En proteínas, por ejemplo, cabe distinguir entre estructura primaria (orden de los aminoácidos), secundaria (primer plegamiento en hélices, hojas, giros…), terciaria (plegamiento de las estructuras tipo hélice/hoja/giro para dar glóbulos) y cuaternaria (organización espacial entre los diferentes glóbulos). Figura 1. Representaciones de la terpenoide, atisano, 3D (centro izquierda) y 2D (derecha). En el modelo 3D de la izquierda, los átomos de carbono están representados por esferas azules; las blancas representan a los átomos de hidrógeno y los cilindros representan los enlaces. El modelo es una representación de la superficies molecular, coloreada por áreas de carga eléctrica positiva (rojo) o negativa (azul). En el modelo 3D del centro, las esferas azul claro representan átomos de carbono, las blancas de hidrógeno y los cilindros entre los átomos son los enlaces simples. Moléculas en la teoría cuántica La mecánica clásica y el electromagnetismo clásico no podían explicar la existencia y estabilidad de las moléculas, ya que de acuerdo con sus ecuaciones una carga eléctrica acelerada emitiría radiación por lo que los electrones necesariamente perderían energía cinética por radiación hasta caer sobre el núcleo atómico. La mecánica cuántica proveyó el primer modelo cualitativamente correcto que además predecía la existencia de átomos estables y proporcionaba explicación cuantitativa muy aproximada para fenómenos empíricos como los espectros de emisión característicos de cada elemento químico. En mecánica cuántica una molécula o un ion poliatómico se describe como un sistema formado por  (1) definido sobre el espacio de funciones antisimetrizadas de cuadrado integrable  (2) donde el primer término representa la interacción de los electrones entre sí, el segundo la interacción de los electrones con los núcleos atómicos, y el tercero las interacciones de los núcleos entre sí. En una molécula neutra se tendrá obviamente que: Si  Aproximación de Born-Oppenheimer Resolver el problema de autovalores y autofunciones para el hamiltoniano cuántico dado por (1) es un problema matemático difícil, por lo que es común simplificarlo de alguna manera. Así dado que los núcleos atómicos son mucho más pesados que los electrones (entre 103 y 105 veces más) puede suponerse que los núcleos atómicos apenas se mueven comparados con los electrones, por lo que se considera que están congelados en posiciones fijas, con lo cual se puede aproximar el hamiltoniano (1) por la aproximación de Born-Oppenheimer dada por: (3) definido sobre el espacio de funciones  Teorema de Kato Los operadores  Tosio Kato La propiedad de ser autoadjunto implicará que las energías son cantidades reales, y el que sean acotados inferiormente implicará que existe un estado fundamental de mínima energía por debajo del cual los electrones no pueden decaer, y por tanto, las moléculas serán estables, ya que los electrones no pueden perder y perder energía como parecían predecir las ecuaciones del electromagnetismo clásico. Dos resultados matemáticos adicionales nos dicen como son las energías permitidas de los electrones dentro de una molécula:[24]​ Teorema HVZ para átomos y moléculas BO El espectro esencial  inf W. Hunziker, C. Van Winter y G. M. Zhislin Además dentro de la mecánica cuántica puede demostrarse que pueden existir iones positivos (cationes, con carga positiva comparable al núcleo atómico), mientras que no es igual de fácil tener iones negativos (aniones), el siguiente resultado matemático implica tiene que ver con la posibilidad de cationes y aniones."
ksampletext_wikipedia_chem_compuestoquimico: str = "Compuesto químico. Un compuesto químico es una sustancia formada por la combinación química de dos o más elementos de la tabla periódica.[1] Los compuestos son representados por una fórmula química. Por ejemplo, el agua (H2O) está constituida por dos átomos de hidrógeno y uno de oxígeno. Los elementos de un compuesto no se pueden dividir ni separar por procesos físicos (decantación, filtración, destilación), sino solo mediante procesos químicos. Los compuestos están formados por moléculas o iones con enlaces estables que no obedece a una selección humana arbitraria. Por lo tanto, no son mezclas o aleaciones como el bronce o el chocolate.[2]​[3] Un elemento químico unido a un elemento químico idéntico no es un compuesto químico, ya que solo está involucrado un elemento, no dos elementos diferentes. Hay cuatro tipos de compuestos, dependiendo de cómo se mantienen unidos los átomos constituyentes: Moléculas unidas por enlaces covalentes. Compuestos iónicos unidos por enlaces iónicos. Compuestos intermetálicos unidos por enlaces metálicos. Ciertos complejos que se mantienen unidos por enlaces covalentes coordinados. Muchos compuestos químicos tienen un identificador numérico único asignado por el Chemical Abstracts Service (CAS): su número CAS. Fórmula Artículo principal: Fórmula molecular En química inorgánica los compuestos se representan mediante fórmulas químicas.[4] Una fórmula química es una forma de expresar información sobre las proporciones de los átomos que constituyen un compuesto químico en particular, utilizando las abreviaturas normalizadas de los elementos químicos y subíndices para indicar el número de átomos involucrados. Por ejemplo, el agua se compone de dos átomos de hidrógeno unidos a uno de oxígeno átomo: la fórmula química es H2O. En el caso de compuestos no estequiométricos, las proporciones pueden ser reproducibles con respecto a su preparación y dar proporciones fijas de sus elementos componentes, pero proporciones que no son integrales [por ejemplo, para el hidruro de paladio, PdH x (0.02 <x <0.58 )].[5]​ El orden de los elementos en la fórmula de los compuestos inorgánicos comienza por la izquierda con el elemento menos electronegativo, hasta la derecha con el más electronegativo. Por ejemplo en el NaCl, el cloro que es más electronegativo que el sodio va en la parte derecha.[6] Para los compuestos orgánicos existen otras varias reglas y se utilizan fórmulas esqueletales o semidesarrolladas para su representación.[7]​ Definiciones Cualquier sustancia que consista en dos o más tipos diferentes de átomos (elementos químicos) en una proporción estequiométrica fija puede denominarse compuesto químico. El concepto se entiende mejor cuando se consideran sustancias químicas puras.[8]​[9]​[10] De la composición de proporciones fijas de dos o más tipos de átomos se desprende que los compuestos químicos se pueden convertir, mediante una reacción química, en compuestos o sustancias, cada uno con menos átomos. Los compuestos químicos tienen una estructura química única y definida que se mantiene unida en una disposición espacial concebida por enlaces químicos. Los compuestos químicos pueden ser compuestos moleculares, mantenidos juntos por enlaces covalentes, sales mantenidas entre sí por enlaces iónicos, compuestos intermetálicos mantenidos juntos por enlaces metálicos, o el subconjunto de complejos químicos que se mantienen unidos por enlaces covalentes coordinados .[11] Los elementos químicos puros generalmente no se consideran compuestos químicos, ya que no cumplen con el requisito de dos o más átomos, aunque a menudo consisten en moléculas compuestas de múltiples átomos (como en la molécula diatómica H2, o la molécula poliatómica S8, etc.)[11] Muchos compuestos químicos tienen un identificador numérico único asignado por el Chemical Abstracts Service (CAS): su número CAS.[12]​ Hay nomenclatura variable y a veces inconsistente para diferenciar sustancias, que incluyen ejemplos verdaderamente no estequiométricos de los compuestos químicos, que requieren que las proporciones sean fijas. Muchas sustancias químicas sólidas, por ejemplo muchos minerales de silicato, no tienen fórmulas simples que reflejen el enlace químico de los elementos entre sí en proporciones fijas; aun así, estas sustancias cristalinas a menudo se denominan compuestos no estequiométricos. Se puede argumentar que están relacionados con dichos productos, en lugar de ser compuestos químicos propiamente dichos, en la medida en que la variabilidad en sus composiciones a menudo se debe a la presencia de elementos extraños atrapados dentro de la estructura cristalina de un compuesto químico verdadero, o debido a perturbaciones en su estructura en relación con el compuesto conocido que surge debido a un exceso o déficit de los elementos constituyentes en lugares de su estructura; tales sustancias no estequiométricas forman la mayor parte de la corteza y el manto de la Tierra. Otros compuestos considerados químicamente idénticos pueden tener cantidades variables de isótopos pesados o ligeros de los elementos constituyentes, lo que cambia ligeramente la proporción en masa de los elementos. Clasificación Se pueden clasificar de acuerdo al tipo de enlace químico o a su composición. Atendiendo al tipo de enlace químico, se pueden dividir en: Moléculas Compuestos iónicos Compuestos intermetálicos Complejos Por su composición, se pueden dividir en dos grandes grupos:[13]​ Compuestos inorgánicos:[14]​ Óxidos básicos. También llamados óxidos metálicos, que están formados por un metal y oxígeno. Ejemplos: el óxido plúmbico, óxido de litio. Óxidos ácidos. También llamados óxidos no metálicos, formados por un no metal y oxígeno. Ejemplos: óxido hipocloroso, óxido selenioso. Hidruros, que pueden ser tanto metálicos como no metálicos. Están compuestos por un elemento e hidrógeno. Ejemplos: hidruro de aluminio, hidruro de sodio. Hidrácidos, son hidruros no metálicos que, cuando se disuelven en agua, adquieren carácter ácido. Por ejemplo, el ácido yodhídrico. Hidróxidos, compuestos formados por la reacción entre un óxido básico y el agua, que se caracterizan por presentar el grupo hidroxilo (OH). Por ejemplo, el hidróxido de sodio, o sosa cáustica. Oxácidos, compuestos obtenidos por la reacción de un óxido ácido y agua. Sus moléculas están formadas por hidrógeno, un no metal y oxígeno. Por ejemplo, ácido clórico. Sales binarias, compuestos formados por un hidrácido más un hidróxido. Por ejemplo, el cloruro de sodio. Oxisales, formadas por la reacción de un oxácido y un hidróxido, como por ejemplo el hipoclorito de sodio. Compuestos orgánicos:[15]​ Compuestos alifáticos, son compuestos orgánicos constituidos por carbono e hidrógeno cuyo carácter no es aromático. Compuestos aromáticos, es un compuesto orgánico cíclico conjugado que posee una mayor estabilidad debido a la deslocalización electrónica en enlaces π. Compuestos heterocíclicos, son compuestos orgánicos cíclicos en los que al menos uno de los componentes del ciclo es de un elemento diferente al carbono.[16]​ Compuestos organometálicos, es un compuesto en el que los átomos de carbono forman enlaces covalentes, es decir, comparten electrones, con un átomo metálico. Polímeros, son macromoléculas formadas por la unión de moléculas más pequeñas llamadas monómeros. Moléculas Artículo principal: Molécula Una molécula es un grupo eléctricamente neutro de dos o más átomos unidos por enlaces químicos.[17]​[18]​[19]​[20]​[21] Una molécula puede ser homonuclear, es decir, estar formada por átomos de un mismo elemento químico, como ocurre con dos átomos en la molécula de oxígeno (O2); o puede ser heteronuclear, es decir, un compuesto químico compuesto por más de un elemento, como el agua (dos átomos de hidrógeno y un átomo de oxígeno; H2O).[22] Los átomos y complejos unidos por enlaces no covalentes como los enlaces de hidrógeno no se suelen considerar como moléculas individuales. Compuestos iónicos Artículo principal: Compuesto iónico Un compuesto iónico es un compuesto químico compuesto de anion que se mantienen unidos por fuerzas electrostáticas denominadas enlace iónico. El compuesto es neutro en general, pero consta de iones cargados positivamente llamados cationes y iones cargados negativamente llamados aniones. Estos pueden ser iones simples como el sodio(Na+) y el cloruro (Cl−) en el cloruro de sodio, o especies poliatómicas como el amonio (NH+ 4) y carbonato (CO2− 3) en el carbonato de amonio.[23] Los iones individuales dentro de un compuesto iónico generalmente tienen múltiples vecinos más cercanos, por lo que no se consideran parte de moléculas, sino parte de una red tridimensional continua, generalmente en una estructura cristalina.[24]​ Los compuestos iónicos que contienen iones básicos hidróxido (OH−) u óxido(O2−) se clasifican como bases. Los compuestos iónicos sin estos iones también se conocen como sales y pueden formarse mediante reacciones ácido-base.[25] Los compuestos iónicos también se pueden producir a partir de sus iones constituyentes por evaporación de su disolvente, precipitación, congelación, una reacción en estado sólido o la reacción de transferencia de electrones de metales reactivos con no metales reactivos, como los gases halógenos. Los compuestos iónicos suelen tener altos puntos de fusión y ebullición, y son duros y quebradizos. Como sólidos, casi siempre son eléctricamente aislantes, pero cuando se funden o disuelven se vuelven altamente conductores, porque se movilizan los iones.[26]​ Compuestos intermetálicos Un compuesto intermetálico es un tipo de aleación metálica que forma un compuesto de estado sólido ordenado entre dos o más elementos metálicos. Los intermetálicos son generalmente duros y quebradizos, con buenas propiedades mecánicas a altas temperaturas.[27]​[28]​[29] Se pueden clasificar como compuestos intermetálicos estequiométricos o no estequiométricos.[27]​ Complejos químicos Artículo principal: Complejo (química) Un complejo de coordinación consiste en un átomo o ion central, que generalmente es metálico y se llama centro de coordinación, y una matriz circundante de moléculas o iones unidos, que a su vez se conocen como ligandos o agentes complejantes.[30]​[31]​[32] Muchos compuestos que contienen metales, especialmente los de metales de transición, son complejos de coordinación.[33] Un complejo de coordinación cuyo centro es un átomo metálico se denomina complejo metálico o elemento de bloque d.[34]​ Enlaces y fuerzas Los compuestos se mantienen unidos por medio de diferentes tipos de enlaces y fuerzas. Las diferencias entre los tipos de enlaces de los compuestos dependen del tipo de elemento presente en el compuesto. Las fuerzas de dispersión de London son las fuerzas más débiles entre las fuerzas intermoleculares. Son fuerzas de atracción temporales que se forman cuando los electrones en dos átomos adyacentes se colocan de manera que crean un dipolo temporal. Además, estas fuerzas son responsables de la condensación de sustancias no polares en líquidos y posterior congelación a un estado sólido dependiendo de la temperatura del ambiente.[35]​ Un enlace covalente, también conocido como enlace molecular, implica el intercambio de electrones entre dos átomos. Principalmente, este tipo de enlace se produce entre elementos que aparecen uno cerca del otro en la tabla periódica de elementos, aunque se observa entre algunos metales y no metales. Esto se debe al mecanismo de este tipo de enlace. Los elementos cercanos en la tabla periódica tienden a tener electronegatividades similares, lo que significa que tienen una afinidad similar por los electrones. Como ninguno de los elementos tiene una afinidad más fuerte para donar o ganar electrones, hace que los elementos compartan electrones de manera que ambos elementos tengan un octeto más estable. El enlace iónico se produce cuando los electrones de valencia se transfieren completamente entre los elementos. Al contrario que el covalente, este enlace químico crea dos iones de carga opuesta. Los metales en enlaces iónicos generalmente pierden sus electrones de valencia, convirtiéndose en cationes, cargados positivamente. El no metal ganará los electrones del metal, haciendo que el no metal sea un anión, es decir, cargado negativamente. Es decir, los enlaces iónicos se producen entre un donador de electrones, generalmente un metal, y un aceptor de electrones, que tiende a ser un no metal.[36]​ El enlace de hidrógeno se produce cuando un átomo de hidrógeno unido a un átomo electronegativo forma una conexión electrostática con otro átomo electronegativo a través de dipolos o cargas que interactúan.[37]​[38]​[39]​ Reacciones Un compuesto se puede convertir en una composición química diferente (productos) mediante la interacción con un segundo compuesto químico (reactivos) a través de una reacción química. En este proceso, los enlaces entre los átomos se rompen en ambos compuestos que interactúan, y luego los enlaces se reforman para obtener nuevas asociaciones entre los mismos átomos. Esquemáticamente, esta reacción podría describirse como AB + CD → AD + CB, donde A, B, C y D son cada uno átomos únicos; y AB, AD, CD y CB son cada uno compuestos ùnicos."

ksampletext_wikipedia_biol_celula: str = "Célula. La célula (del latín cellula, diminutivo de cella, ‘celda’) es la unidad morfológica y funcional de todo ser vivo. De hecho, la célula es el elemento de menor tamaño que puede considerarse vivo.[2] De este modo, puede clasificarse a los organismos vivos según el número de células que posean: si solo tienen una, se les denomina unicelulares (como pueden ser los protozoos o las bacterias, organismos microscópicos); si poseen más, se les llama pluricelulares. En estos últimos el número de células es variable: de unos pocos cientos, como en algunos nematodos, a cientos de billones (1014), como en el caso del ser humano. Las células suelen poseer un tamaño de 10 µm y una masa de 1 ng, si bien existen células mucho mayores. Célula animal La teoría celular, propuesta en 1838 para los vegetales y en 1839 para los animales,[3] por Matthias Jakob Schleiden y Theodor Schwann, postula que todos los organismos están compuestos por células, y que todas las células derivan de otras precedentes. De este modo, todas las funciones vitales emanan de la maquinaria celular y de la interacción entre células adyacentes; además, la tenencia de la información genética, base de la herencia, en su ADN permite la transmisión de aquella de generación en generación.[4]​ La aparición del primer organismo vivo sobre la Tierra suele asociarse al nacimiento de la primera célula. Si bien existen muchas hipótesis que especulan cómo ocurrió, usualmente se describe que el proceso se inició gracias a la transformación de moléculas inorgánicas en orgánicas bajo unas condiciones ambientales adecuadas; tras esto, dichas biomoléculas se asociaron dando lugar a entes complejos capaces de autorreplicarse. Existen posibles evidencias fósiles de estructuras celulares en rocas datadas en torno a 4 o 3,5 miles de millones de años (gigaaños o Ga).[5]​[6]​[nota 1] Se han encontrado evidencias muy fuertes de formas de vida unicelulares fosilizadas en microestructuras en rocas de la formación Strelley Pool, en Australia Occidental, con una antigüedad de 3,4 Ga.[cita requerida] Se trataría de los fósiles de células más antiguos encontrados hasta la fecha. Evidencias adicionales muestran que su metabolismo sería anaerobio y basado en el sulfuro.[7]​ Tipos celulares Existen dos grandes tipos celulares: Célula procariota, propia de los procariontes, que comprende las células de arqueas y bacterias. Célula eucariota, propia de los eucariontes, tales como la célula animal, célula vegetal, y las células de hongos y protistas. Historia y teoría celular La historia de la biología celular ha estado ligada al desarrollo tecnológico que pudiera sustentar su estudio. De este modo, el primer acercamiento a su morfología se inicia con la popularización del microscopio rudimentario de lentes compuestas en el siglo XVII, se suplementa con diversas técnicas histológicas para microscopía óptica en los siglos XIX y XX y alcanza un mayor nivel resolutivo mediante los estudios de microscopía electrónica, de fluorescencia y confocal, entre otros, ya en el siglo XX. El desarrollo de herramientas moleculares, basadas en el manejo de ácidos nucleicos y enzimas permitieron un análisis más exhaustivo a lo largo del siglo XX.[8]​ Descubrimiento Robert Hooke, quien acuñó el término «célula». Las primeras aproximaciones al estudio de la célula surgieron en el siglo XVII;[9] tras el desarrollo a finales del siglo XVI de los primeros microscopios.[10] Estos permitieron realizar numerosas observaciones, que condujeron en apenas doscientos años a un conocimiento morfológico relativamente aceptable. A continuación se enumera una breve cronología de tales descubrimientos: 1665: Robert Hooke publicó los resultados de sus observaciones sobre tejidos vegetales, como el corcho, realizadas con un microscopio de 50 aumentos construido por él mismo. Este investigador fue el primero que, al ver en esos tejidos unidades que se repetían a modo de celdillas de un panal, las bautizó como elementos de repetición, «células» (del latín cellulae, celdillas). Pero Hooke solo pudo observar células muertas por lo que no pudo describir las estructuras de su interior.[11]​ Década de 1670: Anton van Leeuwenhoek observó diversas células eucariotas (como protozoos y espermatozoides) y procariotas (bacterias). 1745: John Needham describió la presencia de «animálculos» o «infusorios»; se trataba de organismos unicelulares. Dibujo de la estructura del corcho observado por Robert Hooke bajo su microscopio y tal como aparece publicado en Micrographia. Década de 1830: Theodor Schwann estudió la célula animal; junto con Matthias Schleiden postularon que las células son las unidades elementales en la formación de las plantas y animales, y que son la base fundamental del proceso vital. 1831: Robert Brown describió el núcleo celular. 1839: Purkinje observó el citoplasma celular. 1857: Kölliker identificó las mitocondrias. 1858: Rudolf Virchow postuló que todas las células provienen de otras células. 1860: Pasteur realizó multitud de estudios sobre el metabolismo de levaduras y sobre la asepsia. 1880: August Weismann descubrió que las células actuales comparten similitud estructural y molecular con células de tiempos remotos. 1931: Ernst Ruska construyó el primer microscopio electrónico de transmisión en la Universidad de Berlín. Cuatro años más tarde, obtuvo una resolución óptica doble a la del microscopio óptico. 1981: Lynn Margulis publica su hipótesis sobre la endosimbiosis serial, que explica el origen de la célula eucariota.[12]​ Teoría celular Artículo principal: Teoría celular El concepto de célula como unidad anatómica y funcional de los organismos surgió entre los años 1830 y 1880, aunque fue en el siglo XVII cuando Robert Hooke describió por vez primera la existencia de las mismas, al observar en una preparación vegetal la presencia de una estructura organizada que derivaba de la arquitectura de las paredes celulares vegetales. En 1830 se disponía ya de microscopios con una óptica más avanzada, lo que permitió a investigadores como Theodor Schwann y Matthias Schleiden definir los postulados de la teoría celular, la cual afirma, entre otras cosas: Que la célula es una unidad morfológica de todo ser vivo: es decir, que en los seres vivos todo está formado por células o por sus productos de secreción. Este primer postulado sería completado por Rudolf Virchow con la afirmación Omnis cellula ex cellula, la cual indica que toda célula deriva de una célula precedente (biogénesis). En otras palabras, este postulado constituye la refutación de la teoría de generación espontánea o ex novo, que hipotetizaba la posibilidad de que se generara vida a partir de elementos inanimados.[13]​ Un tercer postulado de la teoría celular indica que las funciones vitales de los organismos ocurren dentro de las células, o en su entorno inmediato, y son controladas por sustancias que ellas secretan. Cada célula es un sistema abierto, que intercambia materia y energía con su medio. En una célula ocurren todas las funciones vitales, de manera que basta una sola de ellas para que haya un ser vivo (que será un individuo unicelular). Así pues, la célula es la unidad fisiológica de la vida. El cuarto postulado expresa que cada célula contiene toda la información hereditaria necesaria para el control de su propio ciclo y del desarrollo y el funcionamiento de un organismo de su especie, así como para la transmisión de esa información a la siguiente generación celular.[14]​ Definición Se define a la célula como la unidad morfológica y funcional de todo ser vivo. De hecho, la célula es el elemento de menor tamaño que puede considerarse vivo. Como tal posee una membrana de fosfolípidos con permeabilidad selectiva que mantiene un medio interno altamente ordenado y diferenciado del medio externo en cuanto a su composición, sujeta a control homeostático, la cual consiste en biomoléculas y algunos metales y electrolitos. La estructura se automantiene activamente mediante el metabolismo, asegurándose la coordinación de todos los elementos celulares y su perpetuación por replicación a través de un genoma codificado por ácidos nucleicos. La parte de la biología que se ocupa de ella es la citología. Características Las células, como sistemas termodinámicos complejos, poseen una serie de elementos estructurales y funcionales comunes que posibilitan su supervivencia; no obstante, los distintos tipos celulares presentan modificaciones de estas características comunes que permiten su especialización funcional y, por ello, la ganancia de complejidad.[15] De este modo, las células permanecen altamente organizadas a costa de incrementar la entropía del entorno, uno de los requisitos de la vida.[16]​ Características estructurales La existencia de polímeros como la celulosa en la pared vegetal permite sustentar la estructura celular empleando un armazón externo. Individualidad: Todas las células están rodeadas de una envoltura (que puede ser una bicapa lipídica desnuda, en células animales; una pared de polisacárido, en hongos y vegetales; una membrana externa y otros elementos que definen una pared compleja, en bacterias Gram negativas; una pared de peptidoglicano, en bacterias Gram positivas; o una pared de variada composición, en arqueas)[9] que las separa y comunica con el exterior, que controla los movimientos celulares y que mantiene el potencial de membrana. Contienen un medio interno acuoso, el citosol, que forma la mayor parte del volumen celular y en el que están inmersos los orgánulos celulares. Poseen material genético en forma de ADN, el material hereditario de los genes, que contiene las instrucciones para el funcionamiento celular, así como ARN, a fin de que el primero se exprese.[17]​ Tienen enzimas y otras proteínas, que sustentan, junto con otras biomoléculas, un metabolismo activo. Características funcionales Estructura tridimensional de una enzima, un tipo de proteínas implicadas en el metabolismo celular. Las células vivas son un sistema bioquímico complejo. Las características que permiten diferenciar las células de los sistemas químicos no vivos son: Nutrición. Las células toman sustancias del medio, las transforman de una forma a otra, liberan energía y eliminan productos de desecho, mediante el metabolismo. Crecimiento y multiplicación. Las células son capaces de dirigir su propia síntesis. A consecuencia de los procesos nutricionales, una célula crece y se divide, formando dos células, en una célula idéntica a la célula original, mediante la división celular. Diferenciación. Muchas células pueden sufrir cambios de forma o función en un proceso llamado diferenciación celular. Cuando una célula se diferencia, se forman algunas sustancias o estructuras que no estaban previamente formadas y otras que lo estaban dejan de formarse. La diferenciación es a menudo parte del ciclo celular en que las células forman estructuras especializadas relacionadas con la reproducción, la dispersión o la supervivencia. Señalización. Las células responden a estímulos químicos y físicos tanto del medio externo como de su interior y, en el caso de células móviles, hacia determinados estímulos ambientales o en dirección opuesta mediante un proceso que se denomina quimiotaxis. Además, frecuentemente las células pueden interaccionar o comunicar con otras células, generalmente por medio de señales o mensajeros químicos, como hormonas, neurotransmisores, factores de crecimiento... en seres pluricelulares en complicados procesos de comunicación celular y transducción de señales. Evolución. A diferencia de las estructuras inanimadas, los organismos unicelulares y pluricelulares evolucionan. Esto significa que hay cambios hereditarios (que ocurren a baja frecuencia en todas las células de modo regular) que pueden influir en la adaptación global de la célula o del organismo superior de modo positivo o negativo. El resultado de la evolución es la selección de aquellos organismos mejor adaptados a vivir en un medio particular. Las propiedades celulares no tienen por qué ser constantes a lo largo del desarrollo de un organismo: evidentemente, el patrón de expresión de los genes varía en respuesta a estímulos externos, además de factores endógenos.[18] Un aspecto importante a controlar es la pluripotencialidad, característica de algunas células que les permite dirigir su desarrollo hacia un abanico de posibles tipos celulares. En metazoos, la genética subyacente a la determinación del destino de una célula consiste en la expresión de determinados factores de transcripción específicos del linaje celular al cual va a pertenecer, así como a modificaciones epigenéticas. Además, la introducción de otro tipo de factores de transcripción mediante ingeniería genética en células somáticas basta para inducir la mencionada pluripotencialidad, luego este es uno de sus fundamentos moleculares.[19]​ Tamaño, forma y función Comparativa de tamaño entre neutrófilos, células sanguíneas eucariotas (de mayor tamaño), y bacterias Bacillus anthracis, procariotas (de menor tamaño, con forma de bastón). El tamaño y la forma de las células depende de sus elementos más periféricos (por ejemplo, la pared, si la hubiere) y de su andamiaje interno (es decir, el citoesqueleto). Además, la competencia por el espacio tisular provoca una morfología característica: por ejemplo, las células vegetales, poliédricas in vivo, tienden a ser esféricas in vitro.[20] Incluso pueden existir parámetros químicos sencillos, como los gradientes de concentración de una sal, que determinen la aparición de una forma compleja.[21]​ En cuanto al tamaño, la mayoría de las células son microscópicas, es decir, no son observables a simple vista. (un milímetro cúbico de sangre puede contener unos cinco millones de células),[15] A pesar de ser muy pequeñas, el tamaño de las células es extremadamente variable. La célula más pequeña observada, en condiciones normales, corresponde a Mycoplasma genitalium, de 0,2 μm, encontrándose cerca del límite teórico de 0,17 μm.[22] Existen bacterias con 1 y 2 μm de longitud. Las células humanas son muy variables: hematíes de 7 micras, hepatocitos con 20 micras, espermatozoides de 53 μm, óvulos de 150 μm e, incluso, algunas neuronas de en torno a un metro de longitud. En las células vegetales los granos de polen pueden llegar a medir de 200 a 300 μm. Respecto a las células de mayor tamaño; por ejemplo, los xenofióforos,[23] son foraminíferos unicelulares que han desarrollado un gran tamaño, los cuales alcanzar tamaños macroscópicos (Syringammina fragilissima alcanza los 20 cm de diámetro).[24]​ Para la viabilidad de la célula y su correcto funcionamiento siempre se debe tener en cuenta la relación superficie-volumen.[16] Puede aumentar considerablemente el volumen de la célula y no así su superficie de intercambio de membrana, lo que dificultaría el nivel y regulación de los intercambios de sustancias vitales para la célula. Respecto de su forma, las células presentan una gran variabilidad, e, incluso, algunas no la poseen bien definida o permanente. Pueden ser: fusiformes (forma de huso), estrelladas, prismáticas, aplanadas, elípticas, globosas o redondeadas, etc. Algunas tienen una pared rígida y otras no, lo que les permite deformar la membrana y emitir prolongaciones citoplasmáticas (pseudópodos) para desplazarse o conseguir alimento. Hay células libres que no muestran esas estructuras de desplazamiento, pero poseen cilios o flagelos, que son estructuras derivadas de un orgánulo celular (el centrosoma) que dota a estas células de movimiento.[2] De este modo, existen multitud de tipos celulares, relacionados con la función que desempeñan; por ejemplo: Células contráctiles que suelen ser alargadas, como los miocitos esqueléticos. Células con finas prolongaciones, como las neuronas que transmiten el impulso nervioso. Células con microvellosidades o con pliegues, como las del intestino para ampliar la superficie de contacto y de intercambio de sustancias. Células cúbicas, prismáticas o aplanadas como las epiteliales que recubren superficies como las losas de un pavimento. Estudio de las células Los biólogos utilizan diversos instrumentos para lograr el conocimiento de las células. Obtienen información de sus formas, tamaños y componentes, que les sirve para comprender además las funciones que en ellas se realizan. Desde las primeras observaciones de células, hace más de 300 años, hasta la época actual, las técnicas y los aparatos se han ido perfeccionando, originando una rama más de la biología: la microscopía. Dado el pequeño tamaño de la gran mayoría de las células, el uso del microscopio es de enorme valor en la investigación biológica. En la actualidad, los biólogos utilizan dos tipos básicos de microscopio: los ópticos y los electrónicos. Un microscopio óptico utiliza la luz visible para el estudio de muestras. Obteniedo imágenes aumentadas a partir de la desviación de la luz con lentes de cristal. Es utilizado para la observación de tejidos y células desde su invención en el siglo XVII. Los microscopios electrónicos son aquellos que utilizan electrones a alta velocidad para el análisis de muestras. Lo cual ofrece mayores capacidades de aumento que los de tipo óptico. Utilizándose en ramas como la medicina, y el estudio de materiales a nivel atómico. Además de usarse para la observación de células, virus y tejidos a nivel subcelular. Sin embargo, estos presentan limitaciones, debido a una cámara de vacío y a la preparación que requieren las muestras para ser analizadas, no pueden observarse células vivas.[25]​ Escaneo de microscopio electrónico de barrido muestra el SARS-CoV-2 emergiendo de la superficie de las células cultivadas en el laboratorio. Imagen del virus SARS-CoV-2, tomada con un microscopio electrónico de barrido. La célula procariota Artículo principal: Célula procariota Las células procariotas son pequeñas y menos complejas que las eucariotas. Contienen ribosomas, pero carecen de sistemas de endomembranas (esto es, orgánulos delimitados por membranas biológicas, como puede ser el núcleo celular). Por ello poseen el material genético en el citosol. Sin embargo, existen excepciones: algunas bacterias fotosintéticas poseen sistemas de membranas internos.[26] También en el filo Planctomycetota existen organismos como Pirellula que rodean su material genético mediante una membrana intracitoplasmática y Gemmata obscuriglobus que lo rodea con doble membrana. Esta última posee además otros compartimentos internos de membrana, posiblemente conectados con la membrana externa del nucleoide y con la membrana plasmática, que no está asociada a peptidoglucano.[27]​[28]​[29]​ Estudios realizados en 2017, demuestran otra particularidad de Gemmata: presenta estructuras similares al poro nuclear, en la membrana que rodea su cuerpo nuclear.[30]​ Por lo general podría decirse que los procariotas carecen de citoesqueleto. Sin embargo se ha observado que algunas bacterias, como Bacillus subtilis, poseen proteínas tales como MreB y mbl que actúan de un modo similar a la actina y son importantes en la morfología celular.[31] Fusinita van den Ent, en Nature, va más allá, afirmando que los citoesqueletos de actina y tubulina tienen origen procariótico.[32]​ De gran diversidad, los procariotas sustentan un metabolismo extraordinariamente complejo, en algunos casos exclusivo de ciertos taxa, como algunos grupos de bacterias, lo que incide en su versatilidad ecológica.[13] Los procariotas se clasifican, según Carl Woese, en arqueas y bacterias.[33]​ Arqueas Artículo principal: Arquea Estructura bioquímica de la membrana de arqueas (arriba) comparada con la de bacterias y eucariotas (en medio): nótese la presencia de enlaces éter (2) en sustitución de los tipo éster (6) en los fosfolípidos. Las arqueas poseen un diámetro celular comprendido entre 0,1 y 15 μm, aunque las formas filamentosas pueden ser mayores por agregación de células. Presentan multitud de formas distintas: incluso las hay descritas cuadradas y planas.[34] Algunas arqueas tienen flagelos y son móviles. Las arqueas, al igual que las bacterias, no tienen membranas internas que delimiten orgánulos. Como todos los organismos presentan ribosomas, pero a diferencia de los encontrados en las bacterias que son sensibles a ciertos agentes antimicrobianos, los de las arqueas, más cercanos a los eucariotas, no lo son. La membrana celular tiene una estructura similar a la de las demás células, pero su composición química es única, con enlaces tipo éter en sus lípidos.[35] Casi todas las arqueas poseen una pared celular (algunos Thermoplasma son la excepción) de composición característica, por ejemplo, no contienen peptidoglicano (mureína), propio de bacterias. No obstante, pueden clasificarse bajo la tinción de Gram, de vital importancia en la taxonomía de bacterias; sin embargo, en arqueas, poseedoras de una estructura de pared en absoluto común a la bacteriana, dicha tinción es aplicable, pero carece de valor taxonómico. El orden Methanobacteriales tiene una capa de pseudomureína, que provoca que dichas arqueas respondan como positivas a la tinción de Gram.[36]​[37]​[38]​ Como en casi todos los procariotas, las células de las arqueas carecen de núcleo, y presentan un solo cromosoma circular. Existen elementos extracromosómicos, tales como plásmidos. Sus genomas son de pequeño tamaño, sobre 2-4 millones de pares de bases. También es característica la presencia de ARN polimerasas de constitución compleja y un gran número de nucleótidos modificados en los ácidos ribonucleicos ribosomales. Por otra parte, su ADN se empaqueta en forma de nucleosomas, como en los eucariotas, gracias a proteínas semejantes a las histonas y algunos genes poseen intrones.[39] Pueden reproducirse por fisión binaria o múltiple, fragmentación o gemación. Bacterias Artículo principal: Bacteria Estructura de la célula procariota. Las bacterias son organismos relativamente sencillos, de dimensiones muy reducidas, de apenas unas micras en la mayoría de los casos. Como otros procariotas, carecen de un núcleo delimitado por una membrana, aunque presentan un nucleoide, una estructura elemental que contiene una gran molécula generalmente circular de ADN.[17]​[40] Carecen de núcleo celular y demás orgánulos delimitados por membranas biológicas.[41] En el citoplasma se pueden apreciar plásmidos, pequeñas moléculas circulares de ADN que coexisten con el nucleoide y que contienen genes: son comúnmente usados por las bacterias en la parasexualidad (reproducción sexual bacteriana). El citoplasma también contiene ribosomas y diversos tipos de gránulos. En algunos casos, puede haber estructuras compuestas por membranas, generalmente relacionadas con la fotosíntesis.[9]​ Poseen una membrana celular compuesta de lípidos, en forma de una bicapa y sobre ella se encuentra una cubierta en la que existe un polisacárido complejo denominado peptidoglicano; dependiendo de su estructura y subsecuente su respuesta a la tinción de Gram, se clasifica a las bacterias en Gram positivas y Gram negativas. El espacio comprendido entre la membrana celular y la pared celular (o la membrana externa, si esta existe) se denomina espacio periplásmico. Algunas bacterias presentan una cápsula. Otras son capaces de generar endosporas (estadios latentes capaces de resistir condiciones extremas) en algún momento de su ciclo vital. Entre las formaciones exteriores propias de la célula bacteriana destacan los flagelos (de estructura completamente distinta a la de los flagelos eucariotas) y los pili (estructuras de adherencia y relacionadas con la parasexualidad).[9]​ La mayoría de las bacterias disponen de un único cromosoma circular y suelen poseer elementos genéticos adicionales, como distintos tipos de plásmidos. Su reproducción, binaria y muy eficiente en el tiempo, permite la rápida expansión de sus poblaciones, generándose un gran número de células que son virtualmente clones, esto es, idénticas entre sí.[39]​ La célula eucariota Artículo principal: Célula eucariota Las células eucariotas son el exponente de la complejidad celular actual.[15] Presentan una estructura básica relativamente estable caracterizada por la presencia de distintos tipos de orgánulos intracitoplasmáticos especializados, entre los cuales destaca el núcleo, que alberga el material genético. Especialmente en los organismos pluricelulares, las células pueden alcanzar un alto grado de especialización. Dicha especialización o diferenciación es tal que, en algunos casos, compromete la propia viabilidad del tipo celular en aislamiento. Así, por ejemplo, las neuronas dependen para su supervivencia de las células gliales.[13]​ Por otro lado, la estructura de la célula varía dependiendo de la situación taxonómica del ser vivo: de este modo, las células vegetales difieren de las animales, así como de las de los hongos. Por ejemplo, las células animales carecen de pared celular, son muy variables, no tiene plastos, puede tener vacuolas, pero no son muy grandes y presentan centríolos (que son agregados de microtúbulos cilíndricos que contribuyen a la formación de los cilios y los flagelos y facilitan la división celular). Las células de los vegetales, por su lado, presentan una pared celular compuesta principalmente de celulosa, disponen de plastos como cloroplastos (orgánulo capaz de realizar la fotosíntesis), cromoplastos (orgánulos que acumulan pigmentos) o leucoplastos (orgánulos que acumulan el almidón fabricado en la fotosíntesis), poseen vacuolas de gran tamaño que acumulan sustancias de reserva o de desecho producidas por la célula y finalmente cuentan también con plasmodesmos, que son conexiones citoplasmáticas que permiten la circulación directa de las sustancias del citoplasma de una célula a otra, con continuidad de sus membranas plasmáticas.[42]​ Diagrama de una célula animal. (1. Nucléolo, 2. Núcleo, 3. Ribosoma, 4. Vesícula, 5. Retículo endoplasmático rugoso, 6. Aparato de Golgi, 7. Citoesqueleto (microtúbulos), 8. Retículo endoplasmático liso, 9. Mitocondria, 10. Vacuola, 11. Citoplasma, 12. Lisosoma, 13. Centríolos). Diagrama de una célula vegetal Compartimentos Las células son entes dinámicos, con un metabolismo celular interno de gran actividad cuya estructura es un flujo entre rutas anastomosadas. Un fenómeno observado en todos los tipos celulares es la compartimentalización, que consiste en una heterogeneidad que da lugar a entornos más o menos definidos (rodeados o no mediante membranas biológicas) en las cuales existe un microentorno que aglutina a los elementos implicados en una ruta biológica.[43] Esta compartimentalización alcanza su máximo exponente en las células eucariotas, las cuales están formadas por diferentes estructuras y orgánulos que desarrollan funciones específicas, lo que supone un método de especialización espacial y temporal.[2] No obstante, células más sencillas, como los procariotas, ya poseen especializaciones semejantes.[44]​ Membrana plasmática y superficie celular Artículo principal: Membrana plasmática La composición de la membrana plasmática varía entre células dependiendo de la función o del tejido en la que se encuentre, pero posee elementos comunes. Está compuesta por una doble capa de fosfolípidos, por proteínas unidas no covalentemente a esa bicapa, y por glúcidos unidos covalentemente a lípidos o proteínas. Generalmente, las moléculas más numerosas son las de lípidos; sin embargo, las proteínas, debido a su mayor masa molecular, representan aproximadamente el 50 % de la masa de la membrana.[43]​ Un modelo que explica el funcionamiento de la membrana plasmática es el modelo del mosaico fluido, de J. S. Singer y Garth Nicolson (1972), que desarrolla un concepto de unidad termodinámica basada en las interacciones hidrófobas entre moléculas y otro tipo de enlaces no covalentes.[45]​ Esquema de una membrana celular. Se observa la bicapa de fosfolípidos, las proteínas y otras moléculas asociadas que permiten las funciones inherentes a este orgánulo. Dicha estructura de membrana sustenta un complejo mecanismo de transporte, que posibilita un fluido intercambio de masa y energía entre el entorno intracelular y el externo.[43] Además, la posibilidad de transporte e interacción entre moléculas de células aledañas o de una célula con su entorno faculta a estas poder comunicarse químicamente, esto es, permite la señalización celular. Neurotransmisores, hormonas, mediadores químicos locales afectan a células concretas modificando el patrón de expresión génica mediante mecanismos de transducción de señal.[46]​ Sobre la bicapa lipídica, independientemente de la presencia o no de una pared celular, existe una matriz que puede variar, de poco conspicua, como en los epitelios, a muy extensa, como en el tejido conjuntivo. Dicha matriz, denominada glucocalix (glicocáliz), rica en líquido tisular, glucoproteínas, proteoglicanos y fibras, también interviene en la generación de estructuras y funciones emergentes, derivadas de las interacciones célula-célula.[13]​ Estructura y expresión génica Artículo principal: Expresión génica El ADN y sus distintos niveles de empaquetamiento. Las células eucariotas poseen su material genético en, generalmente, un solo núcleo celular, delimitado por una envoltura consistente en dos bicapas lipídicas atravesadas por numerosos poros nucleares y en continuidad con el retículo endoplasmático. En su interior, se encuentra el material genético, el ADN, observable, en las células en interfase, como cromatina de distribución heterogénea. A esta cromatina se encuentran asociadas multitud de proteínas, entre las cuales destacan las histonas, así como ARN, otro ácido nucleico.[47]​ Dicho material genético se encuentra inmerso en una actividad continua de regulación de la expresión génica; las ARN polimerasas transcriben ARN mensajero continuamente, que, exportado al citosol, es traducido a proteína, de acuerdo a las necesidades fisiológicas. Asimismo, dependiendo del momento del ciclo celular, dicho ADN puede entrar en replicación, como paso previo a la mitosis.[39] No obstante, las células eucarióticas poseen material genético extranuclear: concretamente, en mitocondrias y plastos, si los hubiere; estos orgánulos conservan una independencia genética parcial del genoma nuclear.[48]​[49]​ Síntesis y degradación de macromoléculas Dentro del citosol, esto es, la matriz acuosa que alberga a los orgánulos y demás estructuras celulares, se encuentran inmersos multitud de tipos de maquinaria de metabolismo celular: orgánulos, inclusiones, elementos del citoesqueleto, enzimas... De hecho, estas últimas corresponden al 20 % de las enzimas totales de la célula.[13]​ Estructura de los ribosomas; 1) subunidad mayor, 2) subunidad menor. Imagen de un núcleo, el retículo endoplasmático y el aparato de Golgi; 1, Núcleo. 2, Poro nuclear.3, Retículo endoplasmático rugoso (REr).4, Retículo endoplasmático liso (REl). 5, Ribosoma en el RE rugoso. 6, Proteínas siendo transportadas.7, Vesícula (transporte). 8, Aparato de Golgi. 9, Lado cis del aparato de Golgi.10, Lado trans del aparato de Golgi.11, Cisternas del aparato de Golgi. Ribosoma: Los ribosomas, visibles al microscopio electrónico como partículas esféricas,[50] son complejos supramoleculares encargados de ensamblar proteínas a partir de la información genética que les llega del ADN transcrita en forma de ARN mensajero. Elaborados en el núcleo, desempeñan su función de síntesis de proteínas en el citoplasma. Están formados por ARN ribosómico y por diversos tipos de proteínas. Estructuralmente, tienen dos subunidades. En las células, estos orgánulos aparecen en diferentes estados de disociación. Cuando están completos, pueden estar aislados o formando grupos (polisomas). También pueden aparecer asociados al retículo endoplasmático rugoso o a la envoltura nuclear.[39]​ Retículo endoplasmático: El retículo endoplasmático es orgánulo vesicular interconectado que forma cisternas, tubos aplanados y sáculos comunicados entre sí. Intervienen en funciones relacionadas con la síntesis proteica, glicosilación de proteínas, metabolismo de lípidos y algunos esteroides, detoxificación, así como el tráfico de vesículas. En células especializadas, como las miofibrillas o células musculares, se diferencia en el retículo sarcoplásmico, orgánulo decisivo para que se produzca la contracción muscular.[15]​ Aparato de Golgi: El aparato de Golgi es un orgánulo formado por apilamientos de sáculos denominados dictiosomas, si bien, como ente dinámico, estos pueden interpretarse como estructuras puntuales fruto de la coalescencia de vesículas.[51]​[52] Recibe las vesículas del retículo endoplasmático rugoso que han de seguir siendo procesadas. Dentro de las funciones que posee el aparato de Golgi se encuentran la glicosilación de proteínas, selección, destinación, glicosilación de lípidos y la síntesis de polisacáridos de la matriz extracelular. Posee tres compartimientos; uno proximal al retículo endoplasmático, denominado «compartimento cis», donde se produce la fosforilación de las manosas de las enzimas que han de dirigirse al lisosoma; el «compartimento intermedio», con abundantes manosidasas y N-acetil-glucosamina transferasas; y el «compartimento o red trans», el más distal, donde se transfieren residuos de galactosa y ácido siálico, y del que emergen las vesículas con los diversos destinos celulares.[13]​ Lisosoma: Los lisosomas son orgánulos que albergan multitud de enzimas hidrolíticas. De morfología muy variable, no se ha demostrado su existencia en células vegetales.[13] Una característica que agrupa a todos los lisosomas es la posesión de hidrolasas ácidas: proteasas, nucleasas, glucosidasas, lisozima, arilsulfatasas, lipasas, fosfolipasas y fosfatasas. Procede de la fusión de vesículas procedentes del aparato de Golgi, que, a su vez, se fusionan en un tipo de orgánulo denominado endosoma temprano, el cual, al acidificarse y ganar en enzimas hidrolíticos, pasa a convertirse en el lisosoma funcional. Sus funciones abarcan desde la degradación de macromoléculas endógenas o procedentes de la fagocitosis a la intervención en procesos de apoptosis.[53]​ La vacuola regula el estado de turgencia de la célula vegetal. Vacuola vegetal: Las vacuolas vegetales, numerosas y pequeñas en células meristemáticas y escasas y grandes en células diferenciadas, son orgánulos exclusivos de los representantes del mundo vegetal. Inmersas en el citosol, están delimitadas por el tonoplasto, una membrana lipídica. Sus funciones son: facilitar el intercambio con el medio externo, mantener la turgencia celular, la digestión celular y la acumulación de sustancias de reserva y subproductos del metabolismo.[42]​ Inclusión citoplasmática: Las inclusiones son acúmulos nunca delimitados por membrana de sustancias de diversa índole, tanto en células vegetales como animales. Típicamente se trata de sustancias de reserva que se conservan como acervo metabólico: almidón, glucógeno, triglicéridos, proteínas..., aunque también existen de pigmentos.[13]​ Conversión energética El metabolismo celular está basado en la transformación de unas sustancias químicas, denominadas metabolitos, en otras; dichas reacciones químicas transcurren catalizadas mediante enzimas. Si bien buena parte del metabolismo sucede en el citosol, como la glucólisis, existen procesos específicos de orgánulos.[46]​ Modelo de una mitocondria: 1. Membrana interna; 2. Membrana externa; 3. Cresta mitocondrial; 4. Matriz mitocondrial. Mitocondria: Las mitocondrias son orgánulos de aspecto, número y tamaño variable que intervienen en el ciclo de Krebs, fosforilación oxidativa y en la cadena de transporte de electrones de la respiración. Presentan una doble membrana, externa e interna, que dejan entre ellas un espacio perimitocondrial; la membrana interna, plegada en crestas hacia el interior de la matriz mitocondrial, posee una gran superficie. En su interior posee generalmente una sola molécula de ADN, el genoma mitocondrial, típicamente circular, así como ribosomas más semejantes a los bacterianos que a los eucariotas.[13] Según la teoría endosimbiótica, se asume que la primera protomitocondria era un tipo de proteobacteria.[54]​ Estructura de un cloroplasto. Cloroplasto: Los cloroplastos son los orgánulos celulares que en los organismos eucariotas fotosintéticos se ocupan de la fotosíntesis. Están limitados por una envoltura formada por dos membranas concéntricas y contienen vesículas, los tilacoides, donde se encuentran organizados los pigmentos y demás moléculas implicadas en la conversión de la energía lumínica en energía química. Además de esta función, los plastidios intervienen en el metabolismo intermedio, produciendo energía y poder reductor, sintetizando bases púricas y pirimidínicas, algunos aminoácidos y todos los ácidos grasos. Además, en su interior es común la acumulación de sustancias de reserva, como el almidón.[13] Se considera que poseen analogía con las cianobacterias.[55]​ Modelo de la estructura de un peroxisoma. Peroxisoma: Los peroxisomas son orgánulos muy comunes en forma de vesículas que contienen abundantes enzimas de tipo oxidasa y catalasa; de tan abundantes, es común que cristalicen en su interior. Estas enzimas cumplen funciones de detoxificación celular. Otras funciones de los peroxisomas son: las oxidaciones flavínicas generales, el catabolismo de las purinas, la beta-oxidación de los ácidos grasos, el ciclo del glioxilato, el metabolismo del ácido glicólico y la detoxificación en general.[13] Se forman de vesículas procedentes del retículo endoplasmático.[56]​ Citoesqueleto Artículo principal: Citoesqueleto Las células poseen un andamiaje que permite el mantenimiento de su forma y estructura, pero más aún, este es un sistema dinámico que interactúa con el resto de componentes celulares generando un alto grado de orden interno. Dicho andamiaje está formado por una serie de proteínas que se agrupan dando lugar a estructuras filamentosas que, mediante otras proteínas, interactúan entre ellas dando lugar a una especie de retículo. El mencionado andamiaje recibe el nombre de citoesqueleto, y sus elementos mayoritarios son: los microtúbulos, los microfilamentos y los filamentos intermedios.[2]​[nota 2]​[57]​[58]​ Microfilamentos: Los microfilamentos o filamentos de actina están formados por una proteína globular, la actina, que puede polimerizar dando lugar a estructuras filiformes. Dicha actina se expresa en todas las células del cuerpo y especialmente en las musculares, ya que está implicada en la contracción muscular, por interacción con la miosina. Además, posee lugares de unión a ATP, lo que dota a sus filamentos de polaridad.[59] Puede encontrarse en forma libre o polimerizarse en microfilamentos, que son esenciales para funciones celulares tan importantes como la movilidad y la contracción de la célula durante la división celular.[51]​ Citoesqueleto eucariota: microfilamentos en rojo, microtúbulos en verde y núcleo en azul. Microtúbulos: Los microtúbulos son estructuras tubulares de 25 nm de diámetro exterior y unos 12 nm de diámetro interior, con longitudes que varían entre unos pocos nanómetros a micrómetros, que se originan en los centros organizadores de microtúbulos y que se extienden a lo largo de todo el citoplasma. Se hallan en las células eucariotas y están formadas por la polimerización de un dímero de dos proteínas globulares, la alfa y la beta tubulina. Las tubulinas poseen capacidad de unir GTP.[2]​[51] Los microtúbulos intervienen en diversos procesos celulares que involucran desplazamiento de vesículas de secreción, movimiento de orgánulos, transporte intracelular de sustancias, así como en la división celular (mitosis y meiosis) y que, junto con los microfilamentos y los filamentos intermedios, forman el citoesqueleto. Además, constituyen la estructura interna de los cilios y los flagelos.[2]​[51]​ Filamentos intermedios: Los filamentos intermedios son componentes del citoesqueleto. Formados por agrupaciones de proteínas fibrosas, su nombre deriva de su diámetro, de 10 nm, menor que el de los microtúbulos, de 24 nm, pero mayor que el de los microfilamentos, de 7 nm. Son ubicuos en las células animales, y no existen en plantas ni hongos. Forman un grupo heterogéneo, clasificado en cinco familias: las queratinas, en células epiteliales; los neurofilamentos, en neuronas; los gliofilamentos, en células gliales; la desmina, en músculo liso y estriado; y la vimentina, en células derivadas del mesénquima.[13]​ Micrografía al microscopio electrónico de barrido mostrando la superficie de células ciliadas del epitelio de los bronquiolos. Centríolos: Los centríolos son una pareja de estructuras que forman parte del citoesqueleto de células animales. Semejantes a cilindros huecos, están rodeados de un material proteico denso llamado material pericentriolar; todos ellos forman el centrosoma o centro organizador de microtúbulos que permiten la polimerización de microtúbulos de dímeros de tubulina que forman parte del citoesqueleto. Los centríolos se posicionan perpendicularmente entre sí. Sus funciones son participar en la mitosis, durante la cual generan el huso acromático, y en la citocinesis,[60] así como, se postula, intervenir en la nucleación de microtúbulos.[61]​[62]​ Cilios y flagelos: Se trata de especializaciones de la superficie celular con motilidad; con una estructura basada en agrupaciones de microtúbulos, ambos se diferencian en la mayor longitud y menor número de los flagelos, y en la mayor variabilidad de la estructura molecular de estos últimos.[13]​ Ciclo vital Artículo principal: Ciclo celular Diagrama del ciclo celular: la interfase, en naranja, alberga a las fases G1, S y G2; la fase M, en cambio, únicamente consta de la mitosis y citocinesis, si la hubiere. El ciclo celular es el proceso ordenado y repetitivo en el tiempo mediante el cual una célula madre crece y se divide en dos células hijas. Las células que no se están dividiendo se encuentran en una fase conocida como G0, paralela al ciclo. La regulación del ciclo celular es esencial para el correcto funcionamiento de las células sanas, está claramente estructurado en fases[51]​ El estado de no división o interfase. La célula realiza sus funciones específicas y, si está destinada a avanzar a la división celular, comienza por realizar la duplicación de su ADN. El estado de división, llamado fase M, situación que comprende la mitosis y citocinesis. En algunas células la citocinesis no se produce, obteniéndose como resultado de la división una masa celular plurinucleada denominada plasmodio.[nota 3]​ A diferencia de lo que sucede en la mitosis, donde la dotación genética se mantiene, existe una variante de la división celular, propia de las células de la línea germinal, denominada meiosis. En ella, se reduce la dotación genética diploide, común a todas las células somáticas del organismo, a una haploide, esto es, con una sola copia del genoma. De este modo, la fusión, durante la fecundación, de dos gametos haploides procedentes de dos parentales distintos da como resultado un zigoto, un nuevo individuo, diploide, equivalente en dotación genética a sus padres.[63]​ La interfase consta de tres estadios claramente definidos.[2]​[51]​ Fase G1: es la primera fase del ciclo celular, en la que existe crecimiento celular con síntesis de proteínas y de ARN. Es el período que trascurre entre el fin de una mitosis y el inicio de la síntesis de ADN. En él la célula dobla su tamaño y masa debido a la continua síntesis de todos sus componentes, como resultado de la expresión de los genes que codifican las proteínas responsables de su fenotipo particular. Fase S: es la segunda fase del ciclo, en la que se produce la replicación o síntesis del ADN. Como resultado cada cromosoma se duplica y queda formado por dos cromátidas idénticas. Con la duplicación del ADN, el núcleo contiene el doble de proteínas nucleares y de ADN que al principio. Fase G2: es la segunda fase de crecimiento del ciclo celular en la que continúa la síntesis de proteínas y ARN. Al final de este período se observa al microscopio cambios en la estructura celular, que indican el principio de la división celular. Termina cuando los cromosomas empiezan a condensarse al inicio de la mitosis. La fase M es la fase de la división celular en la cual una célula progenitora se divide en dos células hijas idénticas entre sí y a la madre. Esta fase incluye la mitosis, a su vez dividida en: profase, metafase, anafase, telofase; y la citocinesis, que se inicia ya en la telofase mitótica. La incorrecta regulación del ciclo celular puede conducir a la aparición de células precancerígenas que, si no son inducidas al suicidio mediante apoptosis, puede dar lugar a la aparición de cáncer. Los fallos conducentes a dicha desregulación están relacionados con la genética celular: lo más común son las alteraciones en oncogenes, genes supresores de tumores y genes de reparación del ADN.[64]​ Origen Artículos principales: Historia de la vida y Anexo:Cronología de la historia evolutiva de la vida. Origen de la primera célula Artículo principal: Abiogénesis La aparición de la vida, y, por ello, de la célula, probablemente se inició gracias a la transformación de moléculas inorgánicas en orgánicas bajo unas condiciones ambientales adecuadas, produciéndose más adelante la interacción de estas biomoléculas generando entes de mayor complejidad. El experimento de Miller y Urey, realizado en 1953, demostró que una mezcla de compuestos orgánicos sencillos puede transformarse en algunos aminoácidos, glúcidos y lípidos (componentes todos ellos de la materia viva) bajo unas condiciones ambientales que simulan las presentes hipotéticamente en la Tierra primigenia (en torno al eón Hádico).[65] Se ha sugerido que el último antepasado común universal vivió hace más de 4200 millones de años.[66]​ Se postula que dichos componentes orgánicos se agruparon generando estructuras complejas, los coacervados de Oparin, aún acelulares que, en cuanto alcanzaron la capacidad de autoorganizarse y perpetuarse, dieron lugar a un tipo de célula primitiva, el progenote de Carl Woese, antecesor de los tipos celulares actuales.[33] Una vez se diversificó este grupo celular, dando lugar a las variantes procariotas, arqueas y bacterias, pudieron aparecer nuevos tipos de células, más complejos, por endosimbiosis, esto es, captación permanente de unos tipos celulares en otros sin una pérdida total de autonomía de aquellos.[67] De este modo, algunos autores describen un modelo en el cual la primera célula eucariota surgió por introducción de una arquea en el interior de una bacteria, dando lugar esta primera a un primitivo núcleo celular.[68] No obstante, la imposibilidad de que una bacteria pueda efectuar una fagocitosis y, por ello, captar a otro tipo de célula, dio lugar a otra hipótesis, que sugiere que fue una célula denominada cronocito la que fagocitó a una bacteria y a una arquea, dando lugar al primer organismo eucariota. De este modo, y mediante un análisis de secuencias a nivel genómico de organismos modelo eucariotas, se ha conseguido describir a este cronocito original como un organismo con citoesqueleto y membrana plasmática, lo cual sustenta su capacidad fagocítica, y cuyo material genético era el ARN, lo que puede explicar, si la arquea fagocitada lo poseía en el ADN, la separación espacial en los eucariotas actuales entre la transcripción (nuclear), y la traducción (citoplasmática).[69]​ Una dificultad adicional es el hecho de que no se han encontrado organismos eucariotas primitivamente amitocondriados como exige la hipótesis endosimbionte. Además, el equipo de María Rivera, de la Universidad de California, comparando genomas completos de todos los dominios de la vida ha encontrado evidencias de que los eucariotas contienen dos genomas diferentes, uno más semejante a bacterias y otro a arqueas, apuntando en este último caso semejanzas a los metanógenos, en particular en el caso de las histonas.[70]​[71] Esto llevó a Bill Martin y Miklós Müller a plantear la hipótesis de que la célula eucariota surgiera no por endosimbiosis, sino por fusión quimérica y acoplamiento metabólico de un metanógeno y una α-proteobacteria simbiontes a través del hidrógeno (hipótesis del hidrógeno).[72] Esta hipótesis atrae hoy en día posiciones muy encontradas, con detractores como Christian de Duve.[73]​ Harold Morowitz, un físico de la Universidad Yale, ha calculado que las probabilidades de obtener la bacteria viva más sencilla mediante cambios al azar es de 1 sobre 1 seguido por 100 000 000 000 ceros. «Este número es tan grande —dijo Robert Shapiro— que para escribirlo en forma convencional necesitaríamos varios centenares de miles de libros en blanco». Presenta la acusación de que los científicos que han abrazado la evolución química de la vida pasan por alto la evidencia aumentante y «han optado por aceptarla como verdad que no puede ser cuestionada, consagrándola así como mitología».[74]​ Origen de la célula eucariota Artículo principal: Eucariogénesis En la teoría de la simbiogenesis, la fusión entre una arquéa y una bacteria aeróbia creo la célula eucariota, con mitochondrias aeróbicas, hace unos 2500 millones de años. Una segunda fusión, hace 2000 millones de años, añadió los cloroplastos, originando la célula vegetal.[75]​ Las células eucariotas se formaron hace 2500 millones de años en un proceso llamado eucariogénesis.[76] Se acepta ampliamente que esto implicó una simbiogénesis, en la que una arquea y una bacteria se unieron para crear el primer ancestro común eucariota.[77] Esta célula tenía un nuevo nivel de complejidad y capacidad, con un núcleo[78]​[79] y mitocondrias facultativamente aeróbicas.[75] Evolucionó hasta convertirse en una población de organismos unicelulares que incluía al último ancestro común eucariota, acumulando capacidades a lo largo del camino, aunque la secuencia de los pasos involucrados ha sido cuestionada y es posible que no haya comenzado con la simbiogénesis. Presentaba al menos un centriolo y cilio, sexo (meiosis y singamia), peroxisomas y un quiste latente con una pared celular de quitina y/o celulosa.[80]​[81] A su vez, el último ancestro común eucariota dio origen al grupo terminal de los eucariotas, que contiene los ancestros de animales, hongos, plantas y una amplia gama de organismos unicelulares.[82]​[83]​[84] Las células vegetales se formaron hace unos 2000 millones de años con un segundo episodio de simbiogénesis al que se añadieron cloroplastos, derivados de una cianobacteria."
ksampletext_wikipedia_biol_filogenia: str = "Filogenia. La filogenia es la relación de parentesco entre especies o taxones en general. Aunque el término también aparece en lingüística histórica para referirse a la clasificación de las lenguas humanas según su origen común, el término se utiliza principalmente en su sentido biológico. La filogenética es una disciplina de la biología evolutiva[2] que se ocupa de comprender las relaciones históricas entre diferentes grupos de organismos a partir de la distribución en un árbol o cladograma dicotómico de los caracteres derivados (sinapomorfías) de un ancestro común a dos o más taxones que contiene aquellos caracteres plesiomórficos en común. Incluso en el campo del cáncer,[3] la filogenética permite estudiar la evolución clonal (evolución de los clones de la célula cancerosa original, debido a las mutaciones que ocurran)[4] de los tumores y la cronología molecular, viéndose como varían las poblaciones celulares a lo largo de la progresión de la enfermedad, incluso durante el tratamiento de la misma, mediante el empleo de técnicas de secuenciación del genoma completo en muestras de ADN circulante tumoral.[5]​ Para reconstruir la filogenia de un grupo taxonómico (familia, género, subgénero, etc.) es imprescindible construir matrices basadas en datos morfológicos y/o moleculares (ADN, ARN y proteínas).[1]​[6] Las matrices son analizadas con determinados algoritmos que permiten encontrar los árboles filogenéticos más cortos siguiendo el principio de parsimonia,[7] que supone la menor cantidad de cambios bajo el supuesto de que la evolución acontece de la manera más simple, esto es: los árboles que son considerados como la mejor opción filogenética son aquellos más cortos, es decir, más parsimoniosos. Interpretar los árboles obtenidos implica rastrear la historia del grupo bajo un paradigma evolutivo basado en el supuesto de un antecesor común del que van derivando cada uno de los clados, considerando que estos solo se sustentan por homologías. La condición de homología es resultante de la aceptación a priori de la existencia de monofilia. Explicar las relaciones de filogenéticas sobre la base del mapa de caracteres que ofrecen los cladogramas permite construir clasificaciones más naturales, uno de los propósitos centrales de la sistemática, una disciplina cuyos orígenes, en términos académicos, se remontan a los aportes de Linneo. No obstante, muchas clasificaciones han tenido diversos propósitos y responden a metodologías y criterios diferentes. Las primeras han sido artificiales y meramente utilitarias; otras se han basado en criterios que la ciencia ha depuesto en la actualidad, sustituyendo las categorías taxonómicas o los sistemas de clasificación creados bajo esas metodologías por otros que son legitimados por los científicos. Entre las corrientes más relevantes respecto de las clasificaciones biológicas mediadas por la metodología se encuentran en la actualidad dos programas de investigación que en sus inicios se presentaron como antagónicos: el feneticismo[8] y el cladismo[9] y que si bien comparten el propósito de encontrar un sistema que ordene a la diversidad de especies y de categorías taxonómicas se basan en postulados, supuestos y teorías auxiliares diversas y en metodologías diferentes. La sistemática filogenética se ha impuesto con el devenir de los años a causa de que la homología (que no constituye una premisa bajo la lógica feneticista) es consistente con el supuesto de un antecesor común y, por lo tanto, congruente con la evolución y, en consecuencia, con la posibilidad de definir arreglos taxonómicos más naturales. Esta necesidad de conocer la historia evolutiva de los seres vivos inicia con la publicación de El origen de las especies por Charles Darwin en 1859, aunque existen ideas previas que al menos desde Aristóteles[10] han intentado explicar la diversidad de las formas de vida y sus relaciones. No obstante, explicar las relaciones históricas entre especies en función de la evolución es una tarea interminable y provisoria tal como lo es el conocimiento científico, sujeto a marcos teóricos y a coyunturas políticas. Uno de los hitos en relación con la justificación de estas relaciones fueron las contribuciones de Willi Hennig[11] (entomólogo alemán, 1913-1976), Walter Zimmermann[12] (botánico alemán, 1892-1980), Warren H. Wagner, Jr.[13] (botánico estadounidense, 1920-2000) entre otros por la centralidad de sus aportes, tanto desde el punto de vista teórico como metodológico. Técnicas y uso Filogenética molecular Es la técnica de la filogenia que investiga las relaciones de los seres vivos mediante análisis moleculares de la secuencia de ADN, ARN y proteínas. Constituye la herramienta principal de la biología evolutiva moderna para inferir parentescos, especialmente en grupos donde los rasgos morfológicos son escasos o poco informativos, como los microorganismos. En este enfoque, las similitudes en la secuencia de nucleótidos y aminoácidos se interpretan como sinapomorfías en el análisis, es decir, características compartidas derivadas de un ancestro común. No obstante, dichas similitudes pueden variar considerablemente, ya que un organismo puede compartir secuencias idénticas con varios linajes cercanos, lo que puede generar hipótesis filogenéticas alternativas. La filogenética molecular ha permitido identificar numerosos clados evolutivos que no habían sido reconocidos mediante análisis morfológicos y ha corregido múltiples errores derivados del estudio exclusivo de características anatómicas. No obstante, esta aproximación también puede verse afectada por fenómenos como la atracción de ramas largas, en la que linajes con tasas de evolución acelerada se agrupan erróneamente. Por ello, se emplean modelos evolutivos más complejos y métodos estadísticos como la inferencia bayesiana o la máxima verosimilitud para minimizar dichos sesgos y obtener árboles más precisos. En la actualidad, la filogenética molecular se apoya en herramientas bioinformáticas y en bases de datos genómicas de gran escala, lo que permite analizar miles de genes o incluso genomas completos (filogenómica). Estas técnicas han revolucionado la clasificación biológica y la comprensión de la historia evolutiva de los organismos, contribuyendo a redefinir la sistemática moderna y a mejorar la identificación de especies, el estudio de la biodiversidad y la reconstrucción de eventos evolutivos profundos. Filogenética morfológica Es la técnica de la filogenia que investiga las relaciones de los seres vivos mediante análisis morfológicos como anatomía comparada, homología, embriología, alometría y fósiles. Las similitudes morfológicas entre organismos pueden ser un indicativo de parentesco, pero posteriormente se demostró que las similitudes morfológicas pueden evolucionar convergentemente en linajes diferentes. Actualmente, se usan ciertos caracteres morfológicos (sinapomorfías) que pueden emplearse para determinar las relaciones. La filogenética morfológica es empleada por los paleontólogos para determinar las relaciones entre los fósiles y los grupos existentes. También es usada por algunos zoólogos y botánicos evolutivos para determinar ciertos caracteres morfológicos válidos entre sus grupos de estudio (animales y plantas). Antiguamente, se usó para estudiar las relaciones entre los microorganismos, pero su uso quedó obsoleto debido a la ausencia de caracteres morfológicos en estos grupos. Filogenética molecular-estructural Es una técnica filogenética novedosa que investiga las relaciones mediante un tipo de biomolécula específico o similar que porten los organismos, sin tomar en cuenta la secuencia. Es similar a la filogenética morfológica en el hecho de que la sinapomorfía es una biomolécula única o similar que portan dichos organismos sin recurrir a la secuencia. Por ejemplo, las bacterias son un dominio que se caracteriza por tener una pared celular de peptidoglicanos. Las bacterias de Sphingobacteria se caracterizan por tener esfingolípidos. Los dominios de virus Riboviria, Duplodnaviria, Adnaviria y Varidnaviria se determinaron filogenéticamente mediante la presencia de una proteína única o similar estructuralmente. Inferencia de árboles filogenéticos La reconstrucción de árboles filogenéticos, a partir de datos moleculares o morfológicos, requiere de métodos que permitan inferir las relaciones evolutivas entre los taxones de interés. La filogenética computacional es una rama de la bioinformática, que aplica algoritmos y herramientas informáticas para reconstruir y analizar árboles filogenéticos. Entre los enfoques más empleados para estos propósito se encuentran la máxima parsimonia, los métodos basados en distancias, la máxima verosimilitud[14] y la inferencia bayesiana,[15] cada uno basado en distintos supuestos sobre la evolución de los caracteres. Método de máxima parsimonia: es un modelo de reconstrucción filogenética basado en el principio de parsimonia, según el cual se busca el árbol filogenético que implique la menor cantidad posible de cambios evolutivos o transiciones de un estado a otro. Aunque este método no se utiliza con tanta frecuencia en la actualidad, sigue siendo una herramienta importante para la reconstrucción de árboles, especialmente cuando no se cuenta con datos moleculares y se trabaja con caracteres morfológicos.[16]​ Métodos basados en distancias: estos métodos de reconstrucción estiman las distancias evolutivas entre pares de taxones. Esta distancia se calcula generalmente alineando las secuencias de ADN o de proteínas y evaluando cuánto difieren entre sí. Una vez obtenidas las distancias, se reconstruye el árbol filogenético mediante algoritmos de agrupamiento que unen primero a los taxones más similares. Algunos de los algoritmos más utilizados son Neighbor-Joining[17] y UPGMA.[18]​ Método de máxima verosimilitud: en este enfoque de reconstrucción filogenética se asume que el árbol representa un modelo de evolución, y se busca encontrar la topología y la longitud de las ramas que maximizan la probabilidad de que los datos observados hayan ocurrido bajo dicho modelo. Para realizar este proceso, el método requiere un modelo de sustitución que describa la probabilidad de que un nucleótido o un aminoácido cambie por otro a lo largo del tiempo. Aunque es computacionalmente más costoso, es uno de los métodos más utilizados actualmente.[14]​ Método de inferencia bayesiana: este método aplica la inferencia bayesiana para realizar la reconstrucción filogenética. En este método, el modelo de sustitución, la topología del árbol y las longitudes de las ramas se tratan como parámetros del modelo, y se buscan los valores que maximizan la probabilidad posterior, la cual combina la función de verosimilitud de los datos, la probabilidad previa de los parámetros y la probabilidad marginal de los datos. Debido a su alto costo computacional, normalmente se emplea el algoritmo de cadenas de Markov de Monte Carlo (MCMC) para realizar la inferencia de los parametros. Caracteres y estados del carácter El primer paso para reconstruir la filogenia de los organismos es determinar cuanta similitud hay entre sí, ya sea en morfología, anatomía, embriología, biogeografía, moléculas de ADN, ARN o proteínas, ya que en última instancia estos parecidos pueden ser un indicador de su parecido genético, y, por lo tanto, de sus relaciones evolutivas. La evolución es un proceso muy lento, y en la gran mayoría de los casos nadie la ha visto suceder. Lo que se maneja es una serie de hipótesis acerca de cómo ocurrió la diversificación de los organismos, que desembocó en la aparición de las distintas especies variadamente relacionadas entre sí. Esas hipótesis son las que determinan cómo deberían analizarse los organismos para determinar su filogenia. Supongamos una única población ancestral de plantas. Para establecer que los organismos que componen esta población son morfológicamente similares entre sí, determinamos una serie de caracteres: color de pétalo, leñosidad del tallo, presencia o ausencia de tricomas en las hojas, cantidad de estambres, fruto seco o carnoso, y rugosidad de la semilla. Todas las plantas de esta población ancestral comparten los mismos estados del carácter para cada uno de ellos: los pétalos son blancos, el tallo herbáceo, las hojas sin tricomas, los estambres son 5, el fruto es seco, y la semilla lisa. Finalmente, mediante algún mecanismo de aislamiento reproductivo, la población se divide en dos subpoblaciones que no intercambian material genético entre sí. Al cabo de algunas generaciones, se va haciendo evidente que aparecen mutantes en las dos subpoblaciones nuevas. Algunos de ellos son más exitosos reproductivamente que el resto de la población y, por lo tanto, después de unas generaciones más, su genotipo se convierte en el dominante en esa población. Como las mutaciones ocurren al azar en cada subpoblación, y la probabilidad de que ocurra espontáneamente la misma mutación en cada subpoblación es muy baja, las dos subpoblaciones van acumulando diferentes mutaciones exitosas, generando diferentes genotipos, que se pueden ver reflejados en los cambios que ocurren en los estados de los caracteres. Así, por ejemplo, la subpoblación 1 pasó a poseer el tallo leñoso, y la subpoblación 2 pasó a poseer los pétalos rojos (pero conservando el tallo herbáceo ancestral). Como resultado, la última generación de plantas corresponde a dos poblaciones muy similares entre sí, con muchos caracteres compartidos, salvo la leñosidad del tallo y el color de los pétalos. Nosotros, en nuestra corta vida, solo vemos este resultado de la evolución, e hipotetizamos que lo que ocurrió fue el proceso que se indica más arriba. Esta hipótesis se puede reflejar en un árbol filogenético, un diagrama que resume las relaciones de parentesco entre los ancestros y sus descendientes, como el siguiente: Árbol filogenético que muestra cómo, después de un evento de aislamiento reproductivo entre dos poblaciones de la misma especie, apareció una mutación exitosa en cada población, que pasaron a diferenciarse entre ellas mediante la observación de los estados de sus caracteres. En el cladograma, la especie 1 comparte con su ancestro todos los estados de los caracteres salvo el tallo, que es leñoso. La especie 2, a su vez, comparte con su ancestro todos los caracteres salvo el color de los pétalos, que es rojo. Las dos especies comparten entre sí todos los caracteres salvo la leñosidad del tallo y el color de los pétalos. En este ejemplo, se han establecido 2 linajes: secuencias de poblaciones desde el ancestro hasta los descendientes. En los inicios de la sistemática, los caracteres utilizados para comparar a los grupos entre sí eran conspicuos, principalmente morfológicos. A medida que se acumuló más conocimiento se empezó a tomar cada vez más cantidad de caracteres crípticos, como los anatómicos, embriológicos, serológicos, químicos y finalmente caracteres del cariotipo y los derivados del análisis molecular. Los caracteres correspondientes al ancestro de un grupo de organismos que son retenidos por el grupo se dice que son plesiomórficos (ancestrales), mientras que los que fueron adquiridos exclusivamente por ese grupo (en el ejemplo, el tallo leñoso para la especie 1 o los pétalos rojos para la especie 2) se dice que son sinapomórficos o derivados (nuevos). Nótese que solo la presencia de sinapomorfías nos indica que se ha formado un nuevo linaje, nótese también que en árboles filogenéticos más extensos, como el siguiente: Árbol filogenético que muestra un ejemplo de diversificación de una especie ancestral en 5 especies presentes en la actualidad. el mismo carácter puede ser una sinapomorfía o una plesiomorfía, según desde qué porción del árbol se la observe. Por ejemplo, el tallo leñoso es una sinapomorfía de C (y de C+A+B) pero una plesiomorfía para A, ya que comparte ese estado del carácter con B a través de su ancestro común. Otra forma de decirlo es que el tallo leñoso es un carácter derivado desde el punto de vista de la población original, pero es ancestral para A y para B. El aspecto del árbol filogenético (su topología) solo está dado por las conexiones entre sus nodos, y no por el orden en que son diagramados. Así, [[A+B]+C] es el mismo árbol que [C+[A+B]]. La topología tampoco está dada por la posición en que el árbol es dibujado, a veces se los dibuja erectos (con el ancestro abajo y los grupos terminales arriba), a veces se los dibuja recostados (con el ancestro a la izquierda y los grupos terminales a la derecha). Las dos formas de dibujarlos son igualmente válidas. En los árboles filogenéticos como los aquí expuestos, el largo de las ramas tampoco da ninguna información acerca de cuánto diverge ese linaje en términos de sus caracteres ni acerca de en qué momento geológico ocurrió el aislamiento de ese linaje (pero hay árboles que sí dan esa información). Un cladograma es un árbol filogenético que solo muestra las relaciones evolutivas, sin darle un significado a sus ramas. Por el otro lado, hay dos tipos de árboles filogenéticos con significado en la longitud de sus ramas: el cronograma, donde la longitud de las ramas indica el tiempo transcurrido entre un nodo y otro, y la posición en el tiempo de cada nodo con respecto a los otros; y el filograma, donde la longitud de las ramas indica la cantidad de cambio evolutivo desde el ancestro común más cercano.[19]​ Monofilia, parafilia y polifilia Artículos principales: Monofilético, Parafilético y Polifilético. Grupos filogenéticos: monofilético, parafilético, polifilético. Un grupo formado por un ancestro y todos sus descendientes se denomina monofilético, también llamado clado. Al grupo al que se le ha excluido alguno de sus descendientes se lo llama parafilético. Los grupos formados por los descendientes de más de un ancestro se denominan polifiléticos.[cita requerida] Por ejemplo, se cree que las aves y los reptiles descienden de un único ancestro común, luego este grupo taxonómico (amarillo en el diagrama) es considerado monofilético. Los reptiles actuales como grupo también tienen un ancestro común a todos ellos, pero ese grupo (reptiles modernos) no incluye a todos los descendientes de tal ancestro porque se está dejando a las aves fuera (solo incluye los de color cian en el diagrama); por lo que un grupo así se considera como parafilético. Un grupo que incluyera a los vertebrados de sangre caliente contendría solo a los mamíferos y las aves (rojo/naranja en el diagrama) y sería polifilético, porque entre los miembros de este agrupamiento no está el más reciente ancestro común de ellos. Los animales de sangre caliente son todos descendientes de un ancestro de sangre fría. La condición endotérmica (sangre caliente) ha aparecido dos veces, independientemente, en el ancestro de los mamíferos, por un lado, y en el de las aves (y quizá algunos o todos los dinosaurios), por otro.[20]​ Algunos autores sostienen[21] que la diferencia entre grupos parafiléticos y polifiléticos es sutil, y prefieren llamar a estos dos tipos de asemblajes como no monofiléticos. Muchos taxones largamente reconocidos de plantas y animales resultaron ser no monofiléticos según los análisis de filogenia hechos en las últimas décadas, por lo que muchos científicos recomendaron abandonar su uso, ejemplos de estos taxones son Prokaryota, Protista, Pisces, Reptilia, Pteridophyta, Dicotyledoneae, y varios otros más. Como su uso está muy extendido por haber sido tradicionalmente reconocidos, y porque muchos científicos consideran a los taxones parafiléticos válidos (discusión que aún no está terminada en el ambiente científico; el ejemplo más claro de un taxón que muchos[22] desean conservar quizás sean Reptilia), a veces se indica el nombre del taxón, con la salvedad de que su nombre se pone entre comillas, para indicar que el taxón no se corresponde con un clado.[21] La diferencia entre un clado y un taxón es que un clado debe ser un grupo natural (monofilético), mientras que un taxón puede ser o no monofilético, pero los taxones no monofiléticos pierden su validez en la clasificación actual de los organismos.[19]​ El rol de las sinapomorfías en el análisis filogenético Las sinapomorfías que caracterizan a cada grupo monofilético son estados de los caracteres que se originaron en el ancestro común a todos los miembros del grupo, pero que no estaban presentes en los ancestros anteriores a estos, ancestros comunes tanto a los miembros del grupo como a otros grupos más. Hay que tener en cuenta que si bien una sinapomorfía es un estado del carácter que se hipotetiza que está presente en el ancestro del grupo, no necesariamente será encontrada en todos sus descendientes, debido a que la evolución puede modificarla y hasta revertirla a su estado anterior por azar (proceso que se conoce como reversión). Por lo tanto, no está garantizado que una lista de sinapomorfías vaya a encontrarse en todos los miembros de un grupo, y solo mediante un síndrome de caracteres podemos asegurarnos de que cada miembro pertenece a ese clado.[cita requerida] El concepto de sinapomorfía fue formalizado por primera vez por Hennig (1966) y Wagner (1980).[23] Mucho del análisis filogenético actual se basa en la búsqueda de sinapomorfías que permitan establecer grupos monofiléticos. En ese sentido, son revolucionarios los análisis moleculares de ADN que se están realizando desde hace algunos años, que entre otras técnicas determinan la secuencia de bases del mismo trozo de ADN en diferentes taxones, y comparan directamente sus secuencias de bases. En estos análisis, que se realizan con secuencias conservadas de genes concretos (como el ARNr), cada base es un carácter, y los posibles estados del carácter son las 4 posibles bases: adenina, timina, guanina y citosina. Si bien las sinapomorfías encontradas a través de los análisis moleculares de ADN son oscuras y no son útiles para identificar organismos en el campo o para plantear hipótesis acerca de la adaptación de los organismos a su ambiente, poseen ventajas (como la cantidad de caracteres medidos con poca cantidad de recursos, el establecimiento de caracteres menos subjetivos que los basados en fenotipos), que le otorgan a los análisis filogenéticos una precisión sin precedentes, obligando en muchas ocasiones a abandonar hipótesis evolutivas largamente reconocidas. Además, según la hipótesis del reloj molecular, la comparación de secuencias de ADN permite no solo determinar la distancia genética entre dos especies, sino además estimar el tiempo transcurrido desde el último antecesor común.[cita requerida] Sinapomorfías y especies La regla para construir los árboles filogenéticos es el reconocimiento de grupos monofiléticos (clados) a partir de sus sinapomorfías (estados de los caracteres comunes al grupo). Esto es cierto para todos los nodos del árbol salvo el terminal, a nivel de las especies. No se puede establecer monofilia a nivel de las especies debido a que la naturaleza de las relaciones entre los organismos cambia por encima y por debajo del nivel de especie: por encima del nivel de especie, organismos de dos clados diferentes no pueden cruzarse entre sí y dar descendencia fértil, por lo que sus bagajes genéticos se mantienen sin mezclarse. Por debajo del nivel de especie, existe interfertilidad entre los organismos, por lo que el genoma de cada organismo es el resultado del cruce de dos genomas diferentes. Esta diferencia se puede esquematizar como un árbol ramificado para representar a todas las agrupaciones de organismos por encima del nivel de especie, pero en los organismos que pertenecen a la misma especie, las ramas del árbol se entrecruzan entre sí creando una red interconectada de organismos. Como muchas poblaciones del planeta están en diferentes etapas del proceso de especiación, y a veces se reconocen dos poblaciones diferentes como especies diferentes a pesar de ser algo interfértiles, entonces no es fácil determinar si un estado de un carácter es exclusivo de una de las especies o pertenece también en una baja proporción no muestreada a la otra especie, o si pertenecerá en algún momento debido a una hibridación casual, a la otra especie. Avances recientes En los últimos años, el uso de datos genómicos a gran escala ha transformado la filogenia al permitir la reconstrucción de árboles evolutivos con miles de loci —lo que se conoce como filogenómica— y enfrentarse simultáneamente a nuevos retos como la inferencia de ortología/paralogs, la hibridación, el intercambio genético lateral y la incongruencia entre genes y especies (Zaharias et al., 2022). Además, se está reconociendo que en algunos linajes complejos, como ciertos grupos de angiospermas, el modelo clásico de árbol bifurcado podría no bastar y se requiere considerar redes evolutivas o grafos reticulados para reflejar procesos de introgresión y duplicación genómica (Li et al., 2025)."
ksampletext_wikipedia_biol_botanica: str = "Botánica. La botánica (del griego, ‘hierba’) es la rama de la biología que estudia las plantas, en sentido amplio, incluyendo a las algas, hongos y organismos fotosintéticos no necesariamente clasificados como plantas, bajo todos sus aspectos, incluyendo la descripción, clasificación, distribución, identificación, estudio de la reproducción, fisiología, morfología, relaciones recíprocas, relaciones con los otros seres vivos y efectos provocados sobre el medio en el que se encuentran.[1]​[2] Los términos para quien se dedica a esta disciplina son dos: botánico /a y botanista. La botánica estudia las plantas en sentido amplio, abarcando las categorías taxonómicas de las plantas sin flores (criptógamas), las plantas sin flores y sin vasos (briofitas), las plantas sin flores y con vasos (pteridofitas), las plantas con flores (espermatofitas), las plantas con flores y sin fruto (gimnospermas) y las plantas con flores y con fruto (angiospermas), dentro de la clasificación clásica de los organismos vegetales. No obstante, en términos históricos, el objeto de estudio de la botánica no se ha restringido estrictamente al Reino Plantae, sino que ha abarcado un grupo de organismos lejanamente emparentados entre sí, esto es, las cianobacterias, los hongos, las algas y las plantas, los que casi no poseen ningún carácter en común salvo la presencia de cloroplastos (a excepción de los hongos y cianobacterias) o el no poseer capacidad de desplazamiento.[3]​[4] En el campo de la botánica hay que distinguir entre la botánica pura, cuyo objeto es ampliar el conocimiento de la naturaleza, y la botánica aplicada, cuyas investigaciones están al servicio de la tecnología agraria, forestal y farmacéutica. Su conocimiento afecta a muchos aspectos de nuestra vida y por tanto es una disciplina estudiada por biólogos y ambientólogos, pero también por farmacéuticos, ingenieros agrónomos, ingenieros forestales, entre otros.[5]​ La botánica cubre una amplia gama de contenidos, que incluyen aspectos específicos propios de los vegetales, así como de las disciplinas biológicas que se ocupan de la composición química (fitoquímica), de la organización celular y tisular (histología vegetal), del metabolismo y el funcionamiento orgánico (fisiología vegetal), del crecimiento y el desarrollo, de la morfología (fitografía), de la reproducción, de la herencia (genética vegetal), de las enfermedades (fitopatología), de las adaptaciones al ambiente (ecología), de la distribución geográfica (fitogeografía o geobotánica), de los fósiles (paleobotánica) y de la evolución. Los organismos que estudia la botánica La idea de que la naturaleza puede ser dividida en tres reinos (mineral, vegetal y animal) fue propuesta por Nicolás Lemery (1675)[6] y popularizada por Carlos Linneo en el siglo XVIII.[7] Carlos Linneo,[8] a finales del siglo XVIII, introdujo el actual sistema de clasificación. Este incluye los conocimientos sobre las diversas especies vegetales dentro de un sistema más amplio, ofreciendo una versión sintética y enriquecedora. No en vano se ha dicho que el sistema de clasificación de Linneo prefigura lo que después serían las teorías evolutivas. A pesar de que con posterioridad fueron determinados como reinos separados para los hongos (en 1783),[9] protozoarios (en 1858)[10] y bacterias (en 1925)[11] la concepción del siglo XVII de que solo existían dos reinos de organismos dominó la biología por tres siglos. El descubrimiento de los protozoarios en 1675, y de las bacterias en 1683, ambos realizados por Leeuwenhoek,[12]​[13] finalmente comenzó a minar el sistema de dos reinos. No obstante, un acuerdo general entre los científicos acerca de que el mundo viviente debería ser clasificado en al menos cinco reinos,[14]​[15]​[16] solo fue logrado luego de los descubrimientos realizados por la microscopía electrónica en la segunda mitad del siglo XX. Tales hallazgos confirmaron que existían diferencias fundamentales entre las bacterias y los eucariotas y, además, revelaron la tremenda diversidad ultraestructural de los protistas. La aceptación generalizada de la necesidad de utilizar varios reinos para incluir a todos los seres vivos también debe mucho a la síntesis sistemática de Herbert Copeland (1956)[17] y a los influyentes trabajos de Roger Y. Stanier (1961-1962)[18]​[19] y Robert H. Whittaker (1969).[20]​[7]​En el sistema de seis reinos, propuesto por Thomas Cavalier-Smith en 1983[21] y modificado en 1998,[7] las bacterias son tratadas en un único reino (Bacteria) y los eucariotas se dividen en 5 reinos: protozoarios (Protozoa), animales (Animalia), hongos (Fungi), plantas (Plantae) y Chromista (algas cuyos cloroplastos contienen clorofilas a y c, así como otros organismos sin clorofila relacionados con ellas). La nomenclatura de estos tres últimos reinos, clásico objeto de estudio de la botánica, está sujeta a las reglas y recomendaciones del Código Internacional de Nomenclatura Botánica.[22]​ Divisiones de la botánica Familias botánicas Las plantas pueden estudiarse desde varios puntos de vista, así, pueden diferenciarse distintas líneas de trabajo de acuerdo con los niveles de organización que se estudien: desde las moléculas y las células, pasando por los tejidos y los órganos, hasta los individuos, las poblaciones y las comunidades vegetales. Otras posibilidades se refieren al estudio de las plantas que vivieron en épocas geológicas pasadas o al de las que viven en la actualidad, al examen de los distintos grupos sistemáticos y a la investigación de cómo pueden ser utilizados los vegetales por el ser humano.[23]​[24]​ Una de las metas más importantes para la botánica, es que junto a la biotecnología e ingeniería genética puedan llegar a crear vida. En general, todas esas direcciones de trabajo se basan en el análisis comparativo de los fenómenos particulares y de su variabilidad, para llegar a una generalización y al reconocimiento de las relaciones regulares que unen dichos fenómenos entre sí. Siempre deben asociarse los métodos estático y dinámico: por un lado el reconocimiento y la interpretación de las estructuras y formas y, por el otro, el análisis de los procesos vitales, de funciones y de fenómenos de desarrollo. El objetivo final de ambos métodos debe ser en todo caso la comprensión de las formas y de las funciones en su dependencia recíproca y en su evolución. Los distintos puntos de vista descritos y el empleo de diferentes métodos de trabajo han conducido a que dentro de la botánica se hayan desarrollado numerosas disciplinas. En primer lugar, se puede citar a la Morfología, la cual, en sentido amplio, es la teoría general de la estructura y forma de las plantas, e incluye la Citología y la Histología. La primera se ocupa del estudio de la fina constitución de las células y se asocia, en los aspectos relacionados con las moléculas, con algunas partes de la Biología Molecular. La Histología es el estudio de los tejidos de las plantas. Citología e Histología, conjuntamente, son necesarias para comprender la Anatomía de las plantas, o sea, su constitución interna.[25]​[26]​[27]​ Al ocuparse de los procesos de adaptación, la morfología se relaciona con la ecología, disciplina que investiga las relaciones entre la planta y su ambiente. Tales relaciones están basadas en los estudios de la fisiología vegetal, que se ocupa —de modo general— al estudio del modo en que se realizan las funciones de la planta en los campos del metabolismo, del cambio de forma (que incluye el crecimiento y desarrollo de la planta) y de los movimientos. La reproducción de las plantas y el modo en que se heredan y cambian las características a través de las generaciones es el campo de la Genética.[26]​ La botánica sistemática trata de averiguar las afinidades que existen entre los diversos tipos de plantas, basándose en los resultados de todas las disciplinas mencionadas previamente, entre las que, al lado de la morfología, son importantes la citología, la anatomía, el estudio de las esporas y del polen (Palinología), el estudio de la generación sexual y del embrión (Embriología), las sustancias producidas y contenidas en las plantas (fitoquímica), la Genética y la Geobotánica. Como parte de la sistemática, se encuentra principalmente la taxonomía, que se ocupa de la descripción, nomenclatura y ordenación de las especies de plantas existentes, las cuales sobrepasan el número de 330 000. A ella se añade el estudio de la historia evolutiva de las plantas (Filogenia), que se apoya especialmente en la Paleobotánica, el estudio de las plantas que vivieron en otras eras geológicas y en la evolución, que ilustra sobre las leyes y las causas que rigen la formación de las estirpes vegetales.[26]​[28]​ Finalmente, dentro de la botánica existen ramas de estudio que se ocupan de modo especial de grupos particulares de organismos, como la Microbiología (que estudia los microorganismos en general, incluyendo muchos de los que se consideran organismos vegetales), la Bacteriología (que se ocupa de las bacterias), la Micología (que estudia los hongos), la Ficología (que estudia las algas), la Liquenología (estudio de los líquenes), la Briología (estudio de los briófitos: los musgos y las hepáticas), la Pteridología (estudio de los helechos).[29]​[3]​También existen distintas disciplinas aplicadas, que estudian el valor práctico de las plantas para los seres humanos y con ello establecen el enlace con la Agricultura, la Silvicultura y la Farmacia, entre otras. Como ejemplo de estas disciplinas se pueden mencionar el Mejoramiento Genético de Plantas —o fitomejoramiento— (estudia la variabilidad genética y la selección de plantas), la Fitopatología (se ocupa de las enfermedades de las plantas y de los métodos de control de las mismas), la Farmacognosia (estudia las plantas medicinales y sus principios activos).[26]​[30]​ Historia Esta sección es un extracto de Historia de la botánica.[editar] Busto de Teofrasto, considerado como el padre de la botánica. La historia de la botánica es la exposición y narración de las ideas, investigaciones y obras relacionadas con la descripción, clasificación, funcionamiento, distribución y relaciones de los organismos pertenecientes a los reinos Fungi, Chromista y Plantae a través de los diferentes períodos históricos.[n 1]​[n 2]​ Desde la antigüedad, el estudio de los vegetales se ha abordado con dos aproximaciones bastante diferentes: la teórica y la utilitaria. Desde el primer punto de vista, al que se denomina botánica pura, la ciencia de las plantas se erigió por sus propios méritos como una parte integral de la biología. Desde una concepción utilitaria, por otro lado, la denominada botánica aplicada era concebida como una disciplina subsidiaria de la medicina o de la agronomía. En los diferentes períodos de su evolución una u otra aproximación ha predominado, si bien en sus orígenes —que datan del siglo VIII a. C.— la aproximación aplicada fue la preponderante. La botánica, como muchas otras ciencias, alcanzó la primera expresión definida de sus principios y problemas en la Grecia clásica y, posteriormente, continuó su desarrollo durante la época del Imperio romano.[34] Teofrasto, discípulo de Aristóteles y considerado el «padre de la botánica», legó dos obras importantes que se suelen señalar como el origen de esta ciencia: De historia plantarum [Historia de las plantas] y De causis plantarum [Sobre las causas de las plantas].[35] Los romanos contribuyeron poco a los fundamentos de la botánica, pero hicieron una gran contribución al conocimiento de la botánica aplicada a la agricultura.[36] El enciclopedista romano Plinio el Viejo aborda las plantas en los libros XII a XXVI de sus 37 volúmenes de Naturalis Historia.[37]​ Se estima que en la época del imperio romano entre 1300 y 1400 plantas se habían registrado en el oeste.[38] Tras la caída del Imperio en el siglo V, todas las conquistas alcanzadas en la antigüedad clásica tuvieron que redescubrirse a partir del siglo XII, por perderse o ignorarse buena parte de ellas durante la alta Edad Media. La tradición conservadora de la Iglesia y la labor de contadas personalidades hicieron avanzar, aunque muy lentamente, el conocimiento de los vegetales durante este período.[39]​ En los siglos XV y XVI la botánica se desarrolló como una disciplina científica, separada de la herboristería y de la Medicina, si bien continuó contribuyendo a ambas. Diversos factores permitieron el desarrollo y progreso de la botánica durante esos siglos: la invención de la imprenta, la aparición del papel para la elaboración de los herbarios, y el desarrollo de los jardines botánicos, todo ello unido al desarrollo del arte y ciencia de la navegación que permitió la realización de expediciones botánicas. Todos estos factores conjuntamente supusieron un incremento notable en el número de las especies conocidas y permitieron la difusión del conocimiento local o regional a una escala internacional.[40]​[41]​ Impulsada por las obras de Galileo, Kepler, Bacon y Descartes, en el siglo XVII se originó la ciencia moderna. Debido a la creciente necesidad de los naturalistas europeos de intercambiar ideas e información, se comenzaron a fundar las primeras academias científicas. Joachim Jungius fue el primer científico que combinó una mentalidad entrenada en la filosofía con observaciones exactas de las plantas. Tenía la habilidad de definir los términos con exactitud y, por ende, de reducir el uso de términos vagos o arbitrarios en la sistemática. Se lo considera el fundador del lenguaje científico, el que fue desarrollado más tarde por el inglés John Ray y perfeccionado por el sueco Carlos Linneo.[42]​ A Linneo se le atribuyen varias innovaciones centrales en la taxonomía. En primer lugar, la utilización de la nomenclatura binomial de las especies en conexión con una rigurosa caracterización morfológica de las mismas. En segundo lugar, el uso de una terminología exacta. Basado en el trabajo de Jungius, Linneo definió con precisión varios términos morfológicos que serían utilizados en sus descripciones de cada especie o género, en particular aquellos relacionados con la morfología floral y con la morfología del fruto. No obstante, el mismo Linneo notó las fallas de su sistema y buscó en vano nuevas alternativas. Su concepto de la constancia de cada especie fue un obstáculo obvio para lograr establecer un sistema natural ya que esa concepción de la especie negaba la existencia de las variaciones naturales, las cuales son esenciales para el desarrollo de un sistema natural. Esta contradicción permaneció durante mucho tiempo y no fue resuelta hasta 1859 con la obra de Charles Darwin.[42] Durante los siglos XVII y XVIII también se originaron dos disciplinas científicas que, a partir de ese momento, iban a tener una profunda influencia en el desarrollo de todos los ámbitos de la botánica: la anatomía y la fisiología vegetal. Las ideas esenciales de la teoría de la evolución por selección natural de Darwin influirían notablemente en la concepción de la clasificación de los vegetales. De ese modo, aparecieron las clasificaciones filogenéticas, basadas primordialmente en las relaciones de proximidad evolutiva entre las distintas especies, reconstruyendo la historia de su diversificación desde el origen de la vida en la Tierra hasta la actualidad. El primer sistema admitido como filogenético fue el contenido en el Syllabus der Planzenfamilien (1892) de Adolf Engler y conocido más tarde como sistema de Engler cuyas numerosas adaptaciones posteriores han sido la base de un marco universal de referencia según el cual se han ordenado (y se siguen ordenando) muchos tratados de floras y herbarios de todo el mundo, si bien algunos de sus principios para interpretar el proceso evolutivo en las plantas han sido abandonados por la ciencia moderna. Los siglos XIX y XX han sido particularmente fecundos en las investigaciones botánicas, las que han llevado a la creación de numerosas disciplinas como la ecología, la geobotánica, la citogenética y la biología molecular y, en las últimas décadas, a una concepción de la taxonomía basada en la filogenia y en los análisis moleculares de ADN y a la primera publicación de la secuencia del genoma de una angiosperma: Arabidopsis thaliana.[44]​[45]​ La botánica moderna (desde 1945) Esta sección es un extracto de Botánica moderna.[editar] La botánica moderna es una ciencia que considera una gran cantidad de nuevos conocimientos en la actualidad que han sido generados por el estudio de las plantas modelo y sobre la botánica actual, en concreto, ésta comenzó desde 1945. Arabidopsis thaliana motivó a los biólogos actuales a estudiar a fondo este tipo de plantas, esta mala hierba fue una de las primeras plantas en ver su genoma secuenciado. Otros más importantes comercialmente como alimentos básicos como el arroz, trigo, maíz, cebada, centeno, mijo y la soja están teniendo también sus secuencias del genoma. Algunas de éstas son un reto puesto que tienen en sus secuencias más de dos juegos de cromosomas haploides, una condición conocida como poliploidía, común en el reino vegetal. Un alga verde Chlamydomonas reinhardtii (un célula, sola, verde alga) es otro organismo modelo importante que ha sido extensivamente estudiado y provee importantes conocimientos a la biología celular. Significado de la botánica como ciencia Los distintos grupos de vegetales participan de manera fundamental en los ciclos de la biosfera. Las plantas y algas son los productores primarios, responsables de la captación de energía solar de la que depende la mayoría de la vida terrestre, de la creación de materia orgánica y también, como subproducto, de la generación del oxígeno que inunda la atmósfera y causa que casi todos los organismos saquen ventaja del metabolismo aerobio.[46]​ Alimentación humana Casi todo lo que comemos proviene de las plantas, ya sea consumiéndolas directamente (frutas, verduras hortalizas), como indirectamente a través del ganado que se alimenta con plantas que componen el forrajeras. Por lo tanto, las plantas son la base de toda la cadena alimentaria, o lo que los ecólogos llaman el primer nivel trófico. El estudio de las plantas y las técnicas de mejoramiento para producir alimentos son claves para ser capaces de alimentar al mundo y proporcionar una seguridad alimentaria para las generaciones futuras.[47] No obstante, como todas las plantas no son beneficiosas para este fin, la botánica también estudia las especies consideradas nocivas para la agricultura. También estudia los patógenos (fitopatología) que afectan al reino vegetal y la interacción de los humanos con este reino (etnobotánica). Procesos biológicos fundamentales Las plantas son susceptibles de ser estudiadas en sus procesos fundamentales (como la división celular y síntesis proteica por ejemplo), pero sin los problemas éticos que supone estudiar animales o seres humanos. Las leyes de la herencia fueron descubiertas de esta manera por Gregor Mendel, que estudió cómo se hereda la morfología del guisante. Las leyes descubiertas por Mendel a partir del estudio de plantas han conocido desarrollos posteriores, y se han aplicado sobre las propias plantas para conseguir nuevas variedades beneficiosas. Otro estudio clásico efectuado en plantas fue el realizado por Bárbara McClintock, quien descubrió los 'genes saltarines' (o transposones) estudiando el maíz. Son ejemplos que muestran cómo la botánica ha tenido una importancia capital para el entendimiento de los procesos biológicos fundamentales. Aplicaciones de las plantas Muchas de nuestras medicinas y drogas, como el cannabis, vienen directamente del reino vegetal. Otros productos medicinales se derivan de sustancias de origen vegetal; así, la aspirina es un derivado del ácido salicílico, que originalmente se obtenía de la corteza de sauce. La investigación sobre productos farmacéuticamente útiles en las plantas es un campo activo de trabajo que rinde buenos resultados. Estimulantes populares como el café (por su contenido en cafeína), el chocolate, el tabaco (por la nicotina), y el té tienen origen vegetal. Muchas bebidas alcohólicas derivan de la fermentación de plantas como la cebada, el maíz y la uva. Las plantas también nos proveen de muchos materiales, como el algodón, la madera, el papel, el lino, el aceite vegetal, algunos tipos de cuerdas y plásticos. La producción de seda no sería posible sin el cultivo de los árboles de morera. La caña de azúcar y otras plantas han sido recientemente usadas como biomasa para producir una energía renovable alternativa al combustible fósil. Entendimiento de cambios ambientales Las plantas también pueden ayudar al entendimiento de los cambios del medio ambiente de muchas formas. Entendimiento de la destrucción de hábitat y de especies en extinción depende de un catálogo completo y exacto de plantas, de la sistemática y taxonomía. Respuesta de las plantas a radiación ultravioleta puede monitorear problemas como los agujeros en la capa de ozono. El análisis de polen depositado por plantas en miles de millones de años atrás puede ayudar a los científicos a reconstruir los climas del pasado y pronosticar el futuro, una parte esencial de investigaciones sobre cambios climáticos. Recopilar y analizar el tiempo del ciclo de vida es importante para la fenología usado para la investigación de cambios climáticos. Líquenes, sensibles a las condiciones atmosféricas, tienen un uso extensivo como indicadores de contaminación. Las plantas pueden servir como ‘sensores’, una especie de “señales tempranas de aviso” que den la alerta sobre cambios importantes en el ambiente. Por último, las plantas son sumamente valoradas en el aspecto recreativo para millones de personas que disfrutan de su uso en la jardinería, la horticultura y el arte culinario. Disciplinas Subdisciplinas de la botánica Anatomía vegetal u organografía Botánica aplicada Botánica marina Botánica pura o general Botánica sistemática Dendrología Ecología vegetal Ficología Fisiología vegetal geobotánica Histología vegetal Morfología vegetal Paleobotánica Palinología Sistemática vegetal Disciplinas relacionadas Agricultura Agronomía Bioquímica y fitoquímica Ecología Etnobotánica fitoterapia Fitopatología Fitosociología Genética Horticultura Micología Microbiología Métodos de la botánica Herbario Artículo principal: Herbario Secado de especímenes en un herbario de Burkina Faso. Un herbario (del latín herbarium) es una colección de plantas o partes de plantas, preservadas, casi siempre a través de la desecación, procesadas para su conservación, e identificadas, y acompañadas de información importante, como nombre científico y nombre común, utilidad, características de la planta en vivo y del sitio de muestreo, así como la ubicación del punto donde se colectó. Estas plantas se conservan indefinidamente, y constituyen un banco de información que representa la flora o vegetación de una región determinada en un espacio reducido. Estos especímenes se usan con frecuencia como material de referencia para definir el taxón de una planta; pues contienen los holotipos para estas plantas. El tipo nomenclatural o, simplemente, tipo es un ejemplar de una dada especie sobre el que se ha realizado la descripción de la misma y que, de ese modo, valida la publicación de un nombre científico basado en él. El tipo del nombre de una especie es por lo general el espécimen de herbario (o pliego de herbario) a partir del cual se ha perfilado la descripción que valida el nombre. El tipo del nombre de un género es la especie sobre la cual se basó la descripción original que validaba el nombre. El tipo del nombre de una familia es el género sobre el cual fue basada la descripción original válida. En los nombres de taxones de rango superior al de familia no se aplica el principio de tipificación.[48]​ Jardín botánico Artículo principal: Jardín botánico Jardín Botánico de Curitiba. Los jardines botánicos (del latín hortus botanicus) son instituciones habilitadas por un organismo público, privado o asociativo (en ocasiones la gestión es mixta) cuyo objetivo es el estudio, la conservación y divulgación de la diversidad vegetal. Se caracterizan por exhibir colecciones científicas de plantas vivas, que se cultivan para conseguir alguno de estos objetivos: su conservación, investigación, divulgación y enseñanza. En los jardines botánicos se exponen plantas originarias de todo el mundo, generalmente con el objetivo de fomentar el interés de los visitantes hacia el mundo vegetal, aunque algunos de estos jardines se dedican, exclusivamente, a determinadas plantas y a especies concretas. Código Internacional de Nomenclatura para algas, hongos y plantas Estos párrafos son un extracto de Código Internacional de Nomenclatura para algas, hongos y plantas.[editar] El Código Internacional de Nomenclatura para algas, hongos y plantas (ICN)[49] es el compendio de reglas que rigen la nomenclatura taxonómica de los organismos tradicionalmente estudiados por la botánica (plantas, algas y hongos) a efectos de determinar, para cada taxón, un único nombre válido internacionalmente. Hasta el año 2011, con la celebración del XVIII Congreso Internacional de Botánica en Melbourne (Australia), se denominaba Código Internacional de Nomenclatura Botánica (en inglés, ICBN, en español CINB)."
ksampletext_wikipedia_biol_bioquimica: str = "Bioquímica. La bioquímica es una rama de la ciencia que estudia la composición química de los seres vivos, especialmente las proteínas, carbohidratos, lípidos y ácidos nucleicos, además de otras pequeñas moléculas presentes en las células y las reacciones químicas que sufren estos compuestos, como en el metabolismo que les permiten obtener energía (catabolismo) y generar biomoléculas propias (anabolismo). La bioquímica se basa en el concepto de que todo ser vivo contiene carbono y en general las moléculas biológicas están compuestas principalmente de carbono, hidrógeno, oxígeno, nitrógeno, fósforo y azufre. Es la rama de la ciencia que estudia la base química de las moléculas que componen algunas células y los tejidos, que catalizan las reacciones químicas del metabolismo celular como la digestión, la fotosíntesis y la inmunidad, entre otras muchas cosas. Podemos entender la bioquímica como una disciplina científica integradora que elabora el estudio de los biomas y biosistemas. Integra de esta forma las leyes químico-físicas y la evolución biológica que afectan a los biosistemas y a sus componentes. Lo hace desde un punto de vista molecular y trata de entender y aplicar su conocimiento a amplios sectores de la medicina (terapia genética y biomedicina), la agroalimentación, la farmacología. Constituye un pilar fundamental de la biotecnología, y se ha consolidado como una disciplina esencial para abordar los grandes problemas y enfermedades actuales y del futuro, tales como el cambio climático, la escasez de recursos agroalimentarios ante el aumento de población mundial, el agotamiento de las reservas de combustibles fósiles, la aparición de nuevas alergias, el aumento del cáncer, las enfermedades genéticas, la obesidad, etc. La bioquímica es una ciencia experimental y por ello recurrirá al uso de numerosas técnicas instrumentales propias y de otros campos, pero la base de su desarrollo parte del hecho de que lo que ocurre en vivo a nivel subcelular se mantiene o se conserva tras el fraccionamiento subcelular, y a partir de ahí, podemos estudiarlo. Historia Siglo XIX y primera mitad del XX La historia de la bioquímica como la conocemos hoy en día es prácticamente moderna; desde el siglo XIX se comenzó a direccionar una buena parte de la biología y la química a la creación de una nueva disciplina integradora: la química fisiológica o la bioquímica. Pero la aplicación de la bioquímica y su conocimiento probablemente comenzó hace 5000 años, con la producción de pan usando levaduras, en un proceso conocido como fermentación. Es difícil abordar la historia de la bioquímica, en cuanto que, es una mezcla compleja de química orgánica y biología, y en ocasiones, se hace complicado discernir entre lo exclusivamente biológico y lo exclusivamente químico orgánico y es evidente que la contribución a esta disciplina ha sido muy extensa. Aunque es cierto que existen datos experimentales que son básicos en la bioquímica. Se suele situar el inicio de la bioquímica en los descubrimientos en 1828 de Friedrich Wöhler que publicó un artículo acerca de la síntesis de urea, probando que los compuestos orgánicos pueden ser creados artificialmente, en contraste con la creencia comúnmente aceptada durante mucho tiempo, de que la generación de estos compuestos era posible solo en el interior de los seres vivos. La diastasa fue la primera enzima descubierta. En 1833 se extrajo de la solución de malta por Anselme Payen y Jean-François Persoz, dos químicos de una fábrica de azúcar francesa.[1]​ A mediados del siglo XIX, Louis Pasteur demostró los fenómenos de isomería química existente entre las moléculas de ácido tartárico provenientes de los seres vivos y las sintetizadas químicamente en el laboratorio. También estudió el fenómeno de la fermentación y descubrió que intervenían ciertas levaduras, y por tanto no era exclusivamente un fenómeno químico como se había defendido hasta ahora (entre ellos el propio Liebig); así Pasteur escribió: «la fermentación del alcohol es un acto relacionado con la vida y la organización de las células de las levaduras, y no con la muerte y la putrefacción de las células». Además desarrolló un método de esterilización de la leche, el vino y la cerveza (pasteurización) y contribuyó enormemente a refutar la idea de la generación espontánea de los seres vivos. En 1869 se descubre la nucleína y se observa que es una sustancia muy rica en fósforo. Dos años más tarde, Albrecht Kossel concluye que la nucleína es rica en proteínas y contiene las bases púricas adenina y guanina y las pirimidínicas citosina y timina. En 1889 se aíslan los dos componentes mayoritarios de la nucleína: Proteínas (70 %) Sustancias de carácter ácido: ácidos nucleicos (30 %) En 1878 el fisiólogo Wilhelm Kühne acuñó el término enzima para referirse a los componentes biológicos desconocidos que producían la fermentación. La palabra enzima fue usada después para referirse a sustancias inertes tales como la pepsina. En 1897 Eduard Buchner comenzó a estudiar la capacidad de los extractos de levadura para fermentar azúcar a pesar de la ausencia de células vivientes de levadura. En una serie de experimentos en la Universidad Humboldt de Berlín, encontró que el azúcar era fermentado incluso cuando no había elementos vivos en los cultivos de células de levaduras. Llamó a la enzima que causa la fermentación de la sacarosa, “zimasa”. Al demostrar que las enzimas podrían funcionar fuera de una célula viva, el siguiente paso fue demostrar cuál era la naturaleza bioquímica de esos biocatalizadores. El debate fue extenso; muchos, como el bioquímico alemán Richard Willstätter, discrepaban de que la proteína fuera el catalizador enzimático, hasta que en 1926, James B. Sumner demostró que la enzima ureasa era una proteína pura y la cristalizó. La conclusión de que las proteínas puras podían ser enzimas fue definitivamente probada en torno a 1930 por John Howard Northrop y Wendell Meredith Stanley, quienes trabajaron con diversas enzimas digestivas como la pepsina, la tripsina y la quimotripsina. En 1903 Mijaíl Tswett inicia los estudios de cromatografía para separación de pigmentos. En torno a 1915 Gustav Embden y Otto Meyerhof realizan sus estudios sobre la glucólisis. En 1920 se descubre que en las células hay ADN y ARN y que difieren en el azúcar que forma parte de su composición: desoxirribosa o ribosa. El ADN reside en el núcleo. Unos años más tarde, se descubre que en los espermatozoides hay fundamentalmente ADN y proteínas, y posteriormente Feulgen descubre que hay ADN en los cromosomas con su tinción específica para este compuesto. En 1925 Theodor Svedberg demuestra que las proteínas son macromoléculas y desarrolla la técnica de ultracentrifugación analítica. En 1928, Alexander Fleming descubre la penicilina y desarrolla estudios sobre la lisozima. Richard Willstätter (en torno 1910) estudia la clorofila y comprueba la similitud que hay con la hemoglobina. Posteriormente Hans Fischer en torno a 1930, investiga la química de las porfirinas de las que derivan la clorofila o el grupo porfirínico de la hemoglobina. Consiguió sintetizar hemina y bilirrubina. Paralelamente Heinrich Otto Wieland formula teorías sobre las deshidrogenaciones y explica la constitución de muchas otras sustancias de naturaleza compleja, como la pteridina, las hormonas sexuales o los ácidos biliares. En la década de 1940, Melvin Calvin concluye el estudio del ciclo de Calvin en la fotosíntesis y Albert Claude la síntesis del ATP en las mitocondrias. En torno a 1945 Gerty Cori, Carl Cori, y Bernardo Houssay completan sus estudios sobre el ciclo de Cori. En 1953 James Dewey Watson y Francis Crick, gracias a los estudios previos con cristalografía de rayos X de ADN de Rosalind Franklin y Maurice Wilkins, y los estudios de Erwin Chargaff sobre apareamiento de bases nitrogenadas, deducen la estructura de doble hélice del ADN. En 1957, Matthew Meselson y Franklin Stahl demuestran que la replicación del ADN es semiconservativa. Segunda mitad del siglo XX En la segunda mitad del siglo XX, comienza la auténtica revolución de la bioquímica y la biología molecular moderna, especialmente gracias al desarrollo de las técnicas experimentales más básicas como la cromatografía, la centrifugación, la electroforesis, las técnicas radioisotópicas y la microscopía electrónica, y las técnicas más complejas como la cristalografía de rayos X, la resonancia magnética nuclear, la PCR (Kary Mullis), el desarrollo de la inmuno-técnicas. Desde 1950 a 1975 , se conocen en profundidad y detalle aspectos del metabolismo celular inimaginables hasta ahora (fosforilación oxidativa (Peter Dennis Mitchell), ciclo de la urea y ciclo de Krebs (Hans Adolf Krebs), así como otras rutas metabólicas), se produce toda una revolución en el estudio de los genes y su expresión; se descifra el código genético (Francis Crick, Severo Ochoa, Har Gobind Khorana, Robert W. Holley y Marshall Warren Nirenberg), se descubren las enzimas de restricción (finales de 1960, Werner Arber, Daniel Nathans y Hamilton Smith), la ADN ligasa (en 1972, Mertz y Davis) y finalmente en 1973 Stanley Cohen y Herbert Boyer producen el primer ser vivo recombinante, naciendo así la ingeniería genética, convertida en una herramienta poderosísima con la que se supera la frontera entre especies y con la que podemos obtener un beneficio hasta ahora impensable. En 1970, un argentino, Luis Federico Leloir, médico, bioquímico y farmacéutico recibió el Premio Nobel de Química por sus investigaciones sobre los nucleótidos de azúcar, y el rol que cumplen en la fabricación de los hidratos de carbono.[2]​ En 1984, otro argentino, César Milstein, oriundo de la ciudad de Bahía Blanca, recibe el Premio Nobel de Medicina por sus investigaciones sobre anticuerpos monoclonales, hoy utilizados para tratar muchas enfermedades, incluidos algunos tipos de cáncer.[3]​ De 1975 hasta principios del siglo XXI, comienza a secuenciarse el ADN (Allan Maxam, Walter Gilbert y Frederick Sanger), comienzan a crearse las primeras industrias biotecnológicas (Genentech), se aumenta la creación de fármacos y vacunas más eficaces, se eleva el interés por las inmunología y las células madres y se descubre la enzima telomerasa (Elizabeth Blackburn y Carol Greider). En 1989 se utiliza la biorremediación a gran escala en el derrame del petrolero Exxon Valdez en Alaska. Se clonan los primeros seres vivos, se secuencia el ADN de decenas de especies y se publica el genoma completo del hombre (Craig Venter, Celera Genomics y Proyecto Genoma Humano), se resuelven decenas de miles de estructuras proteicas y se publican en PDB, así como genes, en GenBank. Comienza el desarrollo de la bioinformática y la computación de sistemas complejos, que se constituyen como herramientas muy poderosas en el estudio de los sistemas biológicos. Se crea el primer cromosoma artificial y se logra la primera bacteria con genoma sintético (2007, 2009, Craig Venter). Se fabrican las nucleasas con dedos de zinc. Se inducen artificialmente células, que inicialmente no eran pluripotenciales, a células madre pluripotenciales (Shin'ya Yamanaka). Comienzan a darse los primeros pasos. Ramas de la bioquímica Esquema de una célula típica animal con sus orgánulos y estructuras. El pilar fundamental de la investigación bioquímica clásica se centra en las propiedades de las proteínas, muchas de las cuales son enzimas. Sin embargo, existen otras disciplinas que se centran en las propiedades biológicas de carbohidratos (glucobiología)[4] y lípidos (lipobiología).[5]​ Por razones históricas la bioquímica del metabolismo de la célula ha sido intensamente investigada, en importantes líneas de investigación actuales (como el Proyecto Genoma, cuya función es la de identificar y registrar todo el material genético humano), se dirigen hacia la investigación del ADN, el ARN, la síntesis de proteínas, la dinámica de la membrana celular y los ciclos energéticos. Las ramas de la bioquímica son muy amplias y diversas, y han ido variando con el tiempo y los avances de la biología, la química y la física. Bioquímica estructural: es un área de la bioquímica que pretende comprender la arquitectura química de las macromoléculas biológicas, especialmente de las proteínas y de los ácidos nucleicos (ADN y ARN). Así se intenta conocer las secuencias peptídicas, su estructura y conformación tridimensional, y las interacciones físico-químicas atómicas que posibilitan a dichas estructuras. Uno de sus máximos retos es determinar la estructura de una proteína conociendo solo la secuencia de aminoácidos, que supondría la base esencial para el diseño racional de proteínas (ingeniería de proteínas).[6]​ Ciencia que estudia la estructura, propiedades físicas, la reactividad y transformación de los compuestos orgánicos. Química Orgánica Química orgánica: es un área de la química que se encarga del estudio de los compuestos orgánicos (es decir, aquellos que tienen enlaces covalentes carbono-carbono o carbono-hidrógeno) que provienen específicamente de seres vivos. Se trata de una ciencia íntimamente relacionada con la bioquímica clásica, ya que en la mayoría de los compuestos biológicos participa el carbono Mientras que la bioquímica clásica ayuda a comprender los procesos biológicos con base en conocimientos de estructura, enlace químico, interacciones moleculares y reactividad de las moléculas orgánicas, la química bioorgánica intenta integrar los conocimientos de síntesis orgánica, mecanismos de reacción, análisis estructural y métodos analíticos con las reacciones metabólicas primarias y secundarias, la biosíntesis, el reconocimiento celular y la diversidad química de los organismos vivos. De allí surge la Química de Productos Naturales (V. Metabolismo secundario).[10]​ Enzimología: estudia el comportamiento de los catalizadores biológicos o enzimas, como son algunas proteínas y ciertos ARN catalíticos, así como las coenzimas y cofactores como metales y vitaminas. Así se cuestiona los mecanismos de catálisis, los procesos de interacción de las enzimas-sustrato, los estados de transición catalíticos, las actividades enzimáticas, la cinética de la reacción y los mecanismos de regulación y expresión enzimáticas, todo ello desde un punto de vista bioquímico. Estudia y trata de comprender los elementos esenciales del centro activo y de aquellos que no participan, así como los efectos catalíticos que ocurren en la modificación de dichos elementos; en este sentido, utilizan frecuentemente técnicas como la mutagénesis dirigida.[11]​ Bioquímica metabólica: es un área de la bioquímica que pretende conocer los diferentes tipos de rutas metabólicas a nivel celular, y su contexto orgánico. De esta forma son esenciales conocimientos de enzimología y biología celular. Estudia todas las reacciones bioquímicas celulares que posibilitan la vida, y así como los índices bioquímicos orgánicos saludables, las bases moleculares de las enfermedades metabólicas o los flujos de intermediarios metabólicos a nivel global. De aquí surgen disciplinas académicas como la bioenergética (estudio del flujo de energía en los organismos vivos), la bioquímica nutricional (estudio de los procesos de nutrición asociados a| rutas metabólicas)[12] y la bioquímica clínica (estudio de las alteraciones bioquímicas en estado de enfermedad o traumatismo). La metabolómica es el conjunto de ciencias y técnicas dedicadas al estudio completo del sistema constituido por el conjunto de moléculas que constituyen los intermediarios metabólicos, metabolitos primarios y secundarios, que se pueden encontrar en un sistema biológico. Xenobioquímica: es la disciplina que estudia el comportamiento metabólico de los compuestos cuya estructura química no es propia en el metabolismo regular de un organismo determinado. Pueden ser metabolitos secundarios de otros organismos (por ejemplo las micotoxinas, los venenos de serpientes y los fitoquímicos cuando ingresan al organismo humano) o compuestos poco frecuentes o inexistentes en la naturaleza.[13] La farmacología es una disciplina que estudia a los xenobióticos que benefician al funcionamiento celular en el organismo debido a sus efectos terapéuticos o preventivos (fármacos). La farmacología tiene aplicaciones clínicas cuando las sustancias son utilizadas en el diagnóstico, prevención, tratamiento y alivio de síntomas de una enfermedad así como el desarrollo racional de sustancias menos invasivas y más eficaces contra dianas biomoleculares concretas. Por otro lado, la toxicología es el estudio que identifica, estudia y describe, la dosis, la naturaleza, la incidencia, la severidad, la reversibilidad y, generalmente, los mecanismos de los efectos adversos (efectos tóxicos) que producen los xenobióticos. Actualmente la toxicología también estudia el mecanismo de los componentes endógenos, como los radicales libres de oxígeno y otros intermediarios reactivos, generados por xenobióticos y endobióticos. Inmunología: área de la biología, la cual se interesa por la reacción del organismo frente a otros organismos como las bacterias y virus. Todo esto tomando en cuenta la reacción y funcionamiento del sistema inmune de los seres vivos. Es esencial en esta área el desarrollo de los estudios de producción y comportamiento de los anticuerpos. Endocrinología: es el estudio de las secreciones internas llamadas hormonas, las cuales son sustancias producidas por células especializadas cuyo fin es de afectar la función de otras células. La endocrinología trata la biosíntesis, el almacenamiento y la función de las hormonas, las células y los tejidos que las secretan, así como los mecanismos de señalización hormonal. Existen subdisciplinas como la endocrinología médica, la endocrinología vegetal y la endocrinología animal.[15]​ Neuroquímica: es el estudio de las moléculas orgánicas que participan en la actividad neuronal. Este término es empleado con frecuencia para referir a los neurotransmisores y otras moléculas como las drogas neuro-activas que influencian la función neuronal. Quimiotaxonomía: es el estudio de la clasificación e identificación de organismos de acuerdo a sus diferencias y similitudes demostrables en su composición química. Los compuestos estudiados pueden ser fosfolípidos, proteínas, péptidos, heterósidos, alcaloides y terpenos. John Griffith Vaughan fue uno de los pioneros de la quimiotaxonomía. Entre los ejemplos de las aplicaciones de la quimiotaxonomía pueden citarse la diferenciación de las familias Asclepiadaceae y Apocynaceae según el criterio de la presencia de látex; la presencia de agarofuranos en la familia Celastraceae; las sesquiterpenlactonas con esqueleto de germacrano que son características de la familia Asteraceae o la presencia de abietanos en las partes aéreas de plantas del género Salvia del viejo Mundo a diferencia de las del Nuevo Mundo que presentan principalmente neo-clerodanos.[16]​ Ecología química: es el estudio de los compuestos químicos de origen biológico implicados en las interacciones de organismos vivos. Se centra en la producción y respuesta de moléculas señalizadoras (semioquímicos), así como los compuestos que influyen en el crecimiento, supervivencia y reproducción de otros organismos (aleloquímicos). Virología: área de la biología, que se dedica al estudio de los biosistemas más elementales: los virus. Tanto en su clasificación y reconocimiento, como en su funcionamiento y estructura molecular. Pretende reconocer dianas para la actuación de posibles de fármacos y vacunas que eviten su directa o preventivamente su expansión. También se analizan y predicen, en términos evolutivos, la variación y la combinación de los genomas víricos, que podrían hacerlos finalmente, más peligrosos. Finalmente suponen una herramienta con mucha proyección como vectores recombinantes, y han sido ya utilizados en terapia génica.[17]​ Imagen: Proteína mioglobina Genética molecular e ingeniería genética: es un área de la bioquímica y la biología molecular que estudia los genes, su herencia y su expresión. Molecularmente, se dedica al estudio del ADN y del ARN principalmente, y utiliza herramientas y técnicas potentes en su estudio, tales como la PCR y sus variantes, los secuenciadores masivos, los kits comerciales de extracción de ADN y ARN, procesos de transcripción-traducción in vitro e in vivo, enzimas de restricción, ADN ligasas… Es esencial conocer como el ADN se replica, se transcribe y se traduce a proteínas (Dogma Central de la Biología Molecular), así como los mecanismos de expresión basal e inducible de genes en el genoma. También estudia la inserción de genes, el silenciamiento génico y la expresión diferencial de genes y sus efectos. Superando así las barreras y fronteras entre especies en el sentido que el genoma de una especie podemos insertarlo en otro y generar nuevas especies. Uno de sus máximos objetivos actuales es conocer los mecanismos de regulación y expresión genética, es decir, obtener un código epigenético. Constituye un pilar esencial en todas las disciplinas biocientíficas, especialmente en biotecnología. La biotecnología moderna tiene múltiples aplicaciones y variadas e incluyen, además de la fabricación de medicamentos, alimentos, papel, entre otros, el mejoramiento de animales y plantas de interés agronómico.[18]​ Biología Molecular: es la disciplina científica que tiene como objetivo el estudio de los procesos que se desarrollan en los seres vivos desde un punto de vista molecular. Así como la bioquímica clásica investiga detalladamente los ciclos metabólicos y la integración y desintegración de las moléculas que componen los seres vivos, la biología molecular pretende fijarse con preferencia en el comportamiento biológico de las macromoléculas (ADN, ARN, enzimas, hormonas, etc.) dentro de la célula y explicar las funciones biológicas del ser vivo por estas propiedades a nivel molecular.[19]​ Biología celular: (antiguamente citología, de citos=célula y logos=Estudio o Tratado ) es un área de la biología que se dedica al estudio de la morfología y fisiología de las células procariotas y eucariotas. Trata de conocer sus propiedades, estructura, composición bioquímica, funciones, orgánulos que contienen, su interacción con el ambiente y su ciclo vital. Es esencial en esta área conocer los procesos intrínsecos a la vida celular durante el ciclo celular, como la nutrición, la respiración, la síntesis de componentes, los mecanismos de defensa, la división celular y la muerte celular. También se deben conocer los mecanismos de comunicación de células (especialmente en organismos pluricelulares) o las uniones intercelulares. Es un área esencialmente de observación y experimentación en cultivos celulares, que, frecuentemente, tienen como objetivo la identificación y separación de poblaciones celulares y el reconocimiento de orgánulos celulares. Algunas técnicas utilizadas en biología celular tienen que ver con el empleo de técnicas de citoquímica, siembra de cultivos celulares, observación por microscopía óptica y electrónica, inmunocitoquímica, inmunohistoquímica, ELISA o citometría de flujo."

ksampletext_wikipedia_medi_farmacologia: str = "Farmacología. La farmacología (del griego, pharmacon, fármaco y logos, ciencia) es la rama de las ciencias farmacéuticas que estudia la historia, el origen, las propiedades biofisicoquímicas, la presentación, los efectos fisiológicos, los mecanismos de acción, la absorción, la distribución, la biotransformación, la excreción y el uso terapéutico, entre otras actividades biológicas, de las sustancias químicas que interactúan con los organismos vivos. La farmacología estudia como interactúa el fármaco con el organismo, sus acciones, efectos y propiedades.[2]​[3] En un sentido más estricto, se considera la farmacología como el estudio de los fármacos, sea que esas tengan efectos beneficiosos o bien tóxicos. La farmacología tiene aplicaciones clínicas cuando las sustancias son utilizadas en el diagnóstico, prevención y tratamiento de una enfermedad o para el alivio de sus síntomas. Historia Artículo principal: Historia de la farmacia Los orígenes de la farmacología clínica se remontan a la Edad Media, con la farmacognosia y El canon de medicina de Avicena, el Comentario de Pedro de España sobre Isaac Dager y el Comentario de Juan de San Amand sobre el Antedotario de Nicolás. La farmacología temprana se centró en el herbalismo y las sustancias naturales, principalmente extractos de plantas. Las medicinas fueron compiladas en libros llamados farmacopea. Las drogas crudas se han usado desde la prehistoria como una preparación de sustancias de fuentes naturales. Sin embargo, el ingrediente activo de las drogas crudas no se purifica y la sustancia se adulteraba con otras sustancias. La medicina tradicional varía entre culturas y puede ser específica de una cultura particular, como en la medicina tradicional china, mongol, tibetana y coreana. Sin embargo, gran parte de esto se ha considerado como pseudociencia. Las sustancias farmacológicas conocidas como enteógenos pueden tener un uso espiritual y religioso y un contexto histórico. En el siglo XVII, el médico inglés Nicholas Culpeper tradujo y usó textos farmacológicos, en los cuales detalló las plantas y las condiciones que podrían tratar. En el siglo XVIII, gran parte de la farmacología clínica fue establecida por el trabajo de William Withering. La farmacología como disciplina científica no avanzó más hasta mediados del siglo XIX, en medio del gran resurgimiento biomédico de ese período. Antes de la segunda mitad del siglo XIX, la notable potencia y especificidad de drogas como la morfina y la quinina, se explicaron vagamente y con referencia a poderes químicos extraordinarios y afinidades con ciertos órganos o tejidos. El primer departamento de farmacología fue creado por Rudolf Buchheim en 1847, en reconocimiento de la necesidad de comprender cómo las drogas terapéuticas y los venenos producen sus efectos. Posteriormente, el primer departamento de farmacología en Inglaterra se creó en 1905 en el University College de Londres. La farmacología se desarrolló en el siglo XIX como una ciencia biomédica que aplicaba los principios de la experimentación científica a los contextos terapéuticos. El avance de las técnicas de investigación impulsó la investigación farmacológica y su comprensión. El desarrollo de la preparación del baño de órganos, donde las muestras de tejido están conectadas a dispositivos de registro (como un miógrafo) y las respuestas fisiológicas se registran después de la aplicación del medicamento, permitió el análisis de los efectos de los medicamentos en los tejidos. El desarrollo del ensayo de unión al ligando en 1945, permitió la cuantificación de la afinidad de unión de los fármacos en objetivos químicos. Los farmacólogos modernos utilizan técnicas de genética, biología molecular, bioquímica y otras herramientas avanzadas para transformar información sobre mecanismos moleculares y objetivos en terapias dirigidas contra enfermedades, defectos o patógenos, y crear métodos para la atención preventiva, el diagnóstico y, en última instancia, la medicina personalizada. Divisiones La disciplina de la farmacología se puede dividir en muchas subdisciplinas, cada una con un enfoque específico. Sistemas del cuerpo La farmacología también puede centrarse en sistemas específicos que comprenden el cuerpo. Las divisiones relacionadas con los sistemas corporales estudian los efectos de las drogas en diferentes sistemas del cuerpo. Estos incluyen neurofarmacología, en el sistema nervioso central y periférico; inmunofarmacología en el sistema inmune. Otras divisiones incluyen farmacología cardiovascular, renal y endocrina. Psicofarmacología, es el estudio de los efectos de las drogas en la psique, la mente y el comportamiento, como los efectos conductuales de las drogas psicoactivas. Incorpora enfoques y técnicas de neurofarmacología, comportamiento animal y neurociencia conductual, y está interesado en los mecanismos de acción conductuales y neurobiológicos de las drogas psicoactivas. El campo relacionado de la neuropsicofarmacología se centra en los efectos de las drogas en la superposición entre el sistema nervioso y la psique. La farmacometabolómica, también conocida como farmacometabonómica, es un campo que se deriva de la metabolómica. Consiste en la medición directa de metabolitos en los fluidos corporales de un individuo, con el fin de predecir o evaluar el metabolismo de los compuestos farmacéuticos, y para comprender mejor el perfil farmacocinético de un medicamento.[4] La farmacometabolómica se puede aplicar para medir los niveles de metabolitos después de la administración de un medicamento, con el fin de controlar los efectos del medicamento en las vías metabólicas. También estudia el efecto de las variaciones del microbioma en la disposición, acción y toxicidad del fármaco, así como la interacción entre las drogas y el microbioma intestinal. La farmacogenómica es la aplicación de tecnologías genómicas para el descubrimiento de fármacos y la caracterización adicional de fármacos relacionados con el genoma completo de un organismo. Para la farmacología con respecto a genes individuales, la farmacogenética estudia cómo la variación genética da lugar a diferentes respuestas a los fármacos. La farmacoepigenética estudia los patrones de marcado epigenético subyacentes que conducen a variaciones en la respuesta de un individuo al tratamiento médico. Práctica clínica y descubrimiento de fármacos Un toxicólogo trabajando en un laboratorio. La farmacología se puede aplicar dentro de las ciencias clínicas. La farmacología clínica es la ciencia básica de la farmacología que se centra en la aplicación de principios y métodos farmacológicos en la clínica médica y en la atención y los resultados del paciente. Un ejemplo de esto es la posología, que es el estudio de cómo se dosifican los medicamentos. La farmacología está estrechamente relacionada con la toxicología. Tanto la farmacología como la toxicología son disciplinas científicas que se centran en comprender las propiedades y acciones de los productos químicos. Sin embargo, la farmacología enfatiza los efectos terapéuticos de los químicos, usualmente drogas o compuestos que podrían convertirse en drogas, mientras que la toxicología es el estudio de los efectos adversos de los químicos y la evaluación de riesgos. El conocimiento farmacológico se utiliza para aconsejar farmacoterapia en medicina y farmacia. Destino de los fármacos en el organismo Cualquier sustancia que interactúa con un organismo viviente puede ser absorbida por este, distribuida por los distintos órganos, sistemas o espacios corporales, modificada por procesos químicos y finalmente expulsada. La farmacología estudia los procesos en la interacción de fármacos con el hombre y animales llamados procesos LADME que, en orden temporal, son los siguientes: liberación absorción distribución metabolismo excreción El estudio de estos procesos es lo que se conoce como farmacocinética. De la interacción de todos estos procesos, la farmacología puede predecir la biodisponibilidad y vida media de eliminación de un fármaco en el organismo dadas una vía de administración, una dosis y un intervalo de administración. Para que el fármaco ejerza su acción sobre este blanco, debe, generalmente, ser transportado a través de la circulación sanguínea. Absorción Para llegar a la circulación sanguínea el fármaco debe traspasar alguna barrera dada por la vía de administración, que puede ser: cutánea, subcutánea, respiratoria, oral, rectal, muscular, vía ótica, vía oftálmica, vía sublingual. O puede ser inoculada directamente a la circulación por la vía intravenosa. La farmacología estudia la concentración plasmática de un fármaco en relación con el tiempo transcurrido para cada vía de administración y para cada concentración posible, así como las distintas formas de uso de estas vías de administración. Distribución Una vez en la corriente sanguínea, el fármaco, por sus características de tamaño y peso molecular, carga eléctrica, pH, solubilidad, capacidad de unión a proteínas se distribuye entre los distintos compartimientos corporales. La farmacología estudia cómo estas características influyen en el aumento y disminución de concentración del fármaco con el paso del tiempo en distintos sistemas, órganos, tejidos y compartimientos corporales, como por ejemplo, en el líquido cefalorraquídeo, o en la placenta, etc. Metabolismo o biotransformación Muchos fármacos son transformados en el organismo debido a la acción de enzimas. Esta transformación puede consistir en la degradación; (oxidación, reducción o hidrólisis), donde el fármaco pierde parte de su estructura, o en la síntesis de nuevas sustancias con el fármaco como parte de la nueva molécula (conjugación). El resultado de la biotransformación puede ser la inactivación completa o parcial de los efectos del fármaco, el aumento o activación de los efectos, o el cambio por nuevos efectos dependientes de las características de la sustancia sintetizada. La farmacología estudia los mecanismos mediante los cuales se producen estas transformaciones, los tejidos en que ocurre, la velocidad de estos procesos y los efectos de las propias drogas y sus metabolitos sobre los mismos procesos enzimáticos. Excreción Finalmente, el fármaco es eliminado del organismo por medio de algún órgano excretor. Principalmente está el hígado y el riñón, pero también son importantes la piel, las glándulas salivales y lagrimales. Cuando un fármaco es suficientemente hidrosoluble, es derivado hacia la circulación sanguínea, por la cual llega a los riñones y es eliminado por los mismos procesos de la formación de la orina: filtración glomerular, secreción tubular y reabsorción tubular. Si el fármaco, por el contrario, es liposoluble o de tamaño demasiado grande para atravesar los capilares renales, es excretada en la bilis, llegando al intestino grueso donde puede sufrir de la recirculación enterohepática, o bien ser eliminado en las heces. La farmacología estudia la forma y velocidad de depuración de los fármacos y sus metabolitos por los distintos órganos excretores, en relación con las concentraciones plasmáticas del fármaco. El efecto de los fármacos, después de su administración, depende de la variabilidad en la absorción, distribución, metabolismo y excreción. Para que el fármaco alcance su sitio de acción, han de considerarse los siguientes factores: Tasa y grado de absorción a partir del sitio de aplicación. Tasa y grado de distribución en los líquidos y tejidos corporales. Tasa de biotransformación a metabolitos activos o inactivos. Tasa de excreción. Acción de los fármacos sobre el organismo Al estudio del conjunto de efectos sensibles y/o medibles que produce un fármaco en el organismo del ser humano o los animales, su duración y el curso temporal de ellos, se denomina farmacodinámica. Para este estudio, la farmacología entiende al sistema, órgano, tejido o célula destinatario del fármaco u objeto de la sustancia en análisis, como poseedor de receptores con los cuales la sustancia interactúa. La interacción entre sustancia y receptor es un importante campo de estudio, que entre otros aspectos, analiza: Cuantificación de la interacción droga/receptor. Regulación de los receptores, ya sea al aumento, disminución o cambio en el nivel de respuesta. Relación entre dosis y respuesta. La farmacodinámica, define y clasifica a los fármacos de acuerdo con su afinidad, potencia, eficacia y efectos relativos. Algunos de los índices importantes de estas definiciones son la DE50 y la DL50, que son las dosis mínimas necesarias para lograr el efecto deseado y la muerte respectivamente, en el 50% de una población determinada. La relación entre estos valores es el índice terapéutico. De acuerdo con el tipo de efecto preponderante de un fármaco, farmacodinámicamente se les clasifica en: Agonistas farmacológicos, si produce o aumenta el efecto. Antagonistas farmacológicos, si disminuye o elimina el efecto. La farmacodinámica estudia también la variabilidad en los efectos de una sustancia dependientes de factores del individuo tales como: edad, raza, gravidez, estados patológicos, etc. También existe un campo especial de estudio de los efectos farmacológicos de sustancias durante la gestación. En el ser humano, los efectos sobre el embrión y el feto de los fármacos es un campo de intenso estudio. Ramas de la farmacología Farmacocinética: el estudio de los procesos físico-químicos que sufre un fármaco cuando se administra o incorpora a un organismo. Estos procesos serían liberación, absorción, distribución, metabolización y eliminación. Farmacodinámica: ciencia que estudia el mecanismo de acción de los fármacos, es decir estudia como los procesos bioquímicos y fisiológicos dentro del organismo se ven afectados por la presencia del fármaco. Biofarmacia: el estudio de la biodisponibilidad de los fármacos. Farmacognosia: estudio de plantas medicinales y drogas que de ellas se derivan. Química farmacéutica: estudia los fármacos desde el punto de vista químico, lo que comprende el descubrimiento, el diseño, la identificación y preparación de compuestos biológicamente activos, la interpretación de su modo de interacción a nivel molecular, la construcción de su relación estructura-actividad y el estudio de su metabolismo. Farmacia galénica o Farmacotecnia: rama encomendada a la formulación de fármacos como medicamentos. Posología: el estudio de la dosificación de los fármacos. Toxicología: el estudio de los efectos nocivos o tóxicos de los fármacos. Farmacología clínica: evalúa la eficacia y la seguridad de la terapéutica por fármacos. Farmacovigilancia: es una disciplina que permite la vigilancia postcomercialización de los medicamentos a fin de detectar, prevenir y notificar reacciones adversas en grupos de pacientes. Cronofarmacología: El estudio de la correcta administración de medicamentos conforme al ciclo circadiano del ser humano, esto con el fin de maximizar la eficacia y disminuir los efectos colaterales. Margen e índice terapéutico Es un hecho práctico de todos conocido que al incrementar la dosis de un determinado fármaco, se incrementa el riesgo de producción de fenómenos tóxicos o adversos. Para evitar tal situación, los farmacólogos experimentales y clínicos hacen una evaluación de la seguridad del fármaco, con el fin de garantizar que con la dosis empleada se logre el efecto farmacológico deseado con reducción de riesgos de intoxicación. La evaluación más simple y sencilla es la conocida como Margen Terapéutico, que es el margen de dosis que oscila entre la dosis mínima y la dosis máxima terapéutica. De lo anterior se deriva que se puede dosificar un medicamento dentro de este margen, no teniendo sentido alguno el administrar una dosis superior a la máxima terapéutica, ya que con ella no obtendríamos un efecto superior, y nos acercamos a aquella dosis que puede ser tóxicas."
ksampletext_wikipedia_medi_epidemiologia: str = "Epidemiología. La epidemiología es una disciplina científica en el área de la salud pública, no solamente la medicina, que estudia la distribución, frecuencia, magnitud y factores determinantes de las enfermedades existentes en poblaciones humanas definidas. Rich la describió en 1979 como la ciencia que estudia la dinámica de salud en las poblaciones; por lo tanto involucra el análisis e interpretación de las personas que también están sanas.. Quien trabaja como profesional con especialidad en epidemiología se llama epidemiólogo/epidemióloga.[1]​ Principios La epidemiología —que, en sentido estricto, podría denominarse epidemiología humana— constituye una parte muy importante dentro de la salud pública,[1] ocupa un lugar especial en la intersección entre las ciencias biomédicas y las ciencias sociales, e integra los métodos y principios de estas ciencias para estudiar la salud y controlar las enfermedades en grupos humanos bien definidos.[2] Existe también una epidemiología veterinaria, que estudia los mismos aspectos en los padecimientos que afectan la salud de los animales; y también podría hablarse de una epidemiología zoológica y botánica, íntimamente relacionadas con la ecología. En epidemiología se estudian y describen las enfermedades que se presentan en una determinada población, para lo cual se tienen en cuenta una serie de patrones de enfermedad, que se reducen a tres aspectos: tiempo, lugar y persona: el tiempo que tarda en surgir, la temporada del año en la que surge y los tiempos en los que es más frecuente; el lugar (la ciudad, la población, el país, el tipo de zona) en donde se han presentado los casos, y las personas más propensas a padecerla (niños, ancianos, etc., según el caso). La epidemiología surgió del estudio de las epidemias de enfermedades infecciosas; de ahí su nombre. Ya en el siglo XX los estudios epidemiológicos se extendieron también a las enfermedades no infecciosas. Para el análisis adecuado de la información epidemiológica se requiere cada vez con mayor frecuencia un equipo multidisciplinario que prevea la participación de profesionales de otros ámbitos científicos, entre los cuales la demografía y la estadística son especialmente importantes. Ciencia Para causar una enfermedad, un patógeno debe crecer y reproducirse en el hospedador. Los epidemiólogos siguen por esta razón, la historia natural de los patógenos. En muchos casos, un patógeno individual no puede crecer fuera del hospedador; si el hospedador muere, el patógeno también muere. Asimismo, los patógenos que matan al hospedador antes de trasmitirlos a otro hospedador, terminarán por extinguirse. Por tanto, la mayoría de los patógenos dependientes del hospedador deben adaptarse a coexistir con el hospedador. Un patógeno bien adaptado vive en equilibrio con el hospedador, tomando lo que necesita para su existencia, y causando solo un mínimo de daño. Estos patógenos a veces pueden causar infecciones crónicas (infecciones de larga duración) en el hospedador. Cuando existe equilibrio entre el hospedador y el patógeno, ambos sobreviven. Por otra parte, el hospedador puede resultar dañado cuando su resistencia es baja, por factores como una dieta insuficiente, edad avanzada y otros agentes estresantes. Además, algunas veces emergen nuevos patógenos naturales para los cuales el hospedador individual, y algunas veces la especie entera, no ha desarrollado resistencia. Estos patógenos emergentes a menudo causan infecciones agudas, caracterizadas por un comienzo rápido y llamativo. En estos casos, los patógenos pueden actuar como fuerzas selectivas en la evolución del hospedador, igual que el hospedador, al desarrollar resistencia, puede ser una fuerza selectiva en la evolución de los patógenos. En los casos en los que el patógeno no depende del hospedador para sobrevivir, con frecuencia el patógeno puede causar una enfermedad aguda devastadora.[3]​ Objetivos La epidemiología es parte importante de la salud pública y contribuye a: Definir los problemas e inconvenientes de salud importantes de una comunidad; Describir la historia natural de una enfermedad; Descubrir los factores que aumentan el riesgo de contraer una enfermedad (su etiología); Predecir las tendencias de una enfermedad; Determinar si la enfermedad o problema de salud es prevenible o controlable; Determinar la estrategia de intervención (prevención o control) más adecuada; Probar la eficacia de las estrategias de intervención; Cuantificar el beneficio conseguido al aplicar las estrategias de intervención sobre la población; Evaluar los programas de intervención; La medicina moderna, especialmente la mal llamada medicina basada en la evidencia (medicina factual o medicina basada en estudios científicos), está basada en los métodos de la epidemiología.[3]​ Vocabulario Hay una serie de términos que tienen un significado específico para el epidemiólogo. Una enfermedad es una epidemia cuando ocurre en un número inusualmente alto de individuos de una población simultáneamente; una pandemia es una epidemia que se disemina ampliamente, usualmente por todo el mundo. Una enfermedad endémica es la que está constantemente presente en una población, aunque su incidencia suele ser baja. La incidencia de una enfermedad determinada, es el número de nuevos casos de una enfermedad individual en una población de un determinado período de tiempo. La prevalencia de una enfermedad dada, es el número total de casos nuevos y ya existentes informados en una población y durante un determinado período de tiempo. Un brote de una enfermedad ocurre cuando se observa un número de casos, por lo general en un período de tiempo relativamente corto, en un área geográfica que anteriormente solo había presentado casos esporádicos de la enfermedad.[4]​ Mortalidad y morbilidad La mortalidad es la incidencia de muerte en la población. Las enfermedades infecciosas fueron la principal causa de la muerte en 1900 en los países desarrollados, pero ahora son mucho menos significativas. Hoy día, las enfermedades no infecciosas asociadas al estilo de vida, como las enfermedades cardíacas y el cáncer, son mucho más prevalentes y causan mayor mortalidad que las enfermedades infecciosas. Sin embargo, la situación actual podría cambiar rápidamente, si se llegaran a afectar en forma importante las infraestructuras y los servicios de salud públicas. En países en desarrollo, las enfermedades infecciosas son todavía la principal causa de mortalidad. La morbilidad se refiere a la incidencia de enfermedades en la población, incluyendo tanto enfermedades mortales como no mortales. Las estadísticas de la morbilidad definen la salud pública de una población con mayor precisión que las de mortalidad, porque muchas enfermedades tienen una mortalidad relativamente baja.[5]​ Progresión de la enfermedad En términos de sintomatología clínica, el curso de una enfermedad infecciosa aguda puede dividirse en etapas: Infección: el microorganismo invade, coloniza y crece en el hospedador. Período de incubación: el período de tiempo entre la infección y la aparición de los síntomas de la enfermedad. Período agudo: la enfermedad está en su punto culminante, con síntomas claros como fiebre y escalofríos. Período de declive: los síntomas de enfermedad están cediendo, la fiebre disminuye, usualmente después de un período de sudoración intensa, y aparece una sensación de bienestar. Período de convalecencia: el enfermo recupera las fuerzas y vuelve a la normalidad.[5]​ Metodología La epidemiología se basa en el método científico para la obtención de conocimientos, a través de los estudios epidemiológicos. Ante un problema de salud, y los datos disponibles sobre el mismo, se formula una hipótesis, la cual se traduce en una serie de consecuencias contrastables mediante experimentación. Se realiza entonces un proyecto de investigación que comienza con la recolección de datos y su posterior análisis estadístico, que permite obtener medidas de asociación (odds ratio, riesgo relativo, razón de tasas), medidas de efecto (riesgo atribuible) y medidas de impacto (fracción etiológica o riesgo atribuible proporcional), tanto a nivel de los expuestos como a nivel poblacional. De los resultados de esta investigación es posible obtener conocimientos que servirán para realizar recomendaciones de salud pública, pero también para generar nuevas hipótesis de investigación. En la literatura científica reciente se encuentran varios artículos de revisión, revisados por pares, que proveen una valiosa descripción general de las diferentes metodologías de la epidemiología.[6]​[7]​[8]​[9]​[10]​ Etiología de las enfermedades Artículo principal: Etiología Mapa original del Dr. John Snow. Los puntos muestran los casos de muerte por cólera durante la epidemia ocurrida en Londres en 1854. Las cruces representan los pozos de agua de los que bebieron los enfermos. El triángulo epidemiológico causal de las enfermedades está formado por el medio ambiente, los agentes y el huésped. Un cambio en cualquiera de estos tres componentes alterará el equilibrio existente para aumentar o disminuir la frecuencia de la enfermedad, por lo tanto se pueden llamar factores causales o determinantes de la enfermedad. Las bases de la epidemiología moderna fueron sentadas por Girolamo Fracastoro (Verona, 1487-1573) en sus obras De sympathia et antipathia rerum (Sobre la simpatía y la antipatía de las cosas) y De contagione et contagiosis morbis, et eorum curatione (Sobre el contagio y las enfermedades contagiosas y su curación), ambas publicadas en Venecia en 1546, donde Fracastoro expone sucintamente sus ideas sobre el contagio y las enfermedades transmisibles. Se considera al inglés John Graunt (1620-1674) quien publicó en 1662 el libro Natural and Political Observations Made upon the Bills of Mortality —sobre Londres— uno de los precursores de la epidemiología y de la demografía. Sin embargo, es John Snow (1813-1858), a quien se considera el precursor de la epidemiología contemporánea, ya que formuló la hipótesis de la transmisión del cólera por el agua y lo demostró confeccionando un mapa de Londres, en donde un reciente brote epidémico había matado más de 500 personas en un período de 10 días. Snow marcó en el mapa los hogares de los que habían muerto. La distribución mostraba que todas las muertes habían ocurrido en el área de Golden Square. La diferencia clave entre este distrito y el resto de Londres era el origen del agua potable. La compañía de agua privada que suministraba al vecindario de Golden Square extraía el agua de una sección del Támesis especialmente contaminado. Cuando se cambió el agua y comenzó a extraerse río arriba, de una zona menos contaminada, cedió la epidemia de cólera. Un progreso muy importante en el siglo XX, publicado en 1956 con los resultados del estudio de médicos británicos, fue la demostración de la relación causal entre fumar (tabaquismo) y el cáncer de pulmón.[5]​ Transición epidemiológica Constituye un proceso de cambio dinámico a largo plazo en la frecuencia, magnitud y distribución de la morbilidad y mortalidad de la población. La transición epidemiológica, que va acompañada por la transición demográfica, presenta cuatro aspectos a destacar: Desplazamiento en la prevalencia de las enfermedades transmisibles por las no trasmisibles. Desplazamiento en la morbilidad y mortalidad de los grupos jóvenes a los grupos de edad avanzada. Desplazamiento de la mortalidad como fuerza predominante por la morbilidad, sus secuelas e invalideces. Polarización epidemiológica. La polarización epidemiológica sucede cuando en distintas zonas de un país o en distintos barrios de una misma ciudad encontramos diferencias en la morbilidad y mortalidad de la población.[4]​ Ramas relacionadas Epidemiología descriptiva: es la rama de la epidemiología que describe el epidemiológico en tiempo, lugar y persona, cuantificando la frecuencia y distribución del fenómeno mediante medidas de incidencia, prevalencia y mortalidad, con la posterior formulación de hipótesis. Epidemiología analítica: busca, mediante la observación o la experimentación, establecer posibles relaciones causales entre factores a los que se exponen personas y poblaciones y las enfermedades que presentan. Las medidas empleadas en el estudio de esta rama de la epidemiología son los factores de riesgo, cuyo resultado es una probabilidad. Es posible distinguir dos tipos: riesgo absoluto y riesgo relativo. Riesgo absoluto: probabilidad de una enfermedad (baja, moderada, alta); si se considera la probabilidad de la enfermedad durante un periodo de tiempo, de lo que se está hablando es de una incidencia y no de un riesgo absoluto. Riesgo relativo: cuando se comparan dos riesgos absolutos entre sí; se trata de una probabilidad relativa (más alta o más baja que el otro); se ha de tener en cuenta que un riesgo relativo, por muy alto que sea, puede ser irrelevante; por ejemplo, fumar aumenta 100 veces el riesgo de sufrir una enfermedad, el riesgo sin fumar es de 1/100 000 000, por lo que el incremento por fumar es muy pequeño, prácticamente despreciable. Riesgo atribuible: en una población expuesta a un factor de riesgo, es la diferencia entre la incidencia de enfermedad en expuestos y no expuestos al factor de riesgo. La diferencia entre ambos valores proporciona el valor del riesgo de enfermedad en la cohorte expuesta, que se debe exclusivamente a la exposición al factor de riesgo. Epidemiología experimental: busca, mediante el control de las condiciones del grupo a estudiar, sacar conclusiones más complejas que con la mera observación no son deducibles. Se basa en el control de los sujetos a estudiar y en la aleatorización de la distribución de los individuos en dos grupos, un grupo experimental y un grupo control. Se ocupa de realizar estudios en animales de laboratorio y estudios experimentales con poblaciones humanas. Ecoepidemiología: busca, mediante herramientas ecológicas, estudiar integralmente como interaccionan los factores ambientales con las personas y poblaciones en los medios que los rodean y como ello puede influir en la evolución de enfermedades que se producen como consecuencia de dicha interacción."
ksampletext_wikipedia_medi_medicinainterna: str = "Medicina interna. La medicina interna es una especialidad médica que atiende integralmente los problemas de salud en pacientes adultos, ingresados en un centro hospitalario o en consultas ambulatorias.[1]​ Objetivos Guía al enfermo en su compleja trayectoria por el sistema sanitario hospitalario, dirigiendo y coordinando la actuación frente a su enfermedad y coordinando al resto de especialistas necesarios para obtener un diagnóstico y tratamiento adecuados. Los médicos internistas son los expertos a quienes recurren los médicos de atención primaria y el resto de especialistas para atender a enfermos complejos cuyo diagnóstico es difícil, que se encuentran afectados por varias enfermedades o que presentan síntomas en varios órganos, aparatos o sistemas del organismo. Dentro de la extensa formación de los internistas, existe la posibilidad de que algunos de ellos se subespecialicen en ciertos campos de la medicina, focalizándose únicamente en ellos, como el control de los factores de riesgo cardiovascular, enfermedades infecciosas y muy especialmente el VIH, la insuficiencia cardiaca congestiva, la enfermedad tromboembólica venosa y enfermedades autoinmunes, cuidados paliativos o unidades de pacientes crónicos complejos. Generalmente el médico internista requiere la atención de otros especialistas a la hora de la realización de pruebas diagnósticas, como a Radiología en caso de necesitar un TAC o una RM, al Digestivo para endoscopias, al cirujano para toma de biopsias, etc., de tal manera que de forma coordinada selecciona las pruebas que más convengan para lograr un diagnóstico certero del paciente. Historia Artículo principal: Historia de la Medicina A finales del siglo XIX comenzó a desarrollarse la medicina hospitalaria, muy unida a las clínicas universitarias, y surgió una nueva orientación en la medicina general, más ligada a las ciencias básicas biomédicas y a la experimentación, que recibió el nombre de Medicina Interna. El internista ha sido considerado, desde entonces, el clínico por excelencia. Dentro de este campo quedaron excluidas las enfermedades quirúrgicas, las obstétricas y las pediátricas, que, asimismo, constituyeron otras especialidades. Éstas, junto con la Medicina Interna, han sido consideradas, desde esa época, como especialidades básicas. [2]​ La denominación de Medicina Interna parece que tuvo su origen en Alemania, en 1880. En ese año, Strumpell escribió el primer tratado de Enfermedades Internas y, 2 años más tarde, en Wiesbaden, se celebró el I Congreso de Medicina Interna. Se quería indicar un campo de la práctica médica en el que los conceptos se basaban en el nuevo conocimiento que emergía en fisiología, bacteriología y patología, así como la exclusión de los métodos quirúrgicos en la terapéutica empleada. Este nuevo campo también llevaba la connotación de una formación académica y un entrenamiento. Además, estos médicos podían hacer de consultantes de otros especialistas. Es decir, la medicina interna sería como la medicina que trata enfermedades desde dentro, desde el interior del cuerpo, generalmente con medicamentos, en contraposición con la cirugía que trata las enfermedades desde fuera, con intervenciones quirúrgicas.[3]​ A partir de la segunda mitad del siglo XX surgen las especialidades médicas, ramas de la medicina interna. Se puede caer en el error, que perjudica seriamente al paciente, de que los especialistas no se responsabilicen de pacientes que caigan fuera del área de su particular competencia y cada vez ha sido más frecuente que a un mismo enfermo lo estén atendiendo múltiples especialistas, con los más diversos y, a veces, contradictorios enfoques.[cita requerida] Características La medicina interna es la especialidad de la medicina que se encarga de mantener la homeostasis del medio interno. Históricamente es una especialidad exclusivamente hospitalaria, aunque existen tendencias actuales en otras direcciones: consultas en centros periféricos de especialidades, hospitalización domiciliaria con equipos liderados por internistas, e integración en los equipos de Atención Primaria para colaborar como consultores. Un especialista en medicina interna o médico internista no es un médico interno: En España, los médicos internos residentes (MIR) son los médicos que, una vez superada una carrera teórica general en medicina y cirugía de seis años, deben superar el examen MIR y formarse durante 5 años para conseguir una especialidad de médico internista. En México, el médico interno (también conocido como Médico Interno de Pregrado) es aquel que cursa el quinto o sexto año de la carrera de médico cirujano (que dependiendo la universidad tiene una duración de 6 o 7 años) y un médico residente es aquel que, después de haber terminado la carrera de médico cirujano, cursa una especialidad médica (tras haber aprobado el respectivo Examen Nacional de Aspirantes a las Residencias Médicas). En el caso de la Medicina Interna, actualmente tiene una duración de 4 años, realizándose en el último año el servicio social con una duración de 3 a 4 meses, en alguna comunidad rural o ciudad del interior del país. Al término de su especialidad, se le da el diploma correspondiente a la especialidad de Medicina Interna. En el habla popular se le conoce como médico internista."
ksampletext_wikipedia_medi_neurologia: str = "Neurología. La neurología es la rama de la medicina que estudia el sistema nervioso. Específicamente se ocupa de la prevención, diagnóstico, tratamiento y rehabilitación de todas las enfermedades que involucran al sistema nervioso central, sistema nervioso periférico y el sistema nervioso autónomo. Existe gran número de enfermedades neurológicas, las cuales pueden afectar el sistema nervioso central (cerebro y médula espinal), el sistema nervioso periférico, o el sistema nervioso autónomo.[cita requerida] En España, la neurología como especialidad médica nació en el Hospital de la Santa Creu i Sant Pau de Barcelona (entonces Hospital de la Santa Creu) en 1882 de la mano del Dr. Lluís Barraquer i Roviralta. En un primer momento este servicio del Hospital de la Sant Creu se llamó Dispensario de Electroterapia, una consulta dedicada a pacientes con patologías del cerebro y de la médula espinal. Años después, el dispensario se llamó Servicio de Neurología y Electroterapia y finalmente acabó perdiendo la segunda denominación y se quedó como Servicio de Neurología.[2]​ El Dr. Barraquer introdujo, por primera vez en el país, la posibilidad de intervenir determinadas lesiones cerebrales y de ofrecer a estos pacientes la única posibilidad de curación que existía para ellos. A partir de 1910, se intervinieron una serie de pacientes con epilepsia focal, casi todos de origen traumático, que consistían en excisiones de las áreas corticales afectadas por la lesión de la cicatriz, lo que favoreció el nacimiento de la neurocirugía.[2] Uno de sus máximos exponentes fue el Dr. Manuel Corachán i Llort (hijo del Dr. Manuel Corachán Garcia), que en 1936 ya practicaba regularmente estas intervenciones en Sant Pau y que murió durante la guerra civil española.[3]​ Diagnóstico del sujeto con enfermedad neurológica Método clínico en la neurología El objetivo del método clínico en la neurología es servir como base para el tratamiento o la prevención de alguna enfermedad neurológica. En la mayoría de los casos el método consiste en cinco etapas, las cuales son: Pasos para diagnosticar una enfermedad neurológica. Principios de neurología por Adams y Víctor. Identificación de síntomas y signos mediante el interrogatorio y la exploración física. Los síntomas y signos físicos que se consideran importantes respecto al problema en cuestión son interpretados en términos fisiológicos y anatómicos: identificación de trastornos de la función y de la estructura anatómica involucrada. Diagnóstico anatómico/topográfico: Localización del proceso patológico (identificación de las partes del Sistema Nervioso afectadas), donde se reconoce un grupo característico de síntomas y signos, los cuales constituyen un síndrome, lo que nos ayuda a identificar el lugar y la naturaleza de la enfermedad. A esto se le conoce como diagnóstico sindrómico. A partir del diagnóstico anatómico y otros datos médicos (modo, rapidez de inicio, evolución, curso de la enfermedad, afección de sistemas orgánicos extraneurológicos, antecedentes personales y familiares y datos de laboratorio) es posible deducir el diagnóstico patológico. Cuando se identifica el mecanismo y la causalidad de la enfermedad se puede determinar el diagnóstico etiológico. Elaboración del diagnóstico funcional. Esta última etapa se refiere a la valoración del grado de incapacidad, donde se determina si este es temporal o permanente. Es de gran importancia para el tratamiento de la enfermedad y para la estimación del potencial de restablecimiento de la función, es decir, el pronóstico. El método precedente para el diagnóstico de las enfermedades neurológicas puede verse resumido en el diagrama colocado en esta sección. Este enfoque sistemático permite identificar de manera confiable la localización y a menudo el diagnóstico preciso de la enfermedad. Cabe recordar que no siempre es necesario plantear de esta forma la solución a un problema clínico, ya que algunas enfermedades neurológicas tienen cuadros clínicos muy característicos.[4]​ Exploración neurológica Una investigación (1897), obra de Joaquín Sorolla. La pintura muestra el interior del laboratorio del neurólogo Luis Simarro a finales del siglo XIX. Durante un examen neurológico, el neurólogo revisa la historia médica del paciente, con especial atención a sus condiciones recientes. Después le realiza un examen neurológico. Habitualmente, este examen neurológico evalúa el estado mental, las funciones de los nervios craneales, el sistema motor y el sistema sensitivo. Esta información ayuda al neurólogo a determinar si el problema se halla en el sistema nervioso y su localización clínica. La localización de la patología es la clave del proceso por el cual los neurólogos desarrollan sus diferentes diagnósticos. Pueden ser necesarios estudios posteriores para confirmar el diagnóstico, y finalmente una guía y terapia apropiada. La exploración neurológica se inicia con la exploración del paciente en tanto se practica el interrogatorio. La manera en que el paciente cuenta su enfermedad puede manifestar confusión o incoherencia del pensamiento, trastornos de la memoria o del juicio e incluso dificultades para comprender o expresar ideas. El resto de la exploración neurológica debe efectuarse como la última parte de la exploración física general a partir de, como ya se mencionó, la exploración de nervios craneales, cuello y tronco hasta terminar con las pruebas de las funciones motora, refleja y sensitiva de las extremidades superiores e inferiores. Dicha exploración debe modificarse según el estado del paciente. Desde luego muchas partes de la exploración no pueden efectuarse en el paciente comatoso; niños pequeños y lactantes o pacientes con padecimientos psiquiátricos necesitan explorarse de maneras especiales. Procedimientos de exploración y diagnóstico Pruebas de los nervios craneales: la función de los nervios craneales debe investigarse de manera más compleja en los pacientes que presentan síntomas neurológicos que en aquellos que no los experimentan. Si se sospecha una lesión de la fosa anterior debe someterse a prueba el sentido del olfato a través de cada fosa nasal, determinando si el paciente puede distinguir los olores. Los campos visuales se trazan mediante pruebas de confrontación, en algunos casos por investigación de cada ojo por separado buscando cualquier anomalía. La sensibilidad de la cara se somete a prueba con un alfiler y un poco de algodón, debe determinarse la presencia o ausencia de reflejos corneales. Se observan los movimientos faciales cuando el paciente habla y sonríe ya que la debilidad ligera puede ser más evidente en estas circunstancias. Es necesario inspeccionar las cuerdas vocales con instrumentos especiales en caso de sospecha de padecimiento del bulbo raquídeo o del nervio vago sobre todo cuando se presenta ronquera. Pruebas de la función motora: se deben tomar en cuenta las observaciones de la rapidez y fuerza de los movimientos, tamaño, tono y coordinación muscular.  Posiciones prona y supina. Es esencial que el paciente exponga por completo las extremidades para inspeccionarlas por atrofia y fasciculaciones así como para observarlas mientras conserva los brazos estirados en las posiciones prona y supina; que el individuo efectué tareas sencillas como alternar el contacto con su nariz y con el dedo del examinador; hacer que realice movimientos alternos rápidos particularmente los que involucran cambios de dirección, aceleración y desaceleración súbita; que el pulgar toque rápidamente la punta de cada uno de los dedos y efectué movimientos de supinación y pronación del antebrazo; además que complete tareas sencillas como abotonarse la ropa, abrir un broche o manipular herramientas comunes. Pruebas de la función refleja: las pruebas de los reflejos bicipital, tricipital, supinador, rotuliano, aquíleo, cutáneo abdominal y plantar permiten obtener una idea de lo adecuada que es la actividad refleja de la medula espinal. Los reflejos tendinosos requieren que los músculos afectados estén relajados; los reflejos hipoactivos o que apenas pueden descartarse suelen facilitarse mediante contracción voluntaria de otros músculos. La presencia de reflejos cutáneos superficiales de los músculos abdominales, cremasterianos y de otros tipos suele constituir una prueba básica de gran utilidad para identificar lesiones corticospinales. Pruebas de la función sensitiva: esta es la parte más complicada de la exploración neurológica, se reserva para la parte final de la exploración y no debe prolongarse durante más de unos pocos minutos si se requiere que los datos sean confiables. Por lo general se buscan diferencias entre ambos lados del cuerpo, el nivel por debajo del cual se pierde la sensación o la existencia de una zona de anestesia relativa o absoluta. Se explica al paciente con brevedad cada prueba; hablar demasiado sobre estas pruebas con un paciente introspectivo meticuloso puede animarlo para que notifique variaciones menores independientemente de la intensidad del estímulo. No es necesario explorar todas las regiones superficiales de la piel, la investigación rápida de cara, cuello, manos, tronco y pies con un alfiler requiere solo unos cuantos segundos. Las regiones con déficit sensitivo pueden someterse a otras pruebas. El descubrimiento de alguna zona con hiperestesia dirige la atención a un trastorno de sensibilidad superficial. Exploración de la estación y la marcha: ninguna exploración está completa sino se observa al paciente en posición erguida. Quizá la anomalía neurológica más destacada o la única sea la anormalidad de la bipedestación y la marcha, como sucede en algunos trastornos cerebelosos o del lóbulo frontal. Además una alteración de la postura y los movimientos de adaptación automáticos puros al caminar proporciona la pista diagnostica más definitiva en la etapa inicial de la enfermedad de Parkinson y de la parálisis supranuclear progresiva. El paciente médico o quirúrgico sin síntomas neurológicos: para las extremidades superiores suele ser suficiente la observación de los brazos desnudos y estirados en busca de atrofia, debilidad (impulso pronador), temblor o movimientos anormales; la verificación de la fuerza, empuñadura y dorsiflexión a nivel de la muñeca; inquirir acerca de los trastornos sensitivos y desencadenar los reflejos supinador, bicipital y tricipital. El desencadenamiento de los reflejos rotuliano, aquíleo y plantar; las pruebas de vibración y sentido de posición en los dedos de las manos y pies, y la valoración de la coordinación haciendo que el paciente toque de forma alternada su nariz y un dedo del examinador, así como que deslice el talón hacia arriba y abajo por el frente de la pierna opuesta. El paciente comatoso: la exploración cuidadosa del paciente en estupor o comatoso ofrece información considerable en cuanto a la función del sistema nervioso. Se deben reconocer las posturas predominantes de las extremidades y el cuerpo; la presencia o ausencia de movimientos espontáneos en un lado; la posición de la cabeza y los ojos, la velocidad, profundidad y ritmo de la respiración. Se valora la reacción que tiene el paciente al oír su nombre, órdenes sencillas o a estímulos nocivos. Por lo regular es posible determinar si el coma está relacionado con irritación meníngea o enfermedad cerebral focal o del tallo cerebral. En las etapas menos profundas del coma la irritación meníngea produce una resistencia a la flexión pasiva del cuello pero no a la extensión, rotación o inclinación de la cabeza. Diagnóstico de laboratorio: la descripción del método clínico y su aplicación evidencia que la exploración clínica rigurosa debe preceder siempre al empleo de los auxiliares de laboratorio, sin embargo en neurología la finalidad de estos es la prevención. Por tanto en la neurología preventiva la metodología de laboratorio puede adquirir prioridad sobre la metodología clínica. La información genética permite al neurólogo identificar a los pacientes en peligro de desarrollar ciertas enfermedades para iniciar de inmediato la búsqueda de marcadores biológicos antes que los síntomas o signos aparezcan. Las pruebas de investigación bioquímica son aplicables para toda una población y permiten identificar en individuos que aún no muestran síntomas, y en algunas de estas enfermedades es posible aplicar un tratamiento antes de que se sufra una lesión en el sistema nervioso. Trabajo clínico Casos en general Los neurólogos son responsables del diagnóstico, tratamiento y manejo de todas las condiciones mencionadas arriba. Cuando la intervención quirúrgica es requerida, el neurólogo puede referirse al paciente como «neuropaciente». En algunos países, algunas responsabilidades legales de un neurólogo pueden incluir efectuar un diagnóstico de muerte cerebral si el paciente fallece. Suelen tratar personas con enfermedades congénitas si la mayor parte de las manifestaciones son neurológicas. Las punciones lumbares también pueden ser realizadas por estos profesionales. Algunos neurólogos desarrollan un interés a subcampos en particular como las enfermedades cerebrovasculares, los trastornos del movimiento, epilepsia, cefaleas, neurología de la conducta y demencias, trastornos del sueño, control de dolor crónico, esclerosis múltiple o enfermedades neuromusculares. Áreas destacadas Hay superposición de otras especialidades, variando de país en país e incluso en un área geográfica local. El traumatismo craneoencefálico (ETC) agudo es más comúnmente tratado por neurocirujanos, mientras que secuelas de traumas craneoencefálicos pueden ser tratados por neurólogos o especialistas en rehabilitación médica. Aunque los casos de accidente cerebrovascular (ACV) han sido tradicionalmente tratados por médicos internistas u hospitalarios, el surgimiento de neurología vascular y neurólogos intervencionistas han creado una demanda para especialistas en ACV. La organización de JHACO centro certificado en accidentes cerebrovasculares ha incrementado el papel de los neurólogos en el tratamiento de accidentes cerebrovasculares en muchos centros de atención primaria, así como en hospitales de tercer nivel. Algunos casos de enfermedades infecciosas del sistema nervioso son tratados por especialistas en enfermedades infecciosas. La mayoría de los casos de dolor de cabeza son diagnosticados y tratados principalmente por médicos generales, al menos los casos menos severos. Del mismo modo, la mayoría de los casos de ciática y otras radiculopatías mecánicas son atendidos por médicos generales, aunque pueden ser enviados a neurólogos o cirujanos (neurocirujanos o cirujanos ortopédicos). Los trastornos del sueño generalmente son tratados en unidades multidisciplinares en las que participan neurólogos, neumólogos y psiquiatras. Una parálisis cerebral es inicialmente atendida por pediatras, pero el tratamiento puede ser transferido a un neurólogo de adultos después de que el paciente alcanza una cierta edad. Los neuropsicólogos clínicos son usualmente consultados para realizar una evaluación funcional del comportamiento y funciones cognitivas superiores, relacionada con la asistencia en diagnósticos diferenciales, la planificación de estrategias de rehabilitación, el registro de fuerzas y debilidades cognitivas, y la medición de cambios en el tiempo (por ejemplo, para identificar anomalías de envejecimiento o llevando el progreso de una demencia). Relaciones a la neurofisiología clínica En algunos países como Estados Unidos y Alemania, los neurólogos se pueden especializar en neurofisiología clínica, en electroencefalografía, o en el estudio de la conducción nerviosa, en Electromiografías y potenciales evocados. En otros países, es una especialidad independiente (por ejemplo en el Reino Unido y Suecia). Superposición con la psiquiatría A pesar de que las enfermedades mentales son consideradas por algunos de ser desórdenes neurológicos afectando el sistema nervioso central, tradicionalmente se las clasifica por separado, y son tratadas por psiquiatras. En el año 2002, en una reseña del American Journal of Psychiatry, el profesor Joseph B. Martin, decano de Harvard Medical School y neurólogo de profesión, escribió que: «la división en dos categorías es arbitraria, a menudo influenciada por creencias más que por observaciones científicas verificables. Y el hecho de que el cerebro y la mente sean uno solo, hace que esta división sea solamente artificial de todas formas». Esta perspectiva ha propiciado un progresivo acercamiento entre ambas especialidades en las últimas dos décadas, que finalmente se materializó en 2004 con el reconocimiento, en Estados Unidos, de la subespecialidad en «Neurología de la conducta y Neuropsiquiatría». Actualmente, los médicos de esta subespecialidad se encargan del estudio, diagnóstico y tratamiento de las alteraciones de la conducta y los trastornos mentales atribuibles a enfermedades neurológicas. Las enfermedades neurológicas a menudo tienen manifestaciones psiquiátricas, como por ejemplo psicosis, depresión, manía y ansiedad. Estos síndromes neuropsiquiátricos son relativamente habituales en pacientes con ictus, enfermedad de Huntington, parkinsonismos, enfermedad de Alzheimer, enfermedad por cuerpos de Lewy, enfermedad de Pick, encefalitis infecciosas, encefalitis autoinmunes, así como en algunos tipos de epilepsia, por nombrar solo algunas. Efectos del envejecimiento sobre el sistema nervioso Vejez, Emily Samson en la bienvenida al nuevo mundo. De todos los cambios vinculados con la edad tienen una enorme importancia los que tiene el sistema nervioso, algunos signos neurológicos del envejecimiento son: los signos neurooftalmológicos, pérdida de la audición perceptiva progresiva, disminución del sentido del olfato y menor extensión del gusto, reducción de la velocidad y magnitud de actividad motora, tiempo de reacción lento, trastornos de coordinación y agilidad, reducción de la fuerza muscular y adelgazamiento de los músculos, cambios de los reflejos tendinosos y finalmente trastornos del sentido de vibración en los dedos de los pies y en tobillos. Pareja de ancianos. Roger Hsu. Neurología cosmética El emergente campo de la neurología cosmética señala el potencial de terapias para mejorar cuestiones como la eficacia laboral, la atención en la escuela, y una mayor felicidad en la vida personal. A pesar de todo, este campo ha dado también lugar a preguntas acerca de la neuroética o la psicofarmacología. Temas relacionados Temas clásicos Neuroanatomía Neuropediatría Neurootología Neuropsicología y Neurología de la conducta Semiología Métodos Diagnósticos Tomografía axial computarizada Angiografía cerebral Imagen por resonancia magnética (IRM) Electromiografía Tomografía por emisión de positrones Punción lumbar Biopsia cerebral Enfermedades del sistema nervioso central Afasia Anomalías del desarrollo del sistema nervioso central Enfermedades carenciales del sistema nervioso Degeneración combinada subaguda de la médula espinal Encefalopatía de Wernicke Enfermedades cerebrovasculares Enfermedades de la médula espinal Siringomielia Hernia discal Mielitis transversa Enfermedades degenerativas del sistema nervioso central Enfermedad de Alzheimer Atrofia multisistémica Parálisis supranuclear progresiva Enfermedad de Parkinson Esclerosis lateral amiotrófica Enfermedad de Huntington Enfermedades del sistema extrapiramidal Enfermedades desmielinizantes del sistema nervioso central Esclerosis múltiple Enfermedad de Devic Esclerosis concéntrica de Baló Encefalomielitis diseminada aguda Enfermedades infecciosas del sistema nervioso central Meningitis Absceso Cerebral Toxoplasmosis cerebral Encefalitis Enfermedades metabólicas del sistema nervioso central Epilepsias Traumatismos craneoencefálicos Tromboembolismo intracraneal Tumor intracraneal Meningioma Pinealoma Ependimoma Astrocitoma Meduloblastoma Oligodendroglioma Enfermedades del sistema nervioso periférico Síndrome de Guillain-Barré Síndrome de Charcot-Marie-Tooth Enfermedades musculares o miopatías Distrofia muscular de Duchenne Distrofia miotónica de Steinert Enfermedades de la unión neuromuscular Miastenia grave Síndrome miasténico de Lambert-Eaton"

ksampletext_wikipedia_geol_tectonicadeplacas: str = "Tectónica de placas. La tectónica de placas o tectónica global es una teoría que explica la forma en que está estructurada la litosfera (porción externa más fría y rígida de la Tierra). La rama de la Geología que se encarga de su estudio es la tectónica. La teoría da una explicación a las placas tectónicas que forman parte de la superficie de la Tierra y a los deslizamientos que se observan entre ellas en su movimiento sobre el manto terrestre fluido, sus direcciones e interacciones. También explica la formación de las cadenas montañosas (orogénesis). Asimismo, da una explicación satisfactoria al hecho de que los terremotos y los volcanes se concentran en regiones concretas del planeta (como el Cinturón de Fuego del Pacífico) o a la ubicación de las grandes fosas oceánicas junto a los arcos insulares y continentes y no en el centro del océano.[1]​ Las placas tectónicas se desplazan unas respecto de otras con relativa lentitud, a una velocidad nunca perceptible sin instrumentos, pero con tasas diferentes. La mayor velocidad se da en la dorsal del Pacífico Oriental, cerca de la Isla de Pascua, a unos 3400 km de Chile continental, con una velocidad de separación entre placas de más de 15 cm/año y la más lenta se da en la dorsal ártica, con menos de 2,5 cm/año.[2]​[3] Dado que se desplazan sobre la superficie finita de la Tierra, las placas interaccionan unas con otras a lo largo de sus límites provocando intensas deformaciones en la corteza y litosfera de la Tierra, lo que ha dado lugar a la formación de grandes cadenas montañosas (por ejemplo las cordilleras de Himalaya, Alpes, Pirineos, Atlas, Urales, Apeninos, Apalaches, Andes, entre muchos otros) y grandes sistemas de fallas asociadas con estas (por ejemplo, el sistema de fallas de Anatolia del Norte). El contacto por fricción entre los bordes de las placas es responsable de la mayor parte de los terremotos. Otros fenómenos asociados son la creación de volcanes (especialmente notorios en el cinturón de fuego del océano Pacífico) y las fosas oceánicas. Las placas tectónicas se componen de dos tipos distintos de litosfera: la corteza continental, más gruesa, y la corteza oceánica, la cual es relativamente delgada. A la parte superior de la litosfera se la conoce como corteza terrestre, nuevamente de dos tipos (continental y oceánica). Esto significa que una placa litosférica puede ser continental, oceánica, o bien de ambos tipos, en cuyo caso se denomina placa mixta. Uno de los principales puntos de la teoría propone que la cantidad de superficie de las placas (tanto continental como oceánica) que desaparecen en el manto a lo largo de los bordes convergentes de subducción está más o menos en equilibrio con la corteza oceánica nueva que se está formando a lo largo de los bordes divergentes (dorsales oceánicas) a través del proceso conocido como expansión del fondo oceánico. También se suele hablar de este proceso como el principio de la cinta transportadora. En este sentido, el total de la superficie en el globo se mantiene constante, siguiendo la analogía de la cinta transportadora, siendo la corteza la cinta que se desplaza gracias a las fuertes corrientes convectivas de la astenosfera, que hacen las veces de las ruedas que transportan esta cinta, hundiéndose la corteza en las zonas de convergencia, y generándose nuevo piso oceánico en las dorsales. La teoría también explica de forma bastante satisfactoria la forma en que las inmensas masas que componen las placas tectónicas se pueden desplazar, algo que quedaba sin explicar cuando Alfred Wegener propuso la teoría de la deriva continental, aunque existen varios modelos que coexisten: Las placas tectónicas se pueden desplazar porque la litosfera tiene una menor densidad que la astenosfera, que es la capa que se encuentra inmediatamente inferior a la corteza. Esto hace que las placas floten en la astenosfera y el magma líquido más caliente vaya hacia arriba y el más frío y denso hacia abajo, generando una corriente que mueve las placas. Las variaciones de densidad laterales resultan en las corrientes de convección del manto, mencionadas anteriormente. Se cree que las placas son impulsadas por una combinación del movimiento que se genera en el fondo oceánico fuera de la dorsal (debido a variaciones en la topografía y densidad de la corteza, que resultan en diferencias en las fuerzas gravitacionales, arrastre, succión vertical, y zonas de subducción). Una explicación diferente o complementaria se apoya en las diferentes fuerzas que se generan con la rotación del globo terrestre y las fuerzas de marea del Sol y de la Luna; sin embargo, la importancia relativa de cada uno de esos factores no está clara y es objeto de debate.[cita requerida] Placas tectónicas en el mundo Actualmente existen las siguientes placas tectónicas en la superficie de la tierra con límites más o menos definidos, que se dividen en 15 placas mayores (o principales) y 43 placas menores (o secundarias). Las 15 placas mayores Las 15 placas tectónicas mayores. Placa africana Placa antártica Placa arábiga Placa australiana Placa del Caribe Placa de Cocos Placa euroasiática Placa filipina Placa India Placa Juan de Fuca Placa de Nazca Placa norteamericana Placa del Pacífico Placa de Scotia Placa sudamericana Las 42 placas menores Mapa detallado que muestra las placas tectónicas con sus vectores de movimiento. Placa de Altiplano Placa de Amuria Placa de Anatolia Placa de los Andes del Norte Placa Apuliana o Adriática Placa del Arrecife de Balmoral Placa del Arrecife de Conway Placa de Birmania Placa de Bismarck del Norte Placa de Bismarck del Sur Placa Cabeza de Pájaro o Doberai Placa de las Carolinas Placa de Chiloé Placa del Explorador Placa de Futuna Placa Galápagos Placa de Gorda Placa Iraní Placa de Juan Fernández Placa de Kermadec Placa de Manus Placa de Maoke Placa del Mar de Banda Placa del Mar Egeo o Helénica Placa del Mar de las Molucas Placa del Mar de Salomón Placa de las Marianas Placa Niuafo'ou Placa africana Placa de las Nuevas Hébridas Placa de Ojotsk Placa de Okinawa Placa de Panamá Placa de Pascua Placa de Rivera Placa de Sandwich Placa de Shetland Placa somalí Placa de la Sonda Placa de Timor Placa de Tonga Placa Woodlark Placa del Yangtsé Se han identificado tres tipos de bordes: convergentes (dos placas chocan una contra la otra), divergentes (dos placas se separan) y transformantes (dos placas se deslizan una junto a otra). La teoría de la tectónica de placas se divide en dos partes, la de deriva continental, propuesta por Alfred Wegener en la década de 1910, y la de expansión del fondo oceánico, propuesta y aceptada en la década de 1960, que mejoraba y ampliaba a la anterior. Desde su aceptación ha revolucionado las ciencias de la Tierra, con un impacto comparable al que tuvieron las teorías de la gravedad de Isaac Newton y Albert Einstein en la Física o las leyes de Kepler en la Astronomía. Causas del movimiento de las placas Artículo principal: Convección Movimiento por convección. El origen del movimiento de las placas está en unas corrientes de materiales que suceden en el manto, las denominadas corrientes de convección, y sobre todo, en la fuerza de la gravedad. La convección es una de las tres formas de transferencia de calor y se caracteriza porque se produce por intermedio de un fluido (aire, agua) que transporta el calor entre zonas con diferentes temperaturas. La convección se produce únicamente por medio de materiales fluidos. Éstos, al calentarse, aumentan de volumen y, por lo tanto, disminuyen su densidad y ascienden desplazando el fluido que se encuentra en la parte superior y que está a menor temperatura. Lo que se llama convección en sí, es el transporte de calor por medio de las corrientes ascendente y descendente del fluido. Las corrientes de convección se producen por diferencias de temperatura y densidad, de manera que los materiales más calientes pesan menos y ascienden, y los materiales más fríos son más densos, pesados, y descienden. El manto, aunque es sólido, se comporta como un material plástico o dúctil, es decir, se deforma y se estira sin romperse, debido a las altas temperaturas a las que se encuentra, sobre todo el manto inferior. En las zonas profundas el manto hace contacto con el núcleo, el calor es muy intenso, por eso grandes masas de roca se funden parcialmente y al ser más ligeras ascienden lentamente por el manto, produciendo unas corrientes ascendentes de materiales calientes, las plumas o penachos térmicos. Algunos de ellos alcanzan la litosfera, la atraviesan y contribuyen a la fragmentación de los continentes. En las fosas oceánicas, grandes fragmentos de litósfera oceánica fría se hunden en el manto, originando por tanto unas corrientes descendentes, que llegan hasta la base del manto. Las corrientes ascendentes y descendentes del manto podrían explicar el movimiento de las placas, al actuar como una especie de rodillo que las moviera. Antecedentes históricos Deriva continental Artículo principal: Deriva continental A finales del siglo XIX y principios del XX, los geólogos asumían que las principales características de la Tierra eran fijas y que la mayoría de las características geológicas, como el desarrollo de cuencas y cadenas montañosas, podían explicarse por el movimiento vertical de la corteza, descrito en lo que se denomina teoría geosinclinal. Generalmente, esto se colocó en el contexto de un planeta Tierra en contracción debido a la pérdida de calor en el transcurso de un tiempo geológico relativamente corto.[4]​ Ya en 1596 se observó que las costas opuestas del Océano Atlántico (aunque es más preciso hablar de los bordes de las plataformas continentales) tienen formas similares y parecen haber encajado en algún momento pasado. Desde entonces se propusieron muchas teorías para explicar esta aparente complementariedad, pero el supuesto de una Tierra sólida hizo que estas diversas propuestas fueran difíciles de aceptar. El descubrimiento de la radiactividad y sus propiedades de calentamiento asociadas en 1895 impulsó un nuevo examen de la edad aparente de la Tierra. Esto se había estimado previamente por su tasa de enfriamiento bajo el supuesto de que la superficie de la Tierra irradiaba como un cuerpo negro. Esos cálculos habían implicado que, incluso si comenzara con un calor rojo, la Tierra habría caído a su temperatura actual en unas pocas decenas de millones de años. Armados con el conocimiento de una nueva fuente de calor, los científicos se dieron cuenta de que la Tierra sería mucho más antigua y que su núcleo todavía estaba lo suficientemente caliente como para ser líquido. Alfred Wegener en el verano de 1912-13 en Groenlandia. En 1915, después de haber publicado un primer artículo en 1912, Alfred Wegener presentó argumentos serios a favor de la idea de la deriva continental en la primera edición de El origen de los continentes y océanos. En ese libro (reeditado en cuatro ediciones sucesivas hasta la última en 1936), señaló cómo la costa este de América del Sur y la costa oeste de África parecían enacajar (de lo que ya se habían percatado anteriormente Benjamin Franklin entre otros).[5]​. Wegener no fue el primero en notar esto (Abraham Ortelius, Antonio Snider-Pellegrini, Eduard Suess, Roberto Mantovani y Frank Bursley Taylor lo precedieron, solo por mencionar algunos), pero fue el primero en reunir importantes evidencias fósiles, paleo-topográficas y climatológicas para apoyar esta simple observación (y fue apoyado en esto por investigadores como Alex du Toit). También tuvo en cuenta el parecido de la fauna fósil de los continentes septentrionales y ciertas formaciones geológicas. Wegener conjeturó que el conjunto de los continentes actuales estuvieron unidos en el pasado remoto de la Tierra, formando un supercontinente, denominado Pangea.[6] Además, dado que los estratos rocosos de los márgenes de continentes separados son muy similares, sugiere que estas rocas se formaron de la misma manera, lo que implica que estaban unidas en un principio. Por ejemplo, partes de Escocia e Irlanda contienen rocas muy similares a las que se encuentran en Terranova y Nuevo Brunswick. Además, las Montañas Caledonianas de Europa y partes de los montes Apalaches de América del Norte son muy similares en estructura y litología.[7]​ Sin embargo, sus ideas no fueron tomadas en serio por muchos geólogos,[8] quienes señalaron que no existía un mecanismo aparente para la deriva continental. En su tesis original, Wegener propuso que los continentes se desplazaban sobre el manto de la Tierra de la misma forma en que uno desplaza una alfombra sobre el piso de una habitación. Sin embargo, esto no es posible, debido a la enorme fuerza de fricción implicada, lo que motivó el rechazo de la explicación de Wegener, y la puesta en suspenso, como hipótesis interesante pero no probada, de la idea del desplazamiento continental hasta la aparición de la Tectónica de placas. Más concretamente, no vieron cómo la roca continental podría atravesar la roca mucho más densa que forma la corteza oceánica. Wegener no pudo explicar la fuerza que impulsó la deriva continental, y su reivindicación no llegó hasta después de su muerte en 1930.[9]​ Continentes flotantes, paleomagnetismo y zonas sísmicas Como se observó temprano que aunque existía granito en los continentes, el fondo marino parecía estar compuesto de basalto más denso, el concepto predominante durante la primera mitad del siglo XX fue que había dos tipos de corteza, denominada sial (corteza de tipo continental). y sima (corteza de tipo oceánico).[10] Además, se suponía que había una capa estática de estratos debajo de los continentes. Por lo tanto, parecía evidente que una capa de basalto (sial) subyace a las rocas continentales. Sin embargo, basándose en anomalías en la desviación de la plomada de los Andes en Perú, Pierre Bouguer había deducido que las montañas menos densas deben tener una proyección hacia abajo en la capa inferior más densa. El concepto de que las montañas tenían raíces fue confirmado por George B. Airy cien años después, durante un estudio de la gravitación del Himalaya, y los estudios sísmicos detectaron variaciones de densidad correspondientes. Por lo tanto, a mediados de la década de 1950 seguía sin resolverse la cuestión de si las raíces de las montañas estaban apretadas en el basalto circundante o flotaban sobre él como un iceberg. Epicentros de terremotos, 1963–1998. La mayoría de los terremotos tienen lugar en estrechos cinturones que coinciden con los límites entre placas. Durante el siglo XX las mejoras y el mayor uso de instrumentos sísmicos como los sismógrafos permitieron a los científicos comprender que los terremotos tienden a concentrarse en áreas específicas, sobre todo a lo largo de las fosas oceánicas y las dorsales. A finales de la década de 1920 los sismólogos estaban comenzando a identificar varias zonas prominentes de terremotos paralelas a las fosas que normalmente se inclinaban entre 40 y 60° desde la horizontal y se extendían varios cientos de kilómetros hacia el interior de la Tierra. Estas zonas se conocieron más tarde como zonas de Wadati-Benioff, o simplemente zonas de Benioff[11]​, en honor a los sismólogos que las reconocieron por primera vez, Kiyoo Wadati de Japón y Hugo Benioff de Estados Unidos. El estudio de la sismicidad global avanzó enormemente en la década de 1960 con el establecimiento de la Red Mundial de Sismógrafos Estandarizados (WWSSN) para monitorizar el cumplimiento del tratado de 1963 que prohibía las pruebas aéreas de armas nucleares. Los datos muy mejorados de los instrumentos de WWSSN permitieron a los sismólogos mapear con precisión las zonas de concentración de terremotos en todo el mundo. Mientras tanto, se desarrollaron debates en torno al fenómeno de la deriva polar. Desde los primeros debates sobre la deriva continental, los científicos habían discutido y utilizado evidencias de que la deriva polar había ocurrido porque los continentes parecían haberse movido a través de diferentes zonas climáticas durante el pasado. Además, los datos paleomagnéticos habían demostrado que el polo magnético también se había desplazado con el tiempo. Razonando de manera opuesta, los continentes podrían haberse movido y girado, mientras que el polo permanecía relativamente fijo.[12] La primera vez que se utilizó la evidencia de la desviación polar magnética para respaldar los movimientos de los continentes fue en un artículo de Keith Runcorn en 1956, y artículos sucesivos de él y sus estudiantes Ted Irving (quien en realidad fue el primero en estar convencido del hecho de que el paleomagnetismo apoyaba la deriva continental) y Ken Creer. A esto siguió inmediatamente un simposio en Tasmania en marzo de 1956. En este simposio, la evidencia se utilizó en la teoría de una expansión de la corteza global. En esta hipótesis, el desplazamiento de los continentes puede explicarse simplemente por un gran aumento en el tamaño de la Tierra desde su formación. Sin embargo, esto fue insatisfactorio porque sus partidarios no pudieron ofrecer un mecanismo convincente para producir una expansión significativa de la Tierra. Ciertamente, no hay evidencia de que la Luna se haya expandido en los últimos 3 000 millones de años; otros trabajos pronto mostrarían que la evidencia estaba igualmente a favor de la deriva continental en un globo con un radio estable. Durante los años treinta hasta finales de los cincuenta, los trabajos de Vening-Meinesz, Holmes, Umbgrove y muchos otros delinearon conceptos que eran cercanos o casi idénticos a la teoría de la tectónica de placas moderna. En particular, el geólogo inglés Arthur Holmes propuso en 1920 que las uniones de placas podrían encontrarse debajo del mar, y en 1928 que las corrientes de convección dentro del manto podrían ser la fuerza impulsora. A menudo, estas contribuciones se olvidan porque: En ese momento no se aceptaba la deriva continental. Algunas de estas ideas se discutieron en el contexto de ideas fijistas abandonadas de un globo deformante sin deriva continental o una Tierra en expansión. Fueron publicadas durante un episodio de extrema inestabilidad política y económica que obstaculizó la comunicación científica. Muchas fueron publicadas por científicos europeos y al principio no se mencionaron o se les dio poco crédito en los artículos sobre la extensión del fondo marino publicados por los investigadores estadounidenses en la década de 1960. Expansión de la dorsal mediooceánica y convección Artículo principal: Expansión del fondo oceánico Sumergible Alvin, que participó en el proyecto FAMOUS de exploración de la dorsal mesoatlántica. El primer mapa de los fondos oceánicos se consigue elaborar en 1956 gracias a los avances en las tecnologías del sónar. Se investigó el Océano Atlántico y se descubrió que: Había una cordillera submarina, a la que llamaron dorsal. Las rocas cercanas a los continentes eran más antiguas que las del centro. Los epicentros de los terremotos tenían lugar en la dorsal. Existían más de 6000 km de dorsales. Por estas razones en 1960 Harry Hess y en 1961 Robert Dietz sugirieron que el suelo oceánico se expande. En 1963 esta hipótesis se comprobó cuando Vine y Matthews identificaron las líneas de magnetismo de distinta polaridad, es decir, que el campo magnético terrestre se invierte.[13]​ En 1974, dentro del proyecto internacional FAMOUS, un equipo de científicos de la Institución Oceanográfica de Woods Hole (EE. UU.) y del French Centre Oceanologique de Bretagne (Brest, Francia) utilizó buques de investigación en superficie, así como diverso instrumental avanzado que incluía magnetómetros, sonar y sismógrafos, además de dos sumergibles: el Alvin (EE. UU.) y el Archimède (Francia). Las investigaciones confirmaron la existencia de una elevación en el Océano Atlántico central y descubrieron que el fondo del lecho marino, debajo de la capa de sedimentos, consistía en basalto, no en granito, que es el componente principal de los continentes. También encontraron actividad volcánica y sísmica y que la corteza oceánica era mucho más delgada que la corteza continental. Todos estos nuevos hallazgos plantearon preguntas importantes e intrigantes.[14]​ Las fuentes hidrotermales encontradas en las dorsales son consecuencia de una intensa actividad volcánica. Los nuevos datos recopilados sobre las cuencas oceánicas también mostraron características particulares en cuanto a la batimetría. Uno de los principales resultados de estos conjuntos de datos fue que en todo el mundo se detectó un sistema de dorsales oceánicas. Una conclusión importante fue que a lo largo de este sistema se estaba creando un nuevo fondo oceánico, lo que llevó al concepto de la Gran Grieta Global. Esto se describió en el artículo crucial de Bruce Heezen (1960) basado en su trabajo con Marie Tharp, que desencadenaría una verdadera revolución en el pensamiento. Una consecuencia profunda de la expansión del lecho marino es que se crea y se sigue creando una nueva corteza a lo largo de las dorsales oceánicas. Por lo tanto, Heezen defendió la supuesta hipótesis de la Tierra en expansión de S. Warren Carey (ver arriba). Entonces, todavía quedaba la pregunta: ¿cómo se puede agregar continuamente nueva corteza a lo largo de las dorsales oceánicas sin aumentar el tamaño de la Tierra? En realidad, esta cuestión ya había sido resuelta por numerosos científicos durante los años cuarenta y cincuenta, como Arthur Holmes, Vening-Meinesz, Coates y muchos otros: la corteza en exceso desaparece a lo largo de las llamadas fosas oceánicas, donde se produce el proceso conocido como subducción. Por lo tanto, cuando varios científicos a principios de la década de 1960 comenzaron a razonar sobre los datos que tenían a su disposición sobre el fondo del océano, las piezas de la teoría encajaron rápidamente. La pregunta intrigó particularmente a Harry Hammond Hess, un geólogo de la Universidad de Princeton y contraalmirante de la Reserva Naval, y a Robert S. Dietz, un científico de la U.S. National Geodetic Survey, quien acuñó por primera vez el término expansión del fondo oceánico. Dietz y Hess (el primero publicó la misma idea un año antes en Nature, pero la prioridad pertenece a Hess, que ya había distribuido un manuscrito inédito de su artículo de 1962 en 1960) se encontraban entre el pequeño puñado que realmente entendió las amplias implicaciones de la expansión del fondo marino y cómo eventualmente estaría de acuerdo con las ideas, en ese momento poco convencionales y no aceptadas, de la deriva continental y los modelos elegantes y movilistas propuestos por investigadores anteriores como Holmes.[15]​ En el mismo año, Robert R. Coats del U.S. Geological Survey describió las principales características de la subducción del arco insular en las Islas Aleutianas. Su artículo, aunque poco conocido (e incluso ridiculizado) en ese momento, desde entonces ha sido llamado seminal y profético. En realidad, muestra que el trabajo de científicos europeos sobre arcos de islas y cinturones montañosos realizado y publicado durante la década de 1930 hasta la década de 1950 fue aplicado y apreciado también en los Estados Unidos. Lava almohadillada como la producida por la actividad volcánica en las dorsales, apenas cubierta por una fina capa de sedimentos, lo que indica su reciente formación. Si la corteza terrestre se estaba expandiendo a lo largo de las dorsales oceánicas, razonaron Hess y Dietz como Holmes y otros antes que ellos, debe estar encogiéndose en otros lugares. Hess siguió a Heezen, sugiriendo que la nueva corteza oceánica se separa continuamente de las dorsales en un movimiento similar a una cinta transportadora. Y, utilizando los conceptos movilistas desarrollados anteriormente, concluyó correctamente que muchos millones de años después, la corteza oceánica finalmente desciende a lo largo de los márgenes continentales donde se forman fosas oceánicas (cañones estrechos y muy profundos), por ejemplo a lo largo del borde de la cuenca del Océano Pacífico. El paso importante que dio Hess fue que las corrientes de convección serían la fuerza impulsora en este proceso, llegando a las mismas conclusiones que Holmes había obtenido décadas antes con la única diferencia de que el adelgazamiento de la corteza oceánica se realizó utilizando el mecanismo de Heezen de propagación a lo largo de las dorsales. Por lo tanto, Hess concluyó que el Océano Atlántico se estaba expandiendo mientras que el Océano Pacífico se estaba reduciendo. A medida que la vieja corteza oceánica se consume en las fosas (al igual que Holmes y otros, pensó que esto se hacía mediante el engrosamiento de la litosfera continental, no, como se entiende ahora, por el enterramiento a una escala mayor de la propia corteza oceánica en el manto), nuevo magma se eleva y erupciona a lo largo de las dorsales que se extienden para formar una nueva corteza. En efecto, las cuencas oceánicas se están reciclando perpetuamente, con la creación de una nueva corteza y la destrucción de la antigua litosfera oceánica que ocurren simultáneamente. Por lo tanto, los nuevos conceptos movilistas explicaron claramente por qué la Tierra no se agranda con la expansión del fondo del mar, por qué hay tan poca acumulación de sedimentos en el fondo del océano y por qué las rocas oceánicas son mucho más jóvenes que las rocas continentales.[15]​ Inversiones magnéticas y bandeado magnético A partir de la década de 1950, científicos como Victor Vacquier, utilizando instrumentos magnéticos (magnetómetros) adaptados de dispositivos aéreos desarrollados durante la Segunda Guerra Mundial para detectar submarinos, comenzaron a reconocer extrañas variaciones magnéticas en el fondo del océano. Este hallazgo, aunque inesperado, no fue del todo sorprendente porque se sabía que el basalto, la roca volcánica rica en hierro que forma el fondo del océano, contiene un mineral fuertemente magnético (magnetita) y puede distorsionar localmente las lecturas de la brújula. Esta distorsión fue reconocida por los marineros islandeses ya a finales del siglo XVIII. Más importante aún, debido a que la presencia de magnetita le da al basalto propiedades magnéticas mensurables, estas variaciones magnéticas recién descubiertas proporcionaron otro medio para estudiar el fondo del océano profundo. Cuando la roca recién formada se enfriaba, tales materiales magnéticos registraron el campo magnético terrestre en ese momento. Bandeado magnético del fondo marino. La dorsal es el eje de simetría de un patrón de bandas con polaridad alterna normal (color) e invertida (blanco) A medida que se cartografió cada vez más el fondo marino durante la década de 1950, las variaciones magnéticas resultaron no ser ocurrencias aleatorias o aisladas, sino que revelaron patrones reconocibles. Cuando estos patrones magnéticos se mapearon en una amplia región, el fondo del océano mostró un patrón similar a una cebra: una franja con polaridad normal y la franja adyacente con polaridad invertida. El patrón general, definido por estas bandas alternas de roca polarizada normal e inversamente, se conoció como bandas magnéticas y fue publicado por Ron G. Mason y sus colaboradores en 1961, quienes no encontraron, sin embargo, una explicación para estos datos en términos de expansión del fondo marino, como Vine, Matthews y Morley unos años más tarde.[16]​ El descubrimiento de las bandas magnéticas requería una explicación. A principios de la década de 1960, científicos como Heezen, Hess y Dietz habían comenzado a teorizar que las dorsales oceánicas marcan zonas estructuralmente débiles donde el suelo oceánico se estaba partiendo en dos a lo largo de la cresta de la dorsal. El nuevo magma de las profundidades de la Tierra se eleva fácilmente a través de estas zonas débiles y finalmente erupciona a lo largo de la cresta de las dorsales para crear una nueva corteza oceánica. Este proceso, que en un principio se denominó hipótesis de la cinta transportadora y más tarde expansión del fondo oceánico, opera durante muchos millones de años y continúa formando un nuevo fondo oceánico en todo el sistema de cordilleras oceánicas de 64.000 km de longitud.[17]​ Solo cuatro años después de que se publicaran los mapas con el patrón de cebra de bandas magnéticas, el vínculo entre la expansión del fondo oceánico y estos patrones fue establecido, correcta e independientemente, por Lawrence Morley, Fred Vine y Drummond Matthews, en 1963, conocida actualmente como la hipótesis de Vine-Matthews-Morley.[18] Esta hipótesis vinculó estos patrones con reversiones geomagnéticas y fue apoyada por varias líneas de evidencia: Edades de los basaltos del fondo oceánico. En rojo las rocas más jóvenes y en morado las más altiguas las franjas son simétricas alrededor de las crestas de las dorsales oceánicas; en o cerca de la cresta de la dorsal, las rocas son muy jóvenes y envejecen progresivamente lejos de la cresta de la dorsal; las rocas más jóvenes en la cresta de la dorsal siempre tienen la polaridad actual (normal); franjas de roca paralelas a la cresta de la dorsal alternan en polaridad magnética (normal-invertida-normal, etc.), lo que sugiere que se formaron durante diferentes épocas que documentan los episodios normales y de inversión (ya conocidos de estudios independientes) del campo magnético de la Tierra. En las dorsales no existen apenas sedimentos sino rocas volcánicas solidificadas, mientras que la cubierta sedimentaria va aumentando su grosor a ambos lados de la dorsal.[19]​ Al explicar tanto las bandas magnéticas similares a las de una cebra como la construcción del sistema de cordilleras oceánicas, la hipótesis de expansión del fondo oceánico ganó rápidamente adeptos y representó otro avance importante en el desarrollo de la teoría de la tectónica de placas. Además, la corteza oceánica ahora llegó a ser apreciada como una grabación en cinta natural de la historia de las inversiones del campo geomagnético del de la Tierra. En la actualidad, se dedican extensos estudios a la calibración de los patrones de inversión normal en la corteza oceánica, por un lado, y escalas de tiempo conocidas derivadas de la datación de capas de basalto en secuencias sedimentarias (magnetoestratigrafía), por el otro, para llegar a estimaciones de las tasas de propagación pasadas y reconstrucciones de placas.[16]​ La revolución de la tectónica de placas Después de todas estas consideraciones, la tectónica de placas (o, como se llamó inicialmente nueva tectónica global) fue rápidamente aceptada en el mundo científico, y siguieron numerosos artículos que definieron los conceptos implicados: Ciclo de Wilson. En 1965, Tuzo Wilson, quien había sido un promotor de la hipótesis de la extensión del fondo marino y la deriva continental desde el principio, agregó el concepto de fallas transformantes al modelo, completando las clases de tipos de fallas necesarias para hacer que la movilidad de las placas funcionara a nivel global.[20]​ En 1965 se celebró en la Royal Society de Londres un simposio sobre deriva continental que debe considerarse como el inicio oficial de la aceptación de la tectónica de placas por parte de la comunidad científica, y cuyos resúmenes se publican como Blackett, Bullard & Runcorn (1965). En este simposio, Edward Bullard y sus colaboradores mostraron con un cálculo de computadora cómo los continentes a ambos lados del Atlántico encajarían mejor para cerrar el océano, lo que se conoció como el famoso ajuste de Bullard. En 1966 Wilson publicó el artículo que se refería a reconstrucciones de placas tectónicas previas, introduciendo el concepto de lo que ahora se conoce como el ciclo de Wilson.[21]​ En 1967, en la reunión de la Unión Americana de Geofísica, W. Jason Morgan propuso que la superficie de la Tierra consta de 12 placas rígidas que se mueven entre sí. Jason Morgan propuso también la existencia de plumas del manto para explicar los puntos calientes.[22]​ Dos meses después Xavier Le Pichon publicó un modelo completo basado en seis placas principales con sus movimientos relativos, lo que marcó la aceptación final por parte de la comunidad científica de la tectónica de placas. En el mismo año McKenzie y Parker presentaron de forma independiente un modelo similar al de Morgan usando traslaciones y rotaciones en una esfera para definir los movimientos de las placas. La revolución de la tectónica de placas fue el cambio científico y cultural que se desarrolló a partir de la aceptación de la teoría de la tectónica de placas y supuso un cambio de paradigma y una revolución científica que transformó la geología. Límites de placas Son los bordes de una placa y es ahí donde se presenta la mayor actividad tectónica (sismos, formación de montañas, actividad volcánica), ya que es donde se produce la interacción entre placas. Hay tres clases de límite:[23]​ Divergentes: son límites en los que las placas se separan unas de otras y, por lo tanto, emerge magma desde regiones más profundas (por ejemplo, la dorsal mesoatlántica formada por la separación de las placas de Eurasia y Norteamérica y las de África y Sudamérica). Convergentes: son límites en los que una placa choca contra otra, formando una zona de subducción (la placa oceánica se hunde bajo la placa continental) o un cinturón orogénico (si las placas chocan y se comprimen). Son también conocidos como bordes activos. Transformantes: son límites donde los bordes de las placas se deslizan una con respecto a la otra a lo largo de una falla de transformación. En determinadas circunstancias se forman zonas de límite o borde, donde se unen tres o más placas formando una combinación de los tres tipos de límites. Límite divergente o constructivo: las dorsales Dorsal oceánica. Artículo principal: Borde divergente Son las zonas de la litosfera en que se forma nueva corteza oceánica y en las cuales se separan las placas. En los límites divergentes, las placas se alejan y el vacío que resulta de esta separación es rellenado por material de la corteza, que surge del magma de las capas inferiores. Se cree que el surgimiento de bordes divergentes en las uniones de tres placas está relacionado con la formación de puntos calientes. En estos casos se junta material de la astenosfera cerca de la superficie y la energía cinética es suficiente para hacer pedazos la litosfera. El punto caliente que originó la dorsal mesoatlántica se encuentra actualmente debajo de Islandia, y el material nuevo ensancha la isla algunos centímetros cada siglo. Un ejemplo típico de este tipo de límite son las dorsales oceánicas, como la dorsal mesoatlántica entre otras, y en el continente las grietas, como el Gran Valle del Rift. Límite convergente o destructivo La placa oceánica se hunde por debajo de la placa continental. Artículo principal: Borde convergente Las características de los bordes convergentes dependen del tipo de litosfera de las placas que chocan. Con frecuencia las placas no se deslizan en forma continua; sino que se acumula tensión en ambas placas hasta llegar a un nivel de energía acumulada que sobrepasa el necesario para producir el deslizamiento brusco de la placa marina. La energía potencial acumulada es liberada como presión o movimiento; debido a la titánica cantidad de energía almacenada, estos movimientos ocasionan terremotos, de mayor o menor intensidad. Los puntos de mayor actividad sísmica suelen asociarse con este tipo de límites de placas. Cuando una placa oceánica (más densa) choca contra una continental (menos densa) la placa oceánica es empujada debajo, formando una zona de subducción. En la superficie, la modificación topográfica consiste en una fosa oceánica en el agua y un grupo de montañas en tierra. En los Andes centrales, la interacción de la placa de Nazca con la placa sudamericana ha dado lugar a la formación del Oroclinal de Bolivia, una curvatura que refleja la intensa actividad tectónica en esta región.[24]​ Cuando dos placas continentales colisionan (colisión continental), se forman extensas cordilleras formando un borde de obducción. La cadena del Himalaya es el resultado de la colisión entre la placa Indoaustraliana y la placa Euroasiática. Cuando dos placas oceánicas chocan, el resultado es un arco de islas (por ejemplo, Japón). Límite transformante, conservativo o neutro Falla de San Andrés. Artículo principal: Borde transformante El movimiento de las placas a lo largo de las fallas de transformación puede causar considerables cambios en la superficie, lo que es particularmente significativo cuando esto sucede en las proximidades de un asentamiento humano. Debido a la fricción, las placas no se deslizan en forma continua, sino que se acumula tensión en ambas placas hasta llegar a un nivel de energía acumulada que sobrepasa el necesario para producir el movimiento. La energía potencial acumulada es liberada como presión o movimiento en la falla. Debido a la gran cantidad de energía almacenada, estos movimientos ocasionan terremotos de mayor o menor intensidad. Un ejemplo de este tipo de límite es la falla de San Andrés, ubicada en el oeste de Norteamérica, que es parte del sistema de fallas producto del roce entre la placa Norteamericana y la del Pacífico. Medición de la velocidad de las placas tectónicas La medición actual de la velocidad de las placas tectónicas se realiza mediante medidas precisas de GPS. La velocidad antigua de las placas se obtiene mediante la restitución de cortes geológicos (en corteza continental) o mediante la medida de la posición de las inversiones del campo magnético terrestre registradas en el fondo oceánico."
ksampletext_wikipedia_geol_mineral: str = "Mineral. Un mineral es una sustancia natural, de composición química definida. Normalmente es sólido e inorgánico, y tiene una cierta estructura cristalina. Es diferente de una roca, que puede ser un agregado de minerales o no minerales y que no tiene una composición química específica. La definición exacta de mineral es objeto de debate, especialmente con respecto a la exigencia de ser abiogénico, y en menor medida, a si debe tener una estructura atómica ordenada. El estudio de los minerales se llama mineralogía. Hay más de 5300 especies minerales conocidas, de ellas más de 5090 aprobadas por la Asociación Internacional de Mineralogía (IMA por sus siglas en inglés). Continuamente se descubren y describen nuevos minerales, entre 50 y 80 al año.[2] La diversidad y abundancia de especies minerales es controlada por la química de la Tierra. El silicio y el oxígeno constituyen aproximadamente el 75 % de la corteza terrestre, lo que se traduce directamente en el predominio de los minerales de silicato, que componen más del 90% de la corteza terrestre. Los minerales se distinguen por diversas propiedades químicas y físicas. Diferencias en la composición química y en la estructura cristalina distinguen varias especies, y estas propiedades, a su vez, están influidas por el entorno geológico de la formación del mineral. Cambios en la temperatura, la presión o en la composición del núcleo de una masa de roca causan cambios en sus minerales. Los minerales pueden ser descritos por varias propiedades físicas que se relacionan con su estructura química y composición. Las características más comunes que los identifican son la estructura cristalina y el hábito, la dureza, el lustre, la diafanidad, el color, el rayado, la tenacidad, la exfoliación, la fractura, la partición y la densidad relativa. Otras pruebas más específicas para la caracterización de ciertos minerales son el magnetismo, el sabor o el olor, la radioactividad y la reacción a los ácidos fuertes. Los minerales se clasifican por sus componentes químicos clave siendo los dos sistemas dominantes la clasificación de Dana y la clasificación de Strunz. La clase de silicatos se subdivide en seis subclases según el grado de polimerización en su estructura química. Todos los silicatos tienen una unidad básica en forma de tetraedro de sílice [SiO 4]4− , es decir, un catión de silicio unido a cuatro aniones de oxígeno. Estos tetraedros pueden ser polimerizados para dar las subclases: neosilicatos (no polimerizados, y por lo tanto, solo tetraedros), sorosilicatos (dos tetraedros enlazados entre sí), ciclosilicatos (anillos de tetraedros), inosilicatos (cadenas de tetraedros), filosilicatos (láminas de tetraedros), y tectosilicatos (redes en tres dimensiones de tetraedros). Otros grupos minerales importantes son los elementos nativos, sulfuros, óxidos, haluros, carbonatos, sulfatos y fosfatos. Definición Definición básica La definición general de un mineral comprende los siguientes criterios:[3]​ ser de origen natural; ser estable a temperatura ambiente; estar representado por una fórmula química; ser generalmente abiogénico (no resultado de la actividad de los organismos vivos); y tener disposición atómica ordenada. Las tres primeras características generales son menos debatidas que las dos últimas.[3]​: 2–4  El primer criterio significa que un mineral se tiene que formar por un proceso natural, lo que excluye compuestos antropogénicos. La estabilidad a temperatura ambiente, en el sentido más simple, es sinónimo de que el mineral sea sólido. Más específicamente, un compuesto tiene que ser estable o metaestable a 25 °C. Son ejemplos clásicos de excepciones a esta regla el mercurio nativo, que cristaliza a -39 °C, y el hielo de agua, que es sólido solo por debajo de 0 °C; puesto que estos dos minerales se habían descrito con anterioridad a 1959, fueron adoptados por la Asociación Internacional de Mineralogía (IMA).[4]​[5] Los avances modernos suponen un amplio estudio de los cristales líquidos, que también concierne ampliamente a la mineralogía. Los minerales son compuestos químicos, y, como tales, pueden ser descritos por una fórmula fija o una variable. Muchos grupos de minerales y especies están compuestos por una solución sólida; las sustancias puras generalmente no se encuentran debido a la contaminación o sustitución química. Por ejemplo, el grupo del olivino se describe por la fórmula variable (Mg, Fe) 2SiO 4, que es una solución sólida de dos especies de miembro extremo, la forsterita rica en magnesio y la fayalita rica en hierro, que se describen mediante una fórmula química fija. Otras especies minerales podrían tener composiciones variables, tales como el sulfuro de mackinawita, (Fe, Ni) 8, que es principalmente un sulfuro ferroso, pero que tiene una impureza de níquel muy significativa que se refleja en su fórmula.[3]​: 2–4 [6]​ El requisito de que una especie mineral para ser válida ha de ser abiogénica también se ha descrito como similar a que sea inorgánica; sin embargo, este criterio es impreciso y a los compuestos orgánicos se les ha asignado una rama de clasificación separada. Por último, la exigencia de tener una disposición atómica ordenada es generalmente sinónimo de cristalinidad; sin embargo, los cristales también son periódicos, por lo que se utiliza en su lugar el criterio más amplio.[3]​: 2–4  Una disposición atómica ordenada da lugar a una variedad de propiedades físicas macroscópicas, como la forma cristalina, la dureza y la exfoliación.[7]​: 13–14  Ha habido varias propuestas recientes para modificar la definición para considerar las sustancias biogénicas o amorfas como minerales. La definición formal de un mineral aprobada por la IMA en 1995 es: Un mineral es un elemento o compuesto químico que es normalmente cristalino y que se ha formado como resultado de procesos geológicos. IMA (1995)[8]​ Además, las sustancias biogénicas fueron excluidas explícitamente: Las sustancias biogénicas son compuestos químicos producidos totalmente por procesos biológicos sin un componente geológico (por ejemplo, cálculos urinarios, cristales de oxalato en tejidos vegetales, conchas de moluscos marinos, etc.) y no son considerados como minerales. Sin embargo, si hubo procesos geológicos implicados en la génesis del compuesto, entonces el producto puede ser aceptado como un mineral. IMA (1995)[8]​ Avances recientes Los sistemas de clasificación de minerales y sus definiciones están evolucionando para recoger los últimos avances de la ciencia mineral. Los cambios más recientes han sido la adición de una clase orgánica, tanto en el nuevo Dana y en los esquemas de la clasificación de Strunz.[9]​[10] La clase orgánica incluye un grupo muy raro de minerales con hidrocarburos. La Comisión sobre nuevos minerales y nombres de minerales de la IMA aprobó en 2009 un esquema jerárquico para la denominación y clasificación de los grupos minerales y de los nombres de los grupos y estableció siete comisiones y cuatro grupos de trabajo para revisar y clasificar los minerales en una lista oficial de sus nombres publicados.[11]​[12] De acuerdo con estas nuevas reglas, las especies minerales pueden ser agrupadas de diferentes maneras, sobre la base de la química, la estructura cristalina, la aparición, la asociación, la historia genética o los recursos, por ejemplo, dependiendo de la finalidad para que sirva la clasificación. mineral species can be grouped in a number of different ways, on the basis of chemistry, crystal structure, occurrence, association, genetic history, or resource, for example, depending on the purpose to be served by the classification. IMA[11]​ La exclusión de Nickel (1995) de las sustancias biogénicas no fue universalmente respetada. Por ejemplo, Heinz A. Lowenstam (1981) declaró que «los organismos son capaces de formar una gran variedad de minerales, algunos de los cuales no se pueden formar inorgánicamente en la biosfera.»[13] La distinción es una cuestión de clasificación y tiene menos que ver con los constituyentes de los minerales mismos. Skinner (2005) considera todos los sólidos como minerales potenciales e incluye los biominerales en el reino mineral, que son aquellos creados por las actividades metabólicas de los organismos. Skinner amplió la definición previa de un mineral para clasificar como mineral cualquier «elemento o compuesto, amorfo o cristalino, formado a través de los procesos biogeoquímicos».[14]​ Los recientes avances en la genéticas de alta resolución y espectroscopía de absorción de rayos X están proporcionando revelaciones sobre las relaciones biogeoquímicas entre microorganismos y minerales que pueden hacer obsoleta la exclusión biogénica de Nickel (1995) y una necesidad la inclusión biogénica de Skinner (2005).[8]​[14] Por ejemplo, el IMA encargó al Grupo de trabajo de Mineralogía ambiental y Geoquímica[15] tratar de los minerales en la hidrosfera, atmósfera y biosfera. El alcance del grupo incluye microorganismos formadores de minerales, que existen en casi todas las rocas, en el suelo y en la superficie de las partículas que atraviesan el globo hasta una profundidad de al menos 1600 metros por debajo del fondo del mar y 70 kilómetros en la estratosfera (posiblemente se introduzcan en la mesosfera).[16]​[17]​[18] Los ciclos biogeoquímicos han contribuido a la formación de minerales durante miles de millones de años. Los microorganismos pueden precipitar los metales de la disolución, contribuyendo a la formación de yacimientos de mineral. También pueden catalizar la disolución de los minerales.[19]​[20]​[21]​ Antes de la lista de la Asociación Internacional de Mineralogía, más de 60 biominerales ya habían sido descubiertos, nombrados y publicados.[22] Estos minerales (un subconjunto tabulado en Lowenstam (1981)[13]​) se consideran propiamente minerales de acuerdo con la definición de Skinner (2005).[14] Estos biominerales no figuran en la lista oficial de nombres de minerales de la IMA,[23] aunque muchos de estos biominerales representativos se distribuyen entre las 78 clases minerales que figuran en la clasificación de Dana.[14] Otra clase rara de minerales (principalmente de origen biológico) incluye los cristales líquidos minerales que tienen propiedades tanto de líquidos y cristales. Hasta la fecha se han identificado más de 80.000 compuestos cristalinos líquidos.[24]​[25]​ La definición de mineral de Skinner (2005) toma en cuenta esta cuestión afirmando que un mineral puede ser cristalino o amorfo, incluyendo en este último grupo los cristales líquidos.[14] Aunque los biominerales y los cristales líquidos no son la forma más común de minerales,[26] ayudan a definir los límites de lo que constituye propiamente un mineral. La definición formal de Nickel (1995) menciona explícitamente la cristalinidad como una clave para la definición de una sustancia como un mineral. Un artículo de 2011 define la icosahedrita, una aleación de hierro-cobre-aluminio, como mineral; llamada así por su singular simetría icosaédrica natural, es un cuasi cristal. A diferencia de un verdadero cristal, los cuasicristales están ordenados pero no de forma periódica.[27]​[28]​ Rocas, menas y gemas Un esquisto es una roca metamórfica que se caracteriza por la abundancia de placas minerales. En este ejemplo, la roca tiene prominentes porfiroblastos de silimanita (de hasta 3 cm). Los minerales no son equivalentes a las rocas. Una roca puede ser un agregado de uno o más minerales, o no tener ningún mineral.[7]​: 15–16  Rocas como la caliza o la cuarcita se componen principalmente de un mineral —calcita o aragonito en el caso de la caliza, y cuarzo, en la última—.[7]​: 719–721, 747–748  Otras rocas pueden ser definidas por la abundancia relativa de los minerales clave (esenciales); un granito está definido por las proporciones de cuarzo, feldespato alcalino y plagioclasa.[7]​: 694–696  Los otros minerales de la roca se denominan accesorios, y no afectan en gran medida la composición global de la roca. Las rocas también pueden estar compuestas enteramente de material no mineral; el carbón es una roca sedimentaria compuesta principalmente de carbono derivado de manera orgánica.[7]​: 15–16, 728–730  En las rocas, algunas especies y grupos minerales son mucho más abundantes que otros; estos se denominan minerales formativos. Los principales ejemplos son el cuarzo, feldespatos, las micas, los anfíboles, los piroxenos, los olivinos, y la calcita; excepto la última, todos son minerales silicatos.[3]​: 15  En general, alrededor de unos 150 minerales se consideran particularmente importantes, ya sea en términos de su abundancia o valor estético en términos de coleccionismo.[7]​: 14  Los minerales y rocas comercialmente valiosos se conocen como minerales industriales y rocas industriales. Por ejemplo, la moscovita, una mica blanca, puede ser utilizada para ventanas (a veces conocida como isinglass), como material de relleno o como aislante.[7]​: 531–532  Las menas son minerales que tienen una alta concentración de un determinado elemento, normalmente un metal. Ejemplos de ello son el cinabrio (HgS), un mineral de mercurio, esfalerita (ZnS), un mineral de zinc, o la casiterita (SnO 2), un mineral de estaño. Las gemas son minerales con un alto valor ornamental, y se distinguen de las no gemas por su belleza, durabilidad, y por lo general, rareza. Hay alrededor de 20 especies minerales que se califican como minerales gema, que constituyen alrededor de las 35 piedras preciosas más comunes. Los minerales gema están a menudo presentes en diversas variedades, y así un mineral puede dar cuenta de varias piedras preciosas diferentes; por ejemplo, rubí y el zafiro son ambas corindón, Al 3.[7]​: 14–15  Nomenclatura y clasificación Clasificación histórica de los minerales Los minerales se solían clasificar en la antigüedad con criterios de su aspecto físico; Teofrasto, en el s. III a. C., creó la primera lista sistemática cualitativa conocida; Plinio el Viejo (s. I d. C.), en su Historia Natural, realizó una sistemática mineral, trabajo que, en la Edad Media, sirvió de base a Avicena; Linneo (1707-1778) intentó idear una nomenclatura fundándose en los conceptos de género y especie, pero no tuvo éxito y dejó de usarse en el siglo XIX; con el posterior desarrollo de la química, el químico sueco Axel Fredrik Cronstedt (1722-1765) elaboró la primera clasificación de minerales en función de su composición; el geólogo estadounidense James Dwight Dana, en 1837, propuso una clasificación considerando la estructura y composición química. La clasificación más actual se funda en la composición química y la estructura cristalina de los minerales. Las clasificaciones más empleadas son las de Strunz y Kostov. Clasificación moderna Los minerales se clasifican según la variedad, especie, serie y grupo, en orden creciente de generalidad. El nivel básico de definición es el de las especies minerales, que se distinguen de otras especies por sus propiedades químicas y físicas específicas y únicas. Por ejemplo, el cuarzo se define por su fórmula química, SiO 2, y por una estructura cristalina específica que lo distingue de otros minerales con la misma fórmula química (denominados polimorfos). Cuando existe un rango de composición entre dos especies minerales, se define una serie mineral. Por ejemplo, la serie de la biotita está representada por cantidades variables de la endmembers flogopita, siderofilita, annita, y eastonita. Por contraste, un grupo mineral es una agrupación de especies minerales con algunas propiedades químicas comunes que comparten una estructura cristalina. El grupo piroxeno tiene una fórmula común de XY(Si, Al) 6, en donde X e Y son ambos cationes, siendo X generalmente mayor que Y (radio iónico); los piroxenos son silicatos de cadena sencilla que cristalizan en cualquiera de los sistemas cristalinos monoclínico o ortorrómbico. Finalmente, una variedad mineral es un tipo específico de especies minerales que difieren por alguna característica física, como el color o el hábito del cristal. Un ejemplo es la amatista, que es una variedad púrpura del cuarzo.[3]​: 20–22  Para ordenar minerales dos son las clasificaciones más comunes, la de Dana y la de Strunz, ambas basadas en la composición, en especial respecto a los grupos químicos importantes, y en la estructura. James Dwight Dana, un geólogo principal de su tiempo, publicó por primera vez su System of Mineralogy [Sistema de Mineralogía] en 1837; en 1997 se editó su octava edición. La clasificación de Dana asigna un número de cuatro partes a una especie mineral. Su número de clase se basa en los grupos de composición importantes; el número de tipo da la relación de cationes/aniones en el mineral; y los dos últimos números corresponden al grupo de minerales por similitud estructural dentro de un tipo o clase determinada. La clasificación de Strunz —utilizada con menor frecuencia y llamada así por el mineralogista alemán Karl Hugo Strunz— se basa en el sistema de Dana, pero combina tanto criterios químicos como estructurales, estos últimos con respecto a la distribución de los enlaces químicos.[3]​: 558–559  En enero de 2016, la IMA había aprobado 5.090 especies minerales.[29] Se han nombrado en general en honor de una persona (45 %) —ver: Anexo:Minerales nombrados según personas—, seguidos por la ubicación del lugar, mina o yacimiento del descubrimiento (23 %); otras etimologías comunes son los nombres basados en la composición química (14 %) y en las propiedades físicas (8 %).[3]​: 20–22, 556  El sufijo común -ita usado en los nombres de las especies minerales desciende del antiguo sufijo griego - ί τ η ς (-ites), que significa 'relacionado con' o 'que pertenece a'.[30]​ Química mineral Hübnerita, el miembro final rico en manganeso de la serie de la wolframita, con cuarzo menor en el fondo La abundancia y diversidad de minerales es controlada directamente por su composición química, que a su vez, depende de la abundancia de los elementos en la Tierra. La mayoría de los minerales observados derivan de la corteza terrestre. Ocho elementos representan la mayor parte de los componentes clave de los minerales, debido a su abundancia en la corteza terrestre. Estos ocho elementos suponen más del 98 % de la corteza en peso, y son, en orden decreciente: oxígeno, silicio, aluminio, hierro, magnesio, calcio, sodio y potasio. El oxígeno y el silicio son, con mucho, los dos más importantes —el oxígeno compone, en peso, el 46,6 % de la corteza terrestre, y el silicio un 27,7 %.[3]​: 4–7  Los minerales que se forman son controlados directamente por la química mayor del cuerpo matriz. Por ejemplo, un magma rico en hierro y magnesio formará minerales máficos, como el olivino y los piroxenos; por el contrario, un magma más rico en sílice cristalizará para formar minerales que incorporen más SiO 2, como los feldespatos y cuarzos. La caliza, la calcita o la aragonita (todas CaCO 3) se forman porque la roca es rica en calcio y carbonato. Un corolario es que no se encontrará un mineral en una roca cuya química mayor no se parezca a la química mayor del mineral dado, con la excepción de algunas trazas de minerales. Por ejemplo, la cianita, Al 2SiO 5, se forma a partir del metamorfismo de lutitas ricas en aluminio; no sería probable que ocurriera en rocas pobres en aluminio, como la cuarcita. La composición química puede variar entre las especies terminales de una serie de solución sólida. Por ejemplo, los feldespatos plagioclasa comprenden una serie continua que va desde el miembro extremo de la albita, rica en sodio (NaAlSi 8), hasta la anortita, rica en calcio (CaAl 2Si 8), con cuatro variedades intermedias reconocidas entre ellas (recogidas en orden de riqueza del sodio al calcio): oligoclasa, andesina, labradorita y bytownita.[3]​: 586  Otros ejemplos de serie son la serie del olivino, desde la forsterita, rica en magnesio, a la fayalita, rica en hierro, y la serie del wolframita, desde la hübnerita, rica en manganeso, hasta la ferberita, rica en hierro. La sustitución química y la coordinación de poliedros explican esta característica común de los minerales. En la naturaleza, los minerales no son sustancias puras, y se contaminan por otros elementos que están presentes en el sistema químico dado. Como resultado, es posible que un elemento sea sustituido por otro.[3]​: 141  La sustitución química se producirá entre iones de un tamaño y carga similares; por ejemplo, K+  no sustituirá a Si4+  debido a las incompatibilidades químicas y estructurales causadas por la gran diferencia en tamaño y carga. Un ejemplo común de sustitución química es el del Si4+ > por Al3+ , que están próximos en carga, tamaño y abundancia en la corteza terrestre. En el ejemplo de la plagioclasa, hay tres casos de sustitución. Los feldespatos son todos armazones de sílice, que tienen una relación de silicio-oxígeno de 2:1, y el espacio para otros elementos se da por la sustitución del ion Si4+  por el ion Al3+  para dar una unidad de base de [AlSi 8]− ; sin la sustitución, la fórmula puede ser cargada-equilibrada como SiO 2, dando cuarzo.[3]​: 14  La importancia de esta propiedad estructural se explica además por los poliedros de coordinación. La segunda sustitución se produce entre el ion Na+  y el ion Ca2+ ; sin embargo, la diferencia en la carga tiene que contabilizarse haciendo una segunda sustitución del ion Si4+  por el ion Al3+ .[3]​: 585  La coordinación de poliedros es una representación geométrica de cómo un catión está rodeado por un anión. En mineralogía, debido a su abundancia en la corteza terrestre, los poliedros de coordinación se consideran generalmente en términos del oxígeno. La unidad base de los minerales de silicato es el tetraedro de sílice —un ion [SiO 4]4−  rodeado de cuatro O2− —. Una forma alternativa de describir la coordinación del silicato es mediante un número: en el caso del tetraedro de sílice, se dice que tiene un número de coordinación de 4. Diversos cationes tienen un rango específico de posibles números de coordinación; para el silicio, es casi siempre 4, excepto para minerales de muy altas presiones en los que los compuestos se comprimen de tal manera que el silicio está seis veces (octaédrico) coordinado con el oxígeno. Los cationes mayores tienen un número de coordinación más grande debido al aumento en el tamaño relativo en comparación con el oxígeno (la última subcapa orbital de los átomos más pesados es diferente también). Los cambios en los números de coordinación conduce a diferencias físicas y mineralógicas; por ejemplo, a alta presión, tal como en el manto, muchos minerales, especialmente algunos silicatos como el olivino y los granates cambiarán a una estructura de perovskita, en el que el silicio está en coordinación octaédrica. Otro ejemplo son los aluminosilicatos cianita, andalucita y silimanita (polimorfos, ya que comparten la fórmula Al 2SiO 5), que se diferencian por el número de coordinación del Al3+ ; estos minerales transitan de uno al otro como una respuesta a los cambios en la presión y en la temperatura.[3]​: 4–7  En el caso de materiales de silicato, la sustitución del ion Si4+  por Al3+  permite una variedad de minerales, debido a la necesidad de equilibrar las cargas.[3]​: 12–17  Cuando los minerales reaccionan, los productos a veces asumirán la forma del reactivo; el producto mineral se denomina por ser un pseudomorfo de (o después) del reactivo. Aquí se ilustra un pseudomorfo de la caolinita después de la ortoclasa. Aquí, el pseudomorfo conserva la macla Carlsbad común en la ortoclasa. Los cambios de temperatura, de presión y de composición alteran la mineralogía de una roca simple: los cambios en la composición pueden ser causados por procesos como la erosión o metasomatismo (alteración hidrotérmica); los cambios en la temperatura y en la presión se producen cuando la roca madre se somete a movimientos tectónicos o magmáticos en diferentes regímenes físicos; y los cambios en las condiciones termodinámicas favorecen que algunas asociaciones de minerales reaccionen entre sí para producir nuevos minerales. Como tal, es posible que dos rocas tengan una química de roca base idéntica, o muy similar, sin tener una mineralogía similar. Este proceso de alteración mineralógica está relacionado con el ciclo de las rocas. Un ejemplo de una serie de reacciones minerales se ilustra como sigue.[3]​: 549  El feldespato ortoclasa (KAlSi 8) es un mineral que se encuentra comúnmente en el granito, una roca ígnea plutónica. Cuando se expone a la intemperie, reacciona para formar caolinita (Al 2Si 5(OH) 4, un mineral sedimentario, y ácido silícico): 2KAlSi 8 + 5H 2O + 2H+  → Al 2Si 5(OH) 4 + 4H 2SiO 3 + 2K+ Bajo condiciones metamórficas de bajo grado, la caolinita reacciona con el cuarzo para formar pirofilita (Al 2Si 10(OH) 2): 2Si 5(OH) 4 + SiO 2 → Al 2Si 10(OH) A medida que aumenta el grado metamórfico, la pirofilita reacciona para formar cianita y cuarzo: 2Si 10(OH) 2 → Al 2SiO 5 + 3SiO 2 + H Alternativamente, un mineral puede cambiar su estructura cristalina como consecuencia de cambios de temperatura y de presión sin reaccionar. Por ejemplo, el cuarzo se convertirá en una variedad de sus polimorfos de SiO 2, como la tridimita y la cristobalita a altas temperaturas, y en coesita a altas presiones.[3]​: 579  Propiedades físicas de los minerales La caracterización de los minerales puede variar de ser muy simple a muy difícil. Un mineral puede ser identificado por varias propiedades físicas, siendo algunas de ellas suficientes para una plena identificación sin ambigüedades. En otros casos, los minerales solo se pueden clasificar mediante análisis más complejos, ópticos, químicos o de difracción de rayos X; estos métodos, sin embargo, pueden ser costosos y consumen mucho tiempo. Las propiedades físicas que se estudian para la clasificación son la estructura cristalina y el hábito, la dureza y el lustre, la diafanidad, el color, el rayado, la exfoliación y la fractura, y la densidad relativa. Otras pruebas menos generales son la fluorescencia y fosforescencia, el magnetismo, la radioactividad, la tenacidad (respuesta a los cambios mecánicos inducidos de forma), la piezoelectricidad y la reactividad para diluir ácidos.[3]​: 22–23  Estructura cristalina y hábito Acicular natrolita Artículos principales: Sistema cristalino, Hábito cristalino y Macla. El topacio tiene una forma característica de cristal alargado ortorrómbico. La estructura cristalina resulta de la disposición espacial geométrica ordenada de los átomos en la estructura interna de un mineral. Esta estructura cristalina se basa en una disposición atómica o iónica interna regular, que se expresa a menudo en la forma geométrica que el cristal toma. Incluso cuando los granos minerales son demasiado pequeños para ser vistos o son de forma irregular, la estructura cristalina subyacente siempre es periódica y se puede determinar por difracción de rayos X.[3]​: 2–4  Los minerales por lo general son descritos por su contenido de simetría. Los cristales están cristalográficamente restringidos a 32 grupos de puntos, que se diferencian por su simetría. Estos grupos se clasifican a su vez en categorías más amplias, siendo las de mayor alcance seis familias de cristales.[3]​: 69–80  (a veces una de las familias, la hexagonal, también se divide en dos sistemas cristalinos: el trigonal, que tiene un eje tres veces simétrico, y el hexagonal, que tiene un eje seis veces simétrico). Estas familias pueden ser descritas por las longitudes relativas de los tres ejes cristalográficos, y los ángulos que forman entre ellos; estas relaciones corresponden a las operaciones de simetría que definen los grupos de puntos más estrechos. Se resumen a continuación; a, b, y c representan los ejes, y α, β, y γ representan el ángulo opuesto al eje cristalográfico respectivo (por ejemplo, α es el ángulo opuesto al eje a, es decir el ángulo entre los ejes b y c.):[3]​: 69–80  Sistema cristalino Ejes Ángulos entre ejes Ejemplo comunes La química y la estructura cristalina, en conjunto, definen un mineral. Con una restricción a grupos de 32 puntos, los minerales de diferente química pueden tener una estructura cristalina idéntica. Por ejemplo, la halita (NaCl), la galena (PbS) y la periclasa (MgO) pertenecen todas al grupo de puntos hexaoctahedral (familia isométrica), ya que tienen una estequiometría similar entre sus diferentes elementos constitutivos. En contraste, los polimorfos son agrupaciones de minerales que comparten una fórmula química, pero que tienen una estructura diferente. Por ejemplo, la pirita y la marcasita, ambos sulfuros de hierro, tienen la fórmula FeS 2; sin embargo, el primero es isométrico mientras que el último es ortorrómbico. Este polimorfismo se extiende a otros sulfuros de fórmula genérica AX 2; estos dos grupos son conocidos colectivamente como los grupos de la pirita y marcasita.[3]​: 654–655  El polimorfismo se puede extender más allá del contenido de la pura simetría. Los aluminosilicatos son un grupo de tres minerales —cianita, andalucita y silimanita— que comparten la fórmula química Al 2SiO 5. La cianita es triclínica, mientras que la andalucita y la silimanita son ambas ortorrómbicas y pertenecen al grupo de puntos bipiramidal. Estas diferencias surgen en correspondencia a cómo el aluminio se coordina dentro de la estructura cristalina. En todos los minerales, un ion de aluminio está siempre seis veces coordinado con el oxígeno; el silicio, por regla general está en coordinación de cuatro veces en todos los minerales; una excepción es un caso como la stishovita (SiO 2, un polimorfo de cuarzo de ultra-alta presión con estructura de rutilo).[3]​: 581  En la cianita, el segundo aluminio está en coordinación seis veces; su fórmula química se puede expresar como Al [6]Al [6]SiO 5, para reflejar su estructura cristalina. La andalucita tiene el segundo aluminio en coordinación cinco veces (Al [6]Al [5]SiO 5) y la silimanita lo tiene en coordinación de cuatro veces ((Al [6]Al [4]SiO 5).[3]​: 631–632  Las diferencias en la estructura cristalina y la química influyen mucho en otras propiedades físicas del mineral. Los alótropos del carbono, el diamante y el grafito, tienen propiedades muy distintas; el diamante es la sustancia natural más dura, tiene un lustre adamantino, y pertenece a la familia isométrica, mientras que el grafito es muy blando, tiene un lustre grasiento, y cristaliza en la familia hexagonal. Esta diferencia se explica por diferencias en el enlace. En el diamante, los átomos de carbono están en orbitales híbridos sp3, lo que significa que forman un marco o armazón en el que cada carbono está unido covalentemente a cuatro vecinos de una manera tetraédrica. Por otro lado, el grafito forma láminas de átomos de carbono en orbitales híbridos sp2, en los que cada átomo de carbono está unido covalentemente a sólo otros tres. Estas hojas se mantienen unidas por fuerzas mucho más débiles que las fuerzas de van der Waals, y esta discrepancia se traduce en grandes diferencias macroscópicas.[3]​: 166  Maclas de contacto en la espinela La macla es la interpenetración entre dos o más cristales de una única especie mineral. La geometría de la macla está controlada por la simetría del mineral y, como resultado, hay varios tipos: de contacto, reticuladas, geniculadas, de penetración, cíclicas y polisintéticas. Las maclas de contacto, o maclas simples, constan de dos cristales unidos en un plano; este tipo de maclas es común en la espinela; las maclas reticuladas, comunes en forma de rutilo, son cristales entrelazados que se asemejan a un reticulado. Las maclas geniculadas tienen una mezcla en el medio que es causada por el comienzo del maclado. Las maclas de penetración constan de dos cristales individuales que han crecido uno dentro de otro; ejemplos de este hermanamiento son las maclas en forma de cruz de la estaurolita y las maclas de Carlsbad en la ortoclasa. Las maclas cíclicas son causadas por el maclado repetido en torno a un eje de rotación. Se produce alrededor de tres, cuatro, cinco, seis, o ocho ejes de plegado. Las maclas polisintéticas son similares a las maclas cíclicas por la presencia de maclados repetitivos aunque, en lugar de producirse alrededor de un eje de rotación, lo hacen siguiendo planos paralelos, por lo general en una escala microscópica.[3]​: 41–43 [7]​: 39  El hábito cristalino se refiere a la forma general de cristal. Se utilizan varios vocablos para describir esta propiedad: acicular, que describe cristales en forma de aguja como en la natrolita; acuchillado; arborescente o dendrítica (patrón de árbol, común en el cobre nativo); equante, que es típico del granate; prismático (alargado en una dirección); y tabular, que se diferencia de acuchillado en que el primero es plano mientras que este último tiene un alargamiento definido. En relación con la forma cristalina, la calidad de las caras del cristal es diagnóstico de algunos minerales, especialmente con un microscopio petrográfico. Los cristales euhedrales tienen una forma externa definida, mientras que los cristales anhedrales no la tienen; las formas intermedias se denominan subhedrales.[3]​: 32–39 [7]​: 38  Dureza Artículo principal: Escalas de dureza El diamante es el material natural más duro (dureza de Mohs de 10). La dureza de un mineral define cuánto puede resistir el rayado. Esta propiedad física depende de la composición química y de la estructura cristalina, y por ello no es necesariamente constante en todas las caras; la debilidad cristalográfica hace que algunas direcciones sean más blandas que otras.[3]​: 28–29  Un ejemplo de esta propiedad se muestra en la cianita, que tiene una dureza de Mohs de 5½ en la dirección paralela a [001], pero de 7 paralela a [100].[31]​ La escala más común de medición es la escala de dureza de Mohs ordinaria. Definida por diez indicadores, un mineral con un índice más alto raya los minerales que están por debajo de él en la escala. La escala va desde el talco, un silicato estratificado, hasta el diamante, un polimorfo de carbono que es el material natural más duro.[3]​: 28–29  Escala de Mohs de dureza Lustre y diafanidad Artículo principal: Lustre La pirita tiene un brillo metálico La esfalerita tiene un brillo submetálico El lustre o brillo indica cómo se refleja la luz que incide sobre la superficie del mineral, una propiedad que no depende del color y sí de su naturaleza química: es más intenso en sustancias que tienen enlaces metálicos y menor en las de enlaces iónicos o covalentes. El tipo y la intensidad del brillo dependen del índice de refracción y de la relación entre la luz absorbida y la reflejada. Hay numerosos vocablos cualitativos para su descripción, que se agrupan en tres: brillo metálico, cuando reflejan casi toda la luz visible que reciben. Son opacos y con índices de refracción mayores de 3. Suelen ser metales nativos (cuando no están oxidados) y muchos sulfuros (pirita) y óxidos de metales de transición (hematites). brillo submetálico, cuando reflejan una pequeña parte de la luz visible que reciben. Son opacos y su índice de refracción es ligeramente inferior a 3. Suelen ser elementos semimetálicos (grafito), sulfuros y óxidos. brillo no metálico, cuando transmiten la luz en cierto grado. Esta condición es ambigua y se emplean varios vocablos para estimar los matices: vítreo, con índice de refracción 1.33-2.00. Son minerales transparentes, en general compuestos por aniones oxigenados (oxoaniones), como carbonatos, sulfatos, fosfatos, silicatos, nitratos, etc. También varios halogenuros y óxidos (cuarzo hialino o cristal de roca); adamantino, con índice de refracción 2.00-2.50. Es el brillo típico del diamante y de algunas otras variedades aunque a veces para estas se usa el término «subadamantino»; nacarado o perlado, un brillo irisado típico de minerales fácilmente exfoliables, como las micas, el yeso y la apofilita; craso o graso, motivado por la presencia de pequeñas rugosidades en la superficie, a veces microscópicas. Lo tienen algunas blendas, la nefelina y el cuarzo en masa o lechoso; resinoso o céreo, de minerales como el azufre y ciertas blendas y granates; sedoso, característico de minerales fibrosos, como el yeso fibroso, la crisotila y la ulexita; mate, cuando no presentan ningún reflejo, como la creta (calcita) o las arcillas. En este caso también se dice que el mineral no tiene brillo.[3]​: 26–28  Diferentes brillos de minerales Vítreo: cuarzo Vítreo: cuarzo Adamantino: diamantes tallados Adamantino: diamantes tallados Nacarado: moscovita Nacarado: moscovita Craso: ópalo musgoso Craso: ópalo musgoso Resinoso: ámbar Resinoso: ámbar Sedoso: selenita, variedad del yeso Sedoso: selenita, variedad del yeso Ceroso: jade Ceroso: jade Mate: caolinita Mate: caolinita La diafanidad de un mineral describe la capacidad de la luz de pasar a través de él. Los minerales transparentes no disminuyen la intensidad de la luz que pasa a través de ellos. Un ejemplo de estos minerales es la moscovita (mica de potasio); algunas variedades son lo suficientemente claras como para haber sido utilizadas como vidrios en las ventanas. Los minerales translúcidos permiten pasar algo de luz, pero menos que los que son transparentes. La jadeíta y nefrita (formas minerales del jade) son ejemplos de minerales con esta propiedad. Los minerales que no dejan pasar la luz se denominan opacos.[32]​[3]​: 25  La diafanidad de un mineral depende del espesor de la muestra. Cuando un mineral es suficientemente delgado (por ejemplo, en una lámina delgada para petrografía) puede llegar a ser transparente, incluso si esa propiedad no se ve en la muestra de mano. Por el contrario, algunos minerales, como la hematita o la pirita son opacos incluso en láminas delgadas.[3]​: 25  Color y raya Artículo principal: Método de la raya El color, en general, no es una característica que permita caracterizar minerales. Se muestra una uvarovita verde (izquierda) y una grosularia rojo-rosada (derecha), ambos granates. Las propiedades que servirían para el diagnóstico serían los cristales rombododecaédricos, el lustre resinoso, y la dureza, de alrededor de 7. Elbaita dicróica Esmeralda El color es la propiedad más obvia de un mineral, pero a menudo no sirve para caracterizarlo.[3]​: 23  Es causada por la radiación electromagnética que interactúa con los electrones (excepto en el caso de incandescencia, que no se aplica a los minerales).[3]​: 131–144  Por su contribución en el color, se definen tres grandes clases de minerales: minerales idiocromáticos (o 'autocoloreados'), que deben su color a los constituyentes principales y que son diagnosticables.[32]​[3]​: 24  Son minerales siempre del mismo color, como la malaquita (verde), la azurita (azul) y muchos minerales metálicos. Sus colores suelen variar ligeramente debido a la presencia de pequeñas cantidades de otros metales: el oro, por ejemplo, es menos amarillo cuando se mezcla con un poco de plata, y más rosado cuando es mezclado junto con cobre. minerales alocromáticos (o 'coloreados por otros'), que deben su coloración a pequeñas cantidades en la composición consideradas como impurezas, a las que se llama cromóforos, usualmente metales (hierro, cromo, cobre, vanadio o manganeso). Son capaces de adoptar más de una coloración, como el berilo o las dos variedades del corindón, el rubí y el zafiro.[3]​: 24  Algunos minerales alocromáticos que pueden tener prácticamente cada color imaginable, e incluso pueden tener muchos colores en un solo cristal. minerales pseudocromáticos (o 'de color falso'), cuya coloración proviene de la estructura física del cristal y la interferencia con las ondas de luz. Son ejemplos la labradorita, la bornita y el ópalo, que está formado por capas microscópicas de esferas de sílice. Al pasar a su través la luz se separa en los colores que la componen, más o menos como ocurre cuando se refleja en una capa de aceite sobre el agua. Algunos metales, como el hierro, pueden ser tanto alocromático como idiocromático: en el primer caso es considerado como una impureza, mientras que en el segundo forma parte intrínseca del mineral coloreado. El color de algunos minerales puede cambiar, ya sea de manera natural o con un poco de ayuda. Los bajos niveles de radiación, que se dan a menudo en la naturaleza, pueden contribuir a oscurecer algunos minerales incoloros. Los mismos berilos de color amarillo verdoso se tratan artificialmente ahora con calor para darles una coloración más azulada. Además del simple color del cuerpo, los minerales pueden tener otras propiedades ópticas distintivas que pueden implican variabilidad del color: juego de colores, como en el ópalo, significa que la muestra refleja diferentes colores cuando se ilumina, a causa de que la luz se refleja desde las ordenadas esferas de sílice microscópicas de su estructura física.[33]​ pleocroísmo, facultad de absorber las radiaciones luminosas de distinta manera en función de la dirección de vibración: un mismo cristal puede aparecer con coloraciones diferentes dependiendo de la orientación en que haya caído en la preparación microscópica iridiscencia, una variedad del juego de colores por la que la luz se dispersa en un recubrimiento sobre la superficie del cristal, planos de exfoliación o capas desactivas que tienen gradaciones químicas menores.[3]​: 24–26  chatoyancia («ojo de gato») es el efecto de bandas onduladas de color que se observan cuando se rota la muestra; asterismo, una variedad de la chatonyancia, un fenómeno sobre un área que hace aparecer una estrella sobre la superficie reflectante de un corte de cabujón. Se da en algunos rubíes, zafiros y otras gemas (granate-estrella, diópsido-estrella, espinela-estrella, etc.) y particularmente en el corundum de calidad gema.[3]​: 24–26 [33]​ empañamiento Propiedades ópticas Asterismo en un zafiro-estrella azul Asterismo en un zafiro-estrella azul Ojo de tigre Ojo de tigre Iridiscencia en la labradorita Iridiscencia en la labradorita Barras de tungsteno con cristales evaporados, parcialmente oxidados con un colorido empañado Barras de tungsteno con cristales evaporados, parcialmente oxidados con un colorido empañado pleocroísmo en la cordierita, fuertemente dicroica pleocroísmo en la cordierita, fuertemente dicroica Placas de raya con pirita (izqda.) y rodocrosita (dcha.) La raya de un mineral se refiere al color de un mineral en forma de polvo, que puede o no ser idéntico al color de su cuerpo.[3]​: 24  La forma más común de evaluar esta propiedad se hace con una placa de raya, que está hecha de porcelana y es de color blanco o negro. La raya de un mineral es independiente de los elementos traza[32] o de cualquier alteración de la superficie a causa de la intemperie.[3]​: 24  Un ejemplo común de esta propiedad se ilustra con la hematita, que es de color negro, plata o rojo en la muestra, pero que tiene una raya de color rojo cereza.[32] a marrón rojizo.[3]​: 24  La raya es más a menudo distintiva de los minerales metálicos, en contraste con los minerales no metálicos, cuyo color de cuerpo está creada por elementos alocromáticos.[32] La prueba de la raya se ve limitada por la dureza del mineral, ya que los minerales de dureza superior a siete rayan ellos la placa.[3]​: 24  Exfoliación, partición, fractura y tenacidad Artículos principales: Exfoliación, Fractura y Tenacidad. Perfecta exfoliación basal en la biotita (negra), y buena exfoliación en la matrix (ortoclasa rosa) Por definición, los minerales tienen una disposición atómica característica y cualquier debilidad de esa estructura cristalina es la causa de la existencia de los planos de debilidad. La rotura del mineral a lo largo de esos planos se denomina exfoliación. La calidad de la exfoliación puede ser descrita en función de cómo de limpia y fácilmente se rompa el mineral; los vocablos con los que se describen comúnmente esa calidad, en orden decreciente, son «perfecto», «bueno», «distinto» y «pobre». En particular en los minerales transparentes, o en una sección delgada, la exfoliación se puede ver como una serie de líneas paralelas que señalan las superficies planas cuando se ven de lado. La exfoliación no es una propiedad universal de los minerales; por ejemplo, el cuarzo, compuesto por tetraedros de sílice muy interconectados, no tiene ninguna debilidad cristalográfica que le permitiría exfoliarse. Por el contrario, las micas, que tienen una exfoliación basal perfecta, consisten en láminas de tetraedros de sílice que se mantienen juntas muy débilmente.[3]​: 39–40 [7]​: 29–30  Como la exfoliación es función de la cristalografía, hay gran variedad de tipos de exfoliación produciéndose en uno, dos, tres, cuatro o seis direcciones. La exfoliación basal en una única dirección es una característica distintiva de las micas. La exfoliación en dos direcciones, denominada prismática, se produce en anfíboles y piroxenos. Los minerales como la galena o la halita tienen exfoliación cúbica (o isométrica) en tres direcciones, a 90°; cuando hay tres direcciones de exfoliación, pero no a 90°, como en la calcita o en la rodocrosita, se denomina exfoliación romboédrica. La exfoliación octaédrica (cuatro direcciones) está presente en la fluorita y en el diamante, y la esfalerita tiene seis direcciones de exfoliación del dodecaedro.[3]​: 39–40 [7]​: 30–31  Los minerales con muchas exfoliaciones pueden no romper igual de bien en todas las direcciones; por ejemplo, la calcita tiene buena exfoliación en tres direcciones, pero el yeso solo tiene una exfoliación perfecta en una dirección, y pobre en las otras dos. Los ángulos entre los planos de exfoliación varían entre los minerales. Por ejemplo, dado que los anfíboles son silicatos de cadena doble y los piroxenos son silicatos de cadena única, el ángulo entre sus planos de exfoliación es diferente: los piroxenos exfolian en dos direcciones a aproximadamente 90°, mientras que los anfíboles lo hacen claramente en dos direcciones separadas aproximadamente a 120° y 60°. Los ángulos de exfoliación se pueden medir con un goniómetro de contacto, que es similar a un transportador.[3]​: 39–40 [7]​: 30–31  La partición, a veces llamada «exfoliación falsa», es similar en apariencia a la exfoliación pero se produce por defectos estructurales en el mineral en lugar de por una debilidad sistemática. La partición varía de cristal a cristal de un mismo mineral, mientras que todos los cristales de un mineral determinado exfoliaran si la estructura atómica permite tal propiedad. En general, la partición es causada por una cierta tensión aplicada a un cristal. Las fuentes de las tensiones incluyen la deformación (por ejemplo, un aumento de la presión), exsolución o maclado. Los minerales que a menudo muestran partición son los piroxenos, la hematita, la magnetita y el corindón.[3]​: 39–40 [7]​: 30–31  Cuando un mineral se rompe en una dirección que no corresponde a un plano de exfoliación, se habla de fractura. Hay varios tipos: concoidea, cuando se forman superficies redondeadas cóncavas o convexas, de relieve suave. Se produce solo en minerales muy homogéneo, siendo el ejemplo clásico la fractura del cuarzo; lisa, cuando aparecen superficies planas, suaves y sin asperezas; desigual o irregular, cuando surgen superficies rugosas e irregulares. Se da en el cobre nativo[3]​: 31–33 ; fibrosa o astillosa, cuando se rompe como una madera, formando astillas; ganchuda, cuando la superficie de rotura aparece dentada; terrosa, cuando se desmorona como un terrón. La tenacidad está relacionada tanto con la exfoliación y la fractura. Mientras que la fractura y la exfoliación describen las superficies que se crean cuando el mineral se rompe, la tenacidad describe la resistencia que ofrece el mineral a tal ruptura. Los minerales pueden ser:[3]​: 30–31  frágiles, cuando rompen con facilidad con poco esfuerzo; maleables, cuando se laminan mediante golpes; sectiles, cuando se secciona con una cuchilla formando virutas; dúctiles, cuando se puede estirar convirtiéndose en un hilo; flexibles, cuando al ser doblados no recuperan la forma al cesar el esfuerzo; elásticos, cuando al ser doblados recuperan la forma al cesar el esfuerzo. Densidad relativa La galena (PbS) es un mineral de alta densidad relativa. La densidad relativa (a veces llamada gravedad específica) describe numéricamente la densidad de un mineral. Las dimensiones de la densidad son unidades de masa divididas por unidades de volumen: kg/m³ o en g/cm³. La densidad relativa mide la cantidad de agua desplazada por una muestra mineral. Se define como el cociente de la masa de la muestra y la diferencia entre el peso de la muestra en el aire y su correspondiente peso en agua; la densidad relativa es una relación adimensional, sin unidades. Para la mayoría de los minerales, esta propiedad no sirve para caracterizarlos. Los minerales que forman las rocas —normalmente silicatos y ocasionalmente carbonatos— tienen una densidad relativa de 2.5–3.5.<[3]​: 43–44  Una alta densidad relativa si permite diagnosticar algunos minerales. La variación química (y por consiguiente, en la clase mineral) se correlaciona con un cambio en la densidad relativa. Entre los minerales más comunes, los óxidos y sulfuros tienden a tener una alta densidad relativa, ya que incluyen elementos con mayor masa atómica. Una generalización es que los minerales metálicos o con brillo diamantino tienden a tener densidades relativas más altas que las que tienen los minerales no-metálicos o de brillo mate. Por ejemplo, la hematita, Fe 3, tiene una densidad relativa de 5.26[34] mientras que la galena, PbS, tiene una gravedad específica de 7.2–7.6,[35] que es el resultado de su alto contenido en hierro y en plomo, respectivamente. La densidad relativa es muy alta en los metales nativos; la kamacita, una aleación de hierro-níquel común en los meteoritos de hierro, tiene una densidad relativa de 7.9,[36] y el oro tiene una densidad relativa observada entre 15 y 19.3.[3]​: 43–44 [37]​ Otras propiedades Carnotita (amarillo) es un mineral radioactivo Se pueden utilizar otras propiedades para identificar minerales, aunque son menos generales y solo aplicables a ciertos minerales. La inmersión en ácido diluido (a menudo en HCl al 10 %) ayuda a distinguir los carbonatos de otras clases de minerales. El ácido reacciona con el grupo del carbonato ([CO3] 2-), lo que causa que el área afectada sufra efervescencia, con desprendimiento de gas dióxido de carbono. Esta prueba se puede ampliar para poner a prueba el mineral en su forma original de cristal o en polvo. Un ejemplo de esta prueba se realiza para distinguir la calcita de la dolomita, especialmente dentro de las rocas (caliza y dolomía, respectivamente). La efervescencia de la calcita es inmediata en ácido, mientras que para que lo haga la dolomita el ácido debe aplicarse a muestras en polvo o sobre una superficie rayada en una roca.[3]​: 44–45  Los minerales de zeolita no sufren efervescencia en ácido; en vez de eso, se vuelven esmerilados después de 5-10 minutos, y si se dejan en ácido durante un día, se disuelven o se convierten en un gel de sílice.[38]​ El magnetismo es una propiedad muy notable de ciertos minerales. Entre los minerales comunes, la magnetita muestra esta propiedad con fuerza, y también está presente, aunque no con tanta intensidad, en la pirrotita y la ilmenita.[3]​: 44–45  Algunos minerales también pueden identificarse mediante la prueba del sabor u olor. La halita, NaCl, es la sal de mesa; su homólogo de potasio, la silvita, tiene un sabor amargo pronunciado. Los sulfuros tienen un olor característico, sobre todo cuando las muestras están fracturadas, reaccionando o en polvo.[3]​: 44–45  La radiactividad es una propiedad poco frecuente, aunque algunos minerales pueden integrar elementos radiactivos. Pueden ser constituyentes que los definen, como el uranio en la uraninita, la autunita y la carnotita, o como impurezas traza. En este último caso, la desintegración de los elementos radiactivos daña el cristal mineral; el resultado, denominado «halo radiactivo» o «halo pleocroico», es observable mediante diversas técnicas, en especial en las láminas finas de petrografía.[3]​: 44–45  Clases de minerales Dado que la composición de la corteza terrestre está dominada por el silicio y el oxígeno, los elementos con silicatos son, con mucho, la clase de minerales más importante en términos de formación de rocas y diversidad: la mayoría de las rocas se componen en más de un 95% de minerales de silicato, y más del 90% de la corteza terrestre está compuesta por estos minerales.[3]​: 104  Además de los componentes principales, silicio y oxígeno, son comunes en los minerales de silicato otros elementos comunes en la corteza terrestre, como el aluminio, el magnesio, el hierro, el calcio, el sodio y el potasio.[3]​: 5  Los silicatos más importantes que forman rocas son los feldespatos, los cuarzos, los olivinos, los piroxenos, los anfíboles y las micas. A su vez, los minerales no-silicatos se subdividen en varias clases por su química dominante: elementos nativos, sulfuros, haluros, óxidos e hidróxidos, carbonatos y nitratos, boratos, sulfatos, fosfatos y compuestos orgánicos. La mayoría de las especies minerales no silicatos son extremadamente raras (constituyen en total un 8% de la corteza terrestre), aunque algunas son relativamente comunes, como la calcita, pirita, magnetita y hematita. Hay dos estilos estructurales principales observados en los no-silicatos: el empaquetamiento compacto y los tetraedros enlazados como aparecen en los silicatos. Las estructuras compactas son una manera de empaquetar densamente átomos y reducir al mínimo el espacio intersticial. El empaquetado compacto hexagonal consiste en apilar capas en las que cada capa es la misma (ababab), mientras que el empaquetado cúbico consiste en grupos de apilamiento de tres capas (abcabcabc). Análogos a los tetraedros de sílice enlazados son los tetraedros que forman los iones SO 4 (sulfato), PO 4 (fosfato), AsO 4 (arseniato), y VO 4 (vanadato). Los minerales no-silicatos tienen una gran importancia económica, ya que concentran más elementos que los minerales de silicato[3]​: 641–643  y se explotan especialmente como menas.[3]​: 641, 681  Silicatos Artículo principal: Minerales silicatos Esquema del tetraedro [SiO4]4−  base de los silicatos Los silicatos son sales que combinan la sílice SiO 2 con otros óxidos metálicos. La base de la unidad de un mineral de silicato es el tetraedro [SiO4]4− : en la mayoría de casos, el silicio se encuentra coordinado cuatro veces, o en coordinación tetraédrica, con el oxígeno; en situaciones de muy altas presiones, el silicio estará coordinado seis veces, o en coordinación octaédrica, como en la estructura de perovskita o en el cuarzo polimorfo stishovita (SiO2). (En el último caso, el mineral ya no tiene una estructura de silicato, si no de rutilo (TiO 2) y su grupo asociado, que son óxidos simples.) Estos tetraedros de sílice son luego polimerizados en algún grado para crear otras estructuras, como cadenas unidimensionales, láminas bidimensionales o armazones tridimensionales. El mineral de un silicato básico sin polimerización de tetraedros requiere de otros elementos que equilibren la base cargada 4-. En las otras estructuras de silicato son varias las combinaciones de elementos que equilibran esa carga negativa. Es común que el Si4+  sea sustituido por Al3+  debido a la similitud en radio iónico y en carga; en otros casos, los tetraedros de [AlO 4]5−  forman las mismas estructuras que lo hacían los tetraedros no sustituidos, pero los requisitos del equilibrio de cargas son diferentes.[3]​: 104–120  El grado de polimerización puede ser descrito tanto por la estructura formada como por el número de vértices tetraédricos (u oxígenos de coordinación) compartidos (por el aluminio y el silicio en sitios tetraédricos):[3]​: 105  los ortosilicatos (o nesosilicatos) no tienen ninguna vinculación de poliedros, así que los tetraedros no comparten vértices; los disilicatos (o sorosilicatos) tienen dos tetraedros que comparten un átomo de oxígeno; los inosilicatos son silicatos en cadena: los de cadena simple tienen dos vértices compartidos y los de cadena doble dos o tres; los filosilicatos forman una estructura de lámina que requiere tres oxígenos compartidos (en el caso de silicatos de cadena doble, algunos tetraedros deben compartir dos vértices en lugar de tres como harían si resultase una estructura de lámina); los silicatos en armazón o tectosilicatos, tienen tetraedros que comparten los cuatro vértices; los silicatos de anillo, o ciclosilicatos, solo necesitan tetraedros que compartan dos vértices para formar la estructura cíclica.[3]​: 104–117  Se describen a continuación en orden decreciente de polimerización, las subclases de silicato. Enlaces de tetraedros Ortosilicato: tetraedros simples Ortosilicato: tetraedros simples Sorosilicatos: dobles tetraedros Sorosilicatos: dobles tetraedros Inosilicatos: cadenas de tetraedros Inosilicatos: cadenas de tetraedros Inosilicatos: cadenas dobles de tetraedros Inosilicatos: cadenas dobles de tetraedros Ciclosilicatos: Anillos de tetraedros Ciclosilicatos: Anillos de tetraedros Tectosilicatos Artículo principal: Tectosilicato El cuarzo es el principal mineral de la serie de los tectosilicatos (cristal de roca de la mina La Gardette, Francia). Esquema de la estructura interna tridimensional de un cuarzo (cuarzo-β). Las esferas rojas representan iones de oxígeno y las esferas grises iones de silicio. Los tectosilicatos son muy abundantes, constituyendo aproximadamente el 64 % de los minerales de la corteza terrestre.[39]​También conocidos como silicatos de estructura en armazón, tienen el grado de polimerización más alto y tienden a ser químicamente estables como resultado de la fuerza de los enlaces covalentes.[7]​: 502  Son ejemplos el cuarzo, los feldespatos, los feldespatoides, y las zeolitas. Tienen una estructura basada en un entramado tridimensional de tetraedros (ZO 4) con los cuatro vértices ocupados por el ion O2- compartidos, lo que implica relaciones Z:O=1:2.[39] La Z es silicio (Si) (la fórmula resultante es SiO 2, sílice), pero parte del Si4+  puede ser reemplazado por Al3+  (en raras ocasiones por Fe3+ , Ti3+  y B3+ ).[40] Al suceder esto, las cargas negativas resultantes se compensan con la entrada de cationes grandes, como el K+ , el Na+  o el Ca2+  (y con menos frecuencia Ba2+ , Sr2+  y Cs+ ).[40] También pueden tener aniones complementarios F−, Cl−, S2−, CO32−, SO42−.[40]​ El cuarzo (SiO 2) es la especie mineral más abundante, formando el 12 % de la corteza terrestre. Se caracteriza por su alta resistividad química y física. Tiene varios polimorfos, incluyendo la tridimita y la cristobalita a altas temperaturas, la coesita a alta presión y la stishovita a ultra-alta presión. Este último mineral solo puede formarse en la Tierra por impacto de meteoritos, y su estructura está tan compuesta que había cambiado de una estructura de silicato a la de rutilo (TiO 2). El polimorfo de sílice que es más estable en la superficie de la Tierra es el α-cuarzo. Su homólogo, el cuarzo-β, está presente solo a altas temperaturas y presiones (a 1 bar, cambia a cuarzo-α por debajo de 573 °C). Estos dos polimorfos difieren en un retorcimiento de los enlaces; este cambio en la estructura da al cuarzo-β mayor simetría que al cuarzo-α, y por lo tanto también se les llama cuarzo alto (β) y cuarzo bajo (α).[3]​: 104 [3]​: 578–583  Los feldespatos son el grupo más abundante en la corteza terrestre, en torno al 50 %. En los feldespatos, los Al3+  sustitutos de los Si4+  crean un desequilibrio de carga que debe ser explicado por la adición de cationes. La estructura de base se convierte ya en [AlSi 8], ya en [Al 2Si 8]2− . Hay 22 especies minerales de feldespatos, subdivididas en dos grandes subgrupos —alcalino y plagioclasa— y dos grupos menos comunes —celsiana y banalsita—. Los feldespatos alcalinos son los más comunes en una serie que va desde la entre ortoclasa, rica en potasio, a la albita, rica en sodio; en el caso de las plagioclasas, la serie más común varía desde la albita a la anortita, rica en calcio. El maclado de cristales es común en los feldespatos, especialmente con maclas polisintéticas en las plagioclasas y maclas de Carlsbad en los feldespatos alcalinos. Si el último subgrupo se enfría lentamente a partir de una masa fundida, se forma laminillas de exsolución porque los dos componentes —ortoclasa y albita— son inestables en solución sólida. La exsolution puede darse desde una escala microscópica hasta ser fácilmente observable en la muestra de mano; se forma una textura pertitica cuando un feldespato rico en Na exsolve en un huésped rico en K. La textura opuesta (antipertitica), cuando un feldespato rico en K exsolve en un huésped rico en Na, es muy rara.[3]​: 583–588  Los feldespatoides son estructuralmente similares a los feldespatos, pero se diferencian en que se forman en condiciones de carencia de silicio lo que permite una mayor sustitución por Al3+ . Como resultado, los feldespatoides no se pueden asociar con cuarzo. Un ejemplo común de un feldespatoide es la nefelina ((Na, K)AlSiO 4); comparada con los feldespatos alcalinos, la nefelina tiene una relación Al 3: SiO 2 de 1: 2, en lugar de 1:6 en el feldespato.[3]​: 588  Las zeolitas a menudo tienen hábitos de cristal distintivos, produciendo agujas, placas o bloques masivos. Se forman en presencia de agua a bajas temperaturas y presiones, y tienen canales y huecos en su estructura. Las zeolitas tienen varias aplicaciones industriales, especialmente en el tratamiento de aguas residuales.[3]​: 589–593  Ejemplos de tectosilicatos Albita Albita Anortita Anortita Ortoclasa Ortoclasa Nefelina Nefelina Zeolita Zeolita Filosilicatos Artículo principal: Filosilicato Moscovita, una especie mineral del grupo de las micas, dentro de la subclase de los filosilicatos Modelo poliédrico de la lámina de tetraedros de sílice Los filosilicatos son un grupo de minerales muy extendidos en la corteza terrestre, integrantes de muchos tipos de rocas, ígneas, metamórficas y sedimentarias. Las arcillas están formadas fundamentalmente por filosilicatos. La característica principal de los filosilicatos es su disposición en capas, que ocasiona hábitos típicos fácilmente reconocibles (minerales hojosos o escamosos). Además suelen ser minerales blandos y poco densos. Los filosilicatos consisten en apilamientos de láminas de tetraedros polimerizados. Las láminas, desde el punto de vista estructural, son de dos tipos: tetraédricas y octaédricas. Los tetraédricas están enlazados a tres sitios de oxígeno, lo que da una relación característica de silicio:oxígeno de 2:5. Ejemplos importantes son la mica, el grupo de las cloritas y los grupos de caolinita-serpentina. Las láminas están débilmente enlazadas por fuerzas de van der Waals o enlaces de hidrógeno, lo que provoca una debilidad cristalográfica, que a su vez conduce a una prominente exfoliación basal entre los filosilicatos.[7]​: 525  Además de los tetraedros, los filosilicatos tienen una hoja de octaedros (elementos de coordinación seis con oxígeno) que equilibran los tetraedros de base, que tienen una carga negativa (por ejemplo, [Si 10]4− ) Estas hojas de tetraedros (T) y octaedros (O) se apilan en una gran variedad de combinaciones para crear los distintos grupos de los filosilicatos. En una capa octaédrica, hay tres sitios octaédricos en una estructura única; sin embargo, no todos los sitios pueden estar ocupados. En ese caso, el mineral se denomina dioctahédrico, mientras que en otro caso se denomina trioctaédrico.[3]​: 110  El grupo de la caolinita-serpentina consiste en pilas de T-O (minerales de arcilla 1:1); su dureza varía de 2 a 4, cuando las láminas están retenidas por enlaces de hidrógeno. Los minerales de arcilla 2:1 (pirofilita-talco) consisten en pilas T-O-T, pero son más blandos (dureza 1-2), ya que están se mantienen unidos por fuerzas de van der Waals. Estos dos grupos de minerales están divididos en subgrupos según la ocupación octaédrica; específicamente, la caolinita y la pirofilita son dioctaédricos mientras que la serpentina y el talco son trioctaédricos.[3]​: 110–113  Las micas son también filosilicatos T-O-T apilados, pero difieren de los otro miembros de las subclases apiladas T-O-T y T-O en que incorporan aluminio en las láminas tetraédricas (los minerales de arcilla tienen Al3+  en los sitios octaédricos). Ejemplos comunes de micas son la moscovita y las series de la biotita. El grupo de la clorita se relaciona con el grupo de la mica, pero con una capa similar a la brucita (Mg(OH) 2) entre la de las pilas T-O-T.<[3]​: 602–605  A causa de su estructura química, los filosilicatos típicamente tienen capas flexibles, elásticas, transparentes que son aislantes eléctricos y se pueden dividir en escamas muy finas. Las micas se puede utilizar en la electrónica como aislantes, en la construcción, como relleno óptico, o incluso en cosméticos. La crisotila, una especie de serpentina, es la especie mineral más común en el amianto industrial, ya que es menos peligrosa en términos de la salud que los asbestos anfíboles.[3]​: 593–595  Ejemplos de filosilicatos Fuchsita, una mica Fuchsita, una mica Biotita Biotita Crisotilo Crisotilo Brucita Brucita Serpentina Serpentina Inosilicatos Artículo principal: Inosilicato Disposición cristalina de los inosilicatos Tremolita asbestiforme, parte del grupo de los anfiboles en la subclase de los inosilicatos Aegirina, un clinopiroxeno hierro-sodio. Es parte de la subclase inosilicatos. Los inosilicatos son metasilicatos que consisten en tetraedros unidos repetidamente en cadenas. Estas cadenas pueden ser simples —cuando un tetraedro está unido a otros dos para formar una cadena continua— o dobles, cuando dos cadenas sencillas se combinan entre ellas. Los silicatos de cadena individuales tienen una relación de silicio:oxígeno de 1:3 (por ejemplo, [Si 6]4− ), mientras que las variedades de doble cadena tiene una proporción de 4:11, por ejemplo [Si 22]12− . Los inosilicatos tienen dos importantes grupos de minerales que forman rocas; los piroxenos, generalmente silicatos de cadena simple, y los anfiboles, de cadena doble.[7]​: 537  Hay cadenas de orden superior (por ejemplo, cadenas de tres, cuatro o cinco miembros) pero son raras.[41]​ El grupo de los piroxenos consta de 21 especies minerales.[3]​: 112  Los piroxenos tienen una fórmula de estructura general (XYSi 6), siendo X un sitio octaédrico e Y otro que puede variar en número de coordinación de seis a ocho. La mayoría de las variedades de los piroxenos consisten en permutaciones de Ca2+ , Fe2+  y Mg2+  que equilibran la carga negativa de la cadena principal. Los piroxenos son comunes en la corteza terrestre (aproximadamente el 10 %) y son un componente clave de las rocas ígneas máficas.[3]​: 612–613  Los anfiboles tienen una gran variabilidad química, por ello descritos a veces como un «cesto mineralógico» o un «tiburón mineralógico nadando en un mar de elementos». La columna vertebral de los anfíboles es la [Si 22]12− ; está equilibrada por cationes en tres posiciones posibles, aunque la tercera posición no siempre se utiliza y un elemento puede ocupar las restantes. Los anfíboles están generalmente hidratados, es decir, que tienen un grupo hidroxilo ([OH]− ), aunque puede ser reemplazado por un fluoruro, un cloruro, o un ion de óxido.[3]​: 606–612  Debido a su química variable, hay más de 80 especies de anfíboles, aunque las variaciones más comunes, como en los piroxenos, implican mezclas de Ca2+ , Fe2+  y Mg2+ .[3]​: 112  Varias especies minerales de los anfíboles pueden tener un hábito cristalino asbestiforme. Estos minerales de asbesto forman fibras largas, delgadas, flexibles y fuertes, que son aislantes eléctricos, químicamente inertes y resistentes al calor; como tal, tienen varias aplicaciones, especialmente en materiales de construcción. Sin embargo, los asbestos son conocidos carcinógenos, y causan varias enfermedades más, como la asbestosis; los asbestos anfíboles (antofilita, tremolita, actinolita, grunerita y riebeckita) se consideran más peligrosos que el asbesto serpentina crisotilo.[3]​: 611–612  Ejemplos de inosilicatoss Diopsida, un piroxeno Diopsida, un piroxeno Piroxeno Piroxeno Antofilita (anfibol) Antofilita (anfibol) Tremolita (anfibol) Tremolita (anfibol) Crocidolita, variedad de riebeckita (anfíbol) Crocidolita, variedad de riebeckita (anfíbol) Ciclosilicatos Artículo principal: Ciclosilicato Estructura en anillo de la dioptasa La clase de los ciclosilicatos corresponde a la clase 9.C de la clasificación de Strunz y tiene 16 familias. Está integrada por tres o más tetraedros de [SiO4]4− unidos por sus vértices, formando un anillo cerrado, simple o doble, el cual puede tener enlaces iónicos con metales como por ejemplo sodio, calcio, hierro, aluminio, potasio, magnesio, etc.[42] Algunos ejemplos de ciclosilicatos son la turmalina, cordierita, rubelita, benitoita, dioptasa, etc. Los ciclosilicatos, o silicatos de anillo, tienen una relación de silicio a oxígeno de 1:3. Los anillos de seis miembros son los más comunes, con una estructura de base de [Si 28]12− ; ejemplos del grupo son la turmalina y el berilo. Hay otras estructuras de anillo, habiendo sido descritas las de 3, 4, 8, 9 y 12.[3]​: 113–115  Los ciclosilicatos tienden a ser fuertes, con cristales alargados y estriados.[3]​: 558  Los anillos pueden ser simples o ramificados, aislados unos de otros o agrupados en dos. Estos anillos están generalmente apilados en la estructura y determinar canales que puede estar vacíos u ocupados por iones o moléculas. Los ciclosilicates se clasifican según el tipo de anillos, y en particular por el número de tetraedros en el anillo. Las turmalinas tienen una química muy compleja que puede ser descrita por una fórmula general XY 6(BO 3) 3T 18V 3W. El T 18 es la estructura básica del anillo, donde T es generalmente Si4+ , pero pueden ser sustituidos por Al3+  o B3+ . Las turmalinas pueden dividirse en subgrupos por el sitio que ocupe el X, y de ahí se subdividen por la química del sitio W. Los sitios Y y Z pueden acomodar una variedad de cationes, especialmente diversos metales de transición; esta variabilidad en el contenido del metal de transición estructural da al grupo de la turmalina mayor variabilidad en color. Otro ciclosilicato es el berilo, Al 2Be 3Si 18, cuyas variedades incluyen piedras preciosas como la esmeralda (verde) y la aguamarina (azulado). La cordierita es estructuralmente similar al berilo, y es un mineral metamórfico común.[3]​: 617–621  Ejemplos de ciclosilicatoss Elbaita, una turmalina con una distintiva banda coloreada Elbaita, una turmalina con una distintiva banda coloreada Benitoita Benitoita Cordierita Cordierita Dioptasa Dioptasa Berilo Berilo Sorosilicatos Artículo principal: Sorosilicato La epidota a menudo tiene un color verde pistacho distintivo. La clase de los sorosilicatos corresponde a la clase 9.B de la clasificación de Strunz y tiene 10 familias, de dos tipos, el de las epidotas y el de las idocrasas. Los sorosilicatos, también denominados disilicatos, tienen un enlace tetraedro-tetraedro en un oxígeno, lo que resulta en una relación de 2:7 de silicio al oxígeno. El elemento estructural común resultante es el grupo [Si 7]6− . Los disilicatos más comunes son, con mucho, los miembros del grupo de la epidota. Las epidotas se encuentran en diversos entornos geológicos, que van desde las cordilleras oceánicas a los granitos y hasta las metapelitas. Las epidotas se construyen alrededor de la estructura [(Si 4)(Si 7)]10− ; por ejemplo, las especies minerales de epidota tiene calcio, aluminio y hierro férrico para equilibrar las cargas: Ca 2Al 2(Fe3+ ,Al)(SiO 4)(Si 7)O(OH). La presencia de hierro como Fe3+  y Fe2+  ayuda a entender la fugacidad de oxígeno, que a su vez es un factor significativo en petrogénesis.[3]​: 612–627  Otros ejemplos de sorosilicatos son la lawsonita, un mineral metamórfico que forma las facies blueschist (ajuste de zona de subducción con baja temperatura y alta presión), la vesuvianita, que ocupa una cantidad significativa de calcio en su estructura química.[3]​: 612–627 [7]​: 565–573  Ortosilicatos Artículo principal: Ortosilicato Andradita negra, un miembro terminal del grupo de granates Modelo estructural del zirconio La clase de los ortosilicatos corresponde a la clase 9.A de la clasificación de Strunz y tiene 10 familias con cerca de 120 especies. Los ortosilicatos consisten en tetraedros aislados que tienen las cargas equilibrada por otros cationes.[3]​: 116–117  También denominados nesosilicatos, este tipo de silicatos tiene una relación silicio:oxígeno de 1:4 (por ejemplo, SiO 4). Los ortosilicatos típicos tienden a formar bloques de cristales equantes, y son bastante pesados.[7]​: 573  Varios minerales que forman rocas son parte de esta subclase, como los aluminosilicatos, el grupo del olivino o el grupo del granate. Los aluminosilicatos —cianita, andalucita, y silimanita, todos Al 2SiO 5— están estructuralmente compuestos por un tetraedro [SiO 4]−  y un Al3+  en coordinación octaédrica. El restante Al3+  puede estar en coordinación de seis (cianita), cinco (andalucita) o cuatro (silimanita); qué mineral se forma en un entorno dado depende de las condiciones de presión y temperatura. En la estructura del olivino, la serie principal de olivino (Mg, Fe) 2SiO 4 consisten en forsterita, rica en magnesio, y fayalita, rica en hierro. Tanto el hierro como el magnesio están en coordinación octaédrica con el oxígeno. Existen otras especies minerales que tienen esta estructura, como la tefroita, Mn 2SiO 4.[7]​: 574–575  El grupo del granate tiene una fórmula general de X 2(SiO 3, donde X es un gran catión ocho veces coordinado, e Y es un catión menor seis veces coordinado. Hay seis miembros terminales ideales de granate, divididos en dos grupos. Los granates piralspita tienen Al3+  en la posición Y: piropo (Mg 3Al 2(SiO 3), almandino (Fe 3Al 2(SiO 3), y espesartina (Mn 3Al 2(SiO 3). Los granates ugrandita tienen Ca2+  en la posición X: uvarovita (Ca 3Cr 2(SiO 3), grossular (Ca 3Al 2(SiO 3) y andradita (Ca 3Fe 2(SiO 3). Si bien hay dos subgrupos de granate, existen soluciones sólidas entre los seis miembros finales.[3]​: 116–117  Otros ortosilicatos son el zircón, la estaurolita y el topacio. El zirconio (ZrSiO 4) es útil en geocronología ya que el Zr4+  puede ser sustituido por U6+ ; además, debido a su estructura muy resistente, es difícil resetearlo como un cronómetro. La estaurolita es un común mineral índice de grado intermedio metamórfico. Tiene una estructura cristalina particularmente complicada que solo fue descrita plenamente en 1986. El topacio (Al 2SiO 4(F, OH) 2, que se encuentra a menudo en pegmatitas graníticas asociadas con turmalina, piedra preciosa es un mineral común.[3]​: 627–634  Ejemplos de ortosilicatos Andalucita, un aluminosilicato Andalucita, un aluminosilicato Almandina, del grupo del granate Almandina, del grupo del granate Humita Humita Ludwigita, del grupo del olivino Ludwigita, del grupo del olivino Zirconio Zirconio Minerales no silicatos Elementos nativos Artículo principal: Elementos nativos Oro nativo. Raro espécimen de cristales gruesos que crecen fuera de un tallo central (3.7 x 1.1 x 0.4 cm, de Venezuela). Los elementos nativos son aquellos minerales integrados por elementos que no están unidos químicamente a otros elementos. Este grupo incluye minerales metales nativos, semi-metales y no metales, y varias aleaciones sólidas y soluciones. Los metales se mantienen unidos por enlaces metálicos, lo que les confiere propiedades físicas distintivas, como su lustre metálico brillante, ductilidad y maleabilidad, y conductividad eléctrica. Los elementos nativos se subdividen en grupos por su estructura o atributos químicos. El grupo del oro, con una estructura cercana al empaquetamiento cúbico, incluye metales como el oro, la plata y el cobre. El grupo del platino es similar en estructura al grupo de oro. El grupo del hierro-níquel se caracteriza por tener varias especies de aleaciones de hierro-níquel. Dos ejemplos son la kamacita y la taenita, que se encuentran en meteoritos de hierro; estas especies difieren en la cantidad de Ni en la aleación; la kamacita tiene menos de 5–7 % de níquel y es una variedad de hierro nativo, mientras que el contenido de níquel de la taenita es del 7–37 %. Los minerales del grupo del arsénico se componen de semi-metales, que tienen solamente algunos metálicos; por ejemplo, carecen de la maleabilidad de los metales. El carbono nativo aparece en dos alótropos, el grafito y el diamante; el último se forma a muy alta presiones en el manto, lo que le confiere una estructura mucho más fuerte que el grafito.[3]​: 644–648  Sulfuros Artículo principal: Minerales sulfuros Cinabrio rojo (HgS), una mena del mercurio, sobre dolomita La clase de los minerales sulfuros y sulfosales —denominación engañosa pues los sulfuros solo son una parte del grupo— corresponde a la clase 2 de la clasificación de Strunz y en ella se incluyen: minerales sulfuros —con el ion S2− —-, los seleniuros, teluriuros, arseniuros, antimoniuros, bismutiuros, sulfoarseniuros y sulfosales. Los sulfuros se clasifican por la relación del metal o del semimetal con el azufre, M:S igual a 2:1, o 1:1.[3]​: 649  A pesar de que los sulfuros son mucho menos abundantes que los silicatos, su química y sus estructuras son muy variadas, lo que explica porque el número de minerales de sulfuro es muy alto en relación con su abundancia. Se agrupan entre los sulfuros los minerales compuestos de uno o más metales o semimetales con un azufre, que tienen una fórmula de tipo general de M p, donde M es un metal (Ag, Cu, Pb, Zn, Fe, Ni, Hg, As, Sb, Mo, Hg, Tl, V). Los arseniuros, los antimoniuros, los telurios... se clasifican entre los «sulfuros» sensu lato debido a su similitud estructural con los sulfuros. Los sulfuros minerales se caracterizan por la unión covalente, la opacidad y el brillo metálico; se estudian con el microscopio de reflexión. Los sulfuros tienden a ser blandos y frágiles, con un alto peso específico y la mayoría son semiconductores. Muchos sulfuros en polvo, como la pirita, tienen un olor sulfuroso cuando son pulverizados. Los sulfuros son susceptibles a la intemperie, y muchos se disuelven fácilmente en agua; estos minerales disueltos se pueden después volver a redepositar, lo que crea yacimientos de menas secundarias.[3]​: 357  Muchos minerales de sulfuro son importantes económicamente como minerales metálicos; son ejemplos la esfalerita (ZnS), una mena de zinc; la galena (PbS), una mena de plomo; el cinabrio (HgS), una mena de mercurio; y la molibdenita (MoS 2, una mena de molibdeno.[3]​: 651–654  La pirita (FeS 2) es el sulfuro que aparece más y se puede encontrar en la mayoría de entornos geológicos. No es, sin embargo, una mena de hierro, pero puede ser oxidada para producir ácido sulfúrico.[3]​: 654  Relacionados con los sulfuros están las raras sulfosales, en las que un elemento metálico está unido al azufre y a un semimetal, como antimonio, arsénico o bismuto. Al igual que los sulfuros, las sulfosales son típicamente minerales blandos, pesados y frágiles.[7]​: 383  Ejemplos de sulfuros Pirita, disulfuro de hierro Pirita, disulfuro de hierro Esfalerita, mena de zinc Esfalerita, mena de zinc Molibdenita, mena de molibdeno Molibdenita, mena de molibdeno Estannita, mena de estaño Estannita, mena de estaño Reálgar, sulfuro de arsénico Reálgar, sulfuro de arsénico Óxidos Artículo principal: Minerales óxidos La clase de los minerales óxidos e hidróxidos corresponde a la clase 4 de la clasificación de Strunz y en ella se incluyen: óxidos, hidróxidos, vanadatos, arsenitos, antimonitos, bismutitos, sulfitos, selenitos, teluritos y yodatos. Los minerales óxidos se dividen en tres categorías: óxidos simples, hidróxidos y óxidos múltiples. Los óxidos simples se caracterizan por O2−  como anión principal y enlace principalmente iónico. Se pueden subdividir además por la relación del oxígeno a los cationes. El grupo de la periclasa consta de minerales con una relación 1:1. Óxidos con una relación 2:1 incluyen la cuprita (Cu 2O) y el hielo de agua. minerales del grupo del corindón tienen una proporción de 2:3, e incluye minerales como el corindón (Al 3) y la hematita (Fe 3). Los minerales del grupo del rutilo tienen una proporción de 1:2; la especie del mismo nombre, rutilo (TiO 2) es el principal mena del titanio; Otros ejemplos incluyen la casiterita (SnO 2, mena de estaño), y pirolusita (MnO 2, mena de manganeso).[7]​: 400–403 [3]​: 657–660  En hidróxidos, el anión dominante es el ion hidroxilo, OH− . Las bauxitas son la mena principal del aluminio, y son una mezcla heterogénea de minerales de hidróxido de diáspora, gibbsita, y bohmita; se forman en áreas con una alta tasa de meteorización química (principalmente condiciones tropicales).[3]​: 663–664  Por último, varios óxidos son compuestos de dos metales con oxígeno. Un grupo importante dentro de esta clase son las espinelas, con una fórmula general de X2+ Y3+ 4. Ejemplos de especies incluyen la propia espinela (MgAl 4), la cromita (FeCr 4) y la magnetita (Fe 4). Esta última es fácilmente distinguible por su fuerte magnetismo, que se produce ya que tiene hierro en dos estados de oxidación (Fe2+ Fe3+ 4), lo que hace que sea un óxido múltiple en lugar de un óxido simple.[3]​: 660–663  Ejemplos de minerales óxidos Anatasa, dióxido de titanio (TiO 2) Anatasa, dióxido de titanio (TiO Cuprita, óxido de cobre Cuprita, óxido de cobre Casiterita, óxido de estaño Casiterita, óxido de estaño Gibbsita, hidróxido de aluminio Gibbsita, hidróxido de aluminio Magnetita, óxido de hierro Magnetita, óxido de hierro Haluros Cristales de halita cúbica rosa (NaCl; clase haluro) en una matriz de nahcolita (NaHCO 3; un carbonato, y la forma mineral del bicarbonato sódico, que se utilizan como bicarbonato de sodio). baking soda). Artículo principal: Minerales haluros La clase de los minerales haluros corresponde a la clase 3 de la clasificación de Strunz y en ella se incluyen: haluros o halogenuros simples o complejos, con H2O o sin ella, así como derivados oxihaluros, hidroxihaluros y haluros con doble enlace. Los minerales haluros son compuestos en los que un halógeno (flúor, cloro, yodo y bromo) es el anión principal. Estos minerales tienden a ser blandos, débiles, quebradizos y solubles en agua. Los ejemplos más comunes de haluros son la halita (NaCl, sal de mesa), la silvita (KCl) y la fluorita (CaF 2). La halita y la silvita se forman comúnmente como evaporitas, y pueden ser minerales dominantes en las rocas sedimentarias químicas. La criolita, Na 3AlF 6, es un mineral clave en la extracción de aluminio a partir de la bauxita; Sin embargo, dado que la única ocurrencia significativa está en Ivittuut, Groenlandia, en una pegmatita granítica, ya agotada, la criolita sintética se puede hacer a partir de la fluorita.[7]​: 425–430  Carbonatos Artículo principal: Minerales carbonatos Cristales de calcita de la mina Sweetwater, condado de Reynolds, Misuri (6,2 × 6 × 3,3 cm) La clase de los minerales carbonatos y nitratos corresponde a la clase 5 de la clasificación de Strunz y en ella se incluyen carbonatos, carbonatos de uranilo y nitratos. Los minerales carbonatos son aquellos en los que el grupo aniónico principal es un carbonato, [CO 3]2− . Los carbonatos tienden a ser frágiles, muchos tienen exfoliación romboédrica, y todos reaccionan con ácido.[7]​: 431  Debido a la última característica, los geólogos de campo a menudo llevan ácido clorhídrico diluido para distinguir los carbonatos de los no-carbonatos. La reacción del ácido con los carbonatos, que se encuentra más comúnmente como los polimorfos calcita y aragonita (CaCO 3), se refiere a la disolución y precipitación del mineral, que es un elemento clave en la formación de las cuevas de caliza —con elementos como estalactitas y estalagmitas— y los accidentes geográficos kársticos. Los carbonatos se forman con mayor frecuencia en forma de sedimentos biogénicos o químicos en ambientes marinos. El grupo carbonato es estructuralmente un triángulo, donde un catión central de C4+  está rodeado por tres aniones O2− ; diferentes grupos de minerales se forman a partir de diferentes disposiciones de estos triángulos.[3]​: 667  El mineral de carbonato más común es la calcita, que es el componente principal de la sedimentaria caliza y del mármol metamórfico. La calcita, CaCO 3, puede tener una impureza de alto contenido en magnesio; en condiciones de alto magnesio, se formará en su lugar su polimorfo, la aragonita; la geoquímica marina se puede describir, en este sentido, como un mar de aragonito o mar de calcita, dependiendo de qué mineral se forme preferentemente. La dolomita es un carbonato doble, de fórmula CaMg(CO 2. La dolomitization secundaria de la caliza es común, en la que la calcita o la aragonita se convierten en dolomita; esta reacción aumenta el espacio de los poros (el volumen de la celda unidad de la dolomita es el 88 % del de la calcita), lo que puede crear un yacimiento de petróleo y gas. Estas dos especies minerales son miembros de los grupos de minerales del mismo nombre: el grupo de la calcita incluye carbonatos con fórmula general XCO 3 y el de la dolomita la de XY(CO 2.[3]​: 668–669  Ejemplos de minerales carbonatos Rodocrosita Rodocrosita Smithsonita Smithsonita Dolomita con calcita y calcopirita Dolomita con calcita y calcopirita Azurita y malaquita Azurita y malaquita Hanksita, uno de los pocos minerales considerado un carbonato y un sulfato Hanksita, uno de los pocos minerales considerado un carbonato y un sulfato Sulfatos Artículo principal: Minerales sulfatos Rosa del desierto de yeso La clase de los minerales sulfatos corresponde a la clase 7 de la clasificación de Strunz y en ella se incluyen: sulfatos, selenatos, teluratos, cromatos, molibdatos y wolframatos. Los minerales sulfatos tienen todos el anión sulfato, [SO 4]2− . Tienden a ser de transparentes a translúcidos, blandos, y muchos son frágiles.[3]​: 453  Los minerales de sulfato se forman comúnmente como evaporitas, donde se precipitan de la evaporación de las aguas salinas; alternativamente, los sulfatos también se pueden encontrar en los sistemas de vetas hidrotermales asociados con sulfuros,[3]​: 456–457  o como productos de oxidación de sulfuros.[3]​: 674  Los sulfatos se pueden subdividir en minerales anhidros e hidratados. El sulfato hidratado más común, con mucho, es el yeso, CaSO 4⋅2H 2O. Se forma como un evaporita, y se asocia con otros evaporitas como la calcita y la halita; si incorpora granos de arena cuando cristaliza, el yeso puede formar rosas del desierto. El yeso tiene muy baja conductividad térmica y mantiene una temperatura baja cuando se calienta a medida que pierde el calor por deshidratación; como tal, el yeso se utiliza como aislante en materiales de construcción. El equivalente anhidro del yeso es la anhidrita; se puede formar directamente de agua de mar en condiciones muy áridas. El grupo de la barita tiene la fórmula general XSO 4, donde X es un catión grande 12-enlazado. Son ejemplos la barita (BaSO 4), la celestina (SrSO 4), y la anglesita (PbSO 4); la anhidrita no es parte del grupo de la barita, ya que el más pequeño Ca2+  sólo tiene enlace ocho veces.[3]​: 672–673  Ejemplos de minerales sulfatos Barita con cerusita Barita con cerusita Fenicocroíta, un cromato Fenicocroíta, un cromato Lindgrenita, molibdato de cobre Lindgrenita, molibdato de cobre Anhidrita Anhidrita Xocomecatlita, un tellurato Xocomecatlita, un tellurato Fosfatos Artículo principal: Minerales fosfatos La clase de los minerales fosfatos corresponde a la clase 8 de la clasificación de Strunz y en ella se incluyen fosfatos, arseniatos y vanadatos. Son 51 familias agrupadas en 7 divisiones, un grupo grande y diverso, que sin embargo, tiene solo unas pocas especies relativamente comunes. Los minerales fosfatos se caracterizan por el anión fosfato coordinado tetraédricamente [PO 4]3− , aunque la estructura se puede generalizar siendo el fósforo sustituido por antimonio ([SbO 4]3− ), arsénico ([AsO 4]3− ), o vanadio ([VO 4]3− ). Los aniones de cloro (Cl− ), flúor (F− ) e hidróxido (OH− ) también encajan en la estructura cristalina. El fosfato más común es el grupo de la apatita, un nombre genérico que designa fosfatos hexagonales de composición bastante variable, Ca 5(PO 3(OH, Cl,F). Las especies más comunes del grupo son la fluorapatita (Ca 5(PO 3F), la clorapatita (Ca 5(PO 3Cl) y la hidroxiapatita (Ca 5(PO 3(OH)). Los minerales de este grupo son los principales constituyentes cristalinos de los dientes y de los huesos de los vertebrados. Otro grupo relativamente abundante es el grupo de la monacita, que tiene una estructura general de ATO 4, donde T es el fósforo o arsénico, y A es, a menudo, un elemento de las tierras raras. La monacita es importante en dos sentidos: en primer lugar, como sumidero de tierras raras, puede concentrar la cantidad suficiente de estos elementos para convertirse en una mena; en segundo lugar, los elementos del grupo de la monacita pueden incorporar cantidades relativamente grandes de uranio y torio, que pueden ser utilizadas para datar una roca basándose en la desintegración del U y Th en plomo.[3]​: 675–680  Ejemplos de minerales fosfatos Apatita Apatita Vivianita, un fosfato hidratado de hierro Vivianita, un fosfato hidratado de hierro Piromorfita, un cloro-fosfato anhidro de plomo Piromorfita, un cloro-fosfato anhidro de plomo Turquesa, fosfato hidratado de cobre y aluminio Turquesa, fosfato hidratado de cobre y aluminio Lazulita, un fosfato de hierro, aluminio y magnesio Lazulita, un fosfato de hierro, aluminio y magnesio Minerales orgánicos Artículo principal: Minerales compuestos orgánicos La clase de los minerales compuestos orgánicos corresponde a la clase 10 de la clasificación de Strunz y en ella se incluyen sales y ácidos orgánicos que aparezcan en minas y los hidrocarburos. Son 7 familias agrupadas en 3 divisiones, un grupo escaso. Estos raros compuestos contienen carbono orgánico, pero se pueden formar también mediante un proceso geológico. Por ejemplo, la whewellita, CaC 4⋅H 2O es un oxalato que se puede depositar en las venas de menas hidrotermales. Mientras el oxalato de calcio hidratado se puede encontrar en las vetas de carbón y en otros depósitos sedimentarios que comprenden materia orgánica, la ocurrencia hidrotérmica no se considera que está relacionada con la actividad biológica.[3]​: 681  Importancia y utilidad Artículo principal: Mineral industrial Minerales diversos Los minerales tienen gran importancia por sus múltiples aplicaciones en los diversos campos de la actividad humana. La industria moderna depende directa o indirectamente de los minerales. Algunos minerales se utilizan prácticamente tal como se extraen; por ejemplo el azufre, el talco, la sal de mesa, etc. Otros, en cambio, deben ser sometidos a diversos procesos para obtener el producto deseado, como el hierro, cobre, aluminio, estaño, etc. Los minerales constituyen la fuente de obtención de los diferentes metales, base tecnológica de la sociedad actual. Así, de distintos tipos de cuarzo y silicatos, se produce el vidrio. Los nitratos y fosfatos son utilizados como abono para la agricultura. Ciertos materiales, como el yeso, son utilizados profusamente en la construcción. Los minerales que entran en la categoría de piedras preciosas o semipreciosas, como los diamantes, topacios, rubíes, se destinan a la confección de joyas. Astrobiología Se ha sugerido que los biominerales podrían ser indicadores importantes de vida extraterrestre y que por lo tanto podrían desempeñar un papel importante en la búsqueda de vida pasada o presente en el planeta Marte. Por otra parte, se cree que los componentes orgánicos (biofirmas), que a menudo se asocian con los biominerales, juegan un papel crucial tanto en reacciones pre-bióticas como bióticas.[43]​ El 24 de enero de 2014 la NASA informó que los estudios actuales de los astromóviles Curiosity y Opportunity en Marte estarán ahora destinados a la búsqueda de evidencia de vida antigua, incluyendo una biosfera basada en microorganismos autótrofos, quimiótrofos y/o quimiolitoautotróficos, así como en agua antigua, incluyendo ambientes fluvo-lacustres (llanuras relacionadas con antiguos ríos o lagos) que pueden haber sido habitables.[44]​[45]​[46]​[47] La búsqueda de evidencia de habitabilidad, tafonomía (relacionada con los fósiles), y el carbono orgánico en el planeta Marte son ahora un objetivo primordial de la NASA.["
ksampletext_wikipedia_geol_tierra: str = "Tierra. La Tierra (del latín terra) es un planeta del sistema solar que gira alrededor de su estrella —el Sol— en la tercera órbita más interna. Es el más denso y el quinto mayor de los ocho planetas del sistema solar. También es el más grande de los cuatro planetas terrestres o rocosos (planetas interiores). La Tierra se formó hace aproximadamente 4550 millones de años y la vida surgió unos mil millones de años después.[19] Es el hogar de millones de especies, incluidos los seres humanos y actualmente el único cuerpo astronómico donde se conoce la existencia de vida.[20] La atmósfera y otras condiciones abióticas han sido alteradas significativamente por la biosfera del planeta, favoreciendo la proliferación de organismos aerobios, así como la formación de una capa de ozono que junto con el campo magnético terrestre bloquean la radiación solar dañina, permitiendo así la vida en la Tierra.[21] Las propiedades físicas de la Tierra, la historia geológica y su órbita han permitido que la vida siga existiendo. Se estima que el planeta seguirá siendo capaz de sustentar vida durante otros 500 millones de años,[22] ya que según las previsiones actuales, pasado ese tiempo la creciente luminosidad del Sol terminará causando la extinción de la biosfera.[23]​[24]​[25]​ La superficie terrestre o corteza está dividida en varias placas tectónicas que se deslizan sobre el magma durante periodos de varios millones de años. La superficie está cubierta por continentes e islas; estos poseen varios lagos, ríos y otras fuentes de agua, que junto con los océanos de agua salada que representan cerca del 71 % de la superficie constituyen la hidrósfera. No se conoce ningún otro planeta con este equilibrio de agua líquida,[nota 6] que es indispensable para cualquier tipo de vida conocida. Los polos de la Tierra están cubiertos en su mayoría de hielo sólido (indlandsis de la Antártida) o de banquisas (casquete polar ártico). El interior del planeta es geológicamente activo, con una gruesa capa de manto relativamente sólido, un núcleo externo líquido que genera un campo magnético, y un sólido núcleo interior compuesto por aproximadamente un 88 % de hierro.[27]​ La Tierra interactúa gravitatoriamente con otros objetos en el espacio, especialmente el Sol y la Luna. En la actualidad, la Tierra completa una órbita alrededor del Sol cada vez que realiza 366.26 giros sobre su eje, lo cual es equivalente a 365.26 días solares o un año sideral.[nota 7] El eje de rotación de la Tierra se encuentra inclinado 23.4° con respecto a la perpendicular a su plano orbital, lo que produce las variaciones estacionales en la superficie del planeta con un período de un año tropical (365.24 días solares).[28] La Tierra posee un único satélite natural, la Luna, que comenzó a orbitar la Tierra hace 4530 millones de años; esta produce las mareas, estabiliza la inclinación del eje terrestre y reduce gradualmente la velocidad de rotación del planeta. Hace aproximadamente 3800 a 4100 millones de años, durante el llamado bombardeo intenso tardío, numerosos asteroides impactaron en la Tierra, causando significativos cambios en la mayor parte de su superficie. Tanto los minerales del planeta como los productos de la biosfera aportan recursos que se utilizan para sostener a la población humana mundial. Sus habitantes están agrupados en unos 200 estados soberanos independientes, que interactúan a través de la diplomacia, los viajes, el comercio y la acción militar. Las culturas humanas han desarrollado muchas ideas sobre el planeta, incluida la personificación de una deidad, la creencia en una Tierra plana o en la Tierra como centro del universo, y una perspectiva moderna del mundo como un entorno integrado que requiere administración. Eponimia y etimología El nombre del planeta Tierra se diferencia del de otros planetas del sistema solar porque no proviene de la mitología grecorromana de manos de autores griegos o romanos. El término latino «terra» significa literalmente ‘suelo’ o ‘tierra firme’ y de ahí deriva la palabra en español. «La Tierra» también se usa como sinónimo intercambiable por «mundo», «globo» y «planeta».[18] En la Antigüedad la palabra ‘tierra’ se usaba indistintamente para referirse al suelo, a la tierra como uno de los cuatro elementos, así como al mundo habitado, sin distinción clara entre ambos.[29]​ Durante la Edad Media y hasta el Renacimiento, hay textos en latín que usan terra para referirse al mundo habitable y al orbe terrestre. En el tratado De sphaera mundi (~1230) de Johannes de Sacrobosco se refiere a “orbis” al ámbito de la tierra (o mundo terrestre) como esfera. Textos como Cosmographia de Bernardo Silvestre o los geógrafos medievales usaban “orbis terrarum” (círculo de las tierras) para referirse al mundo. En De revolutionibus orbium coelestium (Copérnico, 1543), en latín, aparecen frases como “terra quoque sphaerica sit” («que la Tierra también sea esférica»); Copérnico presentó el Sol como centro y situó la Tierra como uno de los planetas.[30] En los trabajos de Kepler, en obras como Epitome Astronomiae Copernicanae, también aparece “Terra” en contextos genéricos (“in Terra” o “Terra et Luna”). Pero fue Valentín Naboth (o Valentinus Nabodus), un astrónomo y matemático del siglo XVI, en su obra Primae de coelo et terra institutiones (1573), quien asoció la Tierra con la diosa romana Terra o Tellus. Se trata de una costumbre renacentista de armonizar conocimiento científico con la mitología clásica: «La Tierra, llamada en latín Terra o Tellus, es la madre fértil que sostiene todas las criaturas; por eso la designamos con el nombre de la antigua diosa que los romanos veneraban como la dadora de vida y la cuidadora del suelo».[31]​ Cronología Artículos principales: Historia de la Tierra y Edad de la Tierra. Los científicos han podido reconstruir información detallada sobre el pasado de la Tierra. Según estos estudios el material más antiguo del sistema solar se formó hace 4567.2 ± 0.6 millones de años,[32] y en torno a unos 4550 millones de años atrás (con una incertidumbre del 1 %)[19] se habían formado ya la Tierra y los otros planetas del sistema solar a partir de la nebulosa solar, una masa en forma de disco compuesta del polvo y gas remanente de la formación del Sol. Este proceso de formación de la Tierra a través de la acreción tuvo lugar mayoritariamente en un plazo de 10-20 millones de años.[33] La capa exterior del planeta, inicialmente fundida, se enfrió hasta formar una corteza sólida cuando el agua comenzó a acumularse en la atmósfera. La Luna se formó poco antes, hace unos 4530 millones de años.[34]​ Representación gráfica de la teoría del gran impacto. El actual modelo consensuado[35] sobre la formación de la Luna es la teoría del gran impacto, que postula que la Luna se creó cuando un objeto del tamaño de Marte, con cerca del 10 % de la masa de la Tierra,[36] impactó tangencialmente contra esta.[37] En este modelo, parte de la masa de este cuerpo podría haberse fusionado con la Tierra, mientras otra parte habría sido expulsada al espacio, proporcionando suficiente material en órbita como para desencadenar nuevamente un proceso de aglutinamiento por fuerzas gravitatorias, y formando así la Luna. La desgasificación de la corteza y la actividad volcánica produjeron la atmósfera primordial de la Tierra. La condensación de vapor de agua, junto con el hielo y el agua líquida aportada por los asteroides y por protoplanetas, cometas y objetos transneptunianos, produjeron los océanos.[38] El recién formado Sol solo tenía el 70 % de su luminosidad actual: sin embargo, existen evidencias que muestran que los primitivos océanos se mantuvieron en estado líquido; una contradicción denominada la «paradoja del joven Sol débil», ya que aparentemente el agua no debería ser capaz de permanecer en ese estado líquido, sino en el sólido, debido a la poca energía solar recibida.[39] Sin embargo, una combinación de gases de efecto invernadero y mayores niveles de actividad solar contribuyeron a elevar la temperatura de la superficie terrestre, impidiendo así que los océanos se congelaran.[40] Hace 3500 millones de años se formó el campo magnético de la Tierra, lo que ayudó a evitar que la atmósfera fuese arrastrada por el viento solar.[41]​ Se han propuesto dos modelos para el crecimiento de los continentes:[42] el modelo de crecimiento constante,[43] y el modelo de crecimiento rápido en una fase temprana de la historia de la Tierra.[44] Las investigaciones actuales sugieren que la segunda opción es más probable, con un rápido crecimiento inicial de la corteza continental,[45] seguido de un largo período de estabilidad.[23]​[nota 8]​[25] En escalas de tiempo de cientos de millones de años de duración, la superficie terrestre ha estado en constante remodelación, formando y fragmentando continentes. Estos continentes se han desplazado por la superficie, combinándose en ocasiones para formar un supercontinente. Hace aproximadamente 750 millones de años (Ma), uno de los primeros supercontinentes conocidos, Rodinia, comenzó a resquebrajarse. Los continentes más tarde se recombinaron nuevamente para formar Pannotia, entre 600 a 540 Ma, y finalmente Pangea, que se fragmentó hace 180 Ma hasta llegar a la configuración continental actual.[47]​ Evolución de la vida Historia de la vida ver • discusión • editar -4500 —–-4000 —–-3500 —–-3000 —–-2500 —–-2000 —–-1500 —–-1000 —–-500 —–0 — Agua Vida unicelular Fotosíntesis Eucariotas Vida multicelular Vida terrestre Dinosaurios     Mamíferos Flores Tierra primitiva (−4540) Primeras aguas Vida temprana Meteoritos LHB Primeras evidencias de oxígeno Oxígeno atmosférico Gran Oxidación Primeras evidencias de reproducción sexual Biota ediacárica Explosión cámbrica Primeros humanos PongolanoHuronianoCriogénicoAndinoKarooCuaternario Escala vertical: millones de años. Etiquetas color naranja: eras de hielo conocidas. Artículo principal: Historia de la vida La Tierra proporciona el único ejemplo conocido de un entorno que ha dado lugar a la evolución de la vida.[48] Se presume que procesos químicos altamente energéticos produjeron una molécula autorreplicante hace alrededor de 4000 millones de años, y hace entre 3500 y 3800 millones de años existió el último antepasado común universal.[49] El desarrollo de la fotosíntesis permitió que los seres vivos recogiesen de forma directa la energía del Sol; el oxígeno resultante acumulado en la atmósfera formó una capa de ozono (una forma de oxígeno molecular [O3]) en la atmósfera superior. La incorporación de células más pequeñas dentro de las más grandes dio como resultado el desarrollo de las células complejas llamadas eucariotas.[50] Los verdaderos organismos multicelulares se formaron cuando las células dentro de colonias se hicieron cada vez más especializadas. La vida colonizó la superficie de la Tierra en parte gracias a la absorción de la radiación ultravioleta por parte de la capa de ozono.[51]​ En la década de 1960 surgió una hipótesis que afirmaba que durante el período Neoproterozoico, desde 750 hasta los 580 Ma, se produjo una intensa glaciación en la que gran parte del planeta fue cubierto por una capa de hielo. Esta hipótesis ha sido denominada la «Glaciación global», y es de particular interés, ya que este suceso precedió a la llamada explosión del Cámbrico, en la que las formas de vida multicelulares comenzaron a proliferar.[52]​ Tras la explosión del Cámbrico, hace unos 535 Ma se han producido cinco extinciones en masa.[53] De ellas, el evento más reciente ocurrió hace 65 Ma, cuando el impacto de un asteroide provocó la extinción de los dinosaurios no aviarios, así como de otros grandes reptiles, sobreviviendo algunos pequeños animales como los mamíferos, que por aquel entonces eran similares a las actuales musarañas. Durante los últimos 65 millones de años los mamíferos se diversificaron, hasta que hace varios millones de años, un animal africano con aspecto de simio conocido como el Orrorin tugenensis adquirió la capacidad de mantenerse en pie.[54] Esto le permitió utilizar herramientas y favoreció su capacidad de comunicación, proporcionando la nutrición y la estimulación necesarias para desarrollar un cerebro más grande, y permitiendo así la evolución de la especie humana. El desarrollo de la agricultura y de la civilización permitió a los humanos alterar la Tierra en un corto espacio de tiempo como no lo había hecho ninguna otra especie,[55] afectando tanto a la naturaleza como a la diversidad y cantidad de formas de vida. El presente patrón de edades de hielo comenzó hace alrededor de 40 Ma y luego se intensificó durante el Pleistoceno, hace alrededor de 3 Ma. Desde entonces las regiones en latitudes altas han sido objeto de repetidos ciclos de glaciación y deshielo, en ciclos de 40 000-100 000 años. La última glaciación continental terminó hace 10 000 años.[56]​ Véase también: Anexo:Cronología de la historia evolutiva de la vida Futuro Artículo principal: Futuro de la Tierra Ciclo de la vida solar. El futuro del planeta está estrechamente ligado al del Sol. Como resultado de la acumulación constante de helio en el núcleo del Sol, la luminosidad total de la estrella irá poco a poco en aumento. La luminosidad del Sol crecerá en un 10 % en los próximos 1.1 Ga (1100 millones de años) y en un 40 % en los próximos 3.5 Ga.[57] Los modelos climáticos indican que el aumento de la radiación podría tener consecuencias nefastas en la Tierra, incluyendo la pérdida de los océanos del planeta.[58]​ Se espera que la Tierra sea habitable por alrededor de otros 500 millones de años a partir de este momento,[22] aunque este período podría extenderse hasta 2300 millones de años si se elimina el nitrógeno de la atmósfera.[59] El aumento de temperatura en la superficie terrestre acelerará el ciclo del CO2 inorgánico, lo que reducirá su concentración hasta niveles letalmente bajos para las plantas (10 ppm para la fotosíntesis C4) dentro de aproximadamente 500[22] a 900 millones de años. La falta de vegetación resultará en la pérdida de oxígeno en la atmósfera, lo que provocará la extinción de la vida animal a lo largo de varios millones de años más.[60] Después de otros mil millones de años, todas las aguas superficiales habrán desaparecido[61] y la temperatura media global alcanzará los 70 °C.[60] Incluso si el Sol fuese eterno y estable, el continuo enfriamiento interior de la Tierra se traduciría en una gran pérdida de CO2 debido a la reducción de la actividad volcánica,[62] y el 35 % del agua de los océanos podría descender hasta el manto debido a la disminución del vapor de ventilación en las dorsales oceánicas.[63]​ El Sol, siguiendo su evolución natural, se convertirá en una gigante roja en unos 5 Ga. Los modelos predicen que el Sol se expandirá hasta unas 250 veces su tamaño actual, alcanzando un radio cercano a 1 UA (unos 150 millones de kilómetros).[57]​[64] El destino que sufrirá la Tierra entonces no está claro. Siendo una gigante roja, el Sol perderá aproximadamente el 30 % de su masa, por lo que sin los efectos de las mareas, la Tierra se moverá a una órbita de 1.7 UA (unos 250 millones de kilómetros) del Sol cuando la estrella alcance su radio máximo. Por lo tanto se espera que el planeta escape inicialmente de ser envuelto por la tenue atmósfera exterior expandida del Sol. Aun así, cualquier forma de vida restante sería destruida por el aumento de la luminosidad del Sol (alcanzando un máximo de cerca de 5000 veces su nivel actual).[57] Sin embargo, una simulación realizada en 2008 indica que la órbita de la Tierra decaerá debido a los efectos de marea y arrastre, ocasionando que el planeta penetre en la atmósfera estelar y se vaporice.[64]​ Véase también: Extinción humana Composición y estructura Artículo principal: Ciencias de la Tierra La Tierra es un planeta terrestre, lo que significa que es un cuerpo rocoso y no un gigante gaseoso como Júpiter. Es el más grande de los cuatro planetas terrestres del sistema solar en tamaño y masa, y también es el que tiene la mayor densidad, la mayor gravedad superficial, el campo magnético más fuerte y la rotación más rápida de los cuatro.[65] También es el único planeta terrestre con placas tectónicas activas.[66] El movimiento de estas placas produce que la superficie terrestre esté en constante cambio, siendo responsables de la formación de montañas, de la sismicidad y del vulcanismo. El ciclo de estas placas también juega un papel preponderante en la regulación de la temperatura terrestre, contribuyendo al reciclaje de gases con efecto invernadero como el dióxido de carbono, por medio de la renovación permanente de los fondos oceánicos.[67]​ Forma Comparación de tamaño de los planetas interiores (de izquierda a derecha): Mercurio, Venus, Tierra y Marte. Artículo principal: Historia de la geodesia La forma de la Tierra es muy parecida a la de un geoide o esferoide oblato, una esfera achatada por los polos, resultando en un abultamiento alrededor del ecuador.[68] Este abultamiento está causado por la rotación de la Tierra, y ocasiona que el diámetro en el ecuador sea 43 km más largo que el diámetro de un polo a otro.[69] Hace aproximadamente 22 000 años la Tierra tenía una forma más esférica, la mayor parte del hemisferio norte se encontraba cubierto por hielo, y a medida que el hielo se derretía causaba una menor presión en la superficie terrestre en la que se sostenía, causando esto un tipo de «rebote».[70] Este fenómeno siguió ocurriendo hasta mediados de los años noventa, cuando los científicos se percataron de que este proceso se había invertido, es decir, el abultamiento aumentaba.[71] Las observaciones del satélite GRACE muestran que, al menos desde 2002, la pérdida de hielo de Groenlandia y de la Antártida ha sido la principal responsable de esta tendencia. Volcán Chimborazo, el punto terrestre más alejado del centro de la Tierra. La topografía local se desvía de este esferoide idealizado, aunque las diferencias a escala global son muy pequeñas: la Tierra tiene una desviación de aproximadamente una parte entre 584, o el 0.17 %, desde el esferoide de referencia, que es menor que la tolerancia del 0.22 % permitida en las bolas de billar.[72] Las mayores desviaciones locales en la superficie rocosa de la Tierra son el monte Everest (8 848 m sobre el nivel local del mar) y el abismo Challenger, al sur de la fosa de las Marianas (10 911 m bajo el nivel local del mar). Debido a la protuberancia ecuatorial, el punto terrestre más alejado del centro de la Tierra es el volcán Chimborazo en Ecuador.[73]​[74]​[75]​ La idea de que la forma de la Tierra se aproxima a la de un elipsoide data del siglo XVIII por Pierre Louis Maupertuis. Las primeras ideas antiguas sobre la forma de la Tierra sostenían que la Tierra era plana. Así, por ejemplo, en la antigua Mesopotamia, donde el mundo era visto como un disco rodeado por el océano, más allá del cual se levantaban los pilares de un cielo esférico.[76] También lo es de la cosmología bíblica, tal como aparece en libro de Isaías.[77]​[78] Más adelante surgió el concepto de la Tierra esférica como materia de especulación filosófica hasta el siglo III a. C., cuando la astronomía helenística estableció como un hecho, gracias sobre todo a la medición empírica de Eratóstenes. El paradigma helenístico fue gradualmente adoptado en el Viejo Mundo durante la Antigüedad y la Edad Media.[79]​[80]​[81]​[82] Una demostración práctica de la esfericidad de la Tierra fue llevada a cabo por Fernando de Magallanes y Juan Sebastián Elcano en su expedición de circunnavegación del mundo.[83]​ Para un artículo más detallado sobre las pruebas que demuestran la esfericidad de la Tierra, véase Evidencias empíricas de la forma esférica de la Tierra. Tamaño La circunferencia en el ecuador es de 40 091 km. El diámetro en el ecuador es de 12 756 km y en los polos de 12 730 km.[84] El diámetro medio de referencia para el esferoide es de unos 12 742 km, que es aproximadamente 40 000 km/π, ya que el metro se definió originalmente como la diezmillonésima parte de la distancia desde el ecuador hasta el Polo Norte por París, Francia.[85]​ Estimaciones del tamaño de la Tierra aparecieron desde los tiempos de Aristóteles.[86] La primera medición fue hecha por Eratóstenes, el 240 a. C. En esa época se aceptaba que la Tierra era esférica. Eratóstenes calculó el tamaño de la Tierra midiendo el ángulo con que alumbraba el Sol en el solsticio, tanto en Alejandría como en Siena, distante 750 km. El tamaño que obtuvo fue de un diámetro de 12 000 km y una circunferencia de 40 000 km,[87] es decir, con un error de solo el 6 % respecto a los datos actuales. Posteriormente Posidonio de Apamea repitió las mediciones en el año 100 a. C., obteniendo el dato de 29 000 km para la circunferencia, considerablemente más impreciso respecto a los datos actuales. Este último valor fue el que aceptó Ptolomeo, por lo que prevaleció ese valor en los siglos siguientes.[87]​ Por la Edad Media el astrónomo islámico Al-Biruni utilizó un nuevo método para computar la circunferencia terráquea, obteniendo un valor cercano a los valores modernos.[88] En contraste con sus predecesores, Al-Biruni desarrolló un nuevo método utilizando cálculos trigonométricos basado en el ángulo formado entre un plano y la cima de una montaña, con lo que obtuvo mejores mediciones de la circunferencia terrestre e hizo posible el realizar esta medición desde un solo lugar, por una sola persona.[89]​[90] Desde la cima, divisó el ángulo con el horizonte, lo cual, junto con la altura de la montaña (que había calculado previamente), le permitió calcular la curvatura de la Tierra.[91]​[92] También hizo uso del álgebra para formular ecuaciones trigonométricas y utilizó el astrolabio para medir ángulos.[93]​ Composición química de la corteza[94]​ Compuesto Fórmula Composición Continental Oceánica sílice SiO2 60.2 % 48.6 % alúmina Al2O3 15.2 % 16.5 % cal CaO 5.5 % 12.3 % magnesio MgO 3.1 % 6.8 % óxido de hierro (II) FeO 3.8 % 6.2 % óxido de sodio Na2O 3.0 % 2.6 % óxido de potasio K2O 2.8 % 0.4 % óxido de hierro (III) Fe2O3 2.5 % 2.3 % agua H2O 1.4 % 1.1 % dióxido de carbono CO2 1.2 % 1.4 % óxido de titanio TiO2 0.7 % 1.4 % óxido de fósforo P2O5 0.2 % 0.3 % Total 99.6 % 99.9 % Composición química La masa de la Tierra es aproximadamente de 5.98 × 1024 kg. Se compone principalmente de hierro (32.1 %), oxígeno (30.1 %), silicio (15.1 %), magnesio (13.9 %), azufre (2.9 %), níquel (1.8 %), calcio (1.5 %) y aluminio (1.4 %), con el 1.2 % restante formado por pequeñas cantidades de otros elementos. Debido a la segregación de masa, se cree que la zona del núcleo está compuesta principalmente de hierro (88.8 %), con pequeñas cantidades de níquel (5.8 %), azufre (4.5 %), y menos del 1 % formado por trazas de otros elementos.[95]​ El geoquímico F. W. Clarke (1847-1931), llamado «el padre de la geoquímica por haber determinado la composición de la corteza de la Tierra», calculó que un poco más del 47 % de la corteza terrestre se compone de oxígeno. Los componentes de las rocas más comunes de la corteza de la Tierra son casi todos los óxidos. Cloro, azufre y flúor son las únicas excepciones significativas, y su presencia total en cualquier roca es generalmente mucho menor del 1 %. Los principales óxidos son sílice, alúmina, óxido de hierro, de calcio, de magnesio, potasio a y sodio. La sílice actúa principalmente como un ácido, formando silicatos, y los minerales más comunes de las rocas ígneas son de esta naturaleza. A partir de un cálculo sobre la base de 1672 análisis de todo tipo de rocas, Clarke dedujo que un 99.22 % de las rocas están compuestas por 11 óxidos (véase el cuadro a la derecha). Todos los demás compuestos aparecen solamente en cantidades muy pequeñas.[96]​ Véase también: Abundancia de los elementos en la Tierra Estructura interna Artículo principal: Estructura de la Tierra El interior de la Tierra, al igual que el de los otros planetas terrestres, está dividido en capas según su composición química o sus propiedades físicas (reológicas), pero, a diferencia de los otros planetas terrestres, tiene un núcleo interno y externo distintos. Su capa externa es una corteza de silicato sólido, químicamente diferenciado, bajo la cual se encuentra un manto sólido de alta viscosidad. La corteza está separada del manto por la discontinuidad de Mohorovičić, variando el espesor de la misma desde un promedio de 6 km en los océanos a entre 30 y 50 km en los continentes. La corteza y la parte superior fría y rígida del manto superior se conocen comúnmente como la litosfera, y es de la litosfera de lo que están compuestas las placas tectónicas. Debajo de la litosfera se encuentra la astenosfera, una capa de relativamente baja viscosidad sobre la que flota la litosfera. Dentro del manto, entre los 410 y 660 km bajo la superficie, se producen importantes cambios en la estructura cristalina. Estos cambios generan una zona de transición que separa la parte superior e inferior del manto. Bajo el manto se encuentra un núcleo externo líquido de viscosidad extremadamente baja, descansando sobre un núcleo interno sólido.[97] El núcleo interno puede girar con una velocidad angular ligeramente superior que el resto del planeta, avanzando de 0.1 a 0.5° por año.[98]​ Capas geológicas de la Tierra[99]​ Corte de la Tierra desde el núcleo hasta la exosfera (no está a escala). Profundidad[100]​ km Componentes de las capas Densidad g/cm³ 0-60 Litosfera[nota 9] — 0-35 Corteza[nota 10] 2.2-2.9 35-60 Manto superior 3.4-4.4   35-2890 Manto 3.4-5.6 100-700 Astenosfera — 2890-5100 Núcleo externo 9.9-12.2 5100-6378 Núcleo interno 12.8-13.1 Calor El calor interno de la Tierra proviene de una combinación del calor residual de la acreción planetaria (20 %) y el calor producido por la desintegración radiactiva (80 %).[101] Los isótopos con mayor producción de calor en la Tierra son el potasio-40, el uranio-238, el uranio-235 y el torio-232.[102] En el centro del planeta, la temperatura puede llegar hasta los 7000 K y la presión puede alcanzar los 360 GPa.[103] Debido a que gran parte del calor es proporcionado por la desintegración radiactiva, los científicos creen que en la historia temprana de la Tierra, antes de que los isótopos de reducida vida media se agotaran, la producción de calor de la Tierra fue mucho mayor. Esta producción de calor extra, que hace aproximadamente 3000 millones de años era el doble que la producción actual,[101] pudo haber incrementado los gradientes de temperatura dentro de la Tierra, incrementando la convección del manto y la tectónica de placas, permitiendo la producción de rocas ígneas como las komatitas que no se forman en la actualidad.[104]​ Isótopos actuales de mayor producción de calor[105]​ Isótopo Calor emitido Vatios/kg isótopo Vida media años Concentración media del manto kg isótopo/kg manto Calor emitido W/kg manto 238U 9.46 × 10−5 4.47 × 109 30.8 × 10−9 2.91 × 10−12 235U 5.69 × 10−4 7.04 × 108 0.22 × 10−9 1.25 × 10−13 232Th 2.64 × 10−5 1.40 × 1010 124 × 10−9 3.27 × 10−12 40K 2.92 × 10−5 1.25 × 109 36.9 × 10−9 1.08 × 10−12 El promedio de pérdida de calor de la Tierra es de 87 mW m−2, que supone una pérdida global de 4.42 × 1013 W.[106] Una parte de la energía térmica del núcleo es transportada hacia la corteza por plumas del manto, una forma de convección que consiste en afloramientos de roca a altas temperaturas. Estas plumas pueden producir puntos calientes y coladas de basalto.[107] La mayor parte del calor que pierde la Tierra se filtra entre las placas tectónicas, en las surgencias del manto asociadas a las dorsales oceánicas. Casi todas las pérdidas restantes se producen por conducción a través de la litosfera, principalmente en los océanos, ya que allí la corteza es mucho más delgada que en los continentes.[108]​ Placas tectónicas Artículo principal: Tectónica de placas Placas tectónicas[109]​ Muestra de la extensión y los límites de las placas tectónicas, con superposición de contornos en los continentes que se apoyan Nombre de la placa Área 106 km² color #FB9B7A Placa Africana[nota 8]​ 78.0 color #8A9BBE Placa Antártica 60.9           Placa Indoaustraliana 47.2 color #7FA172 Placa Euroasiática 67.8 color #AC8D7F Placa Norteamericana 75.9 color #AD82B0 Placa Sudamericana 43.6 color #FEE6AA Placa Pacífica 103.3 La mecánicamente rígida capa externa de la Tierra, la litosfera, está fragmentada en piezas llamadas placas tectónicas. Estas placas son elementos rígidos que se mueven en relación uno con otro siguiendo uno de estos tres patrones: bordes convergentes, en los que dos placas se aproximan; bordes divergentes, en los que dos placas se separan, y bordes transformantes, en los que dos placas se deslizan lateralmente entre sí. A lo largo de estos bordes de placa se producen los terremotos, la actividad volcánica, la formación de montañas y la formación de fosas oceánicas.[110] Las placas tectónicas se deslizan sobre la parte superior de la astenosfera, la sólida pero menos viscosa sección superior del manto, que puede fluir y moverse junto con las placas,[111] y cuyo movimiento está fuertemente asociado a los patrones de convección dentro del manto terrestre. A medida que las placas tectónicas migran a través del planeta, el fondo oceánico se subduce bajo los bordes de las placas en los límites convergentes. Al mismo tiempo, el afloramiento de material del manto en los límites divergentes crea las dorsales oceánicas. La combinación de estos procesos recicla continuamente la corteza oceánica nuevamente en el manto. Debido a este proceso de reciclaje, la mayor parte del suelo marino tiene menos de 100 millones de años de edad. La corteza oceánica más antigua se encuentra en el Pacífico Occidental, y tiene una edad estimada de unos 200 millones de años.[112]​[113] En comparación, la corteza continental más antigua registrada tiene 4030 millones de años de edad.[114]​ Las siete placas más grandes son la Pacífica, Norteamericana, Euroasiática, Africana Antártica, Indoaustraliana y Sudamericana. Otras placas notables son la placa Índica, la placa arábiga, la placa del Caribe, la placa de Nazca en la costa occidental de América del Sur y la placa Escocesa en el sur del océano Atlántico. La placa de Australia se fusionó con la placa de la India hace entre 50 y 55 millones de años. Las placas con movimiento más rápido son las placas oceánicas, con la placa de Cocos avanzando a una velocidad de 75 mm/año[115] y la placa del Pacífico moviéndose 52-69 mm/año. En el otro extremo, la placa con movimiento más lento es la placa eurasiática, que avanza a una velocidad típica de aproximadamente 21 mm/año.[116]​ Superficie Histograma de elevación de la corteza terrestre. Artículos principales: Superficie terrestre, Accidente geográfico y Anexo:Puntos extremos del mundo. El relieve de la Tierra varía enormemente de un lugar a otro. Cerca del 70.8 %[117] de la superficie está cubierta por agua, con gran parte de la plataforma continental por debajo del nivel del mar. La superficie sumergida tiene características montañosas, incluyendo un sistema de dorsales oceánicas, así como volcanes submarinos,[69] fosas oceánicas, cañones submarinos, mesetas y llanuras abisales. El restante 29.2 % no cubierto por el agua se compone de montañas, desiertos, llanuras, mesetas y otras geomorfologías. La superficie del planeta se moldea a lo largo de períodos de tiempo geológicos, debido a la erosión tectónica. Las características de esta superficie formada o deformada mediante la tectónica de placas están sujetas a una constante erosión a causa de las precipitaciones, los ciclos térmicos y los efectos químicos. La glaciación, la erosión costera, la acumulación de los arrecifes de coral y los grandes impactos de meteoritos[118] también actúan para remodelar el paisaje. Altimetría y batimetría actual. Datos del Modelo Digital de Terreno del National Geophysical Data Center de Estados Unidos. La corteza continental se compone de material de menor densidad, como las rocas ígneas, el granito y la andesita. Menos común es el basalto, una densa roca volcánica que es el componente principal de los fondos oceánicos.[119] Las rocas sedimentarias se forman por la acumulación de sedimentos compactados. Casi el 75 % de la superficie continental está cubierta por rocas sedimentarias, a pesar de que estas solo forman un 5 % de la corteza.[120] El tercer material rocoso más abundante en la Tierra son las rocas metamórficas, creadas a partir de la transformación de tipos de roca ya existentes mediante altas presiones, altas temperaturas, o ambas. Los minerales de silicato más abundantes en la superficie de la Tierra incluyen el cuarzo, los feldespatos, el anfíbol, la mica, el piroxeno y el olivino.[121] Los minerales de carbonato más comunes son la calcita (que se encuentra en piedra caliza) y la dolomita.[122]​ La pedosfera es la capa más externa de la Tierra. Está compuesta de tierra y está sujeta a los procesos de formación del suelo. Existe en el encuentro entre la litosfera, la atmósfera, la hidrosfera y la biosfera. Actualmente el 13.31 % del total de la superficie terrestre es tierra cultivable, y solo el 4.71 % soporta cultivos permanentes.[8] Cerca del 40 % de la superficie emergida se utiliza actualmente como tierras de cultivo y pastizales, estimándose un total de 1.3 × 107 km² para tierras de cultivo y 3.4 × 107 km² para tierras de pastoreo.[123]​ La elevación de la superficie terrestre varía entre el punto más bajo de −418 m en el mar Muerto a una altitud máxima, estimada en 2005, de 8848 m en la cima del monte Everest. La altura media de la tierra sobre el nivel del mar es de 840 m.[124]​ Imágenes satelitales de la Tierra Planisferio terrestre (composición de fotos satelitales). El satélite ambiental Envisat de la ESA desarrolló un retrato detallado de la superficie de la Tierra. A través del proyecto GLOBCOVER se desarrolló la creación de un mapa global de la cobertura terrestre con una resolución tres veces superior a la de cualquier otro mapa por satélite hasta aquel momento. Utilizó reflectores radar con antenas de ancho sintéticas, capturando con sus sensores la radiación reflejada.[125]​ La NASA completó un nuevo mapa tridimensional, que es la topografía más precisa del planeta, elaborada durante cuatro años con los datos transmitidos por el transbordador espacial Endeavour. Los datos analizados corresponden al 80 % de la masa terrestre. Cubre los territorios de Australia y Nueva Zelanda con detalles sin precedentes. También incluye más de mil islas de la Polinesia y la Melanesia en el Pacífico sur, así como islas del Índico y el Atlántico. Muchas de esas islas apenas se levantan unos metros sobre el nivel del mar y son muy vulnerables a los efectos de las marejadas y tormentas, por lo que su conocimiento ayudará a evitar catástrofes; los datos proporcionados por la misión del Endeavour tendrán una amplia variedad de usos, como la exploración virtual del planeta.[126]​ Véase también: Cartografía Hidrosfera Los océanos poseen el mayor volumen de agua en la Tierra. Artículo principal: Hidrosfera La abundancia de agua en la superficie de la Tierra es una característica única que distingue al «Planeta Azul» de otros en el sistema solar. La hidrosfera de la Tierra está compuesta fundamentalmente por océanos, pero técnicamente incluye todas las superficies de agua en el mundo, incluidos los mares interiores, lagos, ríos y aguas subterráneas hasta una profundidad de 2000 m. El lugar más profundo bajo el agua es el abismo Challenger de la fosa de las Marianas, en el océano Pacífico, con una profundidad de −10 911.4 m.[nota 11]​[127]​ La masa de los océanos es de aproximadamente 1.35 × 1018 toneladas métricas, o aproximadamente 1/4400 de la masa total de la Tierra. Los océanos cubren un área de 361.84 × 106 km² con una profundidad media de 3682.2 m, lo que resulta en un volumen estimado de 1.3324 × 109 km³.[128] Si se nivelase toda la superficie terrestre, el agua cubriría la superficie del planeta hasta una altura de más de 2.7 km. El área total de la Tierra es de 5.1 × 108 km². Para la primera aproximación, la profundidad media sería la relación entre los dos, o de 2.7 km. Aproximadamente el 97.5 % del agua es salada, mientras que el restante 2.5 % es agua dulce. La mayor parte del agua dulce, aproximadamente el 68.7 %, se encuentra actualmente en estado de hielo.[129]​ La salinidad media de los océanos es de unos 35 gramos de sal por kilogramo de agua (35 ‰).[130] La mayor parte de esta sal fue liberada por la actividad volcánica, o extraída de las rocas ígneas ya enfriadas.[131] Los océanos son también un reservorio de gases atmosféricos disueltos, siendo estos esenciales para la supervivencia de muchas formas de vida acuática.[132] El agua de los océanos tiene una influencia importante sobre el clima del planeta, actuando como un foco calórico de gran tamaño.[133] Los cambios en la distribución de la temperatura oceánica pueden causar alteraciones climáticas, tales como la Oscilación del Sur, El Niño.[134]​ Atmósfera Artículo principal: Atmósfera terrestre La presión atmosférica media al nivel del mar se sitúa en torno a los 101.325 kPa, con una escala de altura de aproximadamente 8.5 km.[1] Está compuesta principalmente de un 78 % de nitrógeno y un 21 % de oxígeno, con trazas de vapor de agua, dióxido de carbono y otras moléculas gaseosas. La altura de la troposfera varía con la latitud, entre 8 km en los polos y 17 km en el ecuador, con algunas variaciones debido a la climatología y los factores estacionales.[135]​ La biosfera de la Tierra ha alterado significativamente la atmósfera. La fotosíntesis oxigénica evolucionó hace 2700 millones de años, formando principalmente la atmósfera actual de nitrógeno-oxígeno. Este cambio permitió la proliferación de los organismos aeróbicos, así como la formación de la capa de ozono que bloquea la radiación ultravioleta proveniente del Sol, permitiendo la vida fuera del agua. Otras funciones importantes de la atmósfera para la vida en la Tierra incluyen el transporte de vapor de agua, proporcionar gases útiles, quemar los meteoritos pequeños antes de que alcancen la superficie, y moderar la temperatura.[136] Este último fenómeno se conoce como el efecto invernadero: trazas de moléculas presentes en la atmósfera capturan la energía térmica emitida desde el suelo, aumentando así la temperatura media. El dióxido de carbono, el vapor de agua, el metano y el ozono son los principales gases de efecto invernadero de la atmósfera de la Tierra. Sin este efecto de retención del calor, la temperatura superficial media sería de −18 °C y la vida probablemente no existiría.[117]​ Clima y tiempo atmosférico Artículos principales: Clima y Tiempo atmosférico. Imagen satelital de la nubosidad de la Tierra usando el espectroradiómetro de imágenes de media resolución de la NASA. La atmósfera terrestre no tiene unos límites definidos, haciéndose poco a poco más delgada hasta desvanecerse en el espacio exterior. Tres cuartas partes de la masa atmosférica están contenidas dentro de los primeros 11 km de la superficie del planeta. Esta capa inferior se llama troposfera. La energía del Sol calienta esta capa y la superficie bajo esta, causando la expansión del aire. El aire caliente se eleva debido a su menor densidad, siendo sustituido por aire de mayor densidad, es decir, aire más frío. Esto da como resultado la circulación atmosférica que genera el tiempo y el clima a través de la redistribución de la energía térmica.[137]​ Las líneas principales de circulación atmosférica las constituyen los vientos alisios en la región ecuatorial por debajo de los 30° de latitud, y los vientos del oeste en latitudes medias entre los 30° y 60°.[138] Las corrientes oceánicas también son factores importantes para determinar el clima, especialmente la circulación termohalina que distribuye la energía térmica de los océanos ecuatoriales a las regiones polares.[139]​ El vapor de agua generado a través de la evaporación superficial es transportado según los patrones de circulación de la atmósfera. Cuando las condiciones atmosféricas permiten la elevación del aire caliente y húmedo, el agua se condensa y se deposita en la superficie en forma de precipitaciones.[137] La mayor parte del agua es transportada a altitudes más bajas mediante los sistemas fluviales y por lo general regresa a los océanos o es depositada en los lagos. Este ciclo del agua es un mecanismo vital para sustentar la vida en la tierra y es un factor primario de la erosión que modela la superficie terrestre a lo largo de períodos geológicos. Los patrones de precipitación varían enormemente, desde varios metros de agua por año a menos de un milímetro. La circulación atmosférica, las características topológicas y las diferencias de temperatura determinan las precipitaciones medias de cada región.[140]​ La cantidad de energía solar que llega a la Tierra disminuye al aumentar la latitud. En las latitudes más altas la luz solar incide en la superficie en un ángulo menor, teniendo que atravesar gruesas columnas de atmósfera. Como resultado, la temperatura media anual del aire a nivel del mar se reduce en aproximadamente 0.4 °C por cada grado de latitud alejándose del ecuador.[141] La Tierra puede ser subdividida en franjas latitudinales más o menos homogéneas con un clima específico. Desde el ecuador hasta las regiones polares, se encuentran la zona intertropical (o ecuatorial), el clima subtropical, el clima templado y los climas polares.[142] El clima también puede ser clasificado en función de la temperatura y las precipitaciones, en regiones climáticas caracterizadas por masas de aire bastante uniformes. La metodología de clasificación más usada es la clasificación climática de Köppen (modificada por el estudiante de Wladimir Peter Köppen, Rudolph Geiger), que cuenta con cinco grandes grupos (zonas tropicales húmedas, zonas áridas, zonas húmedas con latitud media, clima continental y frío polar), que se dividen en subtipos más específicos.[138]​ Atmósfera superior Imagen de la NASA en la que se observa la Luna parcialmente oscurecida y deformada por la refracción atmosférica. Artículo principal: Espacio exterior Por encima de la troposfera, la atmósfera suele dividir en estratosfera, mesosfera y termosfera.[136] Cada capa tiene un gradiente adiabático diferente, que define la tasa de cambio de la temperatura con respecto a la altura. Más allá de éstas se encuentra la exosfera, que se atenúa hasta penetrar en la magnetosfera, donde los campos magnéticos de la Tierra interactúan con el viento solar.[143] Dentro de la estratosfera se encuentra la capa de ozono; un componente que protege parcialmente la superficie terrestre de la luz ultravioleta, siendo un elemento importante para la vida en la Tierra. La línea de Kármán, definida en los 100 km sobre la superficie de la Tierra, es una definición práctica usada para establecer el límite entre la atmósfera y el espacio.[144]​ La energía térmica hace que algunas de las moléculas en el borde exterior de la atmósfera de la Tierra incrementen su velocidad hasta el punto de poder escapar de la gravedad del planeta. Esto da lugar a una pérdida lenta pero constante de la atmósfera hacia el espacio. Debido a que el hidrógeno no fijado tiene un bajo peso molecular puede alcanzar la velocidad de escape más fácilmente, escapando así al espacio exterior a un ritmo mayor que otros gases.[145] La pérdida de hidrógeno hacia el espacio contribuye a la transformación de la Tierra desde su inicial estado reductor a su actual estado oxidante. La fotosíntesis proporcionó una fuente de oxígeno libre, pero se cree que la pérdida de agentes reductores como el hidrógeno fue una condición previa necesaria para la acumulación generalizada de oxígeno en la atmósfera.[146] Por tanto, la capacidad del hidrógeno para escapar de la atmósfera de la Tierra puede haber influido en la naturaleza de la vida desarrollada en el planeta.[147] En la atmósfera actual, rica en oxígeno, la mayor parte del hidrógeno se convierte en agua antes de tener la oportunidad de escapar. En cambio, la mayor parte de la pérdida de hidrógeno actual proviene de la destrucción del metano en la atmósfera superior.[148]​ Campo magnético Diagrama que muestra las líneas del campo magnético de la magnetosfera de la Tierra. Las líneas son arrastradas de vuelta en el sentido contrario a las solares bajo la influencia del viento solar. Esquema de la magnetosfera de la Tierra. Los flujos de viento solar, de izquierda a derecha Artículo principal: Campo magnético terrestre El campo magnético de la Tierra tiene una forma similar a un dipolo magnético, con los polos actualmente localizados cerca de los polos geográficos del planeta. En el ecuador del campo magnético (ecuador magnético), la fuerza del campo magnético en la superficie es 3.05 × 10−5T, con un momento magnético dipolar global de 7.91 × 1015 T m³.[149] Según la teoría del dínamo, el campo se genera en el núcleo externo fundido, región donde el calor crea movimientos de convección en materiales conductores, generando corrientes eléctricas. Estas corrientes inducen a su vez el campo magnético de la Tierra. Los movimientos de convección en el núcleo son caóticos; los polos magnéticos se mueven y periódicamente cambian de orientación. Esto da lugar a reversiones geomagnéticas a intervalos de tiempo irregulares, unas pocas veces cada millón de años. La inversión más reciente tuvo lugar hace aproximadamente 700 000 años.[150]​[151]​ El campo magnético forma la magnetosfera, que desvía las partículas de viento solar. En dirección al Sol, el arco de choque entre el viento solar y la magnetosfera se encuentra a unas 13 veces el radio de la Tierra. La colisión entre el campo magnético y el viento solar forma los cinturones de radiación de Van Allen; un par de regiones concéntricas, con forma tórica, formadas por partículas cargadas muy energéticas. Cuando el plasma entra en la atmósfera de la Tierra por los polos magnéticos se crean las auroras polares.[152]​ Rotación y órbita Rotación Inclinación del eje de la Tierra (u oblicuidad) y su relación con el eje de rotación y el plano orbital. Artículo principal: Rotación de la Tierra El período de rotación de la Tierra con respecto al Sol, es decir, un día solar, es de alrededor de 86 400 segundos de tiempo solar (86 400.0025 segundos SIU).[153] El día solar de la Tierra es ahora un poco más largo de lo que era durante el siglo XIX debido a la aceleración de marea, los días duran entre 0 y 2 ms SIU más.[154]​[155]​ La rotación de la Tierra fotografiada por DSCOVR EPIC el 29 de mayo de 2016, unas semanas antes del solsticio. El período de rotación de la Tierra en relación con las estrellas fijas, llamado día estelar por el Servicio Internacional de Rotación de la Tierra y Sistemas de Referencia (IERS por sus siglas en inglés), es de 86 164.098903691 segundos del tiempo solar medio (UT1), o de 23h 56m 4.098903691s.[4]​[nota 12] El período de rotación de la Tierra en relación con el equinoccio vernal, mal llamado el día sidéreo, es de 86 164.09053083288 segundos del tiempo solar medio (UT1) (23h 56m 4.09053083288s).[4] Por tanto, el día sidéreo es más corto que el día estelar en torno a 8.4 ms.[156] La longitud del día solar medio en segundos SIU está disponible en el IERS para los períodos 1623-2005[157] y 1962-2005.[158]​ Aparte de los meteoros en la atmósfera y de los satélites en órbita baja, el movimiento aparente de los cuerpos celestes vistos desde la Tierra se realiza hacia al oeste, a una velocidad de 15°/h = 15′/min. Para las masas cercanas al ecuador celeste, esto es equivalente a un diámetro aparente del Sol o de la Luna cada dos minutos (desde la superficie del planeta, los tamaños aparentes del Sol y de la Luna son aproximadamente iguales).[159]​[160]​ Órbita Artículo principal: Traslación de la Tierra Galaxia espiral barrada Ilustración de la galaxia Vía Láctea, mostrando la posición del Sol La Tierra orbita alrededor del Sol a una distancia media de unos 150 millones de kilómetros, completando una órbita cada 365.2564 días solares, o un año sideral. Desde la Tierra, esto genera un movimiento aparente del Sol hacia el este, desplazándose con respecto a las estrellas a un ritmo de alrededor de 1°/día, o un diámetro del Sol o de la Luna cada 12 horas. Debido a este movimiento, en promedio la Tierra tarda 24 horas (un día solar) en completar una rotación sobre su eje hasta que el sol regresa al meridiano. La velocidad orbital de la Tierra es de aproximadamente 29.8 km/s (107 000 km/h), que es lo suficientemente rápida como para recorrer el diámetro del planeta (12 742 km) en siete minutos, o la distancia entre la Tierra y la Luna (384 000 km) en cuatro horas.[1]​ La Luna gira con la Tierra en torno a un baricentro común, debido a que este se encuentra dentro de la Tierra, a 4541 km de su centro, el sistema Tierra-Luna no es un planeta doble, la Luna completa un giro cada 27.32 días con respecto a las estrellas de fondo. Cuando se combina con la revolución común del sistema Tierra-Luna alrededor del Sol, el período del mes sinódico, desde una luna nueva a la siguiente, es de 29.53 días. Visto desde el polo norte celeste, el movimiento de la Tierra, la Luna y sus rotaciones axiales son todas contrarias a la dirección de las manecillas del reloj (sentido antihorario). Visto desde un punto de vista situado sobre los polos norte del Sol y la Tierra, la Tierra parecería girar en sentido antihorario alrededor del Sol. Los planos orbitales y axiales no están alineados: El eje de la Tierra está inclinado unos 23.4 grados con respecto a la perpendicular al plano Tierra-Sol, y el plano entre la Tierra y la Luna está inclinado unos 5 grados con respecto al plano Tierra-Sol. Sin esta inclinación, habría un eclipse cada dos semanas, alternando entre los eclipses lunares y eclipses solares.[1]​[161]​ La esfera de Hill, o la esfera de influencia gravitatoria, de la Tierra tiene aproximadamente 1.5 Gm (o 1 500 000 kilómetros) de radio.[162]​[nota 13] Esta es la distancia máxima en la que la influencia gravitatoria de la Tierra es más fuerte que la de los más distantes Sol y resto de planetas. Los objetos deben orbitar la Tierra dentro de este radio, o terminarán atrapados por la perturbación gravitatoria del Sol. Desde el año de 1772, se estableció que cuerpos pequeños pueden orbitar de manera estable la misma órbita que un planeta, si esta permanece cerca de un punto triangular de Lagrange (también conocido como «punto troyano») los cuales están situados 60° delante y 60° detrás del planeta en su órbita. La Tierra es el cuarto planeta con un asteroide troyano (2010 TK7) después de Júpiter, Marte y Neptuno de acuerdo a la fecha de su descubrimiento[nota 14] Este fue difícil de localizar debido al posicionamiento geométrico de la observación, este fue descubierto en 2010 gracias al telescopio WISE (Wide-Field Infrared Survey Explorer) de la NASA, pero fue en abril de 2011 con el telescopio «Canadá-Francia-Hawái» cuando se confirmó su naturaleza troyana,[165] y se estima que su órbita permanezca estable dentro de los próximos 10 000 años.[166]​ La Tierra, junto con el sistema solar, está situada en la galaxia Vía Láctea, orbitando a alrededor de 28 000 años luz del centro de la galaxia. En la actualidad se encuentra unos 20 años luz por encima del plano ecuatorial de la galaxia, en el brazo espiral de Orión.[167]​ Estaciones e inclinación axial Artículo principal: Oblicuidad de la eclíptica Las estaciones se producen en la Tierra debido a la inclinación de su eje de rotación respecto al plano definido por su órbita (de la eclíptica). En la ilustración es invierno en el hemisferio norte y verano en el hemisferio sur. (La distancia y el tamaño entre los cuerpos no está a escala). Debido a la inclinación del eje de la Tierra, la cantidad de luz solar que llega a un punto cualquiera en la superficie varía a lo largo del año. Esto ocasiona los cambios estacionales en el clima, siendo verano en el hemisferio norte ocurre cuando el Polo Norte está apuntando hacia el Sol, e invierno cuando apunta en dirección opuesta. Durante el verano, el día tiene una duración más larga y la luz solar incide más perpendicularmente en la superficie. Durante el invierno, el clima se vuelve más frío y los días más cortos. En la zona del círculo polar ártico se da el caso extremo de no recibir luz solar durante una parte del año; fenómeno conocido como la noche polar. En el hemisferio sur se da la misma situación pero de manera inversa, con la orientación del Polo Sur opuesta a la dirección del Polo Norte. Espacio oscuro con la Tierra creciente a menor Luna izquierda, media luna en la parte superior derecha, el 30 % del diámetro aparente de la Tierra, cinco veces el diámetro aparente distancia entre la Tierra en la parte izquierda baja, la Luna creciente en la esquina superior derecha, el diámetro aparente de la Tierra es del 30 %; cinco veces el diámetro aparente entre la Tierra desde el espacio; la luz solar proveniente del lado derecho. La Tierra y la Luna vistas desde Marte, imagen del Mars Reconnaissance Orbiter. Desde el espacio, la Tierra puede verse en fases similares a las fases lunares. Por convenio astronómico, las cuatro estaciones están determinadas por solsticios (puntos de la órbita en los que el eje de rotación terrestre alcanza la máxima inclinación hacia el Sol —solsticio de verano— o hacia el lado opuesto —solsticio de invierno—) y por equinoccios, cuando la inclinación del eje terrestre es perpendicular a la dirección del Sol. En el hemisferio norte, el solsticio de invierno se produce alrededor del 21 de diciembre, el solsticio de verano el 21 de junio, el equinoccio de primavera el 20 de marzo y el equinoccio de otoño el 23 de septiembre. En el hemisferio sur la situación se invierte, con el verano y los solsticios de invierno en fechas contrarias a la del hemisferio norte. De igual manera sucede con el equinoccio de primavera y de otoño.[168]​ El ángulo de inclinación de la Tierra es relativamente estable durante largos períodos de tiempo. Sin embargo, la inclinación se somete a nutaciones; un ligero movimiento irregular, con un período de 18.6 años.[169] La orientación (en lugar del ángulo) del eje de la Tierra también cambia con el tiempo, precesando un círculo completo en cada ciclo de 25 800 años. Esta precesión es la razón de la diferencia entre el año sidéreo y el año tropical. Ambos movimientos son causados por la atracción variante del Sol y la Luna sobre el abultamiento ecuatorial de la Tierra. Desde la perspectiva de la Tierra, los polos también migran unos pocos metros sobre la superficie. Este movimiento polar tiene varios componentes cíclicos, que en conjunto reciben el nombre de movimientos cuasiperiódicos. Además del componente anual de este movimiento, existe otro movimiento con ciclos de 14 meses llamado el bamboleo de Chandler. La velocidad de rotación de la Tierra también varía en un fenómeno conocido como variación de duración del día.[170]​ En tiempos modernos, el perihelio de la Tierra se produce alrededor del 3 de enero y el afelio alrededor del 4 de julio. Sin embargo, estas fechas cambian con el tiempo debido a la precesión orbital y otros factores, que siguen patrones cíclicos conocidos como ciclos de Milankovitch. La variación de la distancia entre la Tierra y el Sol resulta en un aumento de alrededor del 6.9 %[nota 15] de la energía solar que llega a la Tierra en el perihelio en relación con el afelio. Puesto que el hemisferio sur está inclinado hacia el Sol en el momento en que la Tierra alcanza la máxima aproximación al Sol, a lo largo del año el hemisferio sur recibe algo más de energía del Sol que el hemisferio norte. Sin embargo, este efecto es mucho menos importante que el cambio total de energía debido a la inclinación del eje, y la mayor parte de este exceso de energía es absorbido por la superficie oceánica, que se extiende en mayor proporción en el hemisferio sur.[171]​ Satélite natural y otros elementos orbitales Características Diámetro 3474.8 km Masa 7.349 × 1022 kg Semieje mayor 384 400 km Periodo orbital 27 d 7 h 43.7 m Luna Artículos principales: Luna y Sistema Tierra-Luna. La Luna es el satélite natural de la Tierra. Es un cuerpo del tipo terrestre relativamente grande: con un diámetro de alrededor de la cuarta parte del de la Tierra, es el segundo satélite más grande del sistema solar en relación con el tamaño de su planeta, después del satélite Caronte de su planeta enano Plutón. Los satélites naturales que orbitan los demás planetas se denominan «lunas» en referencia a la Luna de la Tierra. Detalles del sistema Tierra-Luna. Además del radio de cada objeto, de la distancia entre ellos, y de la inclinación del eje de cada uno, se muestra la distancia del baricentro del sistema Tierra-Luna al centro de la Tierra (4641 km). Imágenes Archivado el 1 de noviembre de 2011 en Wayback Machine. e información de la NASA. El eje de la Luna se localiza por la tercera ley de Cassini. La atracción gravitatoria entre la Tierra y la Luna causa las mareas en la Tierra. El mismo efecto en la Luna ha dado lugar a su acoplamiento de marea, lo que significa que su período de rotación es idéntico a su periodo de traslación alrededor de la Tierra. Como resultado, la luna siempre presenta la misma cara hacia nuestro planeta. A medida que la Luna orbita la Tierra, diferentes partes de su cara son iluminadas por el Sol, dando lugar a las fases lunares. La parte oscura de la cara está separada de la parte iluminada del terminador solar. Debido a la interacción de las mareas, la Luna se aleja de la Tierra a una velocidad de aproximadamente 38 mm al año. Acumuladas durante millones de años, estas pequeñas modificaciones, así como el alargamiento del día terrestre en alrededor de 23 µs, han producido cambios significativos.[172] Durante el período devónico, por ejemplo, (hace aproximadamente 410 millones de años) un año tenía 400 días, cada uno con una duración de 21.8 horas.[173]​ Secuencia de imágenes que muestran la rotación de la Tierra y la traslación de la Luna vistas desde la sonda espacial Galileo. La Luna pudo haber afectado dramáticamente el desarrollo de la vida, moderando el clima del planeta. Evidencias paleontológicas y simulaciones computarizadas muestran que la inclinación del eje terrestre está estabilizada por las interacciones de marea con la Luna.[174] Algunos teóricos creen que sin esta estabilización frente al momento ejercido por el Sol y los planetas sobre la protuberancia ecuatorial de la Tierra, el eje de rotación podría ser caóticamente inestable, mostrando cambios caóticos durante millones de años, como parece ser el caso de Marte.[175]​ Vista desde la Tierra, la Luna está justo a una distancia que la hace que el tamaño aparente de su disco sea casi idéntico al del Sol. El diámetro angular (o ángulo sólido) de estos dos cuerpos coincide porque aunque el diámetro del Sol es unas 400 veces más grande que el de la Luna, también está 400 veces más distante.[160] Esto permite que en la Tierra se produzcan los eclipses solares totales y anulares. La teoría más ampliamente aceptada sobre el origen de la Luna, la teoría del gran impacto, afirma que esta se formó por la colisión de un protoplaneta del tamaño de Marte, llamado Tea, con la Tierra primitiva. Esta hipótesis explica (entre otras cosas) la relativa escasez de hierro y elementos volátiles en la Luna, y el hecho de que su composición sea casi idéntica a la de la corteza terrestre.[176]​ Representación a escala del tamaño y distancia relativa entre la Tierra y la Luna. Representación a escala del tamaño y distancia relativa entre la Tierra y la Luna. Otros elementos orbitales A fecha de 2016, el planeta Tierra tiene nueve cuasisatélites naturales o asteroides coorbitales conocidos: el (3753) Cruithne, el 2002 AA29,[177]​[178] 2003 YN107, 2004 GU9,[179] 2006 FV35, 2010 SO16[180] 2013 LX28, 2014 OL339 y 2016 HO3.[181] El 15 de febrero de 2020 se descubrió que 2020 CD3 es un satélite natural temporal terrestre. A fecha de septiembre de 2021, existen 4550 satélites operativos creados por el hombre orbitando la Tierra.[5]​ Localización de la Tierra Artículo principal: Anexo:Localización de la Tierra en el universo Diagrama de nuestra ubicación dentro del universo observable. (Click aquí para ver en pantalla completa.) Habitabilidad Artículo principal: Habitabilidad planetaria Un planeta que pueda sostener vida se denomina habitable, incluso aunque en él no se originara vida. La Tierra proporciona las (actualmente entendidas como) condiciones necesarias, tales como el agua líquida, un ambiente que permite el ensamblaje de moléculas orgánicas complejas, y la energía suficiente para mantener un metabolismo.[182] Hay otras características que se cree que también contribuyen a la capacidad del planeta para originar y mantener la vida: la distancia entre la Tierra y el Sol, así como su excentricidad orbital, la velocidad de rotación, la inclinación axial, la historia geológica, la permanencia de la atmósfera, y la protección ofrecida por el campo magnético.[183]​ Biosfera Artículo principal: Biosfera Se denomina «biosfera» al conjunto de los diferentes tipos de vida del planeta junto con su entorno físico, modificado por la presencia de los primeros. Generalmente se entiende que la biosfera empezó a evolucionar hace 3500 millones de años. La Tierra es el único lugar donde se sabe que existe vida. La biosfera se divide en una serie de biomas, habitados por plantas y animales esencialmente similares. En tierra, los biomas se separan principalmente por las diferencias en latitud, la altitud sobre el nivel del mar y la humedad. Los biomas terrestres situados en los círculos ártico o antártico, en gran altura o en zonas extremadamente áridas son relativamente estériles de vida vegetal y animal; la diversidad de especies alcanza su máximo en tierras bajas y húmedas, en latitudes ecuatoriales.[184]​ Recursos naturales y uso de la tierra Artículo principal: Recurso natural La Tierra proporciona recursos que son explotados por los seres humanos con diversos fines. Algunos de estos son recursos no renovables, tales como los combustibles fósiles, que son difícilmente renovables a corto plazo. De la corteza terrestre se obtienen grandes depósitos de combustibles fósiles, consistentes en carbón, petróleo, gas natural y clatratos de metano. Estos depósitos son utilizados por los seres humanos para la producción de energía, y también como materia prima para la producción de sustancias químicas. Los cuerpos minerales también se han formado en la corteza terrestre a través de distintos procesos de mineralogénesis, como consecuencia de la erosión y de los procesos implicados en la tectónica de placas.[185] Estos cuerpos albergan fuentes concentradas de varios metales y otros elementos útiles. La biosfera de la Tierra produce muchos productos biológicos útiles para los seres humanos, incluyendo (entre muchos otros) alimentos, madera, fármacos, oxígeno, y el reciclaje de muchos residuos orgánicos. El ecosistema terrestre depende de la capa superior del suelo y del agua dulce, y el ecosistema oceánico depende del aporte de nutrientes disueltos desde tierra firme.[186] Los seres humanos también habitan la tierra usando materiales de construcción para construir refugios. Para 1993, el aprovechamiento de la tierra por los humanos era de aproximadamente: Uso de la tierra Tierra cultivable Cultivos permanentes Pastos permanentes Bosques y tierras arboladas Áreas urbanas Otros Porcentaje 13.13 %[8] 4.71 %[8] 26 % 32 % 1.5 % 30 % La cantidad de tierras de regadío en 1993 se estimaban en 2 481 250 km².[8]​ Medio ambiente y riesgos Grandes áreas de la superficie de la Tierra están sujetas a condiciones climáticas extremas, tales como ciclones tropicales, huracanes, o tifones que dominan la vida en esas zonas. Muchos lugares están sujetos a terremotos, deslizamientos, tsunamis, erupciones volcánicas, tornados, dolinas, ventiscas, inundaciones, sequías y otros desastres naturales. Muchas áreas concretas están sujetas a la contaminación causada por el hombre del aire y del agua, a la lluvia ácida, a sustancias tóxicas, a la pérdida de vegetación (sobrepastoreo, deforestación, desertificación), a la pérdida de vida salvaje, la extinción de especies, la degradación del suelo y su agotamiento, a la erosión y a la introducción de especies invasoras. Según las Naciones Unidas, existe un consenso científico que vincula las actividades humanas con el calentamiento global, debido a las emisiones industriales de dióxido de carbono y el calor residual antropogénico. Se prevé que esto produzca cambios tales como el derretimiento de los glaciares y superficies heladas, temperaturas más extremas, cambios significativos en el clima y un aumento global del nivel del mar.[187]​[188]​[189]​ Geografía humana Artículo principal: Geografía humana La cartografía —el estudio y práctica de la elaboración de mapas—, y subsidiariamente la geografía, han sido históricamente las disciplinas dedicadas a describir la Tierra. La topografía o determinación de lugares y distancias, y en menor medida la navegación, o determinación de la posición y de la dirección, se han desarrollado junto con la cartografía y la geografía, suministrando y cuantificando la información necesaria. La Tierra tiene aproximadamente 8200 millones de habitantes (según datos a julio de 2024).[190] Las proyecciones indicaban que la población humana mundial llegaría a 7000 millones a principios de 2012, pero esta cifra fue superada a mediados de octubre de 2011[191] y se espera llegar a 10 300 millones en 2080.[190] Se piensa que la mayor parte de este crecimiento tendrá lugar en los países en vías de desarrollo. La región del África subsahariana tiene la tasa de natalidad más alta del mundo. La densidad de población varía mucho en las distintas partes del mundo, pero la mayoría de la población vive en Asia. Está previsto que para el año 2020 el 60 % de la población mundial se concentre en áreas urbanas, frente al 40 % en áreas rurales.[192]​ Se estima que solamente una octava parte de la superficie de la Tierra es apta para su ocupación por los seres humanos; tres cuartas partes está cubierta por océanos, y la mitad de la superficie terrestre es: desierto (14 %),[193] alta montaña (27 %),[194] u otros terrenos menos adecuados. El asentamiento permanente más septentrional del mundo es Alert, en la Isla de Ellesmere en Nunavut, Canadá.[195] (82°28′N). El más meridional es la Base Amundsen-Scott, en la Antártida, casi exactamente en el Polo Sur. (90°S) La Tierra de noche. Imagen compuesta a partir de los datos de iluminación del DMSP/OLS, representando una imagen simulada del mundo de noche. Esta imagen no es fotográfica y muchas características son más brillantes de lo que le parecería a un observador directo. Las naciones soberanas independientes reclaman la totalidad de la superficie de tierra del planeta, a excepción de algunas partes de la Antártida y la zona no reclamada de Bir Tawil entre Egipto y Sudán. En el año 2011 existen 204 Estados soberanos, incluidos los 192 Estados miembros de las Naciones Unidas. Hay también 59 territorios dependientes, y una serie de áreas autónomas, territorios en disputa y otras entidades.[8] Históricamente, la Tierra nunca ha tenido un gobierno soberano con autoridad sobre el mundo entero, a pesar de que una serie de estados-nación han intentado dominar el mundo, sin éxito.[196]​ Las Naciones Unidas es una organización mundial intergubernamental que se creó con el objetivo de intervenir en las disputas entre las naciones, a fin de evitar los conflictos armados.[197] Sin embargo, no es un gobierno mundial. La ONU sirve principalmente como un foro para la diplomacia y el derecho internacional. Cuando el consenso de sus miembros lo permite, proporciona un mecanismo para la intervención armada.[198]​ Duración: 48 segundos.0:48 La Tierra de noche. El vídeo de la EEI comienza justo al sureste de Alaska. La primera ciudad que pasa por encima de la Estación Espacial Internacional (vista unos 10 segundos en el vídeo) es la de San Francisco y sus alrededores. Si se mira con mucho cuidado, se puede ver que en el puente Golden Gate se encuentra: una franja más pequeña de luces justo antes de la cercana ciudad de San Francisco, nubes a la derecha de la imagen. También se pueden ver tormentas eléctricas muy evidentes en la costa del océano Pacífico, con nubes. A medida que el video avanza, la EEI pasa por encima de América Central (las luces verdes se pueden ver aquí), con la península de Yucatán a la izquierda. El paseo termina en la Estación Espacial Internacional es la ciudad capital de Bolivia, La Paz. El primer humano en orbitar la Tierra fue Yuri Gagarin el 12 de abril de 1961.[199] Hasta 2004, alrededor de 400 personas visitaron el espacio exterior y alcanzado la órbita de la Tierra. De estos, doce han caminado sobre la Luna.[200]​[201]​[202] En circunstancias normales, los únicos seres humanos en el espacio son los de la Estación Espacial Internacional (EEI). La tripulación de la estación, compuesta en la actualidad por seis personas, suele ser reemplazada cada seis meses.[203] Los seres humanos que más se han alejado de la Tierra se distanciaron 400 171 kilómetros, alcanzados en la década de 1970 durante la misión Apolo 13.[204]​ Véase también: Mundo Perspectiva cultural La primera fotografía hecha por astronautas del «amanecer de la Tierra», tomada desde el Apolo 8. La palabra «Tierra» proviene del latín terra (en minúsculas)[205] y que, en mayúsculas, se asoció a dos diosas arquetipos de la «madre tierra», Gea para los griegos y Tellus para los romanos. Especialmente, en la Edad Contemporánea, se le ha dado el nombre poético de Gaia.[206] El símbolo astronómico estándar de la Tierra consiste en una cruz circunscrita por un círculo.[207]​ A diferencia de lo sucedido con el resto de los planetas del sistema solar, la humanidad no comenzó a ver la Tierra como un objeto en movimiento, en órbita alrededor del Sol, hasta alcanzado el siglo XVI.[208] La Tierra a menudo se ha personificado como una deidad, en particular, una diosa. En muchas culturas la diosa madre también es retratada como una diosa de la fertilidad. En muchas religiones los mitos sobre la creación recuerdan una historia en la que la Tierra es creada por una deidad o deidades sobrenaturales. Varios grupos religiosos, a menudo asociados a las ramas fundamentalistas del protestantismo[209] o el islam,[210] afirman que sus interpretaciones sobre estos mitos de creación, relatados en sus respectivos textos sagrados son la verdad literal, y que deberían ser consideradas junto con los argumentos científicos convencionales de la formación de la Tierra y el desarrollo y origen de la vida, o incluso reemplazarlos.[211] Tales afirmaciones son rechazadas por la comunidad científica[212]​[213] y otros grupos religiosos.[214]​[215]​[216] Un ejemplo destacado es la controversia entre el creacionismo y la teoría de la evolución. En el pasado hubo varias creencias en una Tierra plana,[217] pero esta creencia fue desplazada por el concepto de una Tierra esférica, debido a la gran evidencia de esta como su circunnavegación.[218] La perspectiva humana acerca de la Tierra ha cambiado tras el comienzo de los vuelos espaciales, y actualmente la biosfera se interpreta desde una perspectiva global integrada.[219]​[220] Esto se refleja en el creciente movimiento ecologista, que se preocupa por los efectos que causa la humanidad sobre el planeta."
ksampletext_wikipedia_geol_volcan: str = "Volcán. Un volcán (del portugués, y este del latín Vulcano, dios romano del fuego) es una estructura geológica en la tierra o en el mar, generalmente una montaña, por la que emerge el magma que se divide en lava y gases provenientes del interior de la Tierra.[4] El ascenso del magma ocurre en episodios de actividad violenta denominados erupciones, que pueden variar en intensidad, duración y frecuencia, desde suaves corrientes de lava hasta explosiones extremadamente destructivas. En ocasiones, los volcanes adquieren una forma cónica por la acumulación de material de erupciones anteriores. En la cumbre se encuentra su cráter o caldera. Por lo general los volcanes se forman en los límites de las placas tectónicas, aunque existen los llamados puntos calientes, donde no hay contacto entre placas, como es el caso de las islas Hawái. Aproximadamente el 75% de los volcanes activos del mundo están ubicados en el llamado cinturón de fuego del Pacífico.[5]​ Los volcanes pueden tener muchas formas y despedir distintos materiales. Algunas de las formas más comunes son el estratovolcán, el cono de escoria, la caldera volcánica y el volcán en escudo. También existen numerosos volcanes submarinos ubicados a lo largo de las dorsales mediooceánicas. Algunos volcanes alcanzan una altitud superior a los 6000 metros sobre el nivel del mar. El volcán más alto del mundo es el Nevado Ojos del Salado, en Argentina y Chile, siendo además la segunda cumbre más alta de los hemisferios sur y occidental (solo superado por el cerro argentino Aconcagua).[6]​ Los volcanes no solo existen en la Tierra, sino también en otros planetas y satélites. Algunos están formados por materiales considerados fríos y se denominan criovolcanes. En ellos, el hielo actúa como roca, mientras que el agua fría líquida interna actúa como magma; esto ocurre en la luna de Júpiter llamada Europa. Relación entre vulcanismo y las placas tectónicas Límites de placa divergentes En las crestas oceánicas medias, dos placas tectónicas divergen entre sí a medida que se forma una nueva corteza oceánica por el enfriamiento y la solidificación de la roca fundida caliente. Debido a que la corteza es muy delgada en estas crestas debido al tirón de las placas tectónicas, la liberación de presión conduce a la expansión adiabática (sin transferencia de calor o materia) y al derretimiento parcial del manto, causando vulcanismo y creando una nueva corteza oceánica. La mayoría de los límites de placas divergentes se encuentran en el fondo de los océanos; por lo tanto, la mayor parte de la actividad volcánica en la Tierra es submarina, formando un nuevo fondo marino. Los fumadores negros (también conocidos como respiraderos de aguas profundas) son evidencia de este tipo de actividad volcánica. Donde la cresta oceánica media está sobre el nivel del mar, se forman islas volcánicas; por ejemplo, Islandia. Placas convergentes Las zonas de subducción son lugares donde chocan dos placas, generalmente una placa oceánica y una placa continental. En este caso, la placa oceánica se subduce, o se sumerge, debajo de la placa continental, formando una trinchera oceánica profunda en alta mar. En un proceso llamado fusión de flujo, el agua liberada de la placa subductora reduce la temperatura de fusión de la cuña del manto suprayacente, creando así magma. Este magma tiende a ser extremadamente viscoso debido a su alto contenido de sílice, por lo que a menudo no alcanza la superficie sino que se enfría y solidifica en profundidad. Cuando llega a la superficie, sin embargo, se forma un volcán. Ejemplos típicos son el Etna y los volcanes en el Anillo de Fuego del Pacífico. Puntos calientes Los puntos calientes son áreas volcánicas formadas por plumas de manto, que son columnas de material caliente que se elevan desde el límite núcleo-manto en un espacio fijo que causa la fusión de grandes volúmenes. En algunos casos, debido a que las placas tectónicas se mueven a través de ellas, cada volcán se vuelve inactivo y se forma uno nuevo a medida que la placa avanza sobre el penacho térmico, como en el caso del archipiélago de Hawái; también lo ha hecho la llanura del río Snake, con la caldera de Yellowstone como parte de la placa norteamericana sobre el punto caliente. Otros ejemplos de vulcanismo asociado a punto caliente son las islas Canarias, esta vez con un desplazamiento mínimo de la placa africana, o Islandia, que además coincide con un límite divergente de placas. Tipos de volcanes según su actividad Los volcanes, teniendo en cuenta la frecuencia de sus erupciones, se pueden clasificar en tres tipos: activos, inactivos (durmientes) o extintos. Volcanes activos Los volcanes activos son aquellos que pueden entrar en actividad eruptiva en cualquier momento, es decir, que permanecen en estado de latencia. Esto ocurre con la mayoría de los volcanes, pues ocasionalmente entran en actividad, permaneciendo en reposo la mayor parte del tiempo. El período de actividad eruptiva puede durar desde una hora hasta varios años, como fue el caso del volcán de Pacaya y del Irazú. Hasta el momento, no se ha descubierto ningún método seguro para predecir las erupciones. Volcanes durmientes o inactivos Los volcanes durmientes o inactivos son aquellos que mantienen ciertos signos de actividad, como la presencia de aguas termales, y han entrado en actividad esporádicamente. Dentro de esta categoría suelen incluirse las fumarolas y los volcanes con largos períodos de inactividad entre una erupción y otra. Un volcán se considera durmiente si desde hace siglos no ha tenido una erupción. Volcanes extintos Artículo principal: Volcán extinto Los volcanes extintos son aquellos cuya última erupción fue registrada hace más de 25 000 años. Sin embargo, no se descarta la posibilidad de que puedan despertar y liberar una erupción más fuerte que la de un volcán que está activo, causando grandes desastres. También se les llama extintos cuando han sido alejados de su fuente de magma, perdiendo poco a poco su actividad, esto sucede únicamente en volcanes de punto caliente, a diferencia de los volcanes de zonas de subducción. Tipos de erupciones volcánicas Artículo principal: Erupción volcánica Erupción en el 2011 del volcán Tungurahua, Ecuador. La temperatura, composición, viscosidad y elementos disueltos en el magma son los factores que determinan el tipo de erupción y la cantidad de productos volátiles que la acompañan. Hawaiana Artículo principal: Erupción hawaiana Volcán hawaiano en Kilauea. En este tipo de erupción, la lava generalmente es bastante fluida y no ocurren desprendimientos gaseosos explosivos. Estas lavas se desbordan cuando rebasan el cráter y se deslizan con facilidad por la ladera del volcán, formando verdaderas corrientes que recorren grandes distancias. Por esta razón, los volcanes de tipo hawaiano son de pendiente suave. Algunos residuos de lava, al ser arrastrados por el viento, forman hilos cristalinos que los nativos hawaianos llaman cabellos de la diosa Pele, la diosa del fuego. El volcán hawaiano más famoso es el Kilauea. Estromboliana o mixta Artículo principal: Erupción estromboliana Erupción del Estrómboli en verano de 2015 (animado). Erupción del Estrómboli (Italia) en 1980. Este tipo de erupción recibe el nombre del Estrómboli, volcán de las islas Eolias (mar Tirreno), al norte de Sicilia. Se origina cuando hay alternancia de los materiales en erupción, formándose un cono estratificado en capas de lavas fluidas y materiales sólidos. La lava es fluida, va desprendiendo gases abundantes y violentos con proyecciones de escorias, bombas y lapilli. Debido a que los gases pueden desprenderse con facilidad, no se producen pulverizaciones o cenizas. Cuando la lava rebosa por los bordes del cráter, desciende por las laderas y barrancos, pero no alcanza grandes extensiones como en las erupciones de tipo hawaiano. Vulcaniana Vulcano. Del nombre del volcán Vulcano en las islas Eolias. Esta erupción se caracteriza porque en ella se desprenden grandes cantidades de gases, la lava liberada es poco fluida y se consolida con rapidez. En este tipo de erupción, las explosiones son muy fuertes y pulverizan la lava, produciendo mucha ceniza, la cual es lanzada al aire acompañada de otros materiales fragmentarios. Cuando el magma sale al exterior en forma de lava, se solidifica rápidamente, pero los gases que se desprenden rompen y resquebrajan su superficie, volviéndola áspera y muy irregular y formando lava de tipo Aa. Los conos de estos volcanes son de pendiente muy inclinada. Pliniana o vesubiana Artículo principal: Erupción pliniana Nombrada así en honor a Plinio el Joven, difiere de la erupción volcánica en que en ésta la presión de los gases es muy fuerte y produce explosiones muy violentas, que en los casos extremos (plinianos) puede dar lugar a unas coladas piroclásticas o nubes ardientes que bruscamente se precipitan por las laderas del volcán alcanzando gran rapidez y sepultando en sólo unos minutos una gran extensión de terreno. Estos fenómenos críticos pueden sepultar y abrasar de golpe ciudades enteras, como ocurrió con Pompeya y Herculano por la actividad del volcán Vesubio. Al final de la deposición de esta colada piroclástica ardiente se transforma en la denominada roca ignimbrita; además se genera precipitaciones de cenizas, las cuales también pueden llegar a sepultar grandes extensiones como última capa fría. Se caracteriza por alternar erupciones de piroclasto con erupciones de coladas de lava, dando lugar a una superposición en estratos, lo que hace que este tipo de volcanes alcance grandes dimensiones; que también se denominan «Estratovolcanes». Ejemplo de ellos son el Teide, el Popocatépetl y el Fujiyama. Freatomagmática o surtseyana Artículos principales: Erupción surtseyana y Erupción freatomagmática. Los volcanes de tipo freatomagmático se encuentran en aguas someras, presentan un lago en el interior de su cráter y en ocasiones forman atolones. Sus erupciones son extraordinariamente violentas, ya que a la energía propia del volcán se le suma la expansión del vapor de agua súbitamente calentado. Normalmente no presentan emisiones de lava ni extrusiones de rocas. Algunas de las mayores erupciones freáticas son las del Krakatoa, el Kīlauea y la Isla de Surtsey. Peleana De los volcanes de las Antillas es célebre la Montaña Pelada, ubicada en la isla Martinica, que en la erupción de 1902 destruyó la capital, Saint-Pierre. La lava en esta erupción es extremadamente viscosa y se consolida con gran rapidez, llegando a tapar por completo el cráter formando un pitón o aguja. La enorme presión de los gases sin salida provoca una enorme explosión que levanta el pitón, o bien destroza la parte superior de la ladera. Así ocurrió el 8 de mayo de 1902, cuando las paredes del volcán cedieron a tan enorme empuje que se abrió un conducto por el que salieron con extraordinaria fuerza los gases acumulados a elevada temperatura y que, mezclados con cenizas, formaron una nube ardiente que ocasionó 28 000 víctimas. [cita requerida] Erupciones submarinas Artículo principal: Erupción submarina En el fondo oceánico se producen erupciones volcánicas cuyas lavas pueden formar islas volcánicas si llegan a la superficie. Las erupciones suelen ser de corta duración en la mayoría de los casos, debido al equilibrio isostático de las lavas al enfriarse cuando entran en contacto con el agua y también por la erosión marina. Algunas islas como las Cícladas en Grecia o las islas Canarias en España tienen este origen. Avalanchas de origen volcánico Artículo principal: Lahar Armero después de la tragedia (Colombia). Hay volcanes que generan un número de víctimas elevado, debido a que sus grandes cráteres están durante el periodo de reposo convertidos en lagos o cubiertos de nieve. Al recobrar su actividad, el agua mezclada con cenizas y otros restos, es lanzada formando torrentes y avalanchas o coladas de barro (que se denominan «lahares») que tienen una enorme capacidad destructiva. Un ejemplo de esto fue la erupción del Nevado de Ruiz en Colombia, el 13 de noviembre de 1985. El Nevado del Ruiz es un volcán explosivo en el que la cumbre del cráter (5321 m s.n.m.) estaba recubierta por un casquete de hielo; al ascender la lava se recalentaron las capas de hielo y se formaron unas coladas de barro que invadieron el valle del río Lagunilla, sepultando la ciudad de Armero, dejando 24 000 muertos y decenas de miles de heridos.[cita requerida] Erupciones fisurales Se originan en una larga dislocación de la corteza terrestre, que puede ser desde apenas unos metros hasta varios kilómetros. La lava que fluye a lo largo de la rotura es fluida y recorre grandes extensiones formando amplias mesetas (traps), con uno o más kilómetros de espesor y miles de km². Un ejemplo de vulcanismo fisural es la meseta del Decán en la India. Véase también: Índice de explosividad volcánica Volcán en escudo Artículo principal: Volcán en escudo Columnas de basalto de la «Calzada del Gigante» en Irlanda del Norte. Cuando la lava expulsada por el volcán es fluida, de tipo hawaiano, el volcán adquiere una forma de una estructura amplia y abovedada, que por su apariencia se los denomina en escudo. Los volcanes de escudo se asemejan a la superficie superior de un escudo que reposara en el suelo con el lado convexo hacia arriba. Un volcán en escudo está formado principalmente por lavas basálticas (ricas en hierro) y poco material piroclástico. El mayor volcán de la Tierra es el Mauna Loa, un volcán en escudo en las islas Hawái. El Mauna Loa nace en las profundidades del mar, a unos 5000 metros y se eleva sobre el nivel del mar por unos 4170 metros. Los volcanes en escudo como el Mauna Loa se forman a lo largo de millones de años gracias a ciclos de erupciones de lava que se van superponiendo unas con otras. El volcán de escudo más activo es el Kīlauea, localizado en la Isla de Hawái, al lado de Mauna Loa. En el período histórico el Kilauea ha entrado unas cincuenta veces en erupción y es, por lo tanto, el volcán de este tipo más estudiado. El resultado de erupciones constantes durante millones de años ha dado lugar a la creación de las montañas más grandes de la Tierra (si se tiene en cuenta la altura contando desde la base en el lecho marino). Por ejemplo, el Mauna Loa, desde su base submarina hasta su cúspide, cuenta con una altura de 9.5 km, más alto que el monte Everest. Los geólogos creen que las primeras etapas de formación de los volcanes en escudo consisten en erupciones frecuentes de delgadas coladas de basalto muy líquidas. Además de estas erupciones también se producen erupciones laterales. Normalmente con el cese de cada fase eruptiva se produce el hundimiento del área de la cima. En las últimas fases, las erupciones son más esporádicas y la erupción piroclástica se hace más frecuente. A medida que esto sucede, las coladas de lava tienden a ser más viscosas, lo que provoca que sean más cortas y potentes. Así, va aumentando la pendiente de la ladera del área de la cima. Los volcanes en escudo son muy comunes y también se han identificado en el sistema solar. El más grande conocido hasta la fecha es el monte Olimpo, sobre la superficie de Marte, encontrándose también varios de estos volcanes sobre la superficie de Venus, aunque de apariencia más achatada. Flujo piroclástico Artículo principal: Flujo piroclástico Flujo piroclástico expulsado por el volcán Mayón en Filipinas. Cuando las erupciones de un volcán llegan acompañadas de gases calientes y cenizas se produce lo que se conoce como flujo piroclástico o «nube ardiente». También conocida como avalancha incandescente, el flujo piroclástico se desplaza pendiente abajo a velocidades cercanas a los 200 km/h. La sección basal de estas nubes contienen gases calientes y partículas que flotan en ellos. De esta forma, las nubes transportan fragmentos de rocas que –gracias al rebote de los gases calientes en expansión– se depositan a lo largo de más de 100 km desde su punto de origen. En 1902 una nube ardiente de un pequeño volcán llamado monte Pelée en la isla caribeña de Martinica destruyó la ciudad portuaria de San Pedro. La destrucción fue tan devastadora que murió casi toda la población (unos 28 000 habitantes). A diferencia de Pompeya, que quedó enterrada en un manto de cenizas en un plazo de tres días y las casas quedaron intactas (salvo los techos por el peso de las cenizas), la ciudad de San Pedro fue destruida solo en minutos y la energía liberada fue tal que los árboles fueron arrancados de raíz, las paredes de las casas desaparecieron y las monturas de los cañones se desintegraron. La erupción del monte Pelée muestra cuan distintos pueden ser dos volcanes del mismo tipo. Lahar Artículo principal: Lahar Los conos compuestos también producen coladas de barro llamadas lahar, una palabra de origen indonesio. Estos flujos se producen cuando las cenizas y derrubios volcánicos se saturan de agua y descienden pendiente abajo, normalmente siguiendo los cauces de los ríos. Algunos de los lahares se producen cuando la saturación es provocada por la lluvia, mientras que en otros casos cuando grandes volúmenes de hielo y nieve se funden por una erupción volcánica. En Islandia, el último caso se denomina jökulhlaup y es un fenómeno devastador. Destrucciones importantes de lahares se dieron en 1980 con la erupción del monte Santa Helena, en Estados Unidos, que a pesar de los destrozos producidos, no produjo muchas víctimas debido a que la región está poco poblada. Otro fue en 1985 con la erupción del Nevado del Ruiz, en Colombia, la cual generó un lahar que acabó con la vida de 25 000 personas. Formas volcánicas relacionadas Calderas Artículo principal: Caldera volcánica Caldera Aniakchak, en Alaska. La mayoría de los volcanes presentan en su cima un cráter de paredes empinadas, por el interior. Cuando el cráter supera 1 km de diámetro se denomina caldera volcánica. Las calderas son estructuras de forma circular y la mayoría se forma cuando la estructura volcánica se hunde sobre la cámara magmática parcialmente vacía que se sitúa por debajo. Si bien la mayoría de las calderas se crea por el hundimiento producido después de una erupción explosiva, esto no es así en todos los casos. En el caso de los enormes volcanes en escudo de Hawái, las calderas se crearon por la continua subsidencia a medida que el magma se drenaba desde la cámara magmática durante las erupciones laterales. También las calderas de las islas Galápagos se han ido hundiendo por derrames laterales. Las calderas de gran tamaño se forman cuando un cuerpo lavático granítico (félsico) se ubica cerca de la superficie curvando de esta manera las rocas superiores. Posteriormente, una fractura en el techo permite al magma rico en gases y muy viscoso ascender hasta la superficie, donde expulsa de manera explosiva, enormes volúmenes de material piroclástico, fundamentalmente cenizas y fragmentos de pumita. Estos materiales se denominan coladas piroclásticas y pueden alcanzar velocidades de 100 km/h. Cuando estos materiales se detienen, los fragmentos calientes se fusionan para formar una toba soldada que se asemeja a una colada de lava solidificada. Finalmente, el techo se derrumba dando lugar a una caldera. Este procedimiento puede repetirse varias veces en el mismo lugar. Se conocen al menos 138 calderas que superan los 5 km de diámetro. Muchas de estas calderas son difíciles de ubicar, por lo que han sido identificadas con imágenes de satélites. Entre las más importantes se encuentra La Garita con unos 32 km de diámetro y una longitud de 80 que está ubicada en las montañas de San Juan al sur del estado de Colorado. Erupciones fisurales y llanuras de lava Artículo principal: Fisura volcánica Cono piroclástico en el volcán fisural Laki en Islandia. A pesar de que las erupciones volcánicas están relacionadas con estructuras en forma de cono, la mayor parte del material volcánico es extruido por fracturas en la corteza denominadas fisuras. Estas fisuras permiten la salida de lavas de baja viscosidad que recubren grandes áreas. La meseta del Columbia en el noroeste de Estados Unidos se formó de esta manera. Las erupciones fisurales expulsaron lava basáltica muy líquida. Las coladas siguientes cubrieron el relieve y formaron una llanura de lava (plateau) que en algunos lugares tiene casi 1.5 km de grosor. La fluidez se evidencia en la superficie recorrida por la lava: unos 150 km desde su origen. A estas coladas se las denomina basalto de inundación. Este tipo de coladas sucede principalmente en el suelo oceánico y no puede verse. A lo largo de las dorsales oceánicas, donde la expansión del suelo oceánico es activa, las erupciones fisurales generan nuevo suelo oceánico. Islandia está ubicada encima de la dorsal centroatlántica y ha experimentado numerosas erupciones fisurales. Las erupciones fisurales más grandes de Islandia ocurrieron en 1783 y se denominaron erupciones de Laki. Laki es una fisura o volcán fisural de 25 km de largo que generó más de veinte chimeneas separadas que expulsaron corrientes de lava basáltica muy fluida. El volumen total de lava expulsada por las erupciones de Laki fue superior a los 12 km³. Los gases arruinaron las praderas y mataron al ganado islandés. La hambruna subsiguiente mató cerca de diez mil personas. La caldera está situada muy por debajo de la boca del volcán. Domo de lava Artículo principal: Domo de lava Domos de lava en el cráter del monte Santa Helena (Estados Unidos). La lava rica en sílice es viscosa y por lo tanto, apenas fluye; cuando es extruida fuera de la chimenea puede producir una masa bulbosa de lava solidificada que se denomina domo de lava. Debido a su viscosidad, la mayoría está compuesto por riolitas y otros por obsidianas. La mayoría de los domos volcánicos se desarrollan a partir de una erupción explosiva de un magma rico en gases. Aunque la mayoría de los domos volcánicos están asociados a conos compuestos, algunos se forman de manera independiente. Tal es el caso de la línea de domos riolíticos y de obsidiana en los en California. Chimeneas y pitones volcánicos Artículos principales: Chimenea volcánica y Cuello volcánico. Volcán Teide (Tenerife, España). Los volcanes se alimentan del magma a través de conductos denominados chimeneas. Estas tuberías pueden extenderse hasta unos 200 km de profundidad. En este caso, las estructuras proveen de muestras del manto que han experimentado muy pocas alteraciones durante su ascenso. Las chimeneas volcánicas mejor conocidas son las sudafricanas que están cargadas de diamantes. Las rocas que rellenan estas chimeneas se originaron a profundidades de 150 km, donde la presión es lo bastante elevada como para generar diamantes y otros minerales de alta presión. Debido a que los volcanes están siendo rebajados constantemente por la erosión y la meteorización, los conos de cenizas son desgastados con el tiempo, pero no sucede lo mismo con otros volcanes. Conforme la erosión progresa, la roca que ocupa la chimenea y que es más resistente, puede permanecer de pie sobre el terreno circundante mucho después de que haya desaparecido el cono que la contiene. A estas estructuras de las denomina pitón volcánico. Shiprock, en Nuevo México, es un claro ejemplo de este tipo de estructuras. Cuevas volcánicas Artículo principal: Cueva volcánica Una cueva volcánica es cualquier cavidad formada en rocas volcánicas, aunque el uso común de este término se reserva a cuevas primarias o singenéticas creadas por procesos volcánicos de modo que tanto la oquedad como la roca encajante se forman a la vez. Material volcánico El Pu‘u ‘Ō‘ō, cono volcánico de Hawái. Artículo principal: Roca volcánica El material que se forma por la actividad de un volcán son las rocas volcánicas, efusivas o extrusivas, principalmente basaltos y andesitas. Según su textura pueden ser coladas, piroclastos (lapilli, pumita), obsidiana, etc. Volcanes extraterrestres Monte Olimpo, el volcán más grande del sistema solar situado en el planeta Marte. La Tierra no es el único planeta del sistema solar que tiene actividad volcánica. Venus tiene un intenso vulcanismo con unos cientos de miles de volcanes. Marte tiene la cumbre más alta del sistema solar: el monte Olimpo, un volcán dado por apagado con una base de unos 600 km y más de 27 km de altura. No obstante, este planeta parece tener cierta actividad volcánica apreciable.[7]​ Nuestra Luna está cubierta de inmensos campos de basalto y tiene presencia de domos lunares de origen volcánico similares a un volcán en escudo como por ejemplo el Mons Rümker, lo que sugiere que tuvo una corta pero considerable actividad volcánica que hoy muy probablemente está extinta. Debido a las bajas temperaturas del espacio, algunos volcanes de nuestro sistema solar están formados de hielo que actúa como roca, mientras su agua líquida interna actúa como la magma; esto ocurre -por ejemplo- en la fría luna de Júpiter llamada Europa. Estos reciben el nombre de criovolcán, de los cuales hay también en Encélado. La Voyager 2 descubrió en agosto de 1989, sobre Tritón, rastros de criovulcanismo y géiseres. La búsqueda de vida extraterrestre se ha interesado en buscar rastros de vida en sistemas criovolcánicos donde hay agua líquida y por ende, una fuente de radiación en calor considerable; estos son elementos esenciales para la vida. Existen volcanes un poco más similares a los terrestres, sobre otros satélites de Júpiter como en el caso de Ío. La sonda Voyager 1 permitió fotografiar en marzo de 1979 una erupción en Ío. Los astrofísicos estudian los datos de esta información, que extiende el campo de estudio de la vulcanología. El conocimiento del fenómeno tal como se produce sobre la Tierra pasa en adelante por su estudio en el espacio. La temperatura y composición química de los volcanes del sistema solar varían considerablemente entre los planetas y los satélites. Además, el tipo de materiales que arrojan en sus erupciones es muy diferente de los arrojados en la Tierra.[cita requerida] Peligros Esta sección es un extracto de Peligros volcánicos.[editar] Un diagrama esquemático muestra algunas de las muchas formas en que los volcanes pueden causar problemas a los que están cerca. Un peligro volcánico es la probabilidad de que ocurra una erupción volcánica o un suceso geofísico relacionado, en una determinada área geográfica y dentro de un período de tiempo específico. El riesgo asociado depende de la proximidad y vulnerabilidad de un bien, recurso natural o una población, cerca de donde podría ocurrir un suceso volcánico. Protección civil Véase también: Protección Civil España En España, la Norma Básica de Protección Civil, aprobada por Real Decreto 407/1992, de 24 de abril,[8] dispone en su apartado 6 que el riesgo volcánico será objeto de planes especiales en los ámbitos territoriales que lo requieran. Estos planes especiales habrán de ser elaborados de acuerdo con una Directriz Básica previamente aprobada por el Gobierno. La Directriz Básica de Planificación de Protección Civil ante el Riesgo Volcánico[9] fue aprobada por Acuerdo del Consejo de Ministros del 19 de enero de 1996 y publicada por Resolución de la Secretaría de Estado de Interior el 21 de febrero de 1996. En ella, se consideran tres niveles de planificación: estatal, autonómico y de ámbito local. Por Acuerdo del Consejo de Ministros del 25 de enero de 2013, se aprueba el Plan Estatal de Protección Civil ante el Riesgo Volcánico.[10]​ Creencias tradicionales sobre los volcanes Vulcano forjando los rayos de Júpiter (1636), de Pedro Pablo Rubens. Muchos cuentos antiguos atribuyen las erupciones volcánicas a causas sobrenaturales, tales como la acción de dioses o semidioses. Los antiguos griegos aun pensaban que el poder caprichoso de los volcanes sólo podía ser explicado como un acto divino, mientras que el astrónomo de los siglos XVI-XVII Johannes Kepler creía que eran los conductos lagrimales de la Tierra. Previamente, el jesuita Atanasio Kircher, luego de haber sido testigo de erupciones del Etna y el Estrómboli y haber visitado el cráter del monte Vesubio, publicó su propuesta de que el planeta Tierra tenía un fuego central conectado a numerosos otros causados por la combustión de azufre, betún y carbón. Varias explicaciones fueron propuestas para explicar el comportamiento de los volcanes antes de que el entendimiento moderno de la estructura de la tierra se desarrollara. La acción volcánica solía atribuirse a reacciones químicas y a la delgada capa de piedra fundida cerca de la superficie. Volcanes activos en América del Sur Argentina Artículo principal: Anexo:Volcanes de Argentina Numerosos volcanes se distribuyen a lo largo del territorio de la República Argentina. Algunos volcanes se encuentran definitivamente extintos y otros activos, aunque la proporción va a depender de la definición de activo y extinguido; aquí se consideran activos los que han tenido erupciones probables o verificadas en los últimos 10 000 años. Los volcanes de Argentina son variados tanto en forma como en emplazamiento tectónico. La mayoría de los volcanes argentinos pertenecen al Cinturón volcánico de los Andes, aunque hay grandes y voluminosos volcanes de retroarco. Dada la naturaleza del vulcanismo, es imposible establecer un número exacto de volcanes. Cabe destacar que Argentina junto con Chile acogen al volcán más alto del mundo: Nevado Ojos del Salado. Bolivia Artículo principal: Anexo:Volcanes de Bolivia Bolivia acoge numerosos volcanes activos y extinguidos a través de su territorio. Los volcanes activos se encuentran en el oeste de Bolivia. Nevado Sajama (del aimara: chak xaña ‘oeste’) es un estratovolcán en Bolivia, ubicado en el parque nacional Sajama al oeste del país en el departamento de Oruro. No se sabe con certeza la fecha de su última erupción. Sin embargo, se le considera un volcán extinto. El volcán Ollagüe es un volcán activo situado en la frontera de Bolivia y Chile, en la región de Antofagasta en Chile y el Departamento de Potosí en Bolivia, en la cordillera de los Andes, con una altura de 5870 metros. Acotango es un estratovolcán ubicado en la frontera de Bolivia y Chile, entre el departamento de Oruro y la región de Arica y Parinacota. Su zona de influencia directa está protegida por el parque nacional Lauca, por el lado chileno, y el parque nacional Sajama, por el lado boliviano. Chile El volcán Villarrica es el más activo de Sudamérica, ha presentado alta actividad desde el s. VII d. C.. Artículo principal: Anexo:Volcanes de Chile Los volcanes en Chile son supervisados por el Servicio Nacional de Geología y Minería de Chile (SERNAGEOMIN).[11]​[12] Entre las tareas de este organismo están, desde 1974, la publicación de la revista científica Andean Geology —que se llamaba Revista Geológica de Chile hasta 2009—,[13] y visualizar el Sistema de Información de Geología de Exploración (SIGEX) —que reúne información sistematizada de los proyectos de exploración en Chile y los antecedentes técnicos y administrativos, entre otros—. La información fue obtenida de sitios web y otras fuentes públicas. De este modo, SERNAGEOMIN contribuye a consolidar el conocimiento geológico-minero del país (Art. 21 del Código de Minería de 1988). Según la Red Nacional de Vigilancia Volcánica del Servicio Nacional de Geología y Minería de Chile, el país posee 90 volcanes considerados como «activos»,[14] de entre los cuales destacan: Nevados de Chaitén, Villarrica, Planchón Peteroa y el complejo volcánico Laguna del Maule, este último compartido con Argentina. Colombia Artículo principal: Anexo:Volcanes de Colombia El Nevado del Ruiz: De acuerdo con el Servicio Geológico Colombiano este volcán presenta actividad sísmica regular, así como emisiones de ceniza. Su altura es de 5364 m y se encuentra en la zona cafetera del país. En noviembre de 1985 tuvo una erupción donde fallecieron más de 25 000 habitantes de la población de Armero. El volcán Galeras: Se ubica en el departamento de Nariño y está considerado como el volcán más activo de Colombia. En 1993 unos turistas y un grupo de científicos que se encontraban dentro de su cráter murieron, luego de una erupción. Durante los últimos años ha mantenido una actividad constante, con explosiones pequeñas y expulsión de ceniza y humo ocasional. Ecuador Artículo principal: Anexo:Volcanes de Ecuador Los volcanes activos del Ecuador continental pertenecen a la Zona Volcánica Norte (ZVN) de los Andes, la cual es parte del Cinturón Volcánico de los Andes. La Escuela Politécnica Nacional, también conocida como EPN, es una universidad pública, ubicada en Quito, Ecuador. El Instituto Geofísico dirige en los países volcanes en las montañas de los Andes de Ecuador y en las Islas Galápagos. El Instituto Geofísico EPN dirige desde 1999.[15]​[16]​[17] El Instituto Geofísico de la Escuela Politécnica Nacional (IGEPN) reportó un rápido aumento en la actividad sísmica, el número de explosiones y una nube de cenizas que alcanzó los 2 km (1.2 millas) de altura, llegando la nube de ceniza a la ciudad de Guayaquil. El 26 de abril de 2011 hubo otra erupción de proporciones considerables, lanzando una columna de ceniza que ascendió hasta los 12 km de altura.[18]​[19]​ El Instituto Geofísico E.P.N dispone de equipos internacionales de Sismología y Vulcanología y dirige volcanes en las islas Galápagos. En agosto de 2015, el Volcán Cotopaxi experimentó un incremento significativo de su actividad, motivando incluso la declaración de un estado de excepción[20] en el territorio nacional. Actualmente se encuentra bajo vigilancia constante por parte del Instituto Geofísico de la EPN.[21]​[22]​[23] El 25 de mayo de 2015, Isla Wolf (Galápagos) tuvo una erupción volcánica y ahora está siendo dirigida por el Instituto Geofísico de la Escuela Politécnica Nacional[24]​[25]​[26]​[27] En un informe que detalla la erupción, los investigadores del Instituto Geofísico de Ecuador EPN declararon que la columna de humo alcanzó una altitud de 15 kilómetros aproximadamente. Perú Artículo principal: Anexo:Volcanes del Perú El volcán Ubinas, es el volcán más activo del Perú, ha registrado más de 25 erupciones en los últimos 500 años. El Perú está situado en el cinturón de Fuego del Pacífico, región del planeta que se caracteriza por su gran actividad sísmica y volcánica. Como resultado de ello, el sur del Perú está atravesado por más de 400 volcanes que componen el llamado Arco volcánico del Perú y que forman parte de la Zona Volcánica Central de los Andes (ZVC).[28]​ El Perú cuenta con dos centros de monitoreo volcánico ubicados en la ciudad de Arequipa, los denominados Observatorio Vulcanológico del Sur del Instituto Geofísico del Perú (OVS-IGP) y el Observatorio Vulcanológico del INGEMMET (OVI), que mancomunadamente se han centrado en el objetivo de vigilar permanentemente los 16 volcanes activos y potencialmente activos (Sabancaya, Misti, Ubinas, Coropuna, Tutupaca, Huaynaputina, Ticsani, Chachani, Yucamani, Sara Sara, Ampato, Casiri, Purupuruni, Auquihuato y el Valle de los Volcanes en Andahua y Huambo),[29] para ello cuentan con redes de vigilancia multiparamétricas que proporcionan información valiosa sobre el estado y niveles de actividad de los volcanes a su cargo. El Ubinas es considerado como el volcán más activo del Perú por sus 25 eventos de alta actividad fumarólica y actividad explosiva moderada registrada desde el año de 1550.[29] Está situado en el distrito de Ubinas, departamento de Moquegua. Culmina a 5672 ms y cubre una superficie de 45 km². La más reciente erupción tuvo lugar entre marzo de 2006 a junio de 2009, afectado fuertemente la actividad agrícola en el valle de Ubinas. El inicio de esta crisis eruptiva se presentó dominado por una actividad freática y luego, a partir del 19 de abril de 2006, la actividad deviene en magmática de tipo vulcaniano con emisión de material andesítico básico. Posteriormente, y luego de cuatro años de inactividad, en septiembre de 2013 el volcán Ubinas entró en un nuevo proceso eruptivo, el cual se fue acelerando en febrero de 2014 al tiempo que se registraba Tremores sísmicos de gran energía, eventos de tipo Híbrido, así como emisiones persistentes de gases y ceniza, etc. Finalmente, esta alta actividad sísmica y fumarólica culminó con la ocurrencia de la primera explosión magmática el día 14 de febrero de 2014. A partir de entonces y hasta el presente, la actividad eruptiva del volcán Ubinas ha continuado de manera intermitente."


###############################################################################################


ksampletext_wikipedia_bacilo: str = "Bacilo. En bacteriología: la palabra bacilo se usa para describir cualquier bacteria con forma de barra o vara, y pueden encontrarse en muchos grupos taxonómicos diferentes tipos de bacterias. Sin embargo el nombre Bacillus, se refiere a un género específico de bacteria. El otro nombre Bacilli; hace referencia a una clase de bacilos que incluyen dos órdenes, uno de los cuales contiene al género Bacillus. Los bacilos son bacterias que se encuentran en diferentes ambientes y solo se pueden observar con un microscopio. Los bacilos suelen dividirse en el mismo plano y son solitarios, pero pueden combinarse para formar diplobacilos, estreptobacilos y cocobacilos: Diplobacilos: Dos bacilos dispuestos uno al lado del otro. Estreptobacilos: Bacilos dispuestos en cadenas. Cocobacilos: Ovalados y en forma de bastoncillo. Por tipo de bacteria los bacilos pueden ser: Bacilos Gram positivos: fijan el cristal violeta (tinción de Gram) en la pared celular porque tienen una gruesa capa de peptidoglucano. Bacilos Gram negativos: no fijan el cristal violeta y se tiñen con el colorante de contraste usado en la tinción de Gram que es la safranina, debido a que tienen una fina capa de péptidoglucano en medio de dos bicapas lipídicas en la cual se encuentran los lipopolisacáridos o también llamados endotoxinas (principalmente en la membrana externa). Aunque muchos bacilos son patógenos para el ser humano, algunos no hacen daño, pues producen algunos productos lácteos como el yogur (lactobacilos). A lo largo de la historia de la medicina y de la microbiología, varias de estas bacterias han producido enfermedad en los humanos y por lo general se han adoptado el nombre del científico que los descubría, por ejemplo: Bacilo de Aertrycke: Salmonela. Bacilo de Bang: Brucella abortus. Bacilo de Ducrey: Haemophilus ducreyi. Bacilo de Eberth: Salmonella typhi. Bacilo de Nicolaier: Tétano. Bacilo de Hansen: Mycobacterium leprae. Bacilo de Klebs-Löffler: Corynebacterium diphtheriae. Bacilo de Koch: Mycobacterium tuberculosis. Bacilo de Morex: Género Moraxella. Bacilo de Yersin: Yersinia pestis."

