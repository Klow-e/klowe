

# klowe/sampletexts.py


###############################################################################################


def KSampleTexts():
    """
    Prints every variable that starts with 'ksampletext_', that's to say, every sample text in KlowE.
    """
    for i in dir(): print(i) if i.startswith("ksampletext_") else None


###############################################################################################


ksampletext_wikipedia_math_matematicas: str = "Matemáticas. Las matemáticas o, también, la matemática (del latín mathematĭca, y a la vez del griego, transliterado como, derivado de μάθημα, tr. máthēma ‘lo que se comprende’) es una ciencia formal que estudia los patrones, propiedades, estructuras y relaciones presentes en sistemas lógicos y abstractos creados por los humanos, conceptos tales como cantidad, forma, espacio y número se podrían considerar como el objeto de estudio de la matemática.[5]​[6]​[7]​[8]​[9]​ Descripción Las ciencias naturales han hecho un uso extensivo de la matemática para explicar diversos fenómenos observables, tal como lo expresó Eugene Paul Wigner (Premio Nobel de Física en 1963): «El primer punto es que la enorme utilidad de las matemáticas en las ciencias naturales es algo que roza lo misterioso y que no tiene una explicación racional. En segundo lugar, es precisamente esta extraña utilidad de los conceptos matemáticos lo que plantea la cuestión de la unicidad de nuestras teorías físicas.» [10]​ «El milagro de la adecuación del lenguaje de las matemáticas para la formulación de las leyes de la física es un don maravilloso que no comprendemos ni merecemos.» [11]​ Galileo Galilei, en la misma línea, lo había expresado así: «La filosofía está escrita en este enorme libro, que está continuamente abierto ante nuestros ojos (digo en el nuevo idioma), pero uno no puede entenderlo primero, uno no aprende a entender el idioma y a conocer los caracteres en que está escrito. Está escrito en lenguaje matemático, y los caracteres son triángulos, círculos y otras figuras geométricas, sin las cuales es imposible entender una palabra; sin éstos es un vano vagar por un oscuro laberinto.» [12]​ Mediante la abstracción y el uso de la lógica en el razonamiento, la matemática ha evolucionado basándose en el cálculo y las mediciones, junto con el estudio sistemático de la forma y el movimiento de los objetos físicos.[13] Las matemáticas, desde sus comienzos, han tenido un fin práctico. Las explicaciones que se apoyaban en la lógica aparecieron por primera vez con la matemática helénica, especialmente con los Elementos de Euclides.[14] La matemática siguió desarrollándose, con continuas interrupciones, hasta que en el Renacimiento las innovaciones matemáticas interactuaron con los nuevos descubrimientos científicos. Como consecuencia, hubo una aceleración en la investigación que continúa hasta la actualidad. Hoy día, la matemática se usa en todo el mundo como una herramienta esencial en muchos campos, entre los que se encuentran las ciencias naturales,[15] las ciencias aplicadas, las humanidades,[16]​[17]​[18] la medicina[19] y las ciencias sociales,[20]​[21]​[22] e incluso disciplinas que, aparentemente, no están vinculadas con ella, como la música[23] (por ejemplo, en cuestiones de resonancia armónica, Cuerda vibrante,[24]​[25] etc.) y la literatura.[26]​[27] Las matemáticas aplicadas, rama de la matemática destinada a la aplicación del conocimiento matemático a otros ámbitos, inspiran y hacen uso de los nuevos descubrimientos matemáticos y, en ocasiones, conducen al desarrollo de nuevas disciplinas. Los matemáticos[28] también participan en la matemática pura, sin tener en cuenta sus aplicaciones, aunque estas suelen ser descubiertas con el paso del tiempo. Historia Artículo principal: Historia de las matemáticas Las matemáticas son una de las ciencias más antiguas. Floreció primero antes de la antigüedad en Mesopotamia,[29] en cuanto a la geometría[30] India y China, y más tarde en la antigüedad en Grecia y el helenismo. De ahí data la orientación hacia la tarea de «demostración puramente lógica» y la primera axiomatización, a saber, la geometría euclidiana.[30] En la Edad Media sobrevivió de forma independiente en el primer humanismo de las universidades y en el mundo árabe. A principios de la era moderna, François Viète introdujo variables y René Descartes inauguró un enfoque computacional de la geometría[31]​[32]​[33] mediante el uso de coordenadas. La consideración de las tasas de cambio (fluxión)[34] así como la descripción de las tangentes y la determinación de los contenidos de las superficies (cuadratura)[35] condujeron al cálculo infinitesimal[13] de Gottfried Wilhelm Leibniz e Isaac Newton.[36] La mecánica de Newton y su ley de la gravitación fueron también una fuente de orientación de problemas matemáticos como el problema de los tres cuerpos[37]​[38]​[39] en los siglos siguientes. Otro de los principales problemas de la primera época moderna fue la solución de ecuaciones algebraicas cada vez más complicadas. Para hacer frente a esto, Niels Henrik Abel y Évariste Galois desarrollaron el concepto de grupo, que describe las relaciones entre las simetrías de un objeto.[40]​[41] El álgebra más reciente y, en particular, la geometría algebraica pueden considerarse como una profundización de estas investigaciones. Una idea entonces nueva en el intercambio de cartas entre Blaise Pascal y Pierre de Fermat en 1654 acerca del problema de los juegos de azar,[42]​[43]​[44] aunque existían otras soluciones discutibles como las de Cardano, quien intentó matematizarlas. Pierre-Simon Laplace hace un recuento de los diferentes logros hasta 1812 cuando publica su Ensayo filosófico sobre las posibilidades.[45] Las nuevas ideas y métodos conquistaron muchos campos. Pero durante siglos, la teoría clásica de la probabilidad se dividió en escuelas separadas. Los intentos de definir explícitamente el término «probabilidad» solo tuvieron éxito para casos especiales. Solo la publicación del libro de texto de Andrei Kolmogorov en 1933 Los fundamentos de la Teoría de la Probabilidad [46] completó el desarrollo de los fundamentos de la teoría moderna de la probabilidad. En el transcurso del siglo XIX, el cálculo infinitesimal[13] encontró su forma actual de rigor gracias a los trabajos de Augustin-Louis Cauchy y Karl Weierstrass. La teoría de conjuntos[47] desarrollada por Georg Cantor hacia finales del siglo XIX es también indispensable en la matemática actual, aunque las paradojas del concepto ingenuo de conjuntos dejaron claro, en un primer momento, la incierta base sobre la que se asentaban las matemáticas.[48]​ El desarrollo de la primera mitad del siglo XX estuvo influenciado por la publicación de los problemas de Hilbert. Uno de los problemas intentaba axiomatizar completamente las matemáticas; al mismo tiempo, se hicieron grandes esfuerzos de abstracción, es decir, el intento de reducir los objetos a sus propiedades esenciales. Así, Emmy Noether desarrolló los fundamentos del álgebra moderna,[49] Felix Hausdorff desarrolló la topología general como el estudio de los espacios topológicos, Stefan Banach desarrolló probablemente el concepto más importante del análisis funcional, el espacio de Banach que lleva su nombre. Un nivel de abstracción aún mayor, un marco común para la consideración de construcciones similares de diferentes áreas de las matemáticas, fue finalmente creado por la introducción de la teoría de categorías por Samuel Eilenberg y Saunders Mac Lane. Introducción Etimología La palabra «matemática» (del griego μαθηματικά mathēmatiká, «cosas que se aprenden») viene del griego antiguo μάθημα (máthēma), que quiere decir «campo de estudio o instrucción». Las matemáticas requieren un esfuerzo de instrucción o aprendizaje, refiriéndose a áreas del conocimiento que solo pueden entenderse tras haber sido instruido en las mismas, como la astronomía. «El arte matemática» (μαθηματική τέχνη, mathēmatikḗ tékhnē) se contrapondría en esto a la música, «el arte de las musas» (μουσική τέχνη, mousikē téchnē), que sería un arte, como la poesía, retórica[50]​[51] y similares, que se puede apreciar directamente, «que se puede entender sin haber sido instruido».[52] Aunque el término ya era usado por los pitagóricos (matematikoi) en el siglo VI a. C., alcanzó su significado más técnico y reducido de «estudio matemático» en los tiempos de Aristóteles (siglo IV a. C.). Su adjetivo es μαθηματικός (mathēmatikós), «relacionado con el aprendizaje», lo cual, de manera similar, vino a significar «matemático». En particular, μαθηματική τέχνη (mathēmatikḗ tékhnē; en latín ars mathematica), significa «el arte matemática». La forma más usada es el plural matemáticas (cuyo acortamiento, en algunos países, es «mates»[53]​[54]​), que tiene el mismo significado que el singular[2] y viene de la forma latina mathematica (Cicerón), basada en el plural en griego τα μαθηματικά (ta mathēmatiká), usada por Aristóteles y que significa, a grandes rasgos, «todas las cosas matemáticas». Algunos autores, sin embargo, hacen uso de la forma singular del término; tal es el caso de Bourbaki, en el tratado Elementos de matemática (Élements de mathématique, 1940), destaca la uniformidad de este campo aportada por la visión axiomática moderna, aunque también hace uso de la forma plural como en Éléments d'histoire des mathématiques (1969),[55] posiblemente sugiriendo que es Bourbaki quien finalmente realiza la unificación de las matemáticas.[56] Así mismo, en el escrito L'Architecture des mathématiques (1948) plantea el tema en la sección «¿Matemáticas, singular o plural?» donde defiende la unicidad conceptual de la matemática aunque hace uso de la forma plural en dicho escrito.[57]​[58]​[3]​[4]​ Algunas definiciones de matemática Establecer definiciones claras y precisas es el fundamento de la matemática, aunque encontrar una definición única para ella es improbable.[59] Se muestran algunas reflexiones de reconocidos autores: René Descartes: «Y considerando esto más atentamente al cabo se nota que solamente aquellas en las que se estudia cierto orden y medida hacen referencia a la Mathesis,[60] y que no importa si tal medida ha de buscarse en los números, en las figuras, en los astros, en los sonidos o en cualquier otro objeto;» [61]​ Carl Friedrich Gauss: «El matemático se abstrae totalmente de la naturaleza de los objetos y el contenido de sus relaciones; se preocupa únicamente por la enumeración y la comparación de las relaciones entre ellos [...]» [62]​[63]​ David Hilbert: «[...] nos lleva a una concepción de las matemáticas que considera a éstas como un inventario de fórmulas a las que corresponden, en primer lugar, expresiones concretas de enunciados finitistas y a las que se añaden, en segundo lugar, otras fórmulas que carecen de todo significado y que constituyen los objetos ideales de nuestra teoría.» [64]​[63]​ Benjamin Peirce: «La matemática es la ciencia que extrae conclusiones necesarias.» [65]​ Bertrand Russell: Trató de probar[66]​[67]​ «que toda la Matemática pura trabaja exclusivamente con conceptos definibles en función de un número muy pequeño de conceptos lógicos fundamentales, y de que todas las proposiciones se pueden deducir de un número muy pequeño de principios lógicos fundamentales.» [68]​ John David Barrow: «En el fondo, matemáticas es el nombre que le damos al conjunto de todos los patrones e interrelaciones posibles. Algunos de esos patrones están entre formas, otros están en secuencias de números, mientras que otros son relaciones más abstractas entre estructuras. La esencia de las matemáticas radica en las relaciones entre cantidades y cualidades. Por lo tanto, son las relaciones entre los números, no los números en sí mismos, las que constituyen el foco de interés de los matemáticos modernos.» [5]​ Epistemología y controversia sobre la matemática como ciencia El carácter epistemológico y científico de la matemática ha sido ampliamente discutido. En la práctica, la matemática se emplea para estudiar relaciones cuantitativas, estructuras, relaciones geométricas y las magnitudes variables. Los matemáticos buscan patrones,[6]​[7]​[9]​[69] formulan nuevas conjeturas e intentan alcanzar la verdad matemática mediante deducciones rigurosas. Estas les permiten establecer los axiomas y las definiciones apropiados para dicho fin.[59]​[70] Algunas definiciones clásicas restringen las matemáticas al razonamiento sobre cantidades,[71] aunque solo una parte de la matemática actual usa números,[72] predominando el análisis lógico de construcciones abstractas no cuantitativas. Existe cierta discusión acerca de si los objetos matemáticos, como los números[73] y puntos, realmente existen o simplemente provienen de la imaginación humana. El matemático Benjamin Peirce definió las matemáticas como «la ciencia que señala las conclusiones necesarias».[65] Por otro lado: «cuando las leyes de la matemática se refieren a la realidad, no son exactas; cuando son exactas, no se refieren a la realidad».[74]​ Albert Einstein Se ha discutido el carácter científico de las matemáticas debido a que sus procedimientos y resultados poseen una firmeza e inevitabilidad inexistentes en otras disciplinas como pueden ser la física, la química o la biología. Así, la matemática sería tautológica, infalible y a priori, mientras que otras, como la geología o la fisiología, serían falibles y a posteriori. Son estas características lo que hace dudar de colocarse en el mismo rango que las disciplinas antes citadas pese a las afirmaciones como las de John Stuart Mill quien sostenía en 1843: «En realidad, las leyes de los números son verdades físicas provenientes de la observación.»[75]​ Así, los matemáticos pueden descubrir nuevos procedimientos para resolver integrales o teoremas, pero se muestran incapaces de descubrir un suceso que ponga en duda el Teorema de Pitágoras[76]​[77] o cualquier otro, como sí sucede constantemente con las ciencias de la naturaleza.[78]​ El teorema de Pitágoras es uno de los enunciados más conocidos y antiguos de las matemáticas.[76]​ Un ábaco, instrumento para efectuar operaciones aritméticas sencillas (sumas, restas y también multiplicaciones), fue muy utilizado en otros tiempos. La matemática puede ser entendida como ciencia; si es así debiera señalarse su objeto y su método. Sin embargo, algunos plantean que la matemática es un lenguaje formal, seguro, eficiente, aplicable al entendimiento de la naturaleza, tal como indicó Galileo; además muchos fenómenos de carácter social, otros de carácter biológico[79] o geológico, pueden ser estudiados mediante la aplicación de ecuaciones diferenciales,[80]​[81] cálculo de probabilidades o teoría de conjunto.[47] Precisamente, el avance de la física y de la química ha exigido la invención de nuevos conceptos, instrumentos y métodos en la matemática, sobre todo en el análisis real, análisis complejo y el análisis matricial.[82]​ Aspectos formales, metodológicos y estéticos La inspiración, las matemáticas puras, aplicadas y la estética Isaac Newton (1643-1727), comparte con Leibniz la autoría del desarrollo del cálculo integral y diferencial.[36]​ Es muy posible que el arte de calcular[83]​[84]​[85] haya sido desarrollado antes incluso que la escritura,[86]​[87] relacionado fundamentalmente con la contabilidad y la administración de bienes, el comercio, en la agrimensura y, posteriormente, en la astronomía. Actualmente, todas las ciencias aportan problemas que son estudiados por matemáticos, al mismo tiempo que aparecen nuevos problemas dentro de las propias matemáticas. Por ejemplo, el físico Richard Feynman propuso la integral de caminos como fundamento de la mecánica cuántica, combinando el razonamiento matemático y el enfoque de la física, pero todavía, no se ha logrado una definición plenamente satisfactoria en términos matemáticos. Igualmente, la teoría de cuerdas, una teoría científica en desarrollo que trata de unificar las cuatro fuerzas fundamentales de la física, sigue inspirando a las más modernas matemáticas.[88]​ Algunas matemáticas solo son relevantes en el área en la que estaban inspiradas y son aplicadas para otros problemas en ese campo. Sin embargo, a menudo las matemáticas inspiradas en un área concreta resultan útiles en muchos ámbitos, y se incluyen dentro de los conceptos matemáticos generales aceptados. El notable hecho de que incluso la matemática más pura habitualmente tiene aplicaciones prácticas es lo que Eugene Paul Wigner ha definido como «la irrazonable eficacia de las matemáticas en las Ciencias Naturales».[89]​[15]​ Como en la mayoría de las áreas de estudio, la explosión de los conocimientos en la era científica ha llevado a la especialización de las matemáticas. Hay una importante distinción entre las matemáticas puras y las matemáticas aplicadas. La mayoría de los matemáticos que se dedican a la investigación se centran únicamente en una de estas áreas y, a veces, la elección se realiza cuando comienzan su licenciatura. Varias áreas de las matemáticas aplicadas se han fusionado con otras áreas tradicionalmente fuera de las matemáticas y se han convertido en disciplinas independientes, como pueden ser la estadística, la investigación de operaciones o la informática. Aquellos que sienten predilección por las matemáticas, consideran que prevalece un aspecto estético que define a la mayoría de las matemáticas. Muchos matemáticos hablan de la elegancia de la matemática, su intrínseca estética y su belleza interna. En general, uno de sus aspectos más valorados es la simplicidad. Hay belleza en una simple y contundente demostración, como la demostración de Euclides[14] de la existencia de infinitos números primos, y en un elegante análisis numérico que acelera el cálculo, así como en la transformada rápida de Fourier. Godfrey Harold Hardy en A Mathematician's Apology [90] (Apología de un matemático) expresó la convicción de que estas consideraciones estéticas son, en sí mismas, suficientes para justificar el estudio de las matemáticas puras. Los matemáticos con frecuencia se esfuerzan por encontrar demostraciones de los teoremas que son especialmente elegantes, el excéntrico matemático Paul Erdős se refiere a este hecho como la búsqueda de pruebas de El Libro en el que Dios ha escrito sus demostraciones favoritas.[91]​[92] La popularidad de la matemática recreativa[93]​[94]​[95]​[96] es otra señal que nos indica el placer que produce resolver las preguntas matemáticas. Notación, lenguaje y rigor Artículo principal: Notación matemática Leonhard Euler. Probablemente el más prolífico matemático de todos los tiempos. La mayor parte de la notación[97] matemática que se utiliza hoy en día no se inventó hasta el siglo XVIII.[98]​[99] Antes de eso, las matemáticas eran escritas con palabras, un minucioso proceso que limitaba el avance matemático. En el siglo XVIII, Euler, fue responsable de muchas de las notaciones empleadas en la actualidad. La notación[97] moderna hace que las matemáticas sean mucho más fácil para los profesionales, pero para los principiantes resulta complicada. La notación reduce las matemáticas al máximo, hace que algunos símbolos[99] contengan una gran cantidad de información. Al igual que la notación musical, la notación matemática moderna tiene una sintaxis estricta y codifica la información que sería difícil de escribir de otra manera. El símbolo de infinito en diferentes tipografías. El lenguaje matemático también puede ser difícil para los principiantes. Palabras tales como o y solo si tienen significados más precisos que en lenguaje cotidiano. Además, palabras como abierto y cuerpo tienen significados matemáticos muy concretos. La jerga matemática, o lenguaje matemático, incluye términos técnicos como homeomorfismo o integrabilidad. La razón que explica la necesidad de utilizar la notación y la jerga es que el lenguaje matemático requiere más precisión que el lenguaje cotidiano. Los matemáticos se refieren a esta precisión en el lenguaje y en la lógica como el «rigor». El rigor es una condición indispensable que debe tener una demostración matemática. Los matemáticos quieren que sus teoremas a partir de los axiomas sigan un razonamiento sistemático. Esto sirve para evitar teoremas erróneos, basados en intuiciones falibles, que se han dado varias veces en la historia de esta ciencia.[100] El nivel de rigor previsto en las matemáticas ha variado con el tiempo: los griegos buscaban argumentos detallados, pero en tiempos de Isaac Newton los métodos empleados eran menos rigurosos. Los problemas inherentes de las definiciones que Newton utilizaba dieron lugar a un resurgimiento de un análisis cuidadoso y a las demostraciones oficiales del siglo XIX. Ahora, los matemáticos continúan apoyándose entre ellos mediante demostraciones asistidas por ordenador.[101]​ Un axioma se interpreta tradicionalmente como una «verdad evidente», pero esta concepción es problemática. En el ámbito formal, un axioma no es más que una cadena de símbolos, que tiene un significado intrínseco solo en el contexto de todas las fórmulas derivadas de un sistema axiomático. La matemática como ciencia Carl Friedrich Gauss, apodado el «príncipe de los matemáticos», se refería a la matemática como «la reina de las ciencias». Carl Friedrich Gauss se refería a la matemática como «la reina de las ciencias».[102] Tanto en el latín original Scientiārum Regīna, así como en alemán Königin der Wissenschaften, la palabra ciencia debe ser interpretada como (campo de) conocimiento. Si se considera que la ciencia es el estudio del mundo físico, entonces las matemáticas, o por lo menos las matemáticas puras, no son una ciencia. Muchos filósofos creen que las matemáticas no son experimentalmente falsables y, por ende, no son una ciencia según la definición de Karl Popper.[103] No obstante, en la década de 1930 una importante labor en la lógica matemática demuestra que las matemáticas no pueden reducirse a la lógica[104] y Karl Popper llegó a la conclusión de que «la mayoría de las teorías matemáticas son, como las de física y biología, hipotético-deductivas. Por lo tanto, las matemáticas puras se han vuelto más cercanas a las ciencias naturales[15] cuyas hipótesis son conjeturas, así ha sido hasta ahora».[105] Otros pensadores, en particular Imre Lakatos, han solicitado una versión de Falsacionismo[106]​[107] para las propias matemáticas.[108]​ Una visión alternativa es que determinados campos científicos (como la física teórica) son matemáticas con axiomas que pretenden corresponder a la realidad. De hecho, el físico teórico, John Michael Ziman, propone que la ciencia es «conocimiento público» y, por tanto, incluye a las matemáticas.[109] En cualquier caso, las matemáticas tienen mucho en común con distintos campos de las ciencias físicas, especialmente la exploración de las consecuencias lógicas de las hipótesis. La intuición[110] y la experimentación también desempeñan un papel importante en la formulación de conjeturas tanto en las matemáticas como en las otras ciencias. Las matemáticas experimentales siguen ganando representación dentro de las matemáticas. El cálculo[13] y simulación[111] están jugando un papel cada vez mayor tanto en las ciencias como en las matemáticas, atenuando la objeción de que las matemáticas no se sirven del método científico. En 2002 Stephen Wolfram propuso, en su libro[112] Un nuevo tipo de ciencia, que la matemática computacional merece ser explorada empíricamente como un campo científico. Las opiniones de los matemáticos sobre este asunto son muy variadas. Muchos matemáticos consideran que llamar a su campo ciencia es minimizar la importancia de su perfil estético, además supone negar su historia dentro de las siete artes liberales. Otros consideran que hacer caso omiso de su conexión con las ciencias supone ignorar la evidente conexión entre las matemáticas y sus aplicaciones en la ciencia y la ingeniería, que ha impulsado considerablemente el desarrollo de las matemáticas. Otro asunto de debate, que guarda cierta relación con el anterior, es si la matemática fue creada (como el arte) o descubierta (como la ciencia). Este es uno de los muchos temas de incumbencia de la filosofía de las matemáticas. Los premios matemáticos se mantienen generalmente separados de sus equivalentes en la ciencia. El más prestigioso premio dentro de las matemáticas es la Medalla Fields,[113] fue instaurado en 1936 y se concede cada cuatro años. A menudo se le considera el equivalente del Premio Nobel para la ciencia. Otros premios son el Premio Wolf en matemática, creado en 1978, que reconoce los logros en vida de los matemáticos, y el Premio Abel, otro gran premio internacional, que se introdujo en 2003. Estos dos últimos se conceden por un excelente trabajo, que puede ser una investigación innovadora o la solución de un problema pendiente en un campo determinado. Una famosa lista de esos 23 problemas sin resolver,[114] denominada los «Problemas de Hilbert», fue recopilada en 1900 por el matemático alemán David Hilbert. Esta lista ha alcanzado gran popularidad entre los matemáticos y, al menos, nueve de los problemas ya han sido resueltos. Una nueva lista de siete problemas fundamentales, titulada «Problemas del milenio», se publicó en 2000. La solución de cada uno de los problemas será recompensada con 1 millón de dólares. Curiosamente, tan solo uno (la hipótesis de Riemann) aparece en ambas listas. Ramas de estudio de las matemáticas Artículo principal: Áreas de las matemáticas La Sociedad Matemática Americana distingue unas 5.000 ramas distintas de matemática.[115] En una subdivisión escolarizada de la matemática se distinguen cinco áreas de estudio básicas: la cantidad, la estructura, el espacio, el cambio y la variabilidad que se corresponden con la aritmética, el álgebra, la geometría, el cálculo, la probabilidad y estadística. Como señalaba Richard Courant[116] «Es posible seguir una ruta directa a partir de los elementos fundamentales hasta puntos avanzados» para que puedan divisarse las directrices de la matemática como ciencia. Además, hay ramas de las matemáticas conectadas a otros campos, por ejemplo la lógica, teoría de conjuntos y las matemáticas aplicadas entre muchas otras tal como indica la Sociedad Matemática Americana.[115]​ Véase también: Categoría:Áreas de las matemáticas Matemática pura Artículo principal: Matemáticas puras Cantidad Números naturales Enteros Números racionales Números reales Números complejos Estructura Combinatoria Teoría de números Teoría de grupos Teoría de grafos Teoría del orden Álgebra Espacio Geometría Trigonometría Geometría diferencial Topología Geometría fractal Teoría de la medida Cambio Cálculo Cálculo vectorial Ecuaciones diferenciales Sistemas dinámicos Teoría del caos Análisis complejo Matemática aplicada Artículo principal: Matemáticas aplicadas El concepto «matemática aplicada» se refiere a aquellos métodos y herramientas matemáticas que pueden ser utilizados en el análisis o resolución de problemas pertenecientes al área de las ciencias básicas o aplicadas. Muchos métodos matemáticos han resultado efectivos en el estudio de problemas en física, química, biología,[15] medicina,[117] ciencias sociales,[22] ingeniería, economía,[118] finanzas, ecología entre otras. Sin embargo, una posible diferencia es que en matemática aplicada se procura el desarrollo de la matemática «hacia afuera», es decir su aplicación o transferencia hacia el resto de las áreas. Y en menor grado «hacia dentro» o sea, hacia el desarrollo de la matemática misma. Este último sería el caso de la matemática pura o matemática elemental. La matemática aplicada se usa con frecuencia en distintas áreas tecnológicas para modelado,[119]​[120] simulación[111] y optimización de procesos o fenómenos,[121] como el túnel de viento o el diseño de experimentos. Estadística y ciencias de la decisión La estadística es la rama de la matemática que estudia la variabilidad, así como el proceso aleatorio que la genera siguiendo leyes de probabilidad.[122] Es un conocimiento fundamental para la investigación científica en algunos campos de la tecnología, como informática e ingeniería, y de las ciencias fácticas,[123] como economía,[118] genética, sociología,[124] psicología,[125] medicina,[117] contabilidad, etc. En ocasiones, estas áreas de conocimiento necesitan aplicar técnicas estadísticas durante su proceso de investigación factual, con el fin de obtener nuevos conocimientos basados en la experimentación y en la observación, precisando para ello recolectar, organizar, presentar y analizar un conjunto de datos numéricos y, a partir de ellos y de un marco teórico, hacer las inferencias apropiadas.[117]​[125]​[126]​[127]​[128]​ Se consagra en forma directa al gran problema universal de cómo tomar decisiones inteligentes y acertadas en condiciones de incertidumbre. La estadística descriptiva sirve como fuente de instrucción en los niveles básicos de estadística aplicada a las ciencias fácticas[123] y, por tanto, los conceptos manejados y las técnicas empleadas suelen ser presentadas de la forma más simple y clara posibles. Matemática computacional"
ksampletext_wikipedia_math_calculo: str = "Cálculo. En general el término cálculo (del latín calculus, piedrecita, usado para contar o como ayuda al calcular) hace referencia al resultado correspondiente a la acción de calcular. Calcular, por su parte, consiste en realizar las operaciones necesarias para prever el resultado de una acción previamente concebida, o conocer las consecuencias que se pueden derivar de unos datos previamente conocidos. No obstante, el uso más común del término «cálculo» es el lógico-matemático. Desde esta perspectiva, el cálculo consiste en un procedimiento mecánico o algoritmo, mediante el cual podemos conocer las consecuencias que se derivan de las variables previamente conocidas debidamente formalizadas y simbolizadas. Cálculo como razonamiento y cálculo lógico-matemático Ejemplo de aplicación de un cálculo algebraico a la resolución de un problema, según la interpretación de una teoría física. La expresión del cálculo algebraico  Pero si interpretamos  Al mismo tiempo, según dicha teoría, sirve para resolver el problema de calcular cuántos kilómetros ha recorrido un coche que circula de Madrid a Barcelona a una velocidad constante de 60 km/h durante 4 horas de recorrido. 240 kilómetros recorridos = 60 km/h x 4 h Las dos acepciones del cálculo (la general y la restringida) arriba definidas están íntimamente ligadas. El cálculo es una actividad natural y primordial en el hombre, que comienza en el mismo momento en que empieza a relacionar unas cosas con otras en un pensamiento o discurso. El cálculo lógico natural como razonamiento es el primer cálculo elemental del ser humano. El cálculo en sentido lógico-matemático aparece cuando se toma conciencia de esta capacidad de razonar y trata de formalizarse. Por lo tanto, podemos distinguir dos tipos de operaciones: Operaciones orientadas hacia la consecución de un fin, como prever, programar, conjeturar, estimar, precaver, prevenir, proyectar, configurar, etc. que incluyen en cada caso una serie de complejas actividades y habilidades tanto de pensamiento como de conducta. En su conjunto dichas actividades adquieren la forma de argumento o razones que justifican una finalidad práctica o cognoscitiva. Operaciones formales como algoritmo que se aplica bien directamente a los datos conocidos o a los esquemas simbólicos de la interpretación lógico-matemática de dichos datos; las posibles conclusiones, inferencias o deducciones de dicho algoritmo son el resultado de la aplicación de reglas estrictamente establecidas de antemano. Resultado que es: Conclusión de un proceso de razonamiento. Resultado aplicable directamente a los datos iniciales (resolución de problemas). Modelo de relaciones previamente establecido como teoría científica y significativo respecto a determinadas realidades (Creación de modelos científicos). Mero juego formal simbólico de fundamentación, creación y aplicación de las reglas que constituyen el sistema formal del algoritmo (Cálculo lógico-matemático, propiamente dicho). Dada la importancia que históricamente ha adquirido la actividad lógico-matemática en la cultura humana el presente artículo se refiere a este último sentido. De hecho la palabra, en su uso habitual, casi queda restringida a este ámbito de aplicación; para algunos, incluso, queda reducida a un solo tipo de cálculo matemático, pues en algunas universidades se llamaba «Cálculo» a una asignatura específica de cálculo matemático (como puede ser el cálculo infinitesimal, análisis matemático, cálculo diferencial e integral, etc.). En un artículo general sobre el tema no puede desarrollarse el contenido de lo que supone el cálculo lógico-matemático en la actualidad. Aquí se expone solamente el fundamento de sus elementos más simples, teniendo en cuenta que sobre estas estructuras simples se construyen los cálculos más complejos tanto en el aspecto lógico como en el matemático. Historia del cálculo Artículo principal: Historia del cálculo De la Antigüedad Reconstrucción de un ábaco romano. Un ábaco moderno. El término «cálculo» procede del latín calculus, piedrecita que se mete en el calzado y que produce molestia. Precisamente, tales piedrecitas ensartadas en tiras constituían el ábaco romano que, junto con el suanpan chino, constituyen las primeras máquinas de calcular en el sentido de contar. Los antecedentes de procedimiento de cálculo, como algoritmo, se encuentran en los que utilizaron los geómetras griegos, Eudoxo en particular, en el sentido de llegar por aproximación de restos cada vez más pequeños, a una medida de figuras curvas; así como Diofanto precursor del álgebra. Se considera que Arquímedes fue uno de los matemáticos más grandes de la antigüedad y, en general, de toda la historia.[2]​[3] Usó el método exhaustivo para calcular el área bajo el arco de una parábola con el sumatorio de una serie infinita, y dio una aproximación extremadamente precisa del número Pi.[4] También definió la espiral que lleva su nombre, fórmulas para los volúmenes de las superficies de revolución y un ingenioso sistema para expresar números muy largos. La consideración del cálculo como una forma de razonamiento abstracto aplicado en todos los ámbitos del conocimiento se debe a Aristóteles, quien en sus escritos lógicos fue el primero en formalizar y simbolizar los tipos de razonamientos categóricos (silogismos). Este trabajo sería completado más tarde por los estoicos, los megáricos, la Escolástica. Los algoritmos actuales del cálculo aritmético, utilizados universalmente, son fruto de un largo proceso histórico. De vital importancia son las aportaciones de Muhammad ibn al-Juarismi en el siglo IX;[5]​ En el siglo XIII, Fibonacci introduce en Europa la representación de los números arábigos del sistema decimal. Se introdujo el 0, ya de antiguo conocido en la India y se construye definitivamente el sistema decimal de diez cifras con valor posicional. La escritura antigua de números en Babilonia, en Egipto, en Grecia o en Roma, hacía muy difícil un procedimiento mecánico de cálculo.[6]​ El sistema decimal fue muy importante para el desarrollo de la contabilidad de los comerciantes de la Baja Edad Media, en los inicios del capitalismo. El concepto de función por tablas ya era practicado de antiguo pero adquirió especial importancia en la Universidad de Oxford en el siglo XIV.[7] La idea de un lenguaje o algoritmo capaz de determinar todas las verdades, incluidas las de la fe, aparecen en el intento de Raimundo Lulio en su Ars Magna A fin de lograr una operatividad mecánica se confeccionaban unas tablas a partir de las cuales se podía generar un algoritmo prácticamente mecánico. Este sistema de tablas ha perdurado en algunas operaciones durante siglos, como las tablas de logaritmos, o las funciones trigonométricas; las tablas venían a ser como la calculadora de hoy día; un instrumento imprescindible de cálculo. Las amortizaciones de los créditos en los bancos, por ejemplo, se calculaban a partir de tablas elementales hasta que se produjo la aplicación de la informática en el tercer tercio del siglo XX. A finales de la Edad Media la discusión entre los partidarios del ábaco y los partidarios del algoritmo se decantó claramente por estos últimos.[8] De especial importancia es la creación del sistema contable por partida doble recomendado por Luca Pacioli fundamental para el progreso del capitalismo en el Renacimiento.[9]​ Renacimiento El sistema que usamos actualmente fue introducido por Luca Pacioli en 1494, el cual fue creado y desarrollado para responder a la necesidad de la contabilidad en los negocios de la burguesía renacentista. El desarrollo del álgebra (con la introducción de un sistema de símbolos por un lado, y la resolución de problemas por medio de las ecuaciones) vino de la mano de los grandes matemáticos de la época renacentista como Tartaglia, Stevin, Cardano o Vieta y fue esencial para el planteamiento y solución de los más diversos problemas que surgieron en la época, que dieron como consecuencia los grandes descubrimientos que hicieron posible el progreso científico que surgiría en el siglo XVII.[10]​ Siglos XVII y XVIII Página del artículo de Leibniz Explication de l'Arithmétique Binaire, 1703/1705 En el siglo XVII el cálculo conoció un enorme desarrollo siendo los autores más destacados Descartes,[11] Pascal[12] y, finalmente, Leibniz y Newton[13] con el cálculo infinitesimal que en muchas ocasiones ha recibido simplemente, por absorción, el nombre de cálculo. El concepto de cálculo formal en el sentido de algoritmo reglado para el desarrollo de un razonamiento y su aplicación al mundo de lo real,[14] adquiere una importancia y desarrollo enorme respondiendo a una necesidad de establecer relaciones matemáticas entre diversas medidas, esencial para el progreso de la ciencia física que, debido a esto, es tomada como nuevo modelo de Ciencia frente a la especulación tradicional filosófica, por el rigor y seguridad que ofrece el cálculo matemático. Cambia así el sentido tradicional de la Física como filosofía de la naturaleza y toma el sentido de ciencia que estudia los cuerpos materiales, en cuanto materiales. A partir de entonces el propio sistema de cálculo permite establecer modelos sobre la realidad física, cuya comprobación experimental[15] supone la confirmación de la teoría como sistema. Es el momento de la consolidación del llamado método científico cuyo mejor exponente es en aquel momento la Teoría de la Gravitación Universal y las leyes de la Mecánica de Newton.[16]​ Siglos XIX y XX George Boole Durante el siglo XIX y XX el desarrollo científico y la creación de modelos teóricos fundados en sistemas de cálculo aplicables tanto en mecánica como en electromagnetismo y radioactividad, etc., así como en astronomía fue impresionante. Las geometrías no euclidianas encuentran aplicación en modelos teóricos de astronomía y física. El mundo deja de ser un conjunto de infinitas partículas que se mueven en un espacio-tiempo absoluto y se convierte en un espacio de configuración o espacio de fases de  La lógica asimismo sufrió una transformación radical.[17] La formalización simbólica fue capaz de integrar las leyes lógicas en un cálculo matemático, hasta el punto que la distinción entre razonamiento lógico-formal y cálculo matemático viene a considerarse como meramente utilitaria. En la segunda mitad del siglo XIX y primer tercio del XX, a partir del intento de formalización de todo el sistema matemático, Frege, y de matematización de la lógica, (Bolzano, Boole, Whitehead, Russell) fue posible la generalización del concepto como cálculo lógico. Se lograron métodos muy potentes de cálculo, sobre todo a partir de la posibilidad de tratar como «objeto» conjuntos de infinitos elementos, dando lugar a los números transfinitos de Cantor. Mediante el cálculo la lógica encuentra nuevos desarrollos como lógicas modales y lógicas polivalentes. Los intentos de axiomatizar el cálculo como cálculo perfecto por parte de Hilbert y Poincaré, llevaron, como consecuencia de diversas paradojas (Cantor, Russell, etc.) a nuevos intentos de axiomatización, Axiomas de Zermelo-Fraenkel y a la demostración de Gödel de la imposibilidad de un sistema de cálculo perfecto: consistente, decidible y completo en 1931, de grandes implicaciones lógicas, matemáticas y científicas. Actualidad En la actualidad, el cálculo en su sentido más general, en tanto que cálculo lógico interpretado matemáticamente como sistema binario, y físicamente hecho material mediante la lógica de circuitos electrónicos, ha adquirido una dimensión y desarrollo impresionante por la potencia de cálculo conseguida por los ordenadores, propiamente máquinas computadoras. La capacidad y velocidad de cálculo de estas máquinas hace lo que humanamente sería imposible: millones de operaciones por segundo. El cálculo así utilizado se convierte en un instrumento fundamental de la investigación científica por las posibilidades que ofrece para la modelización de las teorías científicas, adquiriendo especial relevancia en ello el cálculo numérico. Cálculo infinitesimal: breve reseña Artículo principal: Cálculo infinitesimal El cálculo infinitesimal, llamado por brevedad «cálculo», tiene su origen en la antigua geometría griega. Demócrito calculó el volumen de pirámides y conos considerándolos formados por un número infinito de secciones de grosor infinitesimal (infinitamente pequeño). Eudoxo y Arquímedes utilizaron el «método de agotamiento» o exhaución para encontrar el área de un círculo con la exactitud finita requerida mediante el uso de polígonos regulares inscritos de cada vez mayor número de lados. En el periodo tardío de Grecia, el neoplatónico Pappus de Alejandría hizo contribuciones sobresalientes en este ámbito. Sin embargo, las dificultades para trabajar con números irracionales y las paradojas de Zenón de Elea impidieron formular una teoría sistemática del cálculo en el periodo antiguo. En el siglo XVII, Cavalieri y Torricelli ampliaron el uso de los infinitesimales, Descartes y Fermat utilizaron el álgebra para encontrar el área y las tangentes (integración y derivación en términos modernos). Fermat e Isaac Barrow tenían la certeza de que ambos cálculos estaban relacionados, aunque fueron Newton (hacia 1660), en Inglaterra y Leibniz en Alemania (hacia 1670) quienes demostraron que los problemas del área y la tangente son inversos, lo que se conoce como teorema fundamental del cálculo. Leibniz es el creador del simbolismo de la derivada, diferencial y la ∫ estilizada para la integración, en vez de la I de Bernoulli. Usó el nombre de cálculo diferencial y el nombre de cálculo integral propuso Juan Bernoulli, que sustituyó al nombre de 'cálculo sumatorio' de Leibniz. La simbología de Leibniz impulsó el avance del cálculo en Europa continental.[18]​ El descubrimiento de Newton, a partir de su teoría de la gravitación universal, fue anterior al de Leibniz, pero el retraso en su publicación aún provoca controversias sobre quién de los dos fue el primero. Newton utilizó el cálculo en mecánica en el marco de su tratado «Principios matemáticos de filosofía natural», obra científica por excelencia, llamando a su método de «fluxiones». Leibniz utilizó el cálculo en el problema de la tangente a una curva en un punto, como límite de aproximaciones sucesivas, dando un carácter más filosófico a su discurso. Sin embargo, terminó por adoptarse la notación de Leibniz por su versatilidad. En el siglo XVIII aumentó considerablemente el número de aplicaciones del cálculo, pero el uso impreciso de las cantidades infinitas e infinitesimales, así como la intuición geométrica, causaban todavía confusión y duda sobre sus fundamentos. De hecho, la noción de límite, central en el estudio del cálculo, era aún vaga e imprecisa en ese entonces. Uno de sus críticos más notables fue el filósofo George Berkeley. En el siglo XIX el trabajo de los analistas matemáticos sustituyeron esas vaguedades por fundamentos sólidos basados en cantidades finitas: Bolzano y Cauchy definieron con precisión los conceptos de límite en términos de épsilon-delta y de derivada, Cauchy y Riemann hicieron lo propio con las integrales, y Dedekind y Weierstrass con los números reales. Fue el periodo de la fundamentación del cálculo. Por ejemplo, se supo que las funciones diferenciables son continuas y que las funciones continuas son integrables, aunque los recíprocos son falsos. En el siglo XX, el análisis no convencional, legitimó el uso de los infinitesimales, al mismo tiempo que la aparición de las computadoras ha incrementado las aplicaciones y velocidad del cálculo. Actualmente, el cálculo infinitesimal tiene un doble aspecto: por un lado, se ha consolidado su carácter disciplinario en la formación de la sociedad culta del conocimiento, destacando en este ámbito textos propios de la disciplina como el de Louis Leithold, el de Earl W. Swokowski, el de Denis G. Zill o el de James Stewart, entre muchos otros; por otro su desarrollo como disciplina científica que ha desembocado en ámbitos tan especializados como el cálculo fraccional, la teoría de funciones analíticas de variable compleja o el análisis matemático. El éxito del cálculo ha sido extendido con el tiempo a las ecuaciones diferenciales, al cálculo de vectores, al cálculo de variaciones, al análisis complejo y a las topología algebraica y topología diferencial entre muchas otras ramas. El desarrollo y uso del cálculo ha tenido efectos muy importantes en casi todas las áreas de la vida moderna: es fundamento para el cálculo numérico aplicado en casi todos los campos técnicos y/o científicos cuya principal característica es la continuidad de sus elementos, en especial en la física. Prácticamente todos los desarrollos técnicos modernos como la construcción, aviación, transporte, meteorología, etc., hacen uso del cálculo. Muchas fórmulas algebraicas se usan hoy en día en balística, calefacción, refrigeración, etc. Como complemento del cálculo, en relación con sistemas teóricos o físicos cuyos elementos carecen de continuidad, se ha desarrollado una rama especial conocida como Matemática discreta. Recientemente, se ha desarrollado el Cálculo Fraccional de Conjuntos (en inglés, Fractional Calculus of Sets o FCS) como una metodología derivada del Cálculo Fraccional. Esta metodología, mencionada por primera vez en el artículo Sets of Fractional Operators and Numerical Estimation of the Order of Convergence of a Family of Fractional Fixed-Point Methods,[19] tiene como objetivo caracterizar y organizar los elementos del cálculo fraccional mediante el uso de conjuntos, aprovechando la variedad de operadores fraccionales disponibles en la literatura.[20]​[21]​[22]​[23]​[24]​[25]​ Actualmente, el cálculo fraccional carece de una definición unificada de lo que constituye una derivada fraccional. En consecuencia, cuando no es necesario especificar explícitamente la forma de una derivada fraccional, típicamente se denota de la siguiente manera: Los operadores fraccionales tienen varias representaciones, pero una de sus propiedades fundamentales es que recuperan los resultados del cálculo tradicional a medida que  Denotando   y  lim Cálculo lógico Artículo principal: Cálculo lógico El cálculo lógico es un sistema de reglas de inferencia o deducción de un enunciado a partir de otro u otros. El cálculo lógico requiere un conjunto consistente de axiomas y unas reglas de inferencia; su propósito es poder deducir algorítmicamente proposiciones lógicas verdaderas a partir de dichos axiomas. La inferencia es una operación lógica que consiste en obtener una proposición lógica como conclusión a partir de otra(s) (premisas) mediante la aplicación de reglas de inferencia.[27]​ Informalmente interpretamos que alguien infiere —o deduce— T de R si acepta que si R tiene valor de verdad V, entonces, necesariamente, T tiene valor de verdad V. Sin embargo, en el enfoque moderno del cálculo lógico no es necesario acudir al concepto de verdad, para construir el cálculo lógico. Los hombres en nuestra tarea diaria, utilizamos constantemente el razonamiento deductivo. Partimos de enunciados empíricos —supuestamente verdaderos y válidos— para concluir en otro enunciado que se deriva de aquellos, según las leyes de la lógica natural.[28]​ La lógica, como ciencia formal, se ocupa de analizar y sistematizar dichas leyes, fundamentarlas y convertirlas en las reglas que permiten la transformación de unos enunciados —premisas- en otros -conclusiones— con objeto de convertir las operaciones en un algoritmo riguroso y eficaz, que garantiza que dada la verdad de las premisas, la conclusión es necesariamente verdadera. Al aplicar las reglas de un cálculo lógico a los enunciados de un argumento mediante la simbolización adecuada como fórmulas o expresiones bien formadas (EBF) del cálculo, construimos un modelo o sistema deductivo. En ese contexto, las reglas de formación de fórmulas definen la sintaxis de un lenguaje formal de símbolos no interpretados, es decir, sin significado alguno; y las reglas de transformación del sistema permiten transformar dichas expresiones en otras equivalentes; entendiendo por equivalentes que ambas tienen siempre y de forma necesaria el mismo valor de verdad. Dichas transformaciones son meramente tautologías. Un lenguaje formal que sirve de base para el cálculo lógico está formado por varias clases de entidades: Un conjunto de elementos primitivos. Dichos elementos pueden establecerse por enumeración, o definidos por una propiedad tal que permita discernir sin duda alguna cuándo un elemento pertenece o no pertenece al sistema. Un conjunto de reglas de formación de «expresiones bien formadas» (EBF) que permitan en todo momento establecer, sin forma de duda, cuándo una expresión pertenece al sistema y cuándo no. Un conjunto de reglas de transformación de expresiones, mediante las cuales partiendo de una expresión bien formada del cálculo podremos obtener una nueva expresión equivalente y bien formada que pertenece al cálculo. Cuando en un cálculo así definido se establecen algunas expresiones determinadas como verdades primitivas o axiomas, decimos que es un sistema formal axiomático. Un cálculo así definido si cumple al mismo tiempo estas tres condiciones decimos que es un Cálculo Perfecto: Es consistente: No es posible que dada una expresión bien formada del sistema, ƒ, y su negación, no – ƒ, sean ambas teoremas del sistema. No puede haber contradicción entre las expresiones del sistema. Decidible: Dada cualquier expresión bien formada del sistema podemos encontrar un método que nos permita decidir mediante una serie finita de operaciones si dicha expresión es o no es un teorema del sistema. Completo: Cuando dada cualquier expresión bien formada del sistema, podemos establecer la demostración matemática o prueba de que es un teorema del sistema. La misma lógica-matemática ha demostrado que tal sistema de cálculo perfecto «no es posible» (véase el Teorema de Gödel). Sistematización de un cálculo de deducción natural Reglas de formación de fórmulas I. Una letra enunciativa (con o sin subíndice) es una EBF. II. Si A es una EBF, ¬ A también lo es. III. Si A es una EBF y B también, entonces A ∧ B; A ∨ B; A → B; A ↔ B, también lo son. IV. Ninguna expresión es una fórmula del Cálculo sino en virtud de I, II, III. Notas: A, B, … con mayúsculas están utilizadas como metalenguaje en el que cada variable expresa cualquier proposición, atómica (p,q,r,s, …) o molecular (p ∧ q), (p ∨ q), …309>100 A, B, … son símbolos que significan variables; ¬, ∧, ∨, →, ↔, son símbolos constantes. Existen diversas formas de simbolización. Utilizamos aquí la de uso más frecuente en España.[29]​ Reglas de transformación de fórmulas 1) Regla de sustitución (R.T.1): Dada una tesis EBF del cálculo, en la que aparecen variables de enunciados, el resultado de sustituir una, algunas o todas esas variables por expresiones bien formadas (EBF) del cálculo, será también una tesis EBF del cálculo. Y ello con una única restricción, si bien muy importante: cada variable ha de ser sustituida siempre que aparece y siempre por el mismo sustituto. Veamos el ejemplo: 1 [(p ∧ q) ∨ r] → t ∨ s Transformación 2 A ∨ r → B Donde A = (p ∧ q); y donde B = (t ∨ s) 3 C → B Donde C = A ∨ r O viceversa 1 C → B Transformación 2 A ∨ r → B Donde A ∨ r = C 3 [(p ∧ q) ∨ r] → t ∨ s Donde (p ∧ q) = A; y donde (t ∨ s) = B 2) Regla de separación (R.T.2): Si X es una tesis EBF del sistema y lo es también X → Y, entonces Y es una tesis EBF del sistema. Esquemas de inferencia Sobre la base de estas dos reglas, siempre podremos reducir un argumento cualquiera a la forma: [A ∧ B ∧ C … ∧ N] → Y lo que constituye un esquema de inferencia en el que una vez conocida la verdad de cada una de las premisas A, B, … N y, por tanto, de su producto, podemos obtener la conclusión Y con valor de verdad V, siempre y cuando dicho esquema de inferencia sea una ley lógica, es decir su tabla de verdad nos muestre que es una tautología. Por la regla de separación podremos concluir Y, de forma independiente como verdad. Dada la poca operatividad de las tablas de verdad, el cálculo se construye como una cadena deductiva aplicando a las premisas o a los teoremas deducidos las leyes lógicas utilizadas como reglas de transformación, como se expone en cálculo lógico. El lenguaje natural como modelo de un cálculo lógico Naturalmente el cálculo lógico es útil porque puede tener aplicaciones, pero ¿en qué consisten o cómo se hacen tales aplicaciones? Podemos considerar que el lenguaje natural es un modelo de C si podemos someterlo, es decir, aplicarle una correspondencia en C.[30]​ Para ello es necesario someter al lenguaje natural a un proceso de formalización de tal forma que podamos reducir las expresiones lingüísticas del lenguaje natural a EBF de un cálculo mediante reglas estrictas manteniendo el sentido de verdad lógica de dichas expresiones del lenguaje natural. Esto es lo que se expone en cálculo lógico. Las diversas formas en que tratemos las expresiones lingüísticas formalizadas como proposiciones lógicas dan lugar a sistemas diversos de formalización y cálculo: Cálculo proposicional o cálculo de enunciados Cuando se toma la oración simple significativa del lenguaje natural con posible valor de verdad o falsedad como una proposición atómica, como un todo sin analizar. Cálculo como lógica de clases Cuando se toma la oración simple significativa del lenguaje natural con posible valor de verdad o falsedad como resultado del análisis de la oración como una relación de individuos o posibles individuos que poseen o no poseen una propiedad común determinada como pertenecientes o no pertenecientes a una clase natural o a un conjunto como individuos. Cálculo de predicados o cuantificacional Cuando se toma la oración simple significativa del lenguaje natural con posible valor de verdad o falsedad como resultado del análisis de la misma de forma que una posible función predicativa (P), se predica de unos posibles sujetos variables (x) [tomados en toda su posible extensión: (Todos los x); o referente a algunos indeterminados: (algunos x)], o de una constante individual existente (a). Cálculo como lógica de relaciones Cuando se toma la oración simple significativa con posible valor de verdad propio, verdadero o falso, como resultado del análisis de la oración como una relación R que se establece entre un sujeto y un predicado. La simbolización y formación de EBFs en cada uno de esos cálculos, así como las reglas de cálculo se trata en cálculo lógico."
ksampletext_wikipedia_math_algebra: str = "Álgebra. El álgebra (del árabe​) es la rama de la matemática que estudia la combinación de elementos de estructuras abstractas acorde a ciertas reglas.[3] Originalmente esos elementos podían ser interpretados como números o cantidades, por lo que el álgebra en cierto modo fue originalmente una generalización y extensión de la aritmética.[4]​[5] En el álgebra moderna existen áreas del álgebra que en modo alguno pueden considerarse extensiones de la aritmética (álgebra abstracta, álgebra homológica, álgebra exterior, etc.). El álgebra elemental difiere de la aritmética en el uso de abstracciones, como el empleo de letras para representar números que son desconocidos o que pueden tomar muchos valores. Por ejemplo, en  La palabra álgebra también se utiliza en ciertas formas especializadas. Un tipo especial de objeto matemático en el álgebra abstracta se llama álgebra, y la palabra se usa, por ejemplo, en las frases álgebra lineal y topología algebraica. Etimología La palabra álgebra proviene del  y cálculo de datos[2] del título del libro de principios del siglo, La ciencia del restablecimiento y el equilibrio por el matemático y astrónomo persa Muḥammad ibn Mūsā al-Khwārizmī. En su obra, el término al-jabr se refería a la operación de mover un término de un lado de una ecuación al otro, المقابلة al-muqābala equilibrar se refería a añadir términos iguales a ambos lados. Acortada a simplemente algeber o álgebra en latín, la palabra acabó entrando en la lengua inglesa durante el siglo XV, ya sea desde el español, el italiano o el latín medieval. Originalmente se refería al procedimiento quirúrgico de fijar huesos rotos o dislocados. El significado matemático se registró por primera vez (en inglés) en el siglo XVI.[6]​ Introducción A diferencia de la aritmética elemental, que trata de los números y las operaciones fundamentales, en álgebra -para lograr la generalización- se introducen además símbolos (usualmente letras) para representar parámetros (variables o coeficientes), o cantidades desconocidas (incógnitas); las expresiones así formadas son llamadas «fórmulas algebraicas» y expresan una regla o un principio general.[7] El álgebra conforma una de las grandes áreas de las matemáticas, junto a la teoría de números, la geometría y el análisis. Página del libro Kitāb al-mukhtaṣar fī ḥisāb al-ŷabr wa-l-muqābala, de Al-Juarismi La palabra «álgebra» proviene del vocablo árabe الجبر al-ŷabar (en árabe dialectal por asimilación progresiva se pronunciaba [alŷɛbɾ], de donde derivan los términos de las lenguas europeas), que se traduce como 'restauración' o 'reposición, reintegración'. Deriva del tratado escrito alrededor del año 820 e. c. por el matemático y astrónomo persa Muhammad ibn Musa al-Jwarizmi (conocido como Al Juarismi), titulado Al-kitāb al-mukhtaṣar fī ḥisāb al-ŷarabi waˀl-muqābala (Compendio de cálculo por reintegración y comparación), el cual proporcionaba operaciones simbólicas para la solución sistemática de ecuaciones lineales y cuadráticas. Muchos de sus métodos derivan del desarrollo de la matemática en el islam medieval, destacando la independencia del álgebra como una disciplina matemática independiente de la geometría y de la aritmética.[8] Puede considerarse al álgebra como el arte de hacer cálculos del mismo modo que en aritmética, pero con objetos matemáticos no-numéricos.[9]​ El adjetivo «algebraico» denota usualmente una relación con el álgebra, como por ejemplo en estructura algebraica. Por razones históricas, también puede indicar una relación con las soluciones de ecuaciones polinomiales, números algebraicos, extensión algebraica o expresión algebraica. Conviene distinguir entre: Álgebra elemental es la parte del álgebra que se enseña generalmente en los cursos de matemáticas. Álgebra abstracta es el nombre dado al estudio de las «estructuras algebraicas» propiamente. El álgebra usualmente se basa en estudiar las combinaciones de cadenas finitas de signos y, mientras que análisis matemático requiere estudiar límites y sucesiones de una cantidad infinita de elementos. Historia del álgebra Véase también: Historia de la matemática El álgebra en la antigüedad Las raíces del álgebra pueden rastrearse hasta la antigua matemática babilónica,[10] que había desarrollado un avanzado sistema aritmético con el que fueron capaces de hacer cálculos en una forma algorítmica. Con el uso de este sistema lograron encontrar fórmulas y soluciones para resolver problemas que, en la actualidad, suelen resolverse mediante ecuaciones lineales, ecuaciones de segundo grado y ecuaciones indeterminadas. En contraste, la mayoría de los egipcios de esta época, y la mayoría de los matemáticos griegos y chinos del primer milenio antes de Cristo, normalmente resolvían tales ecuaciones por métodos geométricos, tales como los descritos en el Papiro de Rhind, Los Elementos de Euclides y Los nueve capítulos sobre el arte matemático. Papiro de Ahmes; datado entre 2000 al 1800 a. e. c. Papiro de Ahmes; datado entre 2000 al 1800 a. e. c. Elementos de Euclides, ca. Elementos de Euclides, ca. 300 a. e. c. Las nueve lecciones del arte matemático; compilado durante siglos II y III a. e. c. Las nueve lecciones del arte matemático; compilado durante siglos II y III a. e. c. Arithmetica; escrito por Diofanto alrededor de 280 de nuestra era Véase también: Matemática helénica Los matemáticos de la Antigua Grecia introdujeron una importante transformación al crear un álgebra de tipo geométrico, en donde los «términos» eran representados mediante los «lados de objetos geométricos», usualmente líneas a las cuales asociaban letras.[9] Los matemáticos helénicos Herón de Alejandría y Diofanto[11] así como también los matemáticos indios como Brahmagupta, siguieron las tradiciones de Egipto y Babilonia, si bien la Arithmetica de Diofanto y el Brahmasphutasiddhanta de Brahmagupta se hallan a un nivel de desarrollo mucho más alto.[12] Por ejemplo, la primera solución aritmética completa (incluyendo al cero y soluciones negativas) para las ecuaciones cuadráticas fue descrita por Brahmagupta en su libro Brahmasphutasiddhanta. Más tarde, los matemáticos árabes y musulmanes desarrollarían métodos algebraicos a un grado mucho mayor de sofisticación. Diofanto (siglo III), algunas veces llamado «el pádre del álgebra», fue un matemático alejandrino, autor de una serie de libros intitulados Arithmetica. Estos textos tratan de las soluciones a las ecuaciones algebraicas.[13]​ Influencia árabe Véase también: Matemática en el islam medieval Los babilonios y Diofanto utilizaron sobre todo métodos especiales ad hoc para resolver ecuaciones, la contribución de Al-Khwarizmi fue fundamental; resuelve ecuaciones lineales y cuadráticas sin el simbolismo algebraico, números negativos o el cero, por lo que debe distinguir varios tipos de >jab.[14]​ El matemático persa Omar Khayyam desarrolló la geometría algebraica y encontró la solución geométrica de la ecuación cúbica. Otro matemático persa, Sharaf Al-Din al-Tusi, encontró la solución numérica y algebraica a diversos casos de ecuaciones cúbicas; también desarrolló el concepto de función. Los matemáticos indios Mahavirá y Bhaskara II, el matemático persa Al-Karaji, y el matemático chino Zhu Shijie, resolvieron varios casos de ecuaciones de grado tres, cuatro y cinco, así como ecuaciones polinómicas de orden superior mediante métodos numéricos. Edad Moderna Durante la Edad Moderna europea tienen lugar numerosas innovaciones, y se alcanzan resultados que claramente superan los resultados obtenidos por los matemáticos árabes, persas, indios o griegos. Parte de este estímulo viene del estudio de las ecuaciones polinómicas de tercer y cuarto grado. Las soluciones para ecuaciones polinómicas de segundo grado ya era conocida por los matemáticos babilónicos cuyos resultados se difundieron por todo el mundo antiguo. El descrubrimiento del procedimiento para encontrar soluciones algebraicas de tercer y cuarto orden se dieron en la Italia del siglo XVI. También es notable que la noción de determinante fue descubierta por el matemático japonés Kowa Seki en el siglo XVII, seguido por Gottfried Leibniz diez años más tarde, con el fin de resolver sistemas de ecuaciones lineales simultáneas utilizando matrices. Entre los siglos XVI y XVII se consolidó la noción de número complejo, con lo cual la noción de álgebra empezaba a apartarse de cantidades medibles. Gabriel Cramer también hizo un trabajo sobre matrices y determinantes en el siglo XVIII. También Leonhard Euler, Joseph-Louis Lagrange, Adrien-Marie Legendre y numerosos matemáticos del siglo XVIII hicieron avances notables en álgebra. Siglo XIX El álgebra abstracta se desarrolló en el siglo XIX, inicialmente centrada en lo que hoy se conoce como teoría de Galois y en temas de la constructibilidad.[15] Los trabajos de Gauss generalizaron numerosas estructuras algebraicas. La búsqueda de una fundamentación matemática rigurosa y una clasificación de los diferentes tipos de construcciones matemáticas llevó a crear áreas del álgebra abstracta durante el siglo XIX absolutamente independientes de nociones aritméticas o geométricas (algo que no había sucedido con el álgebra de los siglos anteriores). Áreas de matemáticas con la palabra álgebra en su nombre Algunas áreas de las matemáticas que entran en la clasificación de álgebra abstracta tienen la palabra álgebra en su nombre; el álgebra lineal es un ejemplo. Otras no: La teoría de grupos, la teoría de anillos y la teoría de campos son ejemplos. En esta sección, enumeramos algunas áreas de las matemáticas con la palabra álgebra en el nombre. Álgebra elemental, la parte del álgebra que se suele enseñar en los cursos elementales de matemáticas. Álgebra abstracta, en la que se definen e investigan las estructuras algebraicas como grupos, anillos y campos. Álgebra lineal, en la que se estudian las propiedades específicas de las ecuaciones lineales, los espacios vectoriales y las matrices. Álgebra de Boole, una rama del álgebra que abstrae el cálculo con los valor de verdades falso y verdadero. Álgebra conmutativa, el estudio de los anillos conmutativos. Álgebra computacional, la implementación de métodos algebraicos como algoritmos y programas de ordenador. Álgebra homológica, el estudio de las estructuras algebraicas fundamentales para el estudio de los espacios topológicos. Álgebra universal, en la que se estudian propiedades comunes a todas las estructuras algebraicas. Teoría de números algebraicos, en la que se estudian las propiedades de los números desde un punto de vista algebraico. Geometría algebraica, una rama de la geometría, que en su forma primitiva especifica las curvas y superficies como soluciones de ecuaciones polinómicas. Combinatoria algebraica, en la que se utilizan métodos algebraicos para estudiar cuestiones combinatorias. Álgebra relacional: conjunto de relaciones finitas que son cerradas bajo ciertos operadores. Muchas estructuras matemáticas se llaman álgebras: Álgebra sobre un cuerpo o más generalmente álgebra sobre un anillo. Muchas clases de álgebras sobre un campo o sobre un anillo tienen un nombre específico: Álgebra asociativa Álgebra no asociativa Álgebra de Lie Álgebra de Hopf C*-álgebra Álgebra simétrica Álgebra exterior Álgebra tensorial En teoría de la medida, σ-álgebra Álgebra sobre un conjunto En teoría de categorías Álgebra F y co-álgebra F Álgebra T En lógica, Álgebra de relación, un álgebra booleana residuada expandida con una involución llamada conversa. Álgebra booleana, un complementado del Retículo distributivo. Álgebra de Heyting Notación algebraica Notación matemática de raíz cuadrada de x Consiste en que los números se emplean para representar cantidades conocidas y determinadas. Las letras se emplean para representar toda clase de cantidades, ya sean conocidas o desconocidas. Las cantidades conocidas se expresan por las primeras letras del alfabeto: a, b, c, d, … Las cantidades desconocidas se representan por las últimas letras del alfabeto: u, v, w, x, y, z.[16]​ Los signos empleados en álgebra son tres clases: Signos de operación, signos de relación y signos de agrupación.[16]​ Signos de operación En álgebra se verifican con las cantidades las mismas operaciones que en aritmética: suma, resta, multiplicación, elevación a potencias y extracción de raíces, que se indican con los principales signos de aritmética excepto el signo de multiplicación. En lugar del signo × suele emplearse un punto entre los factores y también se indica a la multiplicación colocando los factores entre paréntesis. Así a⋅b y (a)(b) equivale a a × b. Signos de relación Se emplean estos signos para indicar la relación que existe entre dos cantidades. Los principales son: =, que se lee igual a. Así, a=b se lee “a igual a b”. >, que se lee mayor que. Así, x + y > m se lee “x + y mayor que m”. <, que se lee menor que. Así, a < b + c se lee “a menor que b + c”. Signos de agrupación Signos y símbolos más comunes Los signos y símbolos son utilizados en el álgebra —y en general en teoría de conjuntos y álgebra de conjuntos— con los que se constituyen ecuaciones, matrices, series, etc. Sus letras son llamadas variables, ya que se usa esa misma letra en otros problemas y su valor va variando. Aquí algunos ejemplos: Signos y símbolos Expresión Uso + Además de expresar adición, también es usada para expresar operaciones binarias c o k Expresan términos constantes Primeras letras del abecedario a, b, c, … Se utilizan para expresar cantidades conocidas Últimas letras del abecedario …, x, y, z Se utilizan para expresar incógnitas n Expresa cualquier número (1, 2, 3, 4, …, n) Exponentes y subíndices Expresar cantidades de la misma especie, de diferente magnitud. Simbología de Conjuntos[17]​ Símbolo Descripción ∈ Es un elemento del conjunto o pertenece al conjunto. ∉ No es un elemento del conjunto o no pertenece al conjunto. ⎜ Tal que n (C) Cardinalidad del conjunto C U Conjunto Universo Φ Conjunto vacío ⊆ Subconjunto de ⊂ Subconjunto propio de ⊄ No es subconjunto propio de > Mayor que < Menor que ≥ Mayor o igual que ≤ Menor o igual que ∩ Intersección de conjuntos ∪ Unión de Conjuntos A' Complemento del conjunto A = Símbolo de igualdad ≠ No es igual a … El conjunto continúa ⇔ Si y solo si ¬ (en algunos ocasiones ∼) No, negación lógica (es falso que) ∧ Y ∨ O Lenguaje algebraico Lenguaje algebraico[17]​ Lenguaje común Lenguaje algebraico Un número cualquiera m Un número cualquiera aumentado en siete m + 7 La diferencia de dos números cualesquiera f - q El doble de un número excedido en cinco 2x + 5 La división de un número entero entre su antecesor x/(x-1) La mitad de un número d/2 El cuadrado de un número y^2 La media de la suma de dos números (b+c)/2 Las dos terceras partes de un número disminuidos en cinco es igual a 12. 2/3 (x-5) = 12 Tres números naturales consecutivos. x, x + 1, x + 2. La parte mayor de 1200, si la menor es w 1200 - w El cuadrado de un número aumentado en siete b2 + 7 Las tres quintas partes de un número más la mitad de su consecutivo equivalen a tres. 3/5 p + 1/2 (p+1) = 3 El producto de un número con su antecesor equivalen a 30. x(x-1) = 30 El cubo de un número más el triple del cuadrado de dicho número x3 + 3x2 Estructura algebraica Artículo principal: Estructura algebraica En matemáticas, una estructura algebraica es un conjunto de elementos con unas propiedades operacionales determinadas; es decir, lo que define a la estructura del conjunto son las operaciones que se pueden realizar con los elementos de dicho conjunto y las propiedades matemáticas que dichas operaciones poseen. Un objeto matemático constituido por un conjunto no vacío y algunas leyes de composición interna definida en él es una estructura algebraica. Las estructuras algebraicas más importantes son:"
ksampletext_wikipedia_math_numero: str = "Número. Un número es un objeto matemático utilizado para contar (cantidades), medir (magnitudes) y etiquetar. Los números más sencillos, que utilizamos todos en la vida cotidiana, son los números naturales: 1, 2, 3, etc. Se denotan mediante  Los números desempeñan un papel esencial en las ciencias empíricas, ya que permiten cuantificar y describir fenómenos observables. No solo los números naturales son utilizados, sino también diversos conjuntos numéricos desarrollados a lo largo de la historia de las matemáticas. El conjunto de los números enteros (representado por  Desde la antigüedad, se conoce la existencia de números que no pueden expresarse como fracciones de enteros. Por ejemplo, la longitud de la diagonal de un cuadrado de lado unidad es  Con el tiempo, se introdujeron otros tipos de números para ampliar el alcance del análisis matemático, como los imaginarios, complejos y trascendentes, que permiten describir y resolver fenómenos de creciente complejidad en diversas ramas de la ciencia y la ingeniería. Nótese que la teoría de números es una rama de las matemáticas que se ocupa de los enteros (no de números en general). El origen de los números es que los primeros números que el hombre inventó fueron los números naturales, los cuales se utilizaban y se utilizan para contar elementos de un conjunto finito, ya que se procede a enumerar dichos elementos de una manera ordenada seleccionándolos uno tras otro a la vez que se le atribuye a cada uno un número. Tipos de números Clasificación de los números. Los números más conocidos son los números naturales. Denotados mediante  Otro tipo de números ampliamente usados son números fraccionarios, los cuales representan tanto cantidades inferiores a una unidad, como números mixtos (un conjunto de unidades más una parte inferior a la unidad). Los números fraccionarios pueden ser expresados siempre como cocientes de enteros. El conjunto de todos los números fraccionarios es el conjunto de los números racionales (que usualmente se define para que incluya tanto a los racionales positivos, como a los racionales negativos y el cero). Este conjunto de números se designa como  Los números racionales permiten resolver gran cantidad de problemas prácticos, pero desde los antiguos griegos se conoce que ciertas relaciones geométricas (la diagonal de un cuadrado de lado unidad) son números no enteros que tampoco son racionales. Igualmente, la solución numérica de una ecuación polinómica cuyos coeficientes son números racionales, usualmente es un número no racional. Puede demostrarse que cualquier número irracional puede representarse como una sucesión de Cauchy de números racionales que se aproximan a un límite numérico. El conjunto de todos los números racionales y los irracionales (obtenidos como límites de sucesiones de Cauchy de números racionales) es el conjunto de los números reales  Uno de los problemas de los números reales es que no forman un cuerpo algebraicamente cerrado, por lo que ciertos problemas no tienen solución planteados en términos de números reales. Esa es una de las razones por las cuales se introdujeron los números complejos  Al margen de los números reales y complejos, claramente conectados con problemas de las ciencias naturales, existen otros tipos de números que generalizan aún más y extienden el concepto de número de una manera más abstracta y responden más a creaciones deliberadas de matemáticos. La mayoría de estas generalizaciones del concepto de número se usan solo en matemáticas, aunque algunos de ellos han encontrado aplicaciones para resolver ciertos problemas físicos. Entre ellos están los números hipercomplejos, que incluyen a los cuaterniones, útiles para representar rotaciones en un espacio de tres dimensiones, y generalizaciones de estos, como octoniones y los sedeniones. A un nivel un poco más abstracto también se han ideado conjuntos de números capaces de tratar con cantidades infinitas e infinitesimales, como los hiperreales  Lista de los tipos de números existentes La teoría de los números trata básicamente de las propiedades de los números naturales y los enteros, mientras que las operaciones del álgebra y el cálculo permiten definir la mayor parte de los sistemas numéricos, entre los cuales están: Números naturales Número primo Números compuestos Números perfectos Números enteros Números negativos Números pares Números impares Números racionales Números reales Números irracionales Números algebraicos Números trascendentes Extensiones de los números reales Números complejos Números hipercomplejos Cuaterniones Octoniones Números hiperreales Números superreales Números surreales Números usados en teoría de conjuntos Números ordinales Números cardinales Números transfinitos Estructura de los sistemas numéricos En álgebra abstracta y análisis matemático un sistema numérico se caracteriza por una: Estructura algebraica, usualmente un anillo conmutativo o cuerpo matemático (en el caso no conmutativo son un álgebra sobre un cuerpo y en el caso de los números naturales solo un monoide conmutativo). Estructura de orden, usualmente un conjunto ordenado, en el caso de los números naturales, enteros, racionales y reales se trata de conjuntos totalmente ordenados, aunque los números complejos e hipercomplejos solo son conjuntos parcialmente ordenados. Los reales además son un conjunto bien ordenado y con un orden denso.[1]​ Estructura topológica, los conjuntos numéricos numerables usualmente son conjuntos disconexos, sobre los que se considera la topología discreta, mientras que sobre los conjuntos no numerables se considera una topología que los hace adecuados para el análisis matemático. Otra propiedad interesante de muchos conjuntos numéricos es que son representables mediante diagramas de Hasse, diagramas de Euler y diagramas de Venn, pudiéndose tomar una combinación de ambos en un diagrama de Euler-Venn con la forma característica de cuadrilátero y además pudiéndose representar internamente un diagrama de Hasse (es una recta). Tanto históricamente como conceptualmente, los diversos conjuntos numéricos, desde el más simple de los números naturales, hasta extensiones transcendentes de los números reales y complejos, elaboradas mediante la teoría de modelos durante el siglo XX, se construyen desde una estructura más simple hasta otra más compleja.[2]​ Números naturales especiales El estudio de ciertas propiedades que cumplen los números ha producido una enorme cantidad de tipos de números, la mayoría sin un interés matemático específico. Se pueden encuadrar dentro de la matemática recreativa. A continuación se indican algunos: Perfecto: número igual a la suma de sus divisores (incluyendo el 1). Ejemplo: 6 = 1 + 2 + 3. Sheldon: el número 73, es el 21° número primo, que al multiplicar 7 × 3 = 21; Y al dar la vuelta a sus dígitos da 37 que es el 12° número primo. Narcisista: número de n dígitos que resulta ser igual a la suma de las potencias de orden n de sus dígitos. Ejemplo: 153 = 1³ + 5³ + 3³. Omirp: número primo que al invertir sus dígitos da otro número primo. Ejemplo: 1597 y 7951 son primos. Vampiro: número que es el producto de dos números obtenidos a partir de sus dígitos. Ejemplo: 2187 = 27 × 81. Hamsteriano: Su estructura aritmética N= (a×b)2-1, donde a y b son primos los dos, la suma de sus divisores sobrepasa N, y la cantidad de sus divisores es > a×b/2; va como ejemplo: 1224 = (5×7)2-1 Pitagórico: una terna pitagórica son tres números que cumplen las siguientes condiciones: el cuadrado de uno de ellos, más el cuadrado de otro, es igual al cuadrado del tercero, por ejemplo: (3, 4, 5) ya que 32 + 42 = 9 + 16 = 25 = 52 Una vez entendido el problema de la naturaleza y la clasificación de los números, surge otro, más práctico, pero que condiciona todo lo que se va a hacer con ellos: la manera de escribirlos. El sistema que se ha impuesto universalmente es la numeración posicional, gracias al invento del cero, con una base constante. Más formalmente, en Los fundamentos de la aritmética, Gottlob Frege (1848-1925) realiza una definición de «número», la cual fue tomada como referencia por muchos matemáticos (entre ellos Bertrand Russell [1872-1870], cocreador de Principia mathematica): «n» es un número, es entonces la definición de «que existe un concepto “F” para el cual “n” aplica», que a su vez se ve explicado como que «n» es la extensión del concepto «equinumerable con» para «F», y dos conceptos son equinumerables si existe una relación «uno a uno» (véase que no se utiliza el símbolo «1» porque no está definido aún) entre los elementos que lo componen (es decir, una biyección en otros términos). Véase también que Frege, tanto como cualquier otro matemático, se ve inhabilitado para definir al número como la expresión de una cantidad, porque la simbología matemática no hace referencia necesaria a la numerabilidad, y el hecho de «cantidad» referiría a algo numerable, mientras que números se adoptan para definir la cardinalidad de, por ejemplo, los elementos que se encuentran en el intervalo abierto (0, 1), que contiene innumerables elementos (el continuo). Peano, antes de establecer sus cinco proposiciones sobre los números naturales, explícita que supone sabida una definición (quizás debido a su «obviedad») de las palabras o conceptos cero, sucesor y número. De esta manera postula: 0 es un número natural el sucesor de todo número es un número dos números diferentes no tienen el mismo sucesor 0 no es el sucesor de ningún número y la propiedad inductiva Sin embargo, si uno define el concepto cero como el número 100, y el concepto número como los números mayores a 100, entonces las cinco proposiciones mencionadas anteriormente aplican, no a la idea que Peano habría querido comunicar, sino a su formalización. La definición de número se encuentra por ende no totalmente formalizada, aunque se encuentre un acuerdo mayoritario en adoptar la definición enunciada por Frege. Historia del concepto de número Hueso de Ishango. Cognitivamente el concepto de número está asociado a la habilidad de contar y comparar cuál de dos conjuntos de entidades similares tiene mayor cantidad de elementos. Las primeras sociedades humanas se encontraron muy pronto con el problema de determinar cuál de dos conjuntos era «mayor» que otro, o de conocer con precisión cuántos elementos formaban una colección de cosas. Esos problemas podían ser resueltos simplemente contando. La habilidad de contar del ser humano, no es un fenómeno simple, aunque la mayoría de culturas tienen sistemas de cuenta que llegan como mínimo a centenares, algunos pueblos con una cultura material simple, solo disponen de términos para los números 1, 2 y 3 y usualmente usan el término «muchos» para cantidades mayores, aunque cuando es necesario usan recursivamente expresiones traducibles como «3 más 3 y otros 3». El conteo se debió iniciar mediante el uso de objetos físicos (tales como montones de piedras) y de marcas de cuenta, como las encontradas en huesos tallados: el de Lebombo, con 29 muescas grabadas en un hueso de babuino, tiene unos 37 000 años de antigüedad y otro hueso de lobo encontrado en la antigua Checoslovaquia, con 57 marcas dispuestas en cinco grupos de 11 y dos sueltas, se ha estimado en unos 30 000 años de antigüedad. Ambos casos constituyen una de las más antiguas marcas de cuenta conocidas habiéndose sugerido que pudieran estar relacionadas con registros de fases lunares.[3] En cuanto al origen ordinal algunas teorías lo sitúan en rituales religiosos. Los sistemas numerales de la mayoría de familias lingüísticas reflejan que la operación de contar estuvo asociado al conteo de dedos (razón por la cual los sistemas de base decimal y vigesimal son los más abundantes), aunque está testimoniado el empleo de otras bases numéricas. El paso hacia los símbolos numerales, al igual que la escritura, se ha asociado a la aparición de sociedades complejas con instituciones centralizadas constituyendo artificios burocráticos de contabilidad en registros impositivos y de propiedades. Su origen estaría en primitivos símbolos con diferentes formas para el recuento de diferentes tipos de bienes como los que se han encontrado en Mesopotamia inscritos en tablillas de arcilla que a su vez habían venido a sustituir progresivamente el conteo de diferentes bienes mediante fichas de arcilla (constatadas al menos desde el 8000 a. C.) Los símbolos numerales más antiguos encontrados se sitúan en las civilizaciones mesopotámicas usándose como sistema de numeración ya no solo para la contabilidad o el comercio sino también para la agrimensura o la astronomía como, por ejemplo, registros de movimientos planetarios.[4]​ En conjunto, desde hace 5000 años la mayoría de las civilizaciones han contado como lo hacemos hoy aunque la forma de escribir los números (si bien todos representan con exactitud los naturales) ha sido muy diversa. Básicamente la podemos clasificar en tres categorías: Sistemas de notación aditiva. Acumulan los símbolos de todas las unidades, decenas, centenas, …, necesarios hasta completar el número. Aunque los símbolos pueden ir en cualquier orden, adoptaron siempre una determinada posición (de más a menos). De este tipo son los sistemas de numeración: egipcio, hitita, cretense, romano, griego, armenio y judío. Sistemas de notación híbrida. Combinan el principio aditivo con el multiplicativo. En los anteriores 500 se representa con 5 símbolos de 100, en estos se utiliza la combinación del 5 y el 100. El orden de las cifras es ahora fundamental (estamos a un paso del sistema posicional). De este tipo son los sistemas de numeración: chino clásico, asirio, armenio, etíope y maya. Este último utilizaba símbolos para el 1, el 5 y el 0. Siendo este el primer uso documentado del cero tal como lo conocemos hoy (año 36 a. C.) ya que el de los babilonios solo se utilizaba entre otros dígitos. Sistemas de notación posicional. La posición de las cifras nos indica si son unidades, decenas, centenas, …, o en general la potencia de la base. Solo tres culturas además de la india lograron desarrollar un sistema de este tipo: el sistema chino (300 a. C.) que no disponía de 0, el sistema babilónico (2000 a. C.) con dos símbolos, de base 10 aditivo hasta el 60 y posicional (de base 60) en adelante, sin 0 hasta el 300 a. C. Las fracciones unitarias egipcias (Papiro de Ahmes/Rhind) Artículo principal: Fracción egipcia En este papiro adquirido por Alexander Henry Rhind (1833-1863) en 1858, cuyo contenido data del 2000 al 1800 a. C. además del sistema de numeración antes descrito nos encontramos con su tratamiento de las fracciones. No consideran las fracciones en general, solo las fracciones unitarias (inversas de los naturales 1/20) que se representan con un signo oval encima del número, la fracción 2/3 que se representa con un signo especial y en algunos casos fracciones del tipo  Al ser un sistema sumativo la notación es: 1+1/2+1/4. La operación fundamental es la suma y nuestras multiplicaciones y divisiones se hacían por «duplicaciones» y «mediaciones», por ejemplo 69×19=69×(16+2+1), donde 16 representa 4 duplicaciones y 2 una duplicación. Fracciones sexagesimales babilónicas (documentos cuneiformes) En las tablillas cuneiformes de la dinastía Hammurabi (1800-1600 a. C.) aparece el sistema posicional, antes referido, extendido a las fracciones, pero XXX vale para  Para calcular recurrían, como nosotros antes de disponer de máquinas, a las numerosas tablas que disponían: De multiplicar, de inversos, de cuadrados y cubos, de raíces cuadradas y cúbicas, de potencias sucesivas de un número dado no fijo, etc. Por ejemplo, para calcular  Realizaban las operaciones de forma parecida a hoy, la división multiplicando por el inverso (para lo que utilizan sus tablas de inversos). En la tabla de inversos faltan los de 7 y 11 que tienen una expresión sexagesimal infinitamente larga. Sí están 1/59=;1,1,1 (nuestro 1/9=0,111…) y 1/61=;0,59,0,59 (nuestro 1/11=0,0909…), pero no se percataron del desarrollo periódico. Descubrimiento de los inconmensurables Las circunstancias y la fecha de este descubrimiento son inciertas, aunque se atribuye a la escuela pitagórica (se utiliza el teorema de Pitágoras). Aristóteles (384-322 a. C.) menciona una demostración de la inconmensurabilidad de la diagonal de un cuadrado con respecto a su lado basada en la distinción entre lo par y lo impar. La reconstrucción que realiza C. Boyer es: Sean d:diagonal, s:lado y d/s racional, que podremos escribirlo como  La teoría pitagórica de todo es número quedó seriamente dañada. El problema lo resolvería Eudoxo de Cnido (408-355 a. C.) tal como nos indica Euclides en el libro V de Los elementos. Para ello estableció el axioma de Arquímedes: «Dos magnitudes tienen una razón si se puede encontrar un múltiplo de una de ellas que supere a la otra» (excluye el 0). Después, en la definición 5, da la famosa formulación de Eudoxo: «Dos magnitudes están en la misma razón  En el libro Historia de la matemática (1985), de J. P. Colette, se hace la observación de que esta definición está muy próxima a la de número real que dará Dedekind (1831-1916), divide las fracciones en las  Creación del cero Artículo principal: Cero En cualquier sistema de numeración posicional surge el problema de la falta de unidades de determinado orden. Por ejemplo, en el sistema babilónico el número  Hacia el siglo III a. C., en Grecia, se comenzó a representar la nada mediante una o que significa oudos 'vacío', y que no dio origen al concepto de cero como existe hoy en día. La idea del cero como concepto matemático parece haber surgido en la India antes que en ningún otro lugar. La única notación ordinal del Viejo Mundo fue la sumeria, donde el cero se representaba por un vacío. En América, la primera expresión conocida del sistema de numeración vigesimal prehispánico data del siglo III a. C. Se trata de una estela olmeca tardía, la cual ya contaba tanto con el concepto de orden como el de cero. Los mayas inventaron cuatro signos para el cero; los principales eran: el corte de un caracol para el cero matemático, y una flor para el cero calendárico (que implicaba no la ausencia de cantidad, sino el cumplimiento de un ciclo). Números negativos Brahmagupta, en el 628 de nuestra era, considera las dos raíces de las ecuaciones cuadráticas, aunque una de ellas sea negativa o irracional. De hecho en su obra es la primera vez que aparece sistematizada la aritmética (+, -, *, /, potencias y raíces) de los números positivos, negativos y el cero, que él llamaba los bienes, las deudas y la nada. Así, por ejemplo, para el cociente, establece: Positivo dividido por positivo, o negativo dividido por negativo, es afirmativo. Cifra dividido por cifra es nada (0/0=0). Positivo dividido por negativo es negativo. Negativo dividido por afirmativo es negativo. Positivo o negativo dividido por cifra es una fracción que la tiene por denominador (a/0=¿?) No solo utilizó los negativos en los cálculos, sino que los consideró como entidades aisladas, sin hacer referencia a la geometría. Todo esto se consiguió gracias a su despreocupación por el rigor y la fundamentación lógica, y su mezcla de lo práctico con lo formal. Sin embargo, el tratamiento que hicieron de los negativos cayó en el vacío, y fue necesario que transcurrieran varios siglos (hasta el Renacimiento) para que fuese recuperado. Al parecer, los chinos también poseían la idea de número negativo, y estaban acostumbrados a calcular con ellos utilizando varillas negras para los negativos y rojas para los positivos. Transmisión del sistema indo-arábigo a Occidente Varios autores del siglo XIII contribuyeron a esta difusión, destacan Alexandre de Villedieu (1225), Sacrobosco (circa 1195, o 1200-1256) y sobre todo Leonardo de Pisa (1180-1250). Este último, conocido como Fibonacci, viajó por Oriente y aprendió de los árabes el sistema posicional hindú. Escribió un libro, El Liber abaci, que trata en el capítulo I la numeración posicional, en los cuatro siguientes las operaciones elementales, en los capítulos VI y VII las fracciones: comunes, sexagesimales y unitarias (¡no usa los decimales, principal ventaja del sistema!), y en el capítulo XIV los radicales cuadrados y cúbicos. También contiene el problema de los conejos que da la serie:  No aparecen los números negativos, que tampoco consideraron los árabes, debido a la identificación de número con magnitud (¡obstáculo que duraría siglos!). A pesar de la ventaja de sus algoritmos de cálculo, se desataría por diversas causas una lucha encarnizada entre abacistas y algoristas, hasta el triunfo final de estos últimos. Las fracciones continuas Pietro Antonio Cataldi (1548-1626), aunque con ejemplos numéricos, desarrolla una raíz cuadrada en fracciones continuas como hoy: Queremos calcular  Siendo así los números irracionales aceptados con toda normalidad, pues se les podía aproximar fácilmente mediante números racionales. Primera formulación de los números complejos Los números complejos eran en pocos casos aceptados como raíces o soluciones de ecuaciones (M. Stifel (1487-1567), S. Stevin (1548-1620)) y por casi ninguno como coeficientes). Estos números se llamaron inicialmente ficticii 'ficticios' (el término imaginario usado actualmente es reminiscente de estas reticencias a considerarlos números respetables). A pesar de esto G. Cardano (1501-1576) conoce la regla de los signos y R. Bombelli (1526-1573) las reglas aditivas a través de haberes y débitos, pero se consideran manipulaciones formales para resolver ecuaciones, sin entidad al no provenir de la medida o el conteo. Cardano en la resolución del problema dividir 10 en dos partes tales que su producto valga 40 obtiene como soluciones  En la resolución de ecuaciones cúbicas con la fórmula de Cardano-Tartaglia, aunque las raíces sean reales, aparecen en los pasos intermedios raíces de números negativos. En esta situación Bombelli dice en su Álgebra que tuvo lo que llamó una idea loca, esta era que los radicales podían tener la misma relación que los radicandos y operar con ellos, tratando de eliminarlos después. En un texto posterior en 20 años utiliza p.d.m.  Generalización de las fracciones decimales Aunque se encuentra un uso más que casual de las fracciones decimales en la Arabia medieval y en la Europa renacentista, y ya en 1579 Vieta (1540-1603) proclamaba su apoyo a éstas frente a las sexagesimales, y las aceptaban los matemáticos que se dedicaban a la investigación, su uso se generalizó con la obra que Simon Stevin publicó en 1585 De Thiende (La Disme). En su definición primera dice que la Disme es una especie de aritmética que permite efectuar todas las cuentas y medidas utilizando únicamente números naturales. En las siguientes define nuestra parte entera: cualquier número que vaya el primero se dice comienzo y su signo es (0), (primera posición decimal 1/10). El siguiente se dice primera y su signo es (1) (segunda posición decimal 1/100). El siguiente se dice segunda (2). Es decir, los números decimales que escribe: 0,375 como 3(1)7(2)5(3), o 372,43 como 372(0)4(1)3(2). Añade que no se utiliza ningún número roto (fracciones), y el número de los signos, exceptuando el 0, no excede nunca a 9. Esta notación la simplificó Joost Bürgi (1552-1632) eliminando la mención al orden de las cifras y sustituyéndolo por un «.» en la parte superior de las unidades 372·43, poco después Magini (1555-1617) usó el «.» entre las unidades y las décimas: 372.43, uso que se generalizaría al aparecer en la Constructio de Napier (1550-1617) de 1619. La «,» también fue usada a comienzos del siglo XVII por el holandés Willebrord Snellius: 372,43. El principio de inducción matemática Artículo principal: Inducción matemática Su antecedente es un método de demostración, llamado inducción completa, por aplicación reiterada de un mismo silogismo que se extiende indefinidamente y que usó Maurolyco (1494-1575) para demostrar que la suma de los primeros  Si  El hecho de que  entonces  De manera intuitiva se entiende la inducción como un efecto dominó. Suponiendo que se tiene una fila infinita de fichas de dominó, el paso base equivale a tirar la primera ficha; por otro lado, el paso inductivo equivale a demostrar que si alguna ficha se cae, entonces la ficha siguiente también se caerá. La conclusión es que se pueden tirar todas las fichas de esa fila. La interpretación geométrica de los números complejos Esta interpretación suele ser atribuida a Gauss (1777-1855) que hizo su tesis doctoral sobre el teorema fundamental del álgebra, enunciado por primera vez por Harriot y Girard en 1631, con intentos de demostración realizados por D’Alembert, Euler y Lagrange, demostrando que las pruebas anteriores eran falsas y dando una demostración correcta primero para el caso de coeficientes, y después de complejos. También trabajó con los números enteros complejos que adoptan la forma  La representación gráfica de los números complejos había sido descubierta ya por Caspar Wessel (1745-1818) pero pasó desapercibida, y así el plano de los números complejos se llama «plano de Gauss» a pesar de no publicar sus ideas hasta 30 años después. Desde la época de Girard (mitad siglo XVII) se conocía que los números reales se pueden representar en correspondencia con los puntos de una recta. Al identificar ahora los complejos con los puntos del plano los matemáticos se sentirán cómodos con estos números, ver es creer. Descubrimiento de los números trascendentes La distinción entre números irracionales algebraicos y trascendentes data del siglo XVIII, en la época en que Euler demostró que  Liouville (1809-1882) demostró en 1844 que todos los números de la forma  Hermite (1822-1901) en una memoria Sobre la función exponencial de 1873 demostró la trascendencia de  Lindeman (1852-1939) en la memoria Sobre el número  El problema 7 de Hilbert (1862-1943) que plantea si  Teorías de los irracionales Hasta mediados del siglo XIX los matemáticos se contentaban con una comprensión intuitiva de los números y sus sencillas propiedades no son establecidas lógicamente hasta el siglo XIX. La introducción del rigor en el análisis puso de manifiesto la falta de claridad y la imprecisión del sistema de los números reales, y exigía su estructuración lógica sobre bases aritméticas. Bolzano había hecho un intento de construir los números reales basándose en sucesiones de números racionales, pero su teoría pasó desapercibida y no se publicó hasta 1962. Hamilton hizo un intento, haciendo referencia a la magnitud tiempo, a partir de particiones de números racionales: si  cuando  y si  cuando  pero no desarrolló más su teoría. Pero en el mismo año 1872 cinco matemáticos, un francés y cuatro alemanes, publicaron sus trabajos sobre la aritmetización de los números reales: Charles Meray (1835-1911) en su obra Nouveau précis d’analyse infinitesimale define el número irracional como un límite de sucesiones de números racionales,[5]​[6] sin tener en cuenta que la existencia misma del límite presupone una definición del número real. Hermann Heine (1821-1881) publicó, en el Journal de Crelle en 1872, su artículo «Los elementos de la teoría de funciones», donde proponía ideas similares a las de Cantor, teoría que en conjunto se llama actualmente «teorema de Heine-Cantor». Richard Dedekind (1831-1916) publica su Stetigkeit und irrationale zahlen. Su idea se basa en la continuidad de la recta real y en los agujeros que hay si solo consideramos los números racionales. En la sección dedicada al «dominio R» enuncia un axioma por el que se establece la continuidad de la recta: «cada punto de la recta divide los puntos de ésta en dos clases tales que cada punto de la primera se encuentra a la izquierda de cada punto de la segunda clase, entonces existe un único punto que produce esta división». Esta misma idea la utiliza en la sección «creación de los números irracionales» para introducir su concepto de «cortadura». Bertrand Russell apuntaría después que es suficiente con una clase, pues esta define a la otra. Georg Cantor (1845-1918). Define los conceptos de: sucesión fundamental, sucesión elemental, y límite de una sucesión fundamental, y partiendo de ellos define el número real. Karl Weierstrass (1815-1897). No llegó a publicar su trabajo, continuación de los de Bolzano, Abel y Cauchy, pero fue conocido por sus enseñanzas en la Universidad de Berlín. Su caracterización basada en los «intervalos encajados», que pueden contraerse a un número racional pero no necesariamente lo hacen, no es tan generalizable como las anteriores, pero proporciona fácil acceso a la representación decimal de los números reales. Álgebras hipercomplejas La construcción de obtención de los números complejos a partir de los números reales, y su conexión con el grupo de transformaciones afines en el plano sugirió a algunos matemáticos otras generalizaciones similares conocidas como números hipercomplejos. En todas estas generalizaciones los números complejos son un subconjunto de estos nuevos sistemas numéricos, aunque estas generalizaciones tienen la estructura matemática de álgebra sobre un cuerpo, pero en ellos la operación de multiplicación no es conmutativa. Teoría de conjuntos Artículo principal: Teoría de conjuntos La teoría de conjuntos sugirió muchas y variadas formas de extender los números naturales y los números reales de formas diferentes a como los números complejos extendían al conjunto de los números reales. El intento de capturar la idea de conjunto con un número no finito de elementos llevó a la aritmética de números transfinitos que generalizan a los naturales, pero no a los números enteros. Los números transfinitos fueron introducidos por Georg Cantor hacia 1873. Los números hiperreales usados en el análisis no estándar generalizan a los reales pero no a los números complejos (aunque admiten una complejificación que generalizaría también a los números complejos). Aunque parece los números hiperreales no proporcionan resultados matemáticos interesantes que vayan más allá de los obtenibles en el análisis real, algunas demostraciones y pruebas matemáticas parecen más simples en el formalismo de los números hiperreales, por lo que no están exentos de importancia práctica. Socialmente Los números naturales por la necesidad de contar. Los números fraccionarios por la necesidad de medir partes de un todo, y compartir. Los enteros negativos por fenómenos de doble sentido: izquierda-derecha, arriba-abajo, pérdida-ganancia. Los números reales por la necesidad de medir segmentos. Los números complejos por exigencias de resolver ecuaciones algebraicas, como el caso de la cúbicas o de x2 + 1 = 0.[7]​ Sistemas de representación de los números Cifra, dígito y numeral Artículo principal: Cifra (matemática) Una de las formas más frecuentes de representar números por escrito consiste en un «conjunto finito de símbolos» o dígitos que, adecuadamente combinados, permiten formar cifras que funcionan como representaciones de números (cuando una secuencia específica de signos se emplea para representar un número se la llama numeral, aunque una cifra también puede representar simplemente un código identificativo). Base numérica Artículo principal: Base (aritmética) Tanto las lenguas naturales como la mayor parte de sistemas de representación de números mediante cifras, usan un inventario finito de unidades para expresar una cantidad mucho mayor de números. Una manera importante de lograr eso es el uso de una base aritmética en esos sistemas un número se expresa en general mediante suma o multiplicación de números. Los sistemas puramente aritméticos recurren a bases donde cada signo recibe una interpretación diferente según su posición. Así en el siguiente numeral arábigo (base 10): 13568 El <8> por estar en última posición representa unidades, el <6> representa decenas, el <5> centenas, el <3> millares y el <1> decenas de millares. Es decir, ese numeral representara el número: 13568 13568 Muchas lenguas del mundo usan una base decimal, igual que el sistema arábigo, aunque también es frecuente que las lenguas usen sistemas vigesimales (base 20). De hecho la idea de usar un número finito de dígitos o signos para representar números arbitrariamente grandes funciona para cualquier base b, donde b es un número entero mayor o igual que 2. Los ordenadores frecuentemente usan para sus operaciones la base binaria (b = 2), y para ciertos usos también se emplea la base octal (b = 8 ) o hexadecimal (b = 16). La base coincide con el número de signos primarios, si un sistema posicional tiene b símbolos primarios que designaremos por  Designará al número: Números en las lenguas naturales Artículo principal: Numeral (lingüística) Las lenguas naturales usan nombres o numerales para los números frecuentemente basados en el contaje mediante dedos, razón por la cual la mayoría de las lenguas usan sistemas de numeración en base 10 (dedos de las manos) o base 20 (dedos de manos y pies), aunque también existen algunos sistemas exóticos que emplean otras bases."

ksampletext_wikipedia_phys_mecanicacuantica: str = "Mecánica cuántica. La mecánica cuántica es la rama de la física que estudia la naturaleza a escalas espaciales pequeñas, los sistemas atómicos, subatómicos, sus interacciones con la radiación electromagnética y otras fuerzas, en términos de cantidades observables. Se basa en la observación de que todas las formas de energía se liberan en unidades discretas o paquetes llamados cuantos. Las partículas con esta propiedad pueden pertenecer a dos tipos distintos: fermiones o bosones. Algunos de estos últimos están ligados a una -interacción fundamental (por ejemplo, el fotón pertenece a la electromagnética). Sorprendentemente, la teoría cuántica solo permite normalmente cálculos probabilísticos o estadísticos de las características observadas de las partículas elementales, entendidos en términos de funciones de onda. La ecuación de Schrödinger desempeña, en la mecánica cuántica, el papel que las leyes de Newton y la conservación de la energía desempeñan en la mecánica clásica. Es decir, la predicción del comportamiento futuro de un sistema dinámico y es una ecuación de onda en términos de una función de onda la que predice analíticamente la probabilidad precisa de los eventos o resultados. En teorías anteriores de la física clásica, la energía era tratada únicamente como un fenómeno continuo, en tanto que la materia se supone que ocupa una región muy concreta del espacio y que se mueve de manera continua. Según la teoría cuántica, la energía se emite y se absorbe en cantidades discretas y minúsculas. Un paquete individual de energía, llamado cuanto, en algunas situaciones se comporta como una partícula de materia. Por otro lado, se encontró que las partículas exponen algunas propiedades ondulatorias cuando están en movimiento y ya no son vistas como localizadas en una región determinada, sino más bien extendidas en cierta medida. La luz u otra radiación emitida o absorbida por un átomo solo tiene ciertas frecuencias (o longitudes de onda), como puede verse en la línea del espectro asociado al elemento químico representado por tal átomo. La teoría cuántica demuestra que tales frecuencias corresponden a niveles definidos de los cuantos de luz, o fotones, y es el resultado del hecho de que los electrones del átomo solo pueden tener ciertos valores de energía permitidos. Cuando un electrón pasa de un nivel permitido a otro, una cantidad de energía es emitida o absorbida, cuya frecuencia es directamente proporcional a la diferencia de energía entre los dos niveles. La mecánica cuántica surge tímidamente en los inicios del siglo XX dentro de las tradiciones más profundas de la física para dar una solución a problemas para los que las teorías conocidas hasta el momento habían agotado su capacidad de explicar, como la llamada catástrofe ultravioleta en la radiación de cuerpo negro predicha por la física estadística clásica y la inestabilidad de los átomos en el modelo atómico de Rutherford. La primera propuesta de un principio propiamente cuántico se debe a Max Planck en 1900, para resolver el problema de la radiación de cuerpo negro, que fue duramente cuestionado, hasta que Albert Einstein lo convierte en el principio que exitosamente pueda explicar el efecto fotoeléctrico. Las primeras formulaciones matemáticas completas de la mecánica cuántica no se alcanzan hasta mediados de la década de 1920, sin que hasta el día de hoy se tenga una interpretación coherente de la teoría, en particular del problema de la medición. El formalismo de la mecánica cuántica se desarrolló durante la década de 1920. En 1924, Louis de Broglie propuso que, al igual que las ondas de luz presentan propiedades de partículas, como ocurre en el efecto fotoeléctrico, las partículas, también presentan propiedades ondulatorias. Dos formulaciones diferentes de la mecánica cuántica se presentaron después de la sugerencia de Broglie. En 1926, la mecánica ondulatoria de Erwin Schrödinger implica la utilización de una entidad matemática, la función de onda, que está relacionada con la probabilidad de encontrar una partícula en un punto dado en el espacio. En 1925, la mecánica matricial de Werner Heisenberg no hace mención alguna de las funciones de onda o conceptos similares, pero ha demostrado ser matemáticamente equivalente a la teoría de Schrödinger. Un descubrimiento importante de la teoría cuántica es el principio de incertidumbre, enunciado por Heisenberg en 1927, que pone un límite teórico absoluto en la precisión de ciertas mediciones. Como resultado de ello, la asunción clásica de los científicos de que el estado físico de un sistema podría medirse exactamente y utilizarse para predecir los estados futuros tuvo que ser abandonada. Esto supuso una revolución filosófica y dio pie a numerosas discusiones entre los más grandes físicos de la época. La mecánica cuántica propiamente dicha no incorpora a la relatividad en su formulación matemática. La parte de la mecánica cuántica que incorpora elementos relativistas de manera formal para abordar diversos problemas se conoce como mecánica cuántica relativista o ya, en forma más correcta y acabada, teoría cuántica de campos (que incluye a su vez a la electrodinámica cuántica, cromodinámica cuántica y teoría electrodébil dentro del modelo estándar)[1] y más generalmente, la teoría cuántica de campos en espacio-tiempo curvo. La única interacción elemental que no se ha podido cuantizar hasta el momento ha sido la interacción gravitatoria. Este problema constituye entonces uno de los mayores desafíos de la física del siglo XXI. La mecánica cuántica se combinó con la teoría de la relatividad en la formulación de Paul Dirac de 1928, lo que, además, predijo la existencia de antipartículas. Otros desarrollos de la teoría incluyen la estadística cuántica, presentada en una forma por Einstein y Bose (la estadística de Bose-Einstein) y en otra forma por Dirac y Enrico Fermi (la estadística de Fermi-Dirac); la electrodinámica cuántica, interesada en la interacción entre partículas cargadas y los campos electromagnéticos, su generalización, la teoría cuántica de campos y la electrónica cuántica. La mecánica cuántica proporciona el fundamento de la fenomenología del átomo, de su núcleo y de las partículas elementales (lo cual requiere necesariamente el enfoque relativista). También su impacto en teoría de la información, criptografía y química ha sido decisivo entre esta misma. Contexto histórico La mecánica cuántica es, cronológicamente hablando, la última de las grandes ramas de la física. Se formuló a principios del siglo XX, casi al mismo tiempo que la teoría de la relatividad, aunque el grueso de la mecánica cuántica se desarrolló a partir de 1920 (siendo la teoría de la relatividad especial de 1905 y la teoría general de la relatividad de 1915). Además al advenimiento de la mecánica cuántica existían diversos problemas no resueltos en la electrodinámica clásica. El primero de estos problemas era la emisión de radiación de cualquier objeto en equilibrio, llamada radiación térmica, que es la que proviene de la vibración microscópica de las partículas que lo componen. Usando las ecuaciones de la electrodinámica clásica, la energía que emitía esta radiación térmica tendía al infinito, si se suman todas las frecuencias que emitía el objeto, con ilógico resultado para los físicos. También la estabilidad de los átomos no podía ser explicada por el electromagnetismo clásico, y la noción de que el electrón fuera o bien una partícula clásica puntual o bien una cáscara esférica de dimensiones finitas resultaban igualmente problemáticas para esto. Radiación electromagnética El problema de la radiación electromagnética de un cuerpo negro fue uno de los primeros problemas resueltos en el seno de la mecánica cuántica. Es en el seno de la mecánica estadística donde surgen por primera vez las ideas cuánticas en 1900. Al físico alemán Max Planck se le ocurrió un artificio matemático: si en el proceso aritmético se sustituía la integral de esas frecuencias por una suma no continua (discreta), se dejaba de obtener infinito como resultado, con lo que se eliminaba el problema; además, el resultado obtenido concordaba con lo que después era medido. Fue Max Planck quien entonces enunció la hipótesis de que la radiación electromagnética es absorbida y emitida por la materia en forma de «cuantos» de luz o fotones de energía cuantizados introduciendo una constante estadística, que se denominó constante de Planck. Su historia es inherente al siglo XX, ya que la primera formulación cuántica de un fenómeno fue dada a conocer por el mismo Planck el 14 de diciembre de 1900 en una sesión de la Sociedad Física de la Academia de Ciencias de Berlín.[2]​ La idea de Planck habría permanecido muchos años solo como hipótesis sin verificar por completo si Albert Einstein no la hubiera retomado, proponiendo que la luz, en ciertas circunstancias, se comporta como partículas de energía (los cuantos de luz o fotones) en su explicación del efecto fotoeléctrico. Fue Albert Einstein quien completó en 1905 las correspondientes leyes del movimiento su teoría especial de la relatividad, demostrando que el electromagnetismo era una teoría esencialmente no mecánica. Culminaba así lo que se ha dado en llamar física clásica, es decir, la física no-cuántica. Usó este punto de vista llamado por él «heurístico», para desarrollar su teoría del efecto fotoeléctrico, publicando esta hipótesis en 1905, lo que le valió el Premio Nobel de Física de 1921. Esta hipótesis fue aplicada también para proponer una teoría sobre el calor específico, es decir, la que resuelve cuál es la cantidad de calor necesaria para aumentar en una unidad la temperatura de la unidad de masa de un cuerpo. El siguiente paso importante se dio hacia 1925, cuando Louis De Broglie propuso que cada partícula material tiene una longitud de onda asociada, inversamente proporcional a su masa, y a su velocidad. Así quedaba establecida la dualidad onda/materia. Poco tiempo después Erwin Schrödinger formuló una ecuación de movimiento para las «ondas de materia», cuya existencia había propuesto De Broglie y varios experimentos sugerían que eran reales. La mecánica cuántica introduce una serie de hechos contraintuitivos que no aparecían en los paradigmas físicos anteriores; con ella se descubre que el mundo atómico no se comporta como esperaríamos. Los conceptos de incertidumbre o cuantización son introducidos por primera vez aquí. Además la mecánica cuántica es la teoría científica que ha proporcionado las predicciones experimentales más exactas hasta el momento, a pesar de estar sujeta a las probabilidades. Inestabilidad de los átomos clásicos El segundo problema importante que la mecánica cuántica resolvió a través del modelo de Bohr, fue el de la estabilidad de los átomos. De acuerdo con la teoría clásica un electrón orbitando alrededor de un núcleo cargado positivamente debería emitir energía electromagnética perdiendo así velocidad hasta caer sobre el núcleo. La evidencia empírica era que esto no sucedía, y sería la mecánica cuántica la que resolvería este hecho primero mediante postulados ad hoc formulados por Bohr y más tarde mediante modelos como el modelo atómico de Schrödinger basados en supuestos más generales. A continuación se explica el fracaso del modelo clásico. En mecánica clásica, un átomo de hidrógeno es un tipo de problema de los dos cuerpos en que el protón sería el primer cuerpo que tiene más del 99% de la masa del sistema y el electrón es el segundo cuerpo que es mucho más ligero. Para resolver el problema de los dos cuerpos es conveniente hacer la descripción del sistema, colocando el origen del sistema de referencia en el centro de masa de la partícula de mayor masa, esta descripción es correcta considerando como masa de la otra partícula la masa reducida que viene dada por 999 Siendo  watt Ese proceso acabaría con el colapso del átomo sobre el núcleo en un tiempo muy corto dadas las grandes aceleraciones existentes. A partir de los datos de la ecuación anterior el tiempo de colapso sería de 10-8 s, es decir, de acuerdo con la física clásica los átomos de hidrógeno no serían estables y no podrían existir más de una cienmillonésima de segundo. Esa incompatibilidad entre las predicciones del modelo clásico y la realidad observada llevó a buscar un modelo que explicara fenomenológicamente el átomo. El modelo atómico de Bohr era un modelo fenomenológico y provisorio que explicaba satisfactoriamente aunque de manera heurística algunos datos, como el orden de magnitud del radio atómico y los espectros de absorción del átomo, pero no explicaba cómo era posible que el electrón no emitiera radiación perdiendo energía. La búsqueda de un modelo más adecuado llevó a la formulación del modelo atómico de Schrödinger en el cual puede probarse que el valor esperado de la aceleración es nulo, y sobre esa base puede decirse que la energía electromagnética emitida debería ser también nula. Sin embargo, al contrario del modelo de Bohr, la representación cuántica de Schrödinger es difícil de entender en términos intuitivos. Desarrollo histórico Artículo principal: Historia de la mecánica cuántica La teoría cuántica fue desarrollada en su forma básica a lo largo de la primera mitad del siglo XX. El hecho de que la energía se intercambie de forma discreta se puso de relieve por hechos experimentales como los siguientes, inexplicables con las herramientas teóricas anteriores de la mecánica clásica o la electrodinámica: Fig. 1: La función de onda del electrón de un átomo de hidrógeno posee niveles de energía definidos y discretos denotados por un número cuántico n=1, 2, 3,... y valores definidos de momento angular caracterizados por la notación: s, p, d,... Las áreas brillantes en la figura corresponden a densidades elevadas de probabilidad de encontrar el electrón en dicha posición. Espectro de la radiación del cuerpo negro, resuelto por Max Planck con la cuantización de la energía. La energía total del cuerpo negro resultó que tomaba valores discretos más que continuos. Este fenómeno se llamó cuantización, y los intervalos posibles más pequeños entre los valores discretos son llamados quanta (singular: quantum, de la palabra latina para «cantidad», de ahí el nombre de mecánica cuántica). La magnitud de un cuanto es un valor fijo llamado constante de Planck, y que vale: 6,626 ×10-34 J·s. Bajo ciertas condiciones experimentales, los objetos microscópicos como los átomos o los electrones exhiben un comportamiento ondulatorio, como en la interferencia. Bajo otras condiciones, las mismas especies de objetos exhiben un comportamiento corpuscular, de partícula, («partícula» quiere decir un objeto que puede ser localizado en una región concreta del espacio), como en la dispersión de partículas. Este fenómeno se conoce como dualidad onda-partícula. Las propiedades físicas de objetos con historias asociadas pueden ser correlacionadas, en una amplitud prohibida para cualquier teoría clásica, solo pueden ser descritos con precisión si se hace referencia a ambos a la vez. Este fenómeno es llamado entrelazamiento cuántico y la desigualdad de Bell describe su diferencia con la correlación ordinaria. Las medidas de las violaciones de la desigualdad de Bell fueron algunas de las mayores comprobaciones de la mecánica cuántica. Explicación del efecto fotoeléctrico, dada por Albert Einstein, en que volvió a aparecer esa misteriosa necesidad de cuantizar la energía. Efecto Compton. El desarrollo formal de la teoría fue obra de los esfuerzos conjuntos de varios físicos y matemáticos de la época como Schrödinger, Heisenberg, Einstein, Dirac, Bohr y Von Neumann entre otros (la lista es larga). Algunos de los aspectos fundamentales de la teoría están siendo aún estudiados activamente. La mecánica cuántica ha sido también adoptada como la teoría subyacente a muchos campos de la física y la química, incluyendo la física de la materia condensada, la química cuántica y la física de partículas. La región de origen de la mecánica cuántica puede localizarse en la Europa central, en Alemania y Austria, y en el contexto histórico del primer tercio del siglo XX. Suposiciones más importantes Artículo principal: Interpretaciones de la mecánica cuántica Las suposiciones más importantes de esta teoría son las siguientes: Al ser imposible fijar a la vez la posición y el momento de una partícula, se renuncia al concepto de trayectoria, vital en mecánica clásica. En vez de eso, el movimiento de una partícula puede ser explicado por una función matemática que asigna, a cada punto del espacio y a cada instante, la probabilidad de que la partícula descrita se halle en tal posición en ese instante (al menos, en la interpretación de la Mecánica cuántica más usual, la probabilista o interpretación de Copenhague). A partir de esa función, o función de ondas, se extraen teóricamente todas las magnitudes del movimiento necesarias. Existen dos tipos de evolución temporal, si no ocurre ninguna medida el estado del sistema o función de onda evolucionan de acuerdo con la ecuación de Schrödinger, sin embargo, si se realiza una medida sobre el sistema, este sufre un «salto cuántico» hacia un estado compatible con los valores de la medida obtenida (formalmente el nuevo estado será una proyección ortogonal del estado original). Existen diferencias notorias entre los estados ligados y los que no lo están. La energía no se intercambia de forma continua en un estado ligado, sino en forma discreta lo cual implica la existencia de paquetes mínimos de energía llamados cuantos, mientras en los estados no ligados la energía se comporta como un continuo. Descripción de la teoría Interpretación de Copenhague Artículo principal: Interpretación de Copenhague Para describir la teoría de forma general es necesario un tratamiento matemático riguroso, pero aceptando una de las tres interpretaciones de la mecánica cuántica (a partir de ahora la Interpretación de Copenhague), el marco se relaja. La mecánica cuántica describe el estado instantáneo de un sistema (estado cuántico) con una función de onda que codifica la distribución de probabilidad de todas las propiedades medibles, u observables. Algunos observables posibles sobre un sistema dado son la energía, posición, momento y momento angular. La mecánica cuántica no asigna valores definidos a los observables, sino que hace predicciones sobre sus distribuciones de probabilidad. Las propiedades ondulatorias de la materia son explicadas por la interferencia de las funciones de onda. Estas funciones de onda pueden variar con el transcurso del tiempo. Esta evolución es determinista si sobre el sistema no se realiza ninguna medida aunque esta evolución es estocástica y se produce mediante colapso de la función de onda cuando se realiza una medida sobre el sistema (Postulado IV de la MC). Por ejemplo, una partícula moviéndose sin interferencia en el espacio vacío puede ser descrita mediante una función de onda que es un paquete de ondas centrado alrededor de alguna posición media. Según pasa el tiempo, el centro del paquete puede trasladarse, cambiar, de modo que la partícula parece estar localizada más precisamente en otro lugar. La evolución temporal determinista de las funciones de onda es descrita por la ecuación de Schrödinger. Algunas funciones de onda describen estados físicos con distribuciones de probabilidad que son constantes en el tiempo, estos estados se llaman estacionarios, son estados propios del operador hamiltoniano y tienen energía bien definida. Muchos sistemas que eran tratados dinámicamente en mecánica clásica son descritos mediante tales funciones de onda estáticas. Por ejemplo, un electrón en un átomo sin excitar se dibuja clásicamente como una partícula que rodea el núcleo, mientras que en mecánica cuántica es descrito por una nube de probabilidad estática que rodea al núcleo. Cuando se realiza una medición en un observable del sistema, la función de ondas se convierte en una del conjunto de las funciones llamadas funciones propias o estados propios del observable en cuestión. Este proceso es conocido como colapso de la función de onda. Las probabilidades relativas de ese colapso sobre alguno de los estados propios posibles son descritas por la función de onda instantánea justo antes de la reducción. Considerando el ejemplo anterior sobre la partícula en el vacío, si se mide la posición de la misma, se obtendrá un valor impredecible x. En general, es imposible predecir con precisión qué valor de x se obtendrá, aunque es probable que se obtenga uno cercano al centro del paquete de ondas, donde la amplitud de la función de onda es grande. Después de que se ha hecho la medida, la función de onda de la partícula colapsa y se reduce a una que esté muy concentrada en torno a la posición observada x. La ecuación de Schrödinger es determinista en el sentido de que, dada una función de onda a un tiempo inicial dado, la ecuación suministra una predicción concreta de qué función tendremos en cualquier tiempo posterior. Durante una medida, el eigen-estado al cual colapsa la función es probabilista y en este aspecto la mecánica cuántica es no determinista. Así que la naturaleza probabilista de la mecánica cuántica nace del acto de la medida. Esto conduce al problema de definir objetivamente en qué momento se produce la medida y la evolución pasa de lineal y determinista, a no-lineal y estocástica/aleatoria, cuestión que se conoce como problema de la medida y que, además de la interpretación de Copenhague, ha dado lugar a un número elevado de propuestas de resolución, conocidas como interpretaciones de la mecánica cuántica. Formulación matemática Artículos principales: Postulados de la mecánica cuántica y Notación braket. En la formulación matemática rigurosa, desarrollada por Dirac y von Neumann, los estados posibles de un sistema cuántico están representados por vectores unitarios (llamados estados) que pertenecen a un Espacio de Hilbert complejo separable (llamado el espacio de estados). Qué tipo de espacio de Hilbert es necesario en cada caso depende del sistema; por ejemplo, el espacio de estados para los estados de posición y momento es el espacio de funciones de cuadrado integrable  Cada magnitud observable queda representada por un operador lineal hermítico definido sobre un dominio denso del espacio de estados. Cada estado propio de un observable corresponde a un eigenvector del operador, y el valor propio o eigenvalor asociado corresponde al valor del observable en aquel estado propio. El espectro de un operador puede ser continuo o discreto. La medida de un observable representado por un operador con espectro discreto solo puede tomar un conjunto numerable de posibles valores, mientras que los operadores con espectro continuo presentan medidas posibles en intervalos reales completos. Durante una medida, la probabilidad de que un sistema colapse a uno de los eigenestados viene dada por el cuadrado del valor absoluto del producto interno entre el estado propio o auto-estado (que podemos conocer teóricamente antes de medir) y el vector estado del sistema antes de la medida. Podemos así encontrar la distribución de probabilidad de un observable en un estado dado computando la descomposición espectral del operador correspondiente. El principio de incertidumbre de Heisenberg se representa por la aseveración de que los operadores correspondientes a ciertos observables no conmutan. Principio de Incertidumbre Una de las consecuencias del formalismo cuántico es el principio de incertidumbre. En su forma más familiar, establece que ninguna medición de una partícula cuántica puede implicar simultáneamente predicciones precisas para la medición de su posición y la medición de su momento.[3]​[4] Tanto posición como momento son observables, esto significa que son representados por operadores hermíticos. El operador posición  Dado un estado cuántico, la regla de Born nos permite encontrar valores para  y de la misma manera para el momento: El principio de incertidumbre establece que En principio, cualquiera de las desviaciones estándar puede hacerse arbitrariamente pequeña, pero no ambas simultáneamente .[5] Esta desigualdad se generaliza a pares arbitrarios de operadores autoadjuntos  y proporciona el límite inferior en el producto de las desviaciones estándar: Otra consecuencia de la relación de conmutación canónica es que los operadores posición y momento son la transformada de Fourier del otro, de modo que una descripción de un objeto según su momento es la transformada de Fourier de su descripción según su posición. El hecho de que la dependencia en cantidad de movimiento sea la transformada de Fourier de la dependencia en posición significa que el operador de cantidad de movimiento es equivalente (hasta un factor de  Aplicaciones En muchos aspectos, la tecnología moderna opera a una escala en la que los efectos cuánticos son significativos. Las aplicaciones importantes de la teoría cuántica incluyen la química cuántica, la óptica cuántica, la computación cuántica, los imanes superconductores, los diodos emisores de luz, el amplificador óptico y el láser, el transistor y semiconductores como el microprocesador, imágenes médicas y de investigación como la resonancia magnética y el microscopio electrónico.[6] Las explicaciones de muchos fenómenos biológicos y físicos tienen su origen en la naturaleza del enlace químico, sobre todo la macromolécula del ADN. Relatividad y la mecánica cuántica Artículos principales: Teoría cuántica de campos y Segunda cuantización. El mundo moderno de la física se funda notablemente en dos teorías principales, la relatividad general y la mecánica cuántica, aunque ambas teorías usan principios aparentemente incompatibles. Los postulados que definen la teoría de la relatividad de Einstein y la teoría del quántum están apoyados por rigurosa y repetida evidencia empírica. Sin embargo, ambas se resisten a ser incorporadas dentro de un mismo modelo coherente. Desde mediados del siglo XX, aparecieron teorías cuánticas relativistas del campo electromagnético (electrodinámica cuántica) y las fuerzas nucleares (modelo electrodébil, cromodinámica cuántica), pero no se tiene una teoría cuántica relativista del campo gravitatorio que sea plenamente consistente y válida para campos gravitatorios intensos (existen aproximaciones en espacios asintóticamente planos). Todas las teorías cuánticas relativistas consistentes usan los métodos de la teoría cuántica de campos. En su forma ordinaria, la teoría cuántica abandona algunos de los supuestos básicos de la teoría de la relatividad, como por ejemplo el principio de localidad usado en la descripción relativista de la causalidad. El mismo Einstein había considerado absurda la violación del principio de localidad a la que parecía abocar la mecánica cuántica. La postura de Einstein fue postular que la mecánica cuántica si bien era consistente era incompleta. Para justificar su argumento y su rechazo a la falta de localidad y la falta de determinismo, Einstein y varios de sus colaboradores postularon la llamada paradoja de Einstein-Podolsky-Rosen (EPR), la cual demuestra que medir el estado de una partícula puede instantáneamente cambiar el estado de su socio enlazado, aunque las dos partículas pueden estar a una distancia arbitrariamente grande. Modernamente el paradójico resultado de la paradoja EPR se sabe es una consecuencia perfectamente consistente del llamado entrelazamiento cuántico. Es un hecho conocido que si bien la existencia del entrelazamiento cuántico efectivamente viola el principio de localidad, en cambio no viola la causalidad definido en términos de información, puesto que no hay transferencia posible de información. Si bien en su tiempo, parecía que la paradoja EPR suponía una dificultad empírica para la mecánica cuántica, y Einstein consideró que la mecánica cuántica en la interpretación de Copenhague podría ser descartada por experimento, décadas más tarde los experimentos de Alain Aspect (1981) revelaron que efectivamente la evidencia experimental parece apuntar en contra del principio de localidad.[7] Y por tanto, el resultado paradójico que Einstein rechazaba como «sin sentido» parece ser lo que sucede precisamente en el mundo real."
ksampletext_wikipedia_phys_teoriadecuerdas: str = "Teoría de cuerdas. Las teorías de cuerdas son una serie de hipótesis científicas y modelos fundamentales de física teórica que asumen que las partículas subatómicas, aparentemente puntuales, son en realidad estados vibracionales de un objeto extendido más básico llamado cuerda o filamento.[1]​ De acuerdo con estas teorías, un electrón no sería un punto sin estructura interna y de dimensión cero, sino una cuerda minúscula en forma de lazo vibrando en un espacio-tiempo de más de cuatro dimensiones; de hecho, el planteamiento matemático de esta teoría no funciona a menos que el universo tenga diez dimensiones. Mientras que un punto simplemente se movería por el espacio, una cuerda podría hacer algo más: vibrar de diferentes maneras. Si vibrase de cierto modo, veríamos un electrón; pero si lo hiciese de otro, veríamos un fotón, un cuark o cualquier otra partícula del modelo estándar dependiendo de la forma concreta en que estuviese vibrando. Estas teorías, ampliadas con otras como la de las supercuerdas o la teoría M, pretende alejarse de la concepción del punto-partícula. La siguiente formulación de una teoría de cuerdas se debe a Jöel Scherk y John Henry Schwarz, que en 1974 publicaron un artículo en el que mostraban que una teoría basada en objetos unidimensionales o cuerdas en lugar de partículas puntuales podía describir la fuerza gravitatoria, aunque estas ideas no recibieron en ese momento mucha atención hasta la primera revolución de supercuerdas de 1984. De acuerdo con la formulación de la teoría de cuerdas surgida de esta revolución, las teorías de cuerdas pueden considerarse de hecho un caso general de teoría de Kaluza-Klein cuantizada. Las ideas fundamentales son dos: Los objetos básicos de la teoría no serían partículas puntuales, sino objetos unidimensionales extendidos (en las cinco teorías de supercuerdas convencionales estos objetos eran unidimensionales o cuerdas; actualmente en la teoría-M se admiten también de dimensión superior o p-branas). Esto renormaliza algunos infinitos de los cálculos perturbativos. El espacio-tiempo en el que se mueven las cuerdas y p-branas de la teoría no sería el espacio-tiempo ordinario de cuatro dimensiones, sino un espacio de tipo Kaluza-Klein, en el que a las cuatro dimensiones convencionales se añaden seis dimensiones compactadas en forma de variedad de Calabi-Yau. Por tanto convencionalmente en la teoría de cuerdas existe una dimensión temporal, tres dimensiones espaciales ordinarias y seis dimensiones compactadas e inobservables en la práctica. La inobservabilidad de las dimensiones adicionales está relacionada con el hecho de que estas estarían compactadas, y solo serían relevantes a escalas pequeñas comparables con la longitud de Planck. Igualmente, con la precisión de medida convencional las cuerdas cerradas con una longitud similar a la longitud de Planck se asemejarían a partículas puntuales. Desarrollos posteriores Tras la introducción de la teoría de cuerdas, se consideró la conveniencia de introducir el principio de que la teoría fuera supersimétrica; es decir, que admitiera una simetría abstracta que relacionara fermiones y bosones. Actualmente la mayoría de teóricos de cuerdas trabajan en teorías supersimétricas; de ahí que la teoría de cuerdas actualmente se llame teoría de supercuerdas. Esta última teoría es básicamente una teoría de cuerdas supersimétrica; es decir, que es invariante bajo transformaciones de supersimetría. Actualmente existen cinco teorías de supercuerdas relacionadas con los cinco modos que se conocen de implementar la supersimetría en el modelo de cuerdas. Aunque dicha multiplicidad de teorías desconcertó a los especialistas durante más de una década, el saber convencional actual sugiere que las cinco teorías son casos límites de una teoría única sobre un espacio de 10 dimensiones (las tres del espacio y una temporal serían las 4 que ya conocemos más otras seis adicionales resabiadas o compactadas) y una que las engloba formando membranas de las cuales se podría escapar parte de la gravedad de ellas en forma de gravitones. Esta teoría única, llamada teoría M, de la que solo se conocerían algunos aspectos, fue conjeturada en 1995. Variantes de la teoría La teoría de supercuerdas es algo actual. En sus principios (mediados de los años 1980) aparecieron unas cinco teorías de cuerdas, las cuales después fueron identificadas como límites particulares de una sola teoría: la teoría M. Las cinco versiones de la teoría actualmente existentes, entre las que pueden establecerse varias relaciones de dualidad, son: La Teoría de cuerdas de Tipo I, donde aparecen tanto cuerdas y D-branas abiertas como cerradas, que se mueven sobre un espacio-tiempo de diez dimensiones. Las D-branas tienen una, cinco y nueve dimensiones espaciales. La Teoría de cuerdas de Tipo IIA. Es también una teoría de diez dimensiones, pero que emplea solo cuerdas y D-branas cerradas. Incorpora los gravitinos (partículas teóricas asociadas al gravitón mediante relaciones de supersimetría). Usa D-branas de dimensión 0, 2, 4, 6 y 8. La Teoría de cuerdas de Tipo IIB. Difiere de la teoría de tipo IIA principalmente en el hecho de que esta última es no quiral (conservando la paridad). La Teoría de cuerda heterótica SO(32) (Heterótica-O), basada en el grupo de simetría O(32). La Teoría de cuerda heterótica E8xE8 (Heterótica-E), basada en el grupo de Lie excepcional E8. Fue propuesta en 1987 por Gross, Harvey, Martinec y Rohm. El término teoría de cuerdas se refiere en realidad a las teorías de cuerdas bosónicas de 26 dimensiones y la teoría de supercuerdas de diez dimensiones, esta última descubierta al añadir supersimetría a la teoría de cuerdas bosónica. Hoy en día la teoría de cuerdas se suele referir a la variante supersimétrica, mientras que la antigua se conoce por el nombre completo de teoría de cuerdas bosónicas. En 1995, Edward Witten conjeturó que las cinco diferentes teorías de supercuerdas son casos límite de una desconocida teoría de once dimensiones llamada teoría-M. La conferencia donde Witten mostró algunos de sus resultados inició la llamada segunda revolución de supercuerdas. En esta teoría M intervienen como objetos animados físicos fundamentales no solo cuerdas unidimensionales, sino toda una variedad de objetos no perturbativos, extendidos en varias dimensiones, que se llaman colectivamente p-branas (este nombre es una aféresis de membrana). Controversia sobre la teoría Aunque la teoría de cuerdas, según sus defensores, pudiera llegar a convertirse en una de las teorías físicas más predictivas, capaz de explicar algunas de las propiedades más fundamentales de la naturaleza en términos geométricos, los físicos que han trabajado en ese campo hasta la fecha no han podido hacer predicciones concretas con la precisión necesaria para confrontarlas con datos experimentales, al parecer se necesita de una tecnología más avanzada para visualizar las partículas, lo cual llevara muchos años. Dichos problemas de predicción se deberían, según el autor, a que el modelo no es falsable, y por tanto, no es científico,[2] o bien a que «la teoría de las supercuerdas es tan ambiciosa que solo puede ser del todo correcta o del todo equivocada. El único problema es que sus matemáticas son tan nuevas y tan difíciles que durante varias décadas no sabremos cuáles son»,[3] dicho esto en 1990. D. Gross, premio Nobel de física por su trabajo en el modelo estándar, se convirtió en un formidable luchador de la teoría de cuerdas, pero recientemente ha dicho: «No sabemos de qué estamos hablando».[4]​ Si los teóricos de cuerdas se equivocan, no pueden equivocarse solo un poco. Si las nuevas dimensiones y las simetrías no existen, consideraremos a los teóricos de cuerdas unos de los mayores fracasados de la ciencia (...). Su historia constituirá una leyenda moral de cómo no hacer ciencia, de cómo no permitir que se sobrepasen tanto los límites, hasta el punto de convertir la conjetura teórica en fantasía. Lee Smolin[5]​ Otras teorías En 1997, el físico teórico argentino Juan Maldacena propuso un sorprendente modelo del universo según el cual la gravedad surge de cuerdas infinitesimales, delgadas y vibrantes y puede ser reinterpretada en términos físicos. Así, este mundo de cuerdas matemáticamente intrincado, que existe en diez dimensiones espaciales, no sería más que un holograma: la acción real se desarrollaría en un cosmos plano, más simple y en el que no hay gravedad. La idea de Maldacena entusiasmó a los físicos, entre otras razones porque resolvía aparentes inconsistencias entre la física cuántica y la teoría de la gravedad de Einstein. Así, el argentino proporcionó a los científicos una 'piedra Rosetta matemática', una 'dualidad', que les permitía resolver los problemas de un modelo que parecían no tener respuesta en el otro, y viceversa. Pero a pesar de la validez de sus ideas aún no se había logrado hallar ninguna prueba rigurosa de su teoría. Según un artículo publicado en la revista científica Nature, Yoshifumi Hyakutake, de la Universidad de Ibaraki (Japón), y sus colegas, proporcionaron en dos de sus estudios, sino una prueba real, al menos una muestra convincente de que la conjetura de Maldacena es cierta. En uno de los estudios, Hyakutake calculó la energía interna de un agujero negro, la posición de su horizonte de sucesos (el límite entre el agujero negro y el resto del universo), su entropía y otras propiedades a partir de las predicciones de la teoría de cuerdas y de los efectos asociados a las 'partículas virtuales', que aparecen continuamente dentro y fuera de la existencia. En el otro, él y sus colaboradores calcularon la energía interna del correspondiente universo de dimensión inferior sin gravedad. Los dos cálculos informáticos coinciden. Parece que es un cálculo correcto, dice Maldacena, al tiempo que subraya que los hallazgos son una forma interesante de demostrar muchas ideas de la gravedad cuántica y la teoría de cuerdas. Numéricamente han confirmado, tal vez por primera vez, algo de lo que estábamos bastante seguros pero era todavía una conjetura: que la termodinámica de ciertos agujeros negros puede ser reproducida desde un universo dimensional inferior, explica Leonard Susskind, físico teórico de la Universidad de Stanford, en California, quien fue uno de los primeros teóricos en explorar la idea de universos holográficos. Falsacionismo y teoría de cuerdas Artículo principal: Criterio de demarcación La teoría de cuerdas o la teoría M podrían no ser falsables, según sus críticos.[6]​[7]​[8]​[9]​[10] Diversos autores han declarado su preocupación de que la teoría de cuerdas no sea falsable y como tal, siguiendo las tesis del filósofo de la ciencia Karl Popper, la teoría de cuerdas sería equivalente a una pseudociencia.[11]​[12]​[13]​[14]​[15]​[16]​ El filósofo de la ciencia Mario Bunge ha manifestado lo siguiente: La consistencia, la sofisticación y la belleza nunca son suficientes en la investigación científica. La teoría de cuerdas es sospechosa (de pseudociencia). Parece científica porque aborda un problema abierto que es a la vez importante y difícil, el de construir una teoría cuántica de la gravitación. Pero la teoría postula que el espacio físico tiene seis o siete dimensiones, en lugar de tres, simplemente para asegurarse consistencia matemática. Puesto que estas dimensiones extra son inobservables, y puesto que la teoría se ha resistido a la confirmación experimental durante más de tres décadas, parece ciencia ficción, o al menos, ciencia fallida. La física de partículas está inflada con sofisticadas teorías matemáticas que postulan la existencia de entidades extrañas que no interactúan de forma apreciable, o para nada en absoluto, con la materia ordinaria, y como consecuencia, quedan a salvo al ser indetectables. Puesto que estas teorías se encuentran en discrepancia con el conjunto de la Física, y violan el requerimiento de falsacionismo, pueden calificarse de pseudocientíficas, incluso aunque lleven pululando un cuarto de siglo y se sigan publicando en las revistas científicas más prestigiosas. Mario Bunge, 2006.[10]​ Impacto de la promoción de la teoría en el mundo académico Smolin indica que la teoría de cuerdas se ha convertido en el principal camino de exploración de «las grandes cuestiones de la física» debido a una agresiva promoción, considerando que resulta prácticamente un «suicidio profesional» para cualquier joven físico teórico no ingresar en sus filas. Expone además que «a pesar de la escasa inversión en [...] otros campos de investigación, algunos de ellos han avanzado más que el de la teoría de cuerdas» e identifica los siguientes rasgos en las «comunidades de supercuerdas»:[5]​ Tremenda autosuficiencia y conciencia de pertenecer a una élite. Comunidades monolíticas con gran uniformidad de opiniones sobre cuestiones abiertas, generalmente impuestas por los que constituyen la jerarquía de la comunidad. Sentido de identificación con el grupo parecido a la pertenencia a una comunidad religiosa o partido político. Sentido de frontera entre el grupo y otros expertos. Gran desinterés por las ideas y personas que no son del grupo. Una confianza excesiva en interpretar positivamente los resultados e incluso aceptarlos exclusivamente porque son creídos por la mayoría. Una falta de percepción del riesgo que conlleva una nueva teoría."
ksampletext_wikipedia_phys_electromagnetismo: str = "Electromagnetismo. El electromagnetismo es la rama de la física que estudia y unifica los fenómenos eléctricos y magnéticos en una sola teoría. El electromagnetismo describe la interacción de partículas cargadas con campos eléctricos y magnéticos. La interacción electromagnética es una de las cuatro fuerzas fundamentales del universo conocido. El electromagnetismo es una rama de la física que estudia los efectos producidos por el magnetismo, lo cual surge a partir de la corriente eléctrica. Por su parte, el magnetismo es la disciplina que examina los fenómenos asociados a los imanes. Su nombre proviene de Magnesia, un distrito en Asia Menor (actual Turquía), donde se descubrieron por primera vez las piedras llamadas magnetitas, que tienen la capacidad de atraer ciertos metales.[1]​ El electromagnetismo abarca diversos fenómenos del mundo real, como por ejemplo la luz. La luz es un campo electromagnético oscilante que se irradia desde partículas cargadas aceleradas. Aparte de la gravedad, la mayoría de las fuerzas en la experiencia cotidiana son consecuencia del electromagnetismo. Los principios del electromagnetismo encuentran aplicaciones en diversas disciplinas afines, tales como las microondas, antenas, máquinas eléctricas, comunicaciones por satélite, bioelectromagnetismo, plasmas, investigación nuclear, la fibra óptica, la interferencia y la compatibilidad electromagnéticas, la conversión de energía electromecánica, la meteorología por radar, y la observación remota. Los dispositivos electromagnéticos incluyen transformadores, relés, radio/TV, teléfonos, motores eléctricos, líneas de transmisión, guías de onda y láseres. Los fundamentos de la teoría electromagnética fueron presentados por Michael Faraday y formulados por primera vez de modo completo por James Clerk Maxwell en 1865. La formulación consiste en cuatro ecuaciones diferenciales vectoriales que relacionan el campo eléctrico, el campo magnético y sus respectivas fuentes materiales (corriente eléctrica, polarización eléctrica y polarización magnética), conocidas como ecuaciones de Maxwell, lo que ha sido considerada como la «segunda gran unificación de la física», siendo la primera realizada por Isaac Newton. El estudio de los campos electromagnéticos se puede dividir en electrostática —el estudio de las interacciones entre cargas en reposo— y la electrodinámica —el estudio de las interacciones entre cargas en movimiento y la radiación—. La teoría clásica del electromagnetismo se basa en la fuerza de Lorentz y en las ecuaciones de Maxwell. Muchas propiedades ópticas y físicas de la materia también son explicados por la teoría electromagnética. El electromagnetismo es una teoría de campos; es decir, las explicaciones y predicciones que provee se basan en magnitudes físicas vectoriales o tensoriales dependientes de la posición en el espacio y del tiempo. El electromagnetismo describe los fenómenos físicos macroscópicos en los cuales intervienen cargas eléctricas en reposo y en movimiento, usando para ello campos eléctricos y magnéticos y sus efectos sobre las sustancias sólidas, líquidas y gaseosas. Por ser una teoría macroscópica, es decir, aplicable a un número muy grande de partículas y a distancias grandes respecto de las dimensiones de estas, el electromagnetismo no describe los fenómenos atómicos y moleculares. La electrodinámica cuántica proporciona la descripción cuántica de esta interacción, que puede ser unificada con la interacción nuclear débil según el modelo electrodébil. Espectro electromagnético. Historia Esta sección es un extracto de Historia del electromagnetismo.[editar] El físico danés Hans Christian Ørsted, realizando el experimento que le permitió descubrir la relación entre la electricidad y el magnetismo en 1820. La historia del electromagnetismo, considerada como el conocimiento y el uso registrado de las fuerzas electromagnéticas, data de hace más de dos mil años. En la antigüedad ya estaban familiarizados con los efectos de la electricidad atmosférica, en particular del rayo[2] ya que las tormentas son comunes en las latitudes más meridionales, ya que también se conocía el fuego de San Telmo. Sin embargo, se comprendía poco la electricidad y no eran capaces de producir estos fenómenos.[3]​[4]​ Durante los siglos XVII y XVIII, William Gilbert, Otto von Guericke, Stephen Gray, Benjamin Franklin, Alessandro Volta entre otros investigaron estos dos fenómenos de manera separada y llegaron a conclusiones coherentes con sus experimentos. A principios del siglo XIX, Hans Christian Ørsted encontró evidencia empírica de que los fenómenos magnéticos y eléctricos estaban relacionados. De ahí es que los trabajos de físicos como André-Marie Ampère, William Sturgeon, Joseph Henry, Georg Simon Ohm, Michael Faraday en ese siglo, son unificados por James Clerk Maxwell en 1861 con un conjunto de ecuaciones que describían ambos fenómenos como uno solo, como un fenómeno electromagnético.[4]​ Las ahora llamadas ecuaciones de Maxwell demostraban que los campos eléctricos y los campos magnéticos eran manifestaciones de un solo campo electromagnético. Además, describía la naturaleza ondulatoria de la luz, mostrándola como una onda electromagnética.[5] Con una sola teoría consistente que describía estos dos fenómenos antes separados, los físicos pudieron realizar varios experimentos prodigiosos e inventos muy útiles como la bombilla eléctrica por Thomas Alva Edison o el generador de corriente alterna por Nikola Tesla.[6] El éxito predictivo de la teoría de Maxwell y la búsqueda de una interpretación coherente de sus implicaciones, fue lo que llevó a Albert Einstein a formular su teoría de la relatividad que se apoyaba en algunos resultados previos de Hendrik Antoon Lorentz y Henri Poincaré. En la primera mitad del siglo XX, con el advenimiento de la mecánica cuántica, el electromagnetismo tuvo que mejorar su formulación para que fuera coherente con la nueva teoría. Esto se logró en la década de 1940 cuando se completó una teoría cuántica electromagnética conocida como electrodinámica cuántica Historia de la teoría Hans Christian Oersted Originalmente, la electricidad y el magnetismo se consideraban dos fenómenos independientes entre sí. Este punto de vista cambió, sin embargo, con la publicación en 1873 del Tratado de electricidad y magnetismo de James Maxwell , que mostró que la interacción de cargas positivas y negativas está gobernada por una sola fuerza. Hay cuatro efectos principales, resultantes de estas interacciones, que han sido claramente demostrados por experimentos: Las cargas eléctricas son atraídas o repelidas entre sí con una fuerza inversamente proporcional al cuadrado de la distancia entre ellas: las cargas diferentes se atraen, las cargas iguales se repelen. Los polos magnéticos (o estados de polarización en puntos separados) se atraen o repelen entre sí de manera similar y siempre van en pares: cada polo norte no existe por separado del polo sur. La corriente eléctrica en un cable crea un campo magnético circular alrededor del cable, dirigido (en sentido horario o antihorario) según el flujo de corriente. Se induce una corriente en el bucle del cable cuando se acerca o aleja con relación al campo magnético, o cuando el imán se acerca o aleja del bucle del cable; la dirección de la corriente depende de la dirección de estos movimientos. André-Marie Ampere En preparación para la conferencia, la noche del 21 de abril de 1820, Hans Christian Oersted hizo una observación asombrosa. Cuando estaba compilando el material, notó que la aguja de la brújula se desviaba del polo norte magnético cuando se encendía y apagaba la corriente eléctrica de la batería que estaba usando. Esta desviación lo llevó a creer que los campos magnéticos emanan de todos los lados de un cable a través del cual fluye una corriente eléctrica, al igual que la luz y el calor se propagan en el espacio, y esa experiencia indica una conexión directa entre la electricidad y el magnetismo. Michael Faraday En el momento del descubrimiento, Oersted no ofreció una explicación satisfactoria de este fenómeno y no intentó presentar el fenómeno en cálculos matemáticos. Sin embargo, tres meses después, comenzó a realizar investigaciones más intensivas. Poco después, publicó los resultados de su investigación, demostrando que una corriente eléctrica crea un campo magnético cuando fluye a través de cables. En el sistema CGS , la unidad de inducción electromagnética, Oe, recibió su nombre de su contribución al campo del electromagnetismo. James Clerk Maxwell Las conclusiones de Oersted llevaron a un estudio intensivo de electrodinámica por parte de la comunidad científica mundial. Las obras de Dominique François Arago también se remontan a 1820 , quien advirtió que un cable por el que fluye una corriente eléctrica atrae limaduras de hierro . También magnetizó por primera vez alambres de hierro y acero, colocándolos dentro de una bobina de alambres de cobre por donde pasaba la corriente. También logró magnetizar la aguja colocándola en una bobina y descargando la Botella de Leyden a través de la bobina. Independientemente de Arago, Davy descubrió la magnetización del acero y el hierro por la corriente . Las primeras definiciones cuantitativas de la acción de una corriente sobre un imán de la misma forma se remontan a 1820 y pertenecen a científicos franceses Jean-Baptiste Biot y Félix Savart.[7] Los experimentos de Oersted también influyeron en el físico francés André-Marie Ampere , quien presentó la ley electromagnética entre un conductor y una corriente en forma matemática. El descubrimiento de Oersted también representa un paso importante hacia un concepto de campo unificado. Esta unidad, que fue descubierta por Michael Faraday , completada por James Clerk Maxwell , y también refinada por Oliver Heaviside y Heinrich Hertz, es uno de los logros clave del siglo XIX en física matemática . Este descubrimiento tuvo implicaciones de gran alcance, una de las cuales fue comprender la naturaleza de la luz. La luz y otras ondas electromagnéticas toman la forma de fenómenos oscilatorios autopropagantes cuantificados del campo electromagnético llamados fotones. Diferentes frecuencias de vibración conducen a diferentes formas de radiación electromagnética: desde ondas de radio a bajas frecuencias, a luz visible a frecuencias medias, a rayos gamma a altas frecuencias. Oersted no fue la única persona que descubrió la conexión entre la electricidad y el magnetismo. En 1802, Giovanni Domenico Romagnosi , un jurista italiano, desvió una aguja magnética con descargas electrostáticas. Pero, de hecho, la investigación de Romagnosi no utilizó una celda galvánica y no había corriente continua como tal. El informe del descubrimiento se publicó en 1802 en un periódico italiano, pero la comunidad científica apenas lo notó en ese momento.[8]​ Ramas Electrostática Artículo principal: Electrostática La electrostática es el estudio de los fenómenos asociados a los cuerpos cargados en reposo. Como describe la ley de Coulomb, estos cuerpos ejercen fuerzas entre sí. Su comportamiento se puede analizar en términos de la idea de un campo eléctrico que rodea cualquier cuerpo cargado, de manera que otro cuerpo cargado colocado dentro del campo estará sujeto a una fuerza proporcional a la magnitud de su carga y de la magnitud del campo en su ubicación. El que la fuerza sea atractiva o repulsiva depende de la polaridad de la carga. La electrostática tiene muchas aplicaciones, que van desde el análisis de fenómenos como tormentas eléctricas hasta el estudio del comportamiento de los tubos electrónicos. Un electroscopio usado para medir la carga eléctrica de un objeto. Cuando hablamos de electrostática nos referimos a los fenómenos que ocurren debido a una propiedad intrínseca y discreta de la materia, la carga, cuando es estacionaria o no depende del tiempo. La unidad de carga elemental, es decir, la más pequeña observable, es la carga que tiene el electrón.[9] Se dice que un cuerpo está cargado eléctricamente cuando tiene exceso o falta de electrones en los átomos que lo componen. Por definición el defecto de electrones se la denomina carga positiva y al exceso carga negativa.[10] La relación entre los dos tipos de carga es de atracción cuando son diferentes y de repulsión cuando son iguales. La carga elemental es una unidad muy pequeña para cálculos prácticos, por eso en el Sistema Internacional la unidad de carga eléctrica, el culombio, se define como la cantidad de carga transportada en un segundo por una corriente de un amperio de intensidad de corriente eléctrica. que equivale a la carga de 6,25 x 1018 electrones.[9] El movimiento de electrones por un conductor se denomina corriente eléctrica y la cantidad de carga eléctrica que pasa por unidad de tiempo se define como la intensidad de corriente. Se pueden introducir más conceptos como el de diferencia de potencial o el de resistencia, que nos conducirían ineludiblemente al área de circuitos eléctricos, y todo eso se puede ver con más detalle en el artículo principal. El nombre de la unidad de carga se debe a Coulomb, quien en 1785 llegó a una relación matemática de la fuerza eléctrica entre cargas puntuales, que ahora se la conoce como ley de Coulomb: Entre dos cargas puntuales  Las cargas elementales al no encontrarse solas se las debe tratar como una distribución de ellas. Por eso debe implementarse el concepto de campo, definido como una región del espacio donde existe una magnitud escalar o vectorial dependiente o independiente del tiempo. Así el campo eléctrico  Campo eléctrico de cargas puntuales. lim Y así finalmente llegamos a la expresión matemática que define el campo eléctrico: Es importante conocer el alcance de este concepto de campo eléctrico: nos brinda la oportunidad de conocer cuál es su intensidad y qué ocurre con una carga en cualquier parte de dicho campo sin importar el conocimiento de qué lo provoca.[11]​ Una forma de obtener qué cantidad de fuerza eléctrica pasa por cierto punto o superficie del campo eléctrico es usar el concepto de flujo eléctrico. Este flujo eléctrico  El matemático y físico, Carl Friedrich Gauss, demostró que la cantidad de flujo eléctrico en un campo es igual al cociente entre la carga encerrada por la superficie en la que se calcula el flujo,  (1) Véanse también: Carga eléctrica, Ley de Coulomb, Campo eléctrico, Potencial eléctrico y Ley de Gauss. Magnetostática Artículo principal: Magnetostática Líneas de fuerza de una barra magnética. La magnetósfera de la Tierra, empujada por el viento solar. No fue sino hasta el año de 1820, cuando Hans Christian Ørsted descubrió que el fenómeno magnético estaba ligado al eléctrico, que se obtuvo una teoría científica para el magnetismo.[12] La presencia de una corriente eléctrica, o sea, de un flujo de carga debido a una diferencia de potencial, genera una fuerza magnética que no varía en el tiempo. Si tenemos una carga q a una velocidad  Para determinar el valor de ese campo magnético, Jean Baptiste Biot en 1820,[13] dedujo una relación para corrientes estacionarias, ahora conocida como ley de Biot-Savart: Donde  (2) Además en la magnetostática existe una ley comparable a la de Gauss en la electrostática, la ley de Ampère. Esta ley nos dice que la circulación en un campo magnético es igual a la densidad de corriente que exista en una superficie cerrada: Cabe indicar que esta ley de Gauss es una generalización de la ley de Biot-Savart. Además que las fórmulas expresadas aquí son para cargas en el vacío, para más información consúltese los artículos principales. Véanse también: Ley de Ampère, Corriente eléctrica, Campo magnético, Ley de Biot-Savart y Momento magnético dipolar. Electrodinámica clásica Artículo principal: Electrodinámica La electrodinámica es el estudio de los fenómenos asociados a los cuerpos cargados en movimiento y a los campos eléctricos y magnéticos variables. Dado que una carga en movimiento produce un campo magnético, la electrodinámica se refiere a efectos tales como el magnetismo, la radiación electromagnética, y la inducción electromagnética, incluyendo las aplicaciones prácticas, tales como el generador eléctrico y el motor eléctrico. Esta área de la electrodinámica, conocida como electrodinámica clásica, fue sistemáticamente explicada por James Clerk Maxwell, y las ecuaciones de Maxwell describen los fenómenos de esta área con gran generalidad. Una novedad desarrollada más reciente es la electrodinámica cuántica, que incorpora las leyes de la teoría cuántica a fin de explicar la interacción de la radiación electromagnética con la materia. Paul Dirac, Heisenberg y Wolfgang Pauli fueron pioneros en la formulación de la electrodinámica cuántica. La electrodinámica es inherentemente relativista y da unas correcciones que se introducen en la descripción de los movimientos de las partículas cargadas cuando sus velocidades se acercan a la velocidad de la luz. Se aplica a los fenómenos involucrados con aceleradores de partículas y con tubos electrónicos funcionando a altas tensiones y corrientes. En las secciones anteriores se han descrito campos eléctricos y magnéticos que no variaban con el tiempo. Pero los físicos a finales del siglo XIX descubrieron que ambos campos estaban ligados y así un campo eléctrico en movimiento, una corriente eléctrica que varíe, genera un campo magnético y un campo magnético de por sí implica la presencia de un campo eléctrico. Entonces, lo primero que debemos definir es la fuerza que tendría una partícula cargada que se mueva en un campo magnético y así llegamos a la unión de las dos fuerzas anteriores, lo que hoy conocemos como la fuerza de Lorentz: (3) Entre 1890 y 1900 Liénard y Wiechert calcularon el campo electromagnético asociado a cargas en movimiento arbitrario, resultado que se conoce hoy como potenciales de Liénard-Wiechert. Por otro lado, para generar una corriente eléctrica en un circuito cerrado debe existir una diferencia de potencial entre dos puntos del circuito, a esta diferencia de potencial se la conoce como fuerza electromotriz o «fem». Esta fuerza electromotriz es proporcional a la rapidez con que el flujo magnético varía en el tiempo, esta ley fue encontrada por Michael Faraday y es la interpretación de la inducción electromagnética, así un campo magnético que varía en el tiempo induce a un campo eléctrico, a una fuerza electromotriz. Matemáticamente se representa como: (4) El físico James Clerk Maxwell de 1861 relacionó las anteriormente citadas ecuaciones para la ley de Gauss ((1)), ley de Gauss para el campo magnético ((2)), ley de Faraday ((4)) e introdujo el concepto de una corriente de desplazamiento como una densidad de corriente efectiva para llegar a la ley de Ampère generalizada (5): (5) Las cuatro ecuaciones, tanto en su forma diferencial como en la integral aquí descritas, son fruto de la reformulación del trabajo de Maxwell realizada por Oliver Heaviside y Heinrich Rudolf Hertz. Pero el verdadero poder de estas ecuaciones, más la fuerza de Lorentz (3), se centra en que juntas son capaces de describir cualquier fenómeno electromagnético, además de las consecuencias físicas que posteriormente se describirán.[14]​ Esquema de una onda electromagnética. La genialidad del trabajo de Maxwell es que sus ecuaciones describen un campo eléctrico que va ligado inequívocamente a un campo magnético perpendicular a este y a la dirección de su propagación, este campo es ahora llamado campo electromagnético. Dichos campos podían ser derivados de un potencial escalar ( (6) abla \phi } La solución de las ecuaciones de Maxwell implicaba la existencia de una onda que se propagaba a la velocidad de la luz, con lo que además de unificar los fenómenos eléctricos y magnéticos la teoría formulada por Maxwell predecía con absoluta certeza los fenómenos ópticos.[15]​ Así la teoría predecía a una onda que, contraria a las ideas de la época, no necesitaba un medio de propagación; la onda electromagnética se podía propagar en el vacío debido a la generación mutua de los campos magnéticos y eléctricos. Esta onda a pesar de tener una velocidad constante, la velocidad de la luz c, puede tener diferente longitud de onda y consecuentemente dicha onda transporta energía. La radiación electromagnética recibe diferentes nombres al variar su longitud de onda, como rayos gamma, rayos X, espectro visible, etc.; pero en su conjunto recibe el nombre de espectro electromagnético. Espectro electromagnético. Véanse también: Fuerza de Lorentz, Fuerza electromotriz, Ley de Ampère, Ecuaciones de Maxwell y Campo electromagnético. Electrodinámica relativista Artículo principal: Tensor de campo electromagnético Clásicamente, al fijar un sistema de referencia, se puede descomponer los campos eléctricos y magnéticos del campo electromagnético. Pero, en la teoría de la relatividad especial, al tener a un observador con movimiento relativo respecto al sistema de referencia, este medirá efectos eléctricos y magnéticos diferentes de un mismo fenómeno electromagnético. El campo eléctrico y la inducción magnética a pesar de ser elementos vectoriales no se comportan como magnitudes físicas vectoriales, por el contrario la unión de ambos constituye otro ente físico llamado tensor y en este caso el tensor de campo electromagnético.[16]​ Así, la expresión para el campo electromagnético es: Esta representación se conoce como formulación covariante tetradimensional del electromagnetismo. Las expresiones covariantes para las ecuaciones de Maxwell (7) y la fuerza de Lorentz (6) se reducen a: (6) (7) u }}  u }=0} Dada la forma de las ecuaciones anteriores, si el dominio sobre el que se extiende el campo electromagnético es simplemente conexo el campo electromagnético puede expresarse como la derivada exterior de un cuadrivector llamado potencial vector, relacionado con los potenciales del electromagnetismo clásico de la siguiente manera: Donde: La relación entre el cuadrivector potencial y el tensor de campo electromanético resulta ser: El hecho de que la interacción electromagnética pueda representarse por un (cuadri)vector que define completamente el campo electromagnético es la razón por la que se afirma en el tratamiento moderno que la interacción electromagnética es un campo vectorial. En relatividad general el tratamiento del campo electromagnético en un espacio-tiempo curvo es similar al presentado aquí para el espacio-tiempo de Minkowski, solo que las derivadas parciales respecto a las coordenadas deben substituirse por derivadas covariantes. Electrodinámica cuántica Diagrama de Feynman mostrando la fuerza electromagnética entre dos electrones por medio del intercambio de un fotón virtual. Artículo principal: Electrodinámica cuántica Posteriormente a la revolución cuántica de inicios del siglo XX, los físicos se vieron forzados a buscar una teoría cuántica de la interacción electromagnética. El trabajo de Einstein con el efecto fotoeléctrico y la posterior formulación de la mecánica cuántica sugerían que la interacción electromagnética se producía mediante el intercambio de partículas elementales llamadas fotones. La nueva formulación cuántica lograda en la década de 1940 describe la interacción entre los bosones, o partículas portadoras de la interacción, y las otras partículas portadoras de materia (los fermiones).[17]​ La electrodinámica cuántica es principalmente una teoría cuántica de campos renormalizada. Su desarrollo fue obra de Sinitiro Tomonaga, Julian Schwinger, Richard Feynman y Freeman Dyson alrededor de los años 1947 a 1949.[18] En la electrodinámica cuántica, la interacción entre partículas viene descrita por un lagrangiano que posee simetría local, concretamente simetría de gauge. Para la electrodinámica cuántica, el campo de gauge donde los fermiones interactúan es el campo electromagnético, descrito en esta teoría como los estados de bosones (fotones, en este caso) portadores de la interacción.[18]​ Matemáticamente, el lagrangiano para la interacción entre fermiones mediante intercambio de fotones viene dado por: u }\,} Donde el significado de los términos son: Véanse también: Teoría cuántica de campos, Ecuación de Dirac y Modelo estándar."
ksampletext_wikipedia_phys_teoriadelarelatividad: str = "Teoría de la relatividad. La teoría de la relatividad incluye tanto a la teoría de la relatividad especial como la de la relatividad general, formuladas principalmente por Albert Einstein a principios del siglo XX, que pretendían resolver la incompatibilidad existente entre la mecánica newtoniana y el electromagnetismo.[3]​ La teoría de la relatividad especial, publicada en 1905, trata de la física del movimiento de los cuerpos en ausencia de fuerzas gravitatorias, en el que se hacían compatibles las ecuaciones de Maxwell del electromagnetismo con una reformulación de las leyes del movimiento. En la teoría de la relatividad especial, Einstein, Lorentz y Minkowski, entre otros, unificaron los conceptos de espacio y tiempo, en un tramado tetradimensional al que se le denominó espacio-tiempo. La relatividad especial fue una teoría revolucionaria para su época, con la que el tiempo absoluto de Newton quedó relegado y conceptos como la invariabilidad en la velocidad de la luz, la dilatación del tiempo, la contracción de la longitud y la equivalencia entre masa y energía fueron introducidos. Además, con las formulaciones de la relatividad especial, las leyes de la Física son invariantes en todos los sistemas de referencia inerciales; como consecuencia matemática, se encuentra como límite superior de velocidad a la de la luz y se elimina la causalidad determinista que tenía la física hasta entonces. Hay que indicar que las leyes del movimiento de Newton son un caso particular de esta teoría donde la masa, al viajar a velocidades muy pequeñas, no experimenta variación alguna en longitud ni se transforma en energía y al tiempo se le puede considerar absoluto. La teoría de la relatividad general, publicada en 1915, es una teoría de la gravedad que reemplaza a la gravedad newtoniana, aunque coincide numéricamente con ella para campos gravitatorios débiles y velocidades «pequeñas». La teoría general se reduce a la teoría especial en presencia de campos gravitatorios. La relatividad general estudia la interacción gravitatoria como una deformación en la geometría del espacio-tiempo. En esta teoría se introducen los conceptos de la curvatura del espacio-tiempo como la causa de la interacción gravitatoria, el principio de equivalencia que dice que para todos los observadores locales inerciales las leyes de la relatividad especial son invariantes y la introducción del movimiento de una partícula por líneas geodésicas. La relatividad general no es la única teoría que describe la atracción gravitatoria, pero es la que más datos relevantes comprobables ha encontrado. Anteriormente, a la interacción gravitatoria se la describía matemáticamente por medio de una distribución de masas, pero en esta teoría no solo la masa percibe esta interacción, sino también la energía, mediante la curvatura del espacio-tiempo y por eso se necesita otro lenguaje matemático para poder describirla, el cálculo tensorial. Muchos fenómenos, como la curvatura de la luz por acción de la gravedad y la desviación en la órbita de Mercurio, son perfectamente predichos por esta formulación. La relatividad general también abrió otro campo de investigación en la física, conocido como cosmología y es ampliamente utilizado en la astrofísica.[4]​ El 7 de marzo de 2010, la Academia Israelí de Ciencias exhibió públicamente los manuscritos originales de Einstein (redactados en 1905). El documento, que contiene 46 páginas de textos y fórmulas matemáticas escritas a mano, fue donado por Einstein a la Universidad Hebrea de Jerusalén en 1925 con motivo de su inauguración.[5]​[6]​[7]​ Conceptos principales Artículo principal: Anexo:Glosario de relatividad El supuesto básico de la teoría de la relatividad es que la localización de los sucesos físicos, tanto en el tiempo como en el espacio, son relativos al estado de movimiento del observador: así, la longitud de un objeto en movimiento o el instante en que algo sucede, a diferencia de lo que sucede en mecánica newtoniana, no son invariantes absolutos, y diferentes observadores en movimiento relativo entre sí diferirán respecto a ellos (las longitudes y los intervalos temporales, en relatividad son relativos y no absolutos). Relatividad especial Artículo principal: Teoría de la relatividad especial La teoría de la relatividad especial, también llamada teoría de la relatividad restringida, fue publicada por Albert Einstein en 1905 y describe la física del movimiento en el marco de un espacio-tiempo plano. Esta teoría describe correctamente el movimiento de los cuerpos incluso a grandes velocidades y sus interacciones electromagnéticas, se usa básicamente para estudiar sistemas de referencia inerciales (no es aplicable para problemas astrofísicos donde el campo gravitatorio desempeña un papel importante). Estos conceptos fueron presentados anteriormente por Poincaré y Lorentz, que son considerados como precursores de la teoría. Si bien la teoría resolvía un buen número de problemas del electromagnetismo y daba una explicación del experimento de Michelson y Morley, no proporciona una descripción relativista adecuada del campo gravitatorio. Tras la publicación del artículo de Einstein, la nueva teoría de la relatividad especial fue aceptada en unos pocos años por prácticamente la totalidad de los físicos y los matemáticos. De hecho, Poincaré o Lorentz habían estado muy cerca de llegar al mismo resultado que Einstein. La forma geométrica definitiva de la teoría se debe a Hermann Minkowski, antiguo profesor de Einstein en la Politécnica de Zúrich; acuñó el término «espacio-tiempo» (Raumzeit) y le dio la forma matemática adecuada.[nota 1] El espacio-tiempo de Minkowski es una variedad tetradimensional en la que se entrelazaban de una manera indisoluble las tres dimensiones espaciales y el tiempo. En este espacio-tiempo de Minkowski, el movimiento de una partícula se representa mediante su línea de universo (Weltlinie), una curva cuyos puntos vienen determinados por cuatro variables distintas: las tres dimensiones espaciales ( Relatividad general Esta sección es un extracto de Relatividad general.[editar] Representación artística de la explosión de la supernova SN 2006gy, situada a 238 millones de años luz. De ser válido el principio de acción a distancia, las perturbaciones de origen gravitatorio de este estallido nos afectarían inmediatamente y más tarde nos llegarían las de origen electromagnético, que se transmiten a la velocidad de la luz. Esquema bidimensional de la curvatura del espacio-tiempo (cuatro dimensiones) generada por una masa esférica. La teoría general de la relatividad o relatividad general es una teoría del campo gravitatorio y de los sistemas de referencia generales, publicada por Albert Einstein en 1915 y 1916. El nombre de la teoría se debe a que generaliza la llamada teoría especial de la relatividad y el principio de relatividad para un observador arbitrario. Los principios fundamentales introducidos en esta generalización son el principio de equivalencia, que describe la aceleración y la gravedad como aspectos distintos de la misma realidad, la noción de la curvatura del espacio-tiempo y el principio de covariancia generalizado. La teoría de la relatividad general propone que la propia geometría del espacio-tiempo se ve afectada por la presencia de materia, de lo cual resulta una teoría relativista del campo gravitatorio. De hecho la teoría de la relatividad general predice que el espacio-tiempo no será plano en presencia de materia y que la curvatura del espacio-tiempo será percibida como un campo gravitatorio. La intuición básica de Einstein fue postular que en un punto concreto no se puede distinguir experimentalmente entre un cuerpo acelerado uniformemente y un campo gravitatorio uniforme. La teoría general de la relatividad permitió también reformular el campo de la cosmología. Einstein expresó el propósito de la teoría de la relatividad general para aplicar plenamente el programa de Ernst Mach de la relativización de todos los efectos de inercia, incluso añadiendo la llamada constante cosmológica a sus ecuaciones de campo[8] para este propósito. Este punto de contacto real de la influencia de Ernst Mach fue claramente identificado en 1918, cuando Einstein distingue lo que él bautizó como el principio de Mach (los efectos inerciales se derivan de la interacción de los cuerpos) del principio de la relatividad general, que se interpreta ahora como el principio de covariancia general.[9]​ El matemático alemán David Hilbert escribió e hizo públicas las ecuaciones de la covariancia antes que Einstein, ello resultó en no pocas acusaciones de plagio contra Einstein, pero probablemente sea más porque es una teoría (o perspectiva) geométrica. La misma postula que la presencia de masa o energía «curva» el espacio-tiempo, y esta curvatura afecta la trayectoria de los cuerpos móviles e incluso la trayectoria de la luz. Formalismo de la teoría de la relatividad Representación de la línea de universo de una partícula. Como no es posible reproducir un espacio-tiempo de cuatro dimensiones, en la figura se representa solo la proyección sobre 2 dimensiones espaciales y una temporal. Partículas En la teoría de la relatividad una partícula puntual queda representada por un par  Campos Cuando se consideran campos o distribuciones continuas de masa, se necesita algún tipo de generalización para la noción de partícula. Un campo físico posee momentum y energía distribuidos en el espacio-tiempo, el concepto de cuadrimomento se generaliza mediante el llamado tensor de energía-impulso que representa la distribución en el espacio-tiempo tanto de energía como de momento lineal. A su vez un campo dependiendo de su naturaleza puede representarse por un escalar, un vector o un tensor. Por ejemplo el campo electromagnético se representa por un tensor de segundo orden totalmente antisimétrico o 2-forma. Si se conoce la variación de un campo o una distribución de materia, en el espacio y en el tiempo entonces existen procedimientos para construir su tensor de energía-impulso. Magnitudes físicas En relatividad, estas magnitudes físicas son representadas por vectores 4-dimensionales o bien por objetos matemáticos llamados tensores, que generalizan los vectores, definidos sobre un espacio de cuatro dimensiones. Matemáticamente estos 4-vectores y 4-tensores son elementos definidos del espacio vectorial tangente al espacio-tiempo (y los tensores se definen y se construyen a partir del fibrado tangente o cotangente de la variedad que representa el espacio-tiempo). Correspondencia entre E3[nota 2] y M4[nota 3]​ Espacio tridimensional euclídeo Espacio-tiempo de Minkowski Punto Suceso Longitud Intervalo Velocidad Cuadrivelocidad Momentum Cuadrimomentum Igualmente además de cuadrivectores, se definen cuadritensores (tensores ordinarios definidos sobre el fibrado tangente del espacio-tiempo concebido como variedad lorentziana). La curvatura del espacio-tiempo se representa por un 4-tensor (tensor de cuarto orden), mientras que la energía y el momento de un medio continuo o el campo electromagnético se representan mediante 2-tensores (simétrico el tensor de energía-impulso, antisimétrico el de campo electromagnético). Los cuadrivectores son, de hecho, 1-tensores, en esta terminología. En este contexto se dice que una magnitud es un invariante relativista si tiene el mismo valor para todos los observadores, obviamente todos los invariantes relativistas son escalares (0-tensores), frecuentemente formados por la contracción de magnitudes tensoriales. El intervalo relativista El intervalo relativista puede definirse en cualquier espacio-tiempo, sea este plano como en la relatividad especial, o curvo como en relatividad general. Sin embargo, por simplicidad, discutiremos inicialmente el concepto de intervalo para el caso de un espacio-tiempo plano. El tensor métrico del espacio-tiempo plano de Minkowski se designa con la letra  El intervalo, la distancia tetradimensional, se representa mediante la expresión  Reproducción de un cono de luz, en el que se representan dos dimensiones espaciales y una temporal (eje de ordenadas). El observador se sitúa en el origen, mientras que el futuro y el pasado absolutos vienen representados por las partes inferior y superior del eje temporal. El plano correspondiente a t = 0 se denomina plano de simultaneidad o hipersuperficie de presente (también llamado «diagrama de Minkowski»). Los sucesos situados dentro de los conos están vinculados al observador por intervalos temporales. Los que se sitúan fuera, por intervalos espaciales. Los intervalos pueden ser clasificados en tres categorías: Intervalos espaciales (cuando  Los intervalos nulos pueden ser representados en forma de cono de luz, popularizados por el celebérrimo libro de Stephen Hawking, Breve Historia del Tiempo. Sea un observador situado en el origen, el futuro absoluto (los sucesos que serán percibidos por el individuo) se despliega en la parte superior del eje de ordenadas, el pasado absoluto (los sucesos que ya han sido percibidos por el individuo) en la parte inferior, y el presente percibido por el observador en el punto 0. Los sucesos que están fuera del cono de luz no nos afectan, y por lo tanto se dice de ellos que están situados en zonas del espacio-tiempo que no tienen relación de causalidad con la nuestra. Imaginemos, por un momento, que en la galaxia Andrómeda, situada a 2.5 millones de años luz de nosotros, sucedió un cataclismo cósmico hace 100 000 años. Dado que, primero: la luz de Andrómeda tarda 2 millones de años en llegar hasta nosotros y segundo: nada puede viajar a una velocidad superior a la de los fotones, es evidente, que no tenemos manera de enterarnos de lo que sucedió en dicha Galaxia hace tan solo 100 000 años. Se dice, por lo tanto, que el intervalo existente entre dicha hipotética catástrofe cósmica y nosotros, observadores del presente, es un intervalo espacial ( Imagen de la galaxia Andrómeda, tomada por el telescopio Spitzer, tal como era hace 2.5 millones de años (por estar situada a 2.5 millones de años luz). Los sucesos acaecidos 1 000 000 de años atrás se observarán desde la Tierra dentro de un millón y medio de años. Se dice, por tanto, que entre tales eventos y nosotros existe un intervalo espacial. Análisis El único problema con esta hipótesis, es que al entrar en un agujero negro, se anula el espacio-tiempo, y como ya sabemos, algo que contenga algún volumen o masa, debe tener como mínimo un espacio donde ubicarse, el tiempo en ese caso, no tiene mayor importancia, pero el espacio juega un rol muy importante en la ubicación de volúmenes, por lo que esto resulta muy improbable, pero no imposible para la tecnología. Podemos escoger otro episodio histórico todavía más ilustrativo: El de la estrella de Belén, tal y como fue interpretada por Johannes Kepler. Este astrónomo alemán consideraba que dicha estrella se identificaba con una supernova que tuvo lugar el año 5 a. C., cuya luz fue observada por los astrónomos chinos contemporáneos, y que vino precedida en los años anteriores por varias conjunciones planetarias en la constelación de Piscis. Esa supernova probablemente estalló miles de años atrás, pero su luz no llegó a la Tierra sino hasta el año 5 a. C. De ahí que el intervalo existente entre dicho evento y las observaciones de los astrónomos egipcios y megalíticos (que tuvieron lugar varios siglos antes de Cristo) sea un intervalo espacial, pues la radiación de la supernova nunca pudo llegarles. Por el contrario, la explosión de la supernova por un lado, y las observaciones realizadas por los tres magos en Babilonia y por los astrónomos chinos en el año 5 a. C. por el otro, están unidas entre sí por un intervalo temporal, ya que la luz sí pudo alcanzar a dichos observadores. El tiempo propio y el intervalo se relacionan mediante la siguiente equivalencia: Esta invarianza se expresa a través de la llamada geometría hiperbólica: La ecuación del intervalo  Cuadrivelocidad, aceleración y cuadrimomentum Artículos principales: Cuadrivelocidad y Cuadrimomento. En el espacio-tiempo de Minkowski, las propiedades cinemáticas de las partículas se representan fundamentalmente por tres magnitudes: La cuadrivelocidad (o tetravelocidad), la cuadriaceleración y el cuadrimomentum (o tetramomentum). La cuadrivelocidad es un cuadrivector tangente a la línea de universo de la partícula, relacionada con la velocidad coordenada de un cuerpo medida por un observador en reposo cualquiera, esta velocidad coordenada se define con la expresión newtoniana  La velocidad coordenada de un cuerpo con masa depende caprichosamente del sistema de referencia que escojamos, mientras que la cuadrivelocidad propia es una magnitud que se transforma de acuerdo con el principio de covariancia y tiene un valor siempre constante equivalente al intervalo dividido entre el tiempo propio ( La cuadriaceleración puede ser definida como la derivada temporal de la cuadrivelocidad ( Junto con los principios de invarianza del intervalo y la cuadrivelocidad, juega un papel fundamental la ley de conservación del cuadrimomentum. Es aplicable aquí la definición newtoniana del momentum  Como tanto la velocidad de la luz como el cuadrimomentum son magnitudes conservadas, también lo es su producto, al que se le da el nombre de energía conservada  Componentes  Magnitud del cuadrimomentum  Magnitud en cuerpos con masa  Magnitud en fotones (masa = 0)  Energía  Energía en cuerpos con masa (cuerpos en reposo, p=0)  Energía en fotones (masa en reposo = 0)  La aparición de la Relatividad Especial puso fin a la secular disputa que mantenían en el seno de la mecánica clásica las escuelas de los mecanicistas y los energetistas. Los primeros sostenían, siguiendo a Descartes y Huygens, que la magnitud conservada en todo movimiento venía constituida por el momentum total del sistema, mientras que los energetistas —que tomaban por base los estudios de Leibniz— consideraban que la magnitud conservada venía conformada por la suma de dos cantidades: La fuerza viva, equivalente a la mitad de la masa multiplicada por la velocidad al cuadrado ( La mecánica newtoniana dio la razón a ambos postulados, afirmando que tanto el momentum como la energía son magnitudes conservadas en todo movimiento sometido a fuerzas conservativas. Sin embargo, la Relatividad Especial dio un paso más allá, por cuanto a partir de los trabajos de Einstein y Minkowski el momentum y la energía dejaron de ser considerados como entidades independientes y se les pasó a considerar como dos aspectos, dos facetas de una única magnitud conservada: el cuadrimomentum. Componentes y magnitud de los diferentes conceptos cinemáticos Concepto Componentes Expresión algebraica Partículas con masa Fotones Intervalo  ot =0}  Cuadrivelocidad  no definida Aceleración  (sistemas inerciales) ot =0} (sistemas no inerciales) Aceleración no definida Cuadrimomentum  El tensor de energía-impulso (Tab) Artículo principal: Tensor de energía-impulso Tensor de tensión-energía Tres son las ecuaciones fundamentales que en física newtoniana describen el fenómeno de la gravitación universal: la primera, afirma que la fuerza gravitatoria entre dos cuerpos es proporcional al producto de sus masas e inversamente proporcional al cuadrado de su distancia (1); la segunda, que el potencial gravitatorio ( Sin embargo, estas ecuaciones no son compatibles con la Relatividad Especial por dos razones: En primer lugar la masa no es una magnitud absoluta, sino que su medición deriva en resultados diferentes dependiendo de la velocidad relativa del observador. De ahí que la densidad de masa  En segundo lugar, si el concepto de espacio es relativo, también lo es la noción de densidad. Es evidente que la contracción del espacio producida por el incremento de la velocidad de un observador, impide la existencia de densidades que permanezcan invariables ante las transformaciones de Lorentz. Por todo ello, resulta necesario prescindir del término  O lo que es lo mismo: El componente  donde  Además, si los componentes del tensor se miden por un observador en reposo relativo respecto al fluido, entonces, el tensor métrico viene constituido simplemente por la métrica de Minkowski: diag diag Puesto que además la tetravelocidad del fluido respecto al observador en reposo es: como consecuencia de ello, los coeficientes del tensor de tensión-energía son los siguientes: Parte de la materia que cae en el disco de acreción de un agujero negro es expulsada a gran velocidad en forma de chorros. En supuestos como este, los efectos gravitomagnéticos pueden llegar a alcanzar cierta importancia. Donde  Podemos, a partir del tensor de tensión-energía, calcular cuánta masa contiene un determinado volumen del fluido: Retomando la definición de este tensor expuesta unas líneas más arriba, se puede definir al coeficiente  Del mismo modo, es posible deducir matemáticamente a partir del tensor de tensión-energía la definición newtoniana de presión, introduciendo en la mentada ecuación cualquier par de índices que sean diferentes de cero: La hipersuperficie  Finalmente, derivamos parcialmente ambos miembros de la ecuación respecto al tiempo, y teniendo en cuenta que la fuerza no es más que la tasa de incremento temporal del momentum obtenemos el resultado siguiente: Que contiene la definición newtoniana de la presión como fuerza ejercida por unidad de superficie. El tensor electromagnético (Fab) Artículo principal: Tensor de campo electromagnético Las ecuaciones deducidas por el físico escocés James Clerk Maxwell demostraron que electricidad y magnetismo no son más que dos manifestaciones de un mismo fenómeno físico: el campo electromagnético. Ahora bien, para describir las propiedades de este campo los físicos de finales del siglo XIX debían utilizar dos vectores diferentes, los correspondientes los campos eléctrico y magnético. Fue la llegada de la relatividad especial la que permitió describir las propiedades del electromagnetismo con un solo objeto geométrico, el vector cuadripotencial, cuyo componente temporal se correspondía con el potencial eléctrico, mientras que sus componentes espaciales eran los mismos que los del potencial magnético. De este modo, el campo eléctrico puede ser entendido como la suma del gradiente del potencial eléctrico más la derivada temporal del potencial magnético: y el campo magnético, como el rotacional del potencial magnético: abla  imes A} Las propiedades del campo electromagnético pueden también expresarse utilizando un tensor de segundo orden denominado tensor de Faraday y que se obtiene diferenciando exteriormente al vector cuadripotencial  La fuerza de Lorentz puede deducirse a partir de la siguiente expresión: Donde "

ksampletext_wikipedia_chem_valenciaquimica: str = "Valencia (química). La valencia es el número de electrones que le faltan o debe ceder un elemento químico para completar su último nivel de energía. Estos electrones son los que pone en juego durante una reacción química o para establecer un enlace químico con otro elemento. Hay elementos con más de una valencia, por ello fue reemplazado este concepto con el de números de oxidación que finalmente representa lo mismo. A través del siglo XX, el concepto de valencia ha evolucionado en una amplia gama de aproximaciones para describir el enlace químico, incluyendo la estructura de Lewis (1916), la teoría del enlace de valencia (1927), la teoría de los orbitales moleculares (1928), la teoría de repulsión de pares electrónicos de la capa de valencia (1958) y todos los métodos avanzados de química cuántica. En química, históricamente se ha considerado la valencia de un elemento como su capacidad de combinarse con otros elementos, para formar compuestos moleculares. Diferentes autores utilizan distintas definiciones de valencia, lo que crea un debate sobre cuál es la correcta. Por ejemplo, algunos autores confunden número de coordinación con valencia o estado de oxidación con valencia, a pesar de que esos tres términos son diferentes entre sí. Historia La etimología de la palabra «valencia» proviene de 1543, significando molde, del latín valentía poder, capacidad, y el significado químico refiriéndose al «poder combinante de un elemento» está registrado desde 1884, del alemán Valenz.[1]​ En 1890, William Higgins publicó bocetos sobre lo que él llamó combinaciones de partículas últimas, que esbozaban el concepto de enlaces de valencia.[2] Si, por ejemplo, de acuerdo a Higgins, la fuerza entre la partícula última de oxígeno y la partícula última de nitrógeno era 6, luego la fuerza del enlace debería ser dividida acordemente, y de modo similar para las otras combinaciones de partículas últimas: estas son las de la tabla periódica. Combinaciones de partículas últimas de William Higgins (1789). Sin embargo, el origen no exacto de la teoría de las valencias químicas puede ser rastreado a una publicación de Edward Frankland, en la que combinó las viejas teorías de los radicales libres y «teoría de tipos» con conceptos sobre afinidad química para mostrar que ciertos elementos tienen la tendencia a combinarse con otros elementos para formar compuestos conteniendo tres equivalentes del átomo unido, por ejemplo, en los grupos de tres átomos (vg. NO3, NH3, NI3, etc.) o cinco, por ejemplo en los grupos de cinco átomos (vg. N2O5, NH4O, P2O5, etc.) Es en este modo, según Franklin, que sus afinidades están mejor satisfechas. Siguiendo estos ejemplos y postulados, Franklin declaró cuán obvio esto es que:[3]​ Una tendencia o ley prevalece (aquí), y que, no importa qué puedan ser los caracteres de los átomos que se unen, el poder combinante de los elementos atrayentes, si me puedo permitir el término, se satisface siempre por el mismo número de estos átomos. Descripción La capacidad combinatoria o afinidad de un átomo de un elemento dado viene determinada por el número de átomos de hidrógeno con los que se combina. En el metano, el carbono tiene una valencia de 4; en el amoníaco, el nitrógeno tiene una valencia de 3; en el agua, el oxígeno tiene una valencia de 2; y en el cloruro de hidrógeno, el cloro tiene una valencia de 1. El cloro, al tener una valencia de uno, puede sustituir al hidrógeno. El fósforo tiene una valencia de 5 en el pentacloruro de fósforo, PCl 5. Los diagramas de valencia de un compuesto representan la conectividad de los elementos, con líneas dibujadas entre dos elementos, a veces llamadas enlaces, que representan una valencia saturada para cada elemento.[2] Las dos tablas siguientes muestran algunos ejemplos de diferentes compuestos, sus diagramas de valencia y las valencias para cada elemento del compuesto. Compuesto H Hidrógeno CH Metano C Propano C Propileno C Acetileno Diagrama      Valencias  Hidrógeno: 1 Carbono: 4 Hidrógeno: 1 Carbono: 4 Hidrogeno: 1 Carbono: 4 Hidrógeno: 1 Carbono: 4 Hidrógeno: 1 Compuesto NH Amoníaco NaCN Cianuro de sodio PSCl Thiophosphoryl chloride H Ácido sulfhídrico H 2SO Ácido sulfúrico H Dithionic acid Cl Óxido perclórico XeO Tetraóxido de xenón Diagrama         Valencias  Nitrógeno: 3 Hidrógeno: 1 Sodio: 1 Carbono: 4 Nitrógeno: 3 Fósforo: 5 Azufre: 2 Cloro: 1 Azufre: 2 Hidrógeno: 1 Azufre: 6 Oxígeno: 2 Hidrógeno: 1 Azufre: 6 Oxígeno: 2 Hidrógeno: 1 Cloro: 7 Oxígeno: 2 Xenón: 8 Oxígeno: 2 Definiciones modernas La valencia es definida por la IUPAC como:[4]​ El número máximo de átomos univalentes (originalmente átomos de hidrógeno o de cloro) que pueden combinarse con un átomo del elemento considerado, o con un fragmento, o por el que puede sustituirse un átomo de este elemento. Una descripción moderna alternativa es: El número de átomos de hidrógeno que pueden combinarse con un elemento en un hidruro binario o el doble del número de átomos de oxígeno que se combinan con un elemento en su óxido u óxidos. Esta definición difiere de la definición de la IUPAC, ya que se puede decir que un elemento tiene más de una valencia. Una definición moderna muy similar dada en un artículo reciente define la valencia de un átomo particular en una molécula como el número de electrones que un átomo utiliza en la unión, con dos fórmulas equivalentes para calcular la valencia:[5]​ valencia = número de electrones en la capa de valencia del átomo libre - número de electrones no enlazantes del átomo en la molécula valencia = número de enlaces + carga formal. Sin embargo esta definición de valencia es incorrecta. Por esta definición, el átomo de nitrógeno en el ion de amonio [NH4]+ es pentavalente, y en el ion de amida [NH2]- es monovalente, que obviamente es falso, porque el átomo de nitrógeno en los iones de amonio y amida es trivalente. Por lo tanto, esta definición es engañosa porque puede dar resultados falsos. Desarrollo histórico La etimología de la palabra valencia se remonta a 1425, con el significado de extracto, preparado, del latín valentia fuerza, capacidad, del anterior valor valía, valor, y el significado químico referido al poder combinatorio de un elemento se registra a partir de 1884, del alemán Valenz.[6]​ William Higgins' combinaciones de partículas últimas (1789) El concepto de valencia se desarrolló en la segunda mitad del siglo XIX y ayudó a explicar con éxito la estructura molecular de los compuestos inorgánicos y orgánicos.[7]​La búsqueda de las causas subyacentes de la valencia condujo a las teorías modernas del enlace químico, incluyendo el átomo cúbico (1902), estructura de Lewiss (1916), teoría del enlace de valencia (1927), orbitales molecularess (1928), teoría de repulsión de pares de electrones de la corteza de valencia (1958), y todos los métodos avanzados de la química cuántica. En 1789, William Higgins publicó opiniones sobre lo que denominó combinaciones de partículas últimas, que prefiguraron el concepto de enlaces de valencia.[8] Si, por ejemplo, según Higgins, la fuerza entre la partícula última de oxígeno y la partícula última de nitrógeno fuera 6, entonces la intensidad de la fuerza se dividiría en consecuencia, y lo mismo para las demás combinaciones de partículas últimas (véase la ilustración). Sin embargo, el origen exacto de la teoría de las valencias químicas se remonta a un trabajo de Edward Frankland de 1852, en el que combinó la antigua teoría de los radicales con ideas sobre la afinidad química para demostrar que ciertos elementos tienen tendencia a combinarse con otros elementos para formar compuestos que contienen 3, es decir, en los grupos de 3 átomos (por ejemplo, NO 3, NH 3, NI 3, etc.) o 5, es decir, en los grupos de 5 átomos (por ejemplo, NO 5, NH 4O, PO 5, etc.), equivalentes de los elementos unidos. Según él, ésta es la manera en que mejor se satisfacen sus afinidades, y siguiendo estos ejemplos y postulados, declara lo obvio que es que[9]​ Una tendencia o ley prevalece (aquí), y es que, cualesquiera que sean los caracteres de los átomos que se unen, el poder combinatorio del elemento que atrae, si se me permite el término, se satisface siempre con el mismo número de estos átomos. En 1857 August Kekulé propuso valencias fijas para muchos elementos, como 4 para el carbono, y las utilizó para proponer fórmulas estructurales para muchas moléculas de orgánica, que todavía se aceptan hoy en día. Lothar Meyer en su libro de 1864, Die modernen Theorien der Chemie, que contenía una primera versión de la tabla periódica con 28 elementos, clasificó por primera vez los elementos en seis familias según su valencia. Los trabajos sobre la organización de los elementos por peso atómico, hasta entonces se habían visto obstaculizados por el uso generalizado de peso equivalentes para los elementos, en lugar de pesos atómicos.[10]​ La mayoría de los químicos del siglo XIX definían la valencia de un elemento como el número de sus enlaces sin distinguir diferentes tipos de valencia o de enlace. Sin embargo, en 1893 Alfred Werner describió metal de transición complejos de coordinaciónes como [Co(NH 6]Cl 3, en los que distinguió valencias principales y subsidiarias (en alemán: 'Hauptvalenz' y 'Nebenvalenz'), correspondientes a los conceptos modernos de estado de oxidación y número de coordinación respectivamente. Para los elementos del grupo principal, en 1904 Richard Abegg consideró valencias positivas y negativas (estados de oxidación máximo y mínimo), y propuso la regla de Abegg según la cual su diferencia es a menudo 8. Electrones y valencia El modelo de Rutherford del átomo nuclear (1911) demostró que el exterior de un átomo está ocupado por electrones, lo que sugiere que los electrones son responsables de la interacción de los átomos y de la formación de enlaces químicos. En 1916, Gilbert N. Lewis explicó la valencia y el enlace químico en términos de una tendencia de los átomos (del grupo principal) a alcanzar una octeto estable de 8 electrones de valencia. Según Lewis, el enlace covalente conduce a octetos por la compartición de electrones, y el enlace iónico conduce a octetos por la transferencia de electrones de un átomo a otro. El término covalencia se atribuye a Irving Langmuir, quien afirmó en 1919 que el número de pares de electrones que un átomo dado comparte con los átomos adyacentes se denomina covalencia de ese átomo.[11] El prefijo co- significa juntos, de modo que un enlace covalente significa que los átomos comparten una valencia. Posteriormente, ahora es más común hablar de enlaces covalentes en lugar de valencia, que ha caído en desuso en trabajos de nivel superior a partir de los avances en la teoría del enlace químico, pero sigue siendo ampliamente utilizado en estudios elementales, donde proporciona una introducción heurística al tema. En la década de 1930, Linus Pauling propuso que también existen enlaces covalentes polares, que son intermedios entre los covalentes y los iónicos, y que el grado de carácter iónico depende de la diferencia de electronegatividad de los dos átomos enlazados. Pauling también consideró las moléculas hipervalentes, en las que los elementos del grupo principal tienen valencias aparentes superiores a la máxima de 4 permitida por la regla del octeto. Por ejemplo, en la molécula de hexafluoruro de azufre (SF 6), Pauling consideró que el azufre forma 6 enlaces verdaderos de dos electrones utilizando orbitales atómicos híbridos sp3d2, que combinan un orbital s, tres orbitales p y dos orbitales d. Sin embargo, más recientemente, cálculos cuántico-mecánicos sobre esta molécula y otras similares han demostrado que el papel de los orbitales d en el enlace es mínimo, y que la molécula SF 6 debería describirse como una molécula con 6 enlaces covalentes polares (en parte iónicos) formados por sólo cuatro orbitales en el azufre (un s y tres p) de acuerdo con la regla del octeto, junto con seis orbitales en los fluorinos.[12] Cálculos similares en moléculas de metales de transición muestran que el papel de los orbitales p es menor, de modo que un orbital s y cinco orbitales d en el metal son suficientes para describir el enlace.[13]​ Tipos de valencia Valencia positiva máxima: es el número positivo que refleja la máxima capacidad de combinación de un átomo. Este número coincide con el grupo de la tabla periódica de los elementos al cual pertenece. Por ejemplo, el cloro (Cl) pertenece al grupo 7, por lo que su valencia positiva máxima es 7. Valencia negativa solo para el grupo A no para el grupo B: es el número negativo que refleja la capacidad que tiene un átomo de combinarse con otro pero que esté actuando con valencia positiva. Este número negativo se puede determinar contando lo que le falta a la valencia positiva máxima para llegar a 8, pero con signo -. Por ejemplo: a la valencia máxima positiva del átomo de cloro es 7, por lo que le falta un electrón para cumplir el octeto, entonces su valencia negativa será -1. Vista general El concepto fue desarrollado a mediados del siglo XIX, en un intento por racionalizar la fórmula química de compuestos químicos diferentes. En 1919, Irving Langmuir, tomó prestado el término para explicar el modelo del átomo cúbico de Gilbert N. Lewis al enunciar que el número de pares de electrones que cualquier átomo dado comparte con el átomo adyacente es denominado la covalencia del átomo. El prefijo co- significa «junto», así que un enlace covalente significa que los átomos comparten valencia. De ahí, si un átomo, por ejemplo, tiene una valencia +1, significa que perdió un electrón, y otro con una valencia de -1, significa que tiene un electrón adicional. Luego, un enlace entre estos dos átomos resultaría porque se complementarían o compartirían sus tendencias en el balance de la valencia. Subsecuentemente, actualmente es más común hablar de enlace covalente en vez de valencia, que ha caído en desuso del nivel más alto de trabajo, con los avances en la teoría del enlace químico, pero aún es usado ampliamente en estudios elementales donde provee una introducción heurística a la materia. Definición del número de enlaces Se creía originalmente que el número de enlaces formados por un elemento dado era una propiedad química fija y, en efecto, en muchos casos, es una buena aproximación. Por ejemplo, en muchos de sus compuestos, el carbono forma cuatro enlaces, el oxígeno dos y el hidrógeno uno. Sin embargo, pronto se hizo evidente que, para muchos elementos, la valencia podría variar entre compuestos diferentes. Uno de los primeros ejemplos en ser identificado era el fósforo, que algunas veces se comporta como si tuviera una valencia de tres, y otras como si tuviera una valencia de cinco. Un método para resolver este problema consiste en especificar la valencia para cada compuesto individual: aunque elimina mucho de la generalidad del concepto, esto ha dado origen a la idea de número de oxidación (usado en la nomenclatura Stock y a la notación lambda en la nomenclatura IUPAC de química inorgánica). Definición de IUPAC La Unión Internacional de Química Pura y Aplicada (IUPAC) ha hecho algunos intentos de llegar a una definición desambigua de valencia. La versión actual, adoptada en 1994, es la siguiente:[14]​ La valencia es el máximo número de átomos univalentes (originalmente átomos de hidrógeno o cloro) que pueden combinarse con un átomo del elemento en consideración, o con un fragmento, o para el cual un átomo de este elemento puede ser sustituido. Esta definición reimpone una valencia única para cada elemento a expensas de despreciar, en muchos casos, una gran parte de su química. La mención del hidrógeno y el cloro es por razones históricas, aunque ambos en la práctica forman compuestos principalmente en los que sus átomos forman un enlace simple. Las excepciones en el caso del hidrógeno incluyen el ion bifluoruro, [HF2]−, y los diversos hidruros de boro tales como el diborano: estos son ejemplos de enlace de tres centros. El cloro forma un número de fluoruro—ClF, ClF3 y ClF5—y su valencia, de acuerdo a la definición de la IUPAC, es cinco. El flúor es el elemento para el que el mayor número de átomos se combinan con átomos de otros elementos: es univalente en todos sus compuestos, excepto en el ion [H2F]+. En efecto, la definición IUPAC sólo puede ser resuelta al fijar las valencias del hidrógeno y el flúor como uno, convención que ha sido seguida acá. Valencias de los elementos Artículo principal: Anexo:Estados de oxidación de los elementos Las valencias de la mayoría de los elementos se basan en el fluoruro más alto conocido.["
ksampletext_wikipedia_chem_quimicaorganica: str = "Química orgánica. La química orgánica es la rama de la química que estudia una clase numerosa de moléculas, que, en su mayoría contienen carbono formando enlaces covalentes: carbono-carbono o carbono-hidrógeno y otros heteroátomos, también conocidos como compuestos orgánicos. Debido a la omnipresencia del carbono en los compuestos que esta rama de la química estudia, esta disciplina también es llamada química del carbono.[1]​ Historia El trabajo de Friedrich Wöhler sobre la síntesis de la urea es considerado por muchos como el inicio de la química orgánica, y en particular de la síntesis orgánica. La química orgánica constituyó o se instituyó como disciplina en los años treinta. El desarrollo de nuevos métodos de análisis de las sustancias de origen animal y vegetal, basados en el empleo de disolventes, como el éter o el alcohol, permitió el aislamiento de un gran número de sustancias orgánicas que recibieron el nombre de principios inmediatos. La aparición de la química orgánica se asocia a menudo al descubrimiento, en 1828, por el químico alemán Friedrich Wöhler, de que la sustancia inorgánica cianato de amonio podía convertirse en urea, una sustancia orgánica que se encuentra en la orina de muchos animales. Antes de este descubrimiento, los químicos creían que para sintetizar sustancias orgánicas, era necesaria la intervención de lo que llamaban ‘la fuerza vital’, es decir, los organismos vivos. El experimento de Wöhler[2] rompió la barrera entre sustancias orgánicas e inorgánicas. De esta manera, los químicos modernos consideran compuestos orgánicos a aquellos que contienen carbono e hidrógeno, y otros elementos (que pueden ser uno o más), siendo los más comunes: oxígeno, nitrógeno, azufre y los halógenos. En 1856, sir William Henry Perkin, mientras trataba de estudiar la quinina, accidentalmente fabricó el primer colorante orgánico ahora conocido como malva de Perkin.[3]​ La diferencia entre la química orgánica y la química biológica,es que en la segunda las moléculas de ADN tienen una historia y, por ende, en su estructura nos hablan de su historia, del pasado en el que se han constituido, mientras que una molécula orgánica, creada hoy, es solo testigo de su presente, sin pasado y sin evolución histórica.[4]​ Cronología Artículo principal: Cronología de la Química orgánica 1675: Lémery clasifica los productos químicos naturales, según su origen en minerales, vegetales y animales 1784: Antoine Lavoisier demuestra que todos los productos vegetales y animales están formados básicamente por carbono e hidrógeno y, en menor proporción, nitrógeno, oxígeno y azufre 1807: Jöns Jacob Berzelius clasifica los productos químicos en: Orgánicos: los que proceden de organismos vivos. Inorgánicos: los que proceden de la materia inanimada. 1816: Michel Eugène Chevreul prepara distintos jabones a partir de diferentes fuentes de ácidos grasos y diversas bases, produciendo así distintas sales de ácidos grasos (o jabones), que no resultaron ser más que productos orgánicos nuevos derivados de productos naturales (grasas animales y vegetales). 1828: Friedrich Wöhler, a partir de sustancias inorgánicas y con técnicas normales de laboratorio, sintetizó la sustancia urea, la segunda sustancia orgánica obtenida artificialmente, luego del oxalato de amonio. Fórmula desarrollada urea 1856: Sir William Perkin sintetiza el primer colorante orgánico por accidente. 1865: August Kekulé propuso que los átomos de carbono que forman el benceno se unen formando cadenas cerradas o anillos. Primeros compendios La tarea de presentar la química orgánica de manera sistemática y global se realizó mediante una publicación surgida en Alemania, fundada por el químico Friedrich Konrad Beilstein (1838-1906). Su Handbuch der organischen Chemie (Manual de la química orgánica) comenzó a publicarse en Hamburgo en 1880 y consistió en dos volúmenes que recogían información de unos quince mil compuestos orgánicos conocidos. Cuando la Deutsche chemische Gesellschaft (Sociedad Alemana de Química) trató de elaborar la cuarta reedición, en la segunda década del siglo XX, la cifra de compuestos orgánicos se había multiplicado por diez. Treinta y siete volúmenes fueron necesarios para la edición básica, que aparecieron entre 1916 y 1937. Un suplemento de 27 volúmenes se publicó en 1938, recogiendo información aparecida entre 1910 y 1919. En la actualidad, se está editando el Fünftes Ergänzungswerk (quinta serie complementaria), que recoge la documentación publicada entre 1960 y 1979. Para ofrecer con más prontitud sus últimos trabajos, el Beilstein Institut ha creado el servicio Beilstein On line, que funciona desde 1988. Recientemente, se ha comenzado a editar periódicamente un CD-ROM, Beilstein Current Facts in Chemistry, que selecciona la información química procedente de importantes revistas. Actualmente, la citada información está disponible a través de internet. El alma de la química orgánica: el carbono Estructura tetraédrica del metano. La gran cantidad de compuestos orgánicos que existen tiene su explicación en las características del átomo de carbono, que tiene cuatro electrones en su capa de valencia: según la regla del octeto necesita ocho para completarla, por lo que forma cuatro enlaces (valencia = 4) con otros átomos. Esta especial configuración electrónica da lugar a una variedad de posibilidades de hibridación orbital del átomo de carbono (hibridación química). La molécula orgánica más sencilla que existe es el metano. En esta molécula, el carbono presenta hibridación sp3, con los átomos de hidrógeno formando un tetraedro. El carbono forma enlaces covalentes con facilidad para alcanzar una configuración estable, estos enlaces los forma con facilidad con otros carbonos, lo que permite formar frecuentemente cadenas abiertas (lineales o ramificadas) y cerradas (anillos). Clasificación de compuestos orgánicos La clasificación de los compuestos orgánicos puede realizarse de diversas maneras: atendiendo a su origen (natural o sintético), a su estructura (p. ej.: alifático o aromático), a su funcionalidad (p. ej.: alcoholes o cetonas), o a su peso molecular (p. ej.: monómeros o polímeros). Clasificación según su origen La clasificación de los compuestos orgánicos según el origen es de dos tipos: naturales o sintéticos. A menudo, los de origen natural se entiende que son los presentes en los seres vivos, pero no siempre es así, ya que algunas moléculas orgánicas también se sintetizan ex-vivo, es decir en ambientes inertes, como por ejemplo el ácido fórmico en el cometa Halle-Bopp. Natural In-vivo Los compuestos orgánicos presentes en los seres vivos o biosintetizados constituyen una gran familia de compuestos orgánicos. Su estudio tiene interés en medicina, farmacia, perfumería, cocina y muchos otros campos más. Carbohidratos Los carbohidratos están compuestos fundamentalmente de carbono (C), oxígeno (O) e hidrógeno (H). Son a menudo llamados azúcares, pero esta nomenclatura no es del todo correcta. Tienen una gran presencia en el reino vegetal (fructosa, celulosa, almidón, alginatos), pero también en el animal (glucógeno, glucosa). Se suelen clasificar según su grado de polimerización en: Monosacáridos (glucosa, fructosa, ribosa y desoxirribosa) Disacáridos (sacarosa, lactosa, maltosa) Trisacáridos (maltotriosa, rafinosa) Polisacáridos (alginatos, ácido algínico, celulosa, almidón, etc.) Lípidos Los lípidos son un conjunto de moléculas orgánicas, la mayoría biomoléculas, compuestas principalmente por carbono e hidrógeno y en menor medida oxígeno, aunque también pueden contener fósforo, azufre y nitrógeno. Tienen como característica principal el ser hidrófobas (insolubles en agua) y solubles en disolventes orgánicos como la bencina, el benceno y el cloroformo. En el uso coloquial, a los lípidos se les llama incorrectamente grasas, ya que las grasas son solo un tipo de lípidos procedentes de animales. Los lípidos cumplen funciones diversas en los organismos vivientes, entre ellas la de reserva energética (como los triglicéridos), la estructural (como los fosfolípidos de las bicapas) y la reguladora (como las hormonas esteroides). Proteínas fórmula química de un aminoácido. Las proteínas son polipéptidos, es decir están formados por la polimerización de péptidos, y estos por la unión de aminoácidos. Pueden considerarse así poliamidas naturales, ya que el enlace peptídico es análogo al enlace amida. Comprenden una familia muy importante de moléculas en los seres vivos, pero en especial en el reino animal. Por otra parte, son producto de la expresión de genes contenidos en el ADN. Algunos ejemplos de proteínas son el colágeno, las fibroínas, o la seda de araña. Ácidos nucleicos Los ácidos nucleicos son polímeros formados por la repetición de monómeros denominados nucleótidos, unidos mediante enlaces fosfodiéster. Se forman, así, largas cadenas; algunas moléculas de ácidos nucleicos llegan a alcanzar pesos moleculares gigantescos, con millones de nucleótidos encadenados. Están formados por la moléculas de carbono, hidrógeno, oxígeno, nitrógeno y fosfato. Los ácidos nucleicos almacenan la información genética de los organismos vivos y son los responsables de la transmisión hereditaria. Existen dos tipos básicos, el ADN y el ARN. Moléculas pequeñas Estructura de la testosterona. Una hormona, que se puede clasificar como molécula pequeña en el argot-químico-orgánico. Las moléculas pequeñas son compuestos orgánicos de peso molecular moderado (generalmente se consideran pequeñas aquellas con peso molecular menor a 1000 g/mol) y que aparecen en pequeñas cantidades en los seres vivos, pero no por ello su importancia es menor. A ellas pertenecen distintos grupos de hormonas como la testosterona, el estrógeno u otros grupos como los alcaloides. Las moléculas pequeñas tienen gran interés en la industria farmacéutica por su relevancia en el campo de la medicina. Ex-vivo Son compuestos orgánicos que han sido sintetizados sin la intervención de ningún ser vivo, en ambientes extracelulares y extravirales. Procesos geológicos Sello alemán de 1964 conmemorativo de la descripción de la estructura del benceno por Friedrich August Kekulé en 1865. El petróleo es una sustancia clasificada como mineral en la cual se presentan una gran cantidad de compuestos orgánicos. Muchos de ellos, como el benceno, son empleados por el hombre tal cual, pero muchos otros son tratados o derivados para conseguir una gran cantidad de compuestos orgánicos, como por ejemplo los monómeros para la síntesis de materiales poliméricos o plásticos. Procesos atmosféricos El sistema climático está constituido por la atmósfera, la hidrósfera, la biosfera, la geosfera y sus interacciones. Las variaciones en el equilibrio climático pueden generar diversos procesos como el calentamiento global, el efecto invernadero o la disminución de la capa de ozono. Procesos de síntesis planetaria En el año 2000 el ácido fórmico, un compuesto orgánico sencillo, también fue hallado en la cola del cometa Hale-Bopp.[5]​[6] Puesto que la síntesis orgánica de estas moléculas es inviable bajo las condiciones espaciales, este hallazgo parece sugerir que a la formación del sistema solar debió anteceder un periodo de calentamiento durante su colapso final.[6]​ Sintético Desde la síntesis de Wöhler de la urea un altísimo número de compuestos orgánicos han sido sintetizados químicamente para beneficio humano. Estos incluyen fármacos, desodorantes, perfumes, detergentes, jabones, fibras textiles sintéticas, materiales plásticos, polímeros en general, o colorantes orgánicos. Cadenas hidrocarbonadas sencillas Hidrocarburos El compuesto más simple es el metano, un átomo de carbono con cuatro de hidrógeno (valencia = 1), pero también puede darse la unión carbono-carbono, formando cadenas de distintos tipos, ya que pueden darse enlaces simples, dobles o triples. Cuando el resto de enlaces de estas cadenas son con hidrógeno, se habla de hidrocarburos, que pueden ser: Saturados: con enlaces covalentes simples, alcanos. Insaturados: con dobles enlaces covalentes (alquenos) o triples (alquinos). Hidrocarburos cíclicos: Hidrocarburos saturados con cadena cerrada, como el ciclohexano. Aromáticos: estructura cíclica. Radicales y ramificaciones de cadena Estructura de un hidrocarburo ramificado nombrado 5-butil-3,9-dimetil-undecano. Los radicales[7] son fragmentos de cadenas de carbonos que cuelgan de la cadena principal. Su nomenclatura se hace con la raíz correspondiente (en el caso de un carbono met-, dos carbonos et-, tres carbonos prop-, cuatro carbonos but-, cinco carbonos pent-, seis carbonos hex-, y así sucesivamente) y el sufijo -il. Además, se indica con un número, colocado delante, la posición que ocupan. El compuesto más simple que se puede hacer con radicales es el 2-metilpropano. En caso de que haya más de un radical, se nombrarán por orden alfabético de las raíces. Por ejemplo, el 2-etil, 5-metil, 8-butil, 10-docoseno. Clasificación según los grupos funcionales Los compuestos orgánicos también pueden contener otros elementos, también otros grupos de átomos además del carbono e hidrógeno, llamados grupos funcionales. Un ejemplo es el grupo hidroxilo, que forma los alcoholes: un átomo de oxígeno enlazado a uno de hidrógeno (-OH), al que le queda una valencia libre. Asimismo también existen funciones alqueno (dobles enlaces), éteres, ésteres, aldehídos, cetonas, carboxílicos, carbamoilos,[8] azo, nitro o sulfóxido, entre otros.[9]​ Alquino Alquino Hidroxilo Hidroxilo Éter Éter Amina Amina Aldehído Aldehído Cetona Cetona Carboxilo Carboxilo Éster Éster Amida Amida Azo Azo Nitro Nitro Sulfóxido Sulfóxido Monómero de la celulosa. Oxigenados Son cadenas de carbonos con uno o varios átomos de oxígeno. Pueden ser: Alcoholes: Las propiedades físicas de un alcohol se basan principalmente en su estructura. El alcohol está compuesto por un alcano y agua. Contiene un grupo hidrofóbico (sin afinidad por el agua) del tipo de un alcano, y un grupo hidroxilo que es hidrófilo (con afinidad por el agua), similar al agua. De estas dos unidades estructurales, el grupo –OH da a los alcoholes sus propiedades físicas características, y el alquilo es el que las modifica, dependiendo de su tamaño y forma. El grupo –OH es muy polar y, lo que es más importante, es capaz de establecer puentes de hidrógeno: con sus moléculas compañeras o con otras moléculas neutras. Dependiendo de la cantidad de grupos -OH que forman parte del alcohol, el mismo puede ser clasificado como monohidroxilado (presencia de un hidroxilo) o polihidroxilado (dos o más grupos hidroxilos en la molécula). Aldehídos: Los aldehídos son compuestos orgánicos caracterizados por poseer el grupo funcional -CHO. Se denominan como los alcoholes correspondientes, cambiando la terminación -ol por -al: Es decir, el grupo carbonilo H-C=O está unido a un solo radical orgánico. Cetonas: Una cetona es un compuesto orgánico caracterizado por poseer un grupo funcional carbonilo unido a dos átomos de carbono, a diferencia de un aldehído, en donde el grupo carbonilo se encuentra unido al menos a un átomo de hidrógeno. Cuando el grupo funcional carbonilo es el de mayor relevancia en dicho compuesto orgánico, las cetonas se nombran agregando el sufijo -ona al hidrocarburo del cual provienen (hexano, hexanona; heptano, heptanona; etc). También se puede nombrar posponiendo cetona a los radicales a los cuales está unido (por ejemplo: metilfenil cetona). Cuando el grupo carbonilo no es el grupo prioritario, se utiliza el prefijo oxo- (ejemplo: 2-oxopropanal). El grupo funcional carbonilo consiste en un átomo de carbono unido con un doble enlace covalente a un átomo de oxígeno. El tener dos átomos de carbono unidos al grupo carbonilo, es lo que lo diferencia de los ácidos carboxílicos, aldehídos, ésteres. El doble enlace con el oxígeno, es lo que lo diferencia de los alcoholes y éteres. Las cetonas suelen ser menos reactivas que los aldehídos dado que los grupos alquílicos actúan como dadores de electrones por efecto inductivo. Ácidos carboxílicos: Los ácidos carboxílicos constituyen un grupo de compuestos que se caracterizan porque poseen un grupo funcional llamado grupo carboxilo o grupo carboxi (–COOH); se produce cuando coinciden sobre el mismo carbono un grupo hidroxilo (-OH) y carbonilo (C=O). Se puede representar como COOH o CO2H... Ésteres: Los ésteres presentan el grupo éster (-O-CO-) en su estructura. Algunos ejemplos de sustancias con este grupo incluyen el ácido acetil salicílico, componente de la aspirina, o algunos compuestos aromáticos como el acetato de isoamilo, con característico olor a plátano. Los aceites también son ésteres de ácidos grasos con glicerol. Éteres: Los éteres presentan el grupo éter(-O-) en su estructura. Suelen tener bajo punto de ebullición y son fácilmente descomponibles. Por ambos motivos, los éteres de baja masa molecular suelen ser peligrosos ya que sus vapores pueden ser explosivos. Nitrogenados Aminas: Las aminas son compuestos orgánicos caracterizados por la presencia del grupo amina (-N<). Las aminas pueden ser primarias (R-NH2), secundarias (R-NH-R) o terciarias (R-NR´-R). Las aminas suelen dar compuestos ligeramente amarillentos y con olores que recuerdan a pescado u orina. Amidas: Las amidas son compuestos orgánicos caracterizados por la presencia del grupo amida (-NH-CO-) en su estructura. Las proteínas o polipéptidos son poliamidas naturales formadas por enlaces peptídicos entre distintos aminoácidos. Isocianatos: Los isocianatos tienen el grupo isocianato (-N=C=O). Este grupo es muy electrófilo, reaccionando fácilmente con el agua para descomponerse mediante la transposición de Hofmann dar una amina y anhídrico carbónico, con los hidroxilos para dar uretanos, y con las aminas primarias o secundarias para dar ureas. Cíclicos Son compuestos que contienen un ciclo saturado. Un ejemplo de estos son los norbornanos, que en realidad son compuestos bicíclicos, los terpenos, u hormonas como el estrógeno, progesterona, testosterona u otras biomoléculas como el colesterol. Aromáticos El furano (C4H4O) es un ejemplo de compuesto aromático. Estructura tridimensional del furano mostrando la nube electrónica de electrones π. Los compuestos aromáticos tienen estructuras cíclicas insaturadas. El benceno es el claro ejemplo de un compuesto aromático, entre cuyos derivados están el tolueno, el fenol o el ácido benzoico. En general se define un compuesto aromático aquel que tiene anillos que cumplen la regla de Hückel, es decir que tienen 4n+2 electrones en orbitales π (n=0,1,2,...). A los compuestos orgánicos que tienen otro grupo distinto al carbono en sus cilos (normalmente N, O u S) se denominan compuestos aromáticos heterocíclicos. Así los compuestos aromáticos se suelen dividir en: Derivados del benceno: Policíclicos (antraceno, naftaleno, fenantreno, etc.), fenoles, aminas aromáticas, fulerenos, etc. Compuestos heterocíclicos: Piridina, furano, tiofeno, pirrol, porfirina, etc. Isómeros Isómeros del C6H12. Ya que el carbono puede enlazarse de diferentes maneras, una cadena puede tener diferentes configuraciones de enlace dando lugar a los llamados isómeros, moléculas tienen la misma fórmula química, pero distintas estructuras y propiedades. Existen distintos tipos de isomería: isomería de cadena, isomería de función, tautomería, estereoisomería, y estereoisomería configuracional. El ejemplo mostrado a la izquierda es un caso de isometría de cadena en la que el compuesto con fórmula C6H12 puede ser un ciclo (ciclohexano) o un alqueno lineal, el 1-hexeno. Un ejemplo de isomería de función sería el caso del propanal y la acetona, ambos con fórmula C3H6O. Compuestos orgánicos Artículo principal: Compuesto orgánico Los compuestos orgánicos pueden dividirse de manera muy general en: Compuestos alifáticos Compuestos aromáticos Compuestos heterocíclicos Compuestos organometálicos Polímeros Relación con la biología Una de las principales relaciones entre la química orgánica y la biología es el estudio de la síntesis y estructura de moléculas orgánicas de importancia en los procesos moleculares realizados por los organismos vivos, es decir en el metabolismo.[10] La bioquímica es el campo interdisciplinar científico que estudia los seres vivos, y ya que estos usan compuestos que contienen carbono, la química orgánica es imprescindible para comprender los procesos metabólicos. En términos biológicos la química orgánica es de gran importancia sobre todo en un contexto celular y esto lo podemos ejemplificar con moléculas como los carbohidratos, presentes desde la membrana plasmática así como en la estructura química del ADN, los lípidos quienes son la base principal de la membrana plasmática, las proteínas que ayudan a dar sostén a un organismo o sus funciones como enzimas y el ADN, molécula encargada de resguardar la información genética de los organismos vivos.["
ksampletext_wikipedia_chem_molecula: str = "Molécula. En química, una molécula (del nuevo latín molecula, que es un diminutivo de la palabra moles, 'masa') es un grupo eléctricamente neutro y suficientemente estable de al menos dos átomos en una configuración definida, unidos por enlaces químicos fuertes covalentes.[4]​[5]​[6]​[7]​[8]​[9]​ En este estricto sentido, las moléculas se diferencian de los iones poliatómicos. En la química orgánica y la bioquímica, el término molécula se utiliza de manera menos estricta y se aplica también a los compuestos orgánicos (moléculas orgánicas) y en las biomoléculas. Antes, se definía la molécula de forma menos general y precisa, como la más pequeña parte de una sustancia que podía tener existencia independiente y estable conservando aún sus propiedades fisicoquímicas. De acuerdo con esta definición, podían existir moléculas monoatómicas. En la teoría cinética de los gases, el término molécula se aplica a cualquier partícula gaseosa con independencia de su composición. De acuerdo con esta definición, los átomos de un gas noble se considerarían moléculas aunque se componen de átomos no enlazados.[10]​ Una molécula puede consistir en varios átomos de un único elemento químico, como en el caso del oxígeno diatómico (O2),[11] o de diferentes elementos, como en el caso del agua (H2O).[12] Los átomos y complejos unidos por enlaces no covalentes como los enlaces de hidrógeno o los enlaces iónicos no se suelen considerar como moléculas individuales. Las moléculas como componentes de la materia son comunes en las sustancias orgánicas (y por tanto en la bioquímica). También conforman la mayor parte de los océanos y de la atmósfera. Sin embargo, un gran número de sustancias sólidas familiares, que incluyen la mayor parte de los minerales que componen la corteza, el manto y el núcleo de la Tierra, contienen muchos enlaces químicos, pero no están formados por moléculas. Además, ninguna molécula típica puede ser definida en los cristales iónicos (sales) o en cristales covalentes, aunque estén compuestos por celdas unitarias que se repiten, ya sea en un plano (como en el grafito) o en tres dimensiones (como en el diamante o el cloruro de sodio). Este sistema de repetir una estructura unitaria varias veces también es válida para la mayoría de las fases condensadas de la materia con enlaces metálicos, lo que significa que los metales sólidos tampoco están compuestos por moléculas. En el vidrio (sólidos que presentan un estado vítreo desordenado), los átomos también pueden estar unidos por enlaces químicos sin que se pueda identificar ningún tipo de molécula, pero tampoco existe la regularidad de la repetición de unidades que caracteriza a los cristales. Casi toda la química orgánica y buena parte de la química inorgánica se ocupan de la síntesis y reactividad de moléculas y compuestos moleculares. La química física y, especialmente, la química cuántica también estudian, cuantitativamente, en su caso, las propiedades y reactividad de las moléculas. La bioquímica está íntimamente relacionada con la biología molecular, ya que ambas estudian a los seres vivos a nivel molecular. El estudio de las interacciones específicas entre moléculas, incluyendo el reconocimiento molecular es el campo de estudio de la química supramolecular. Estas fuerzas explican las propiedades físicas como la solubilidad o el punto de ebullición de un compuesto molecular.[13]​ Las moléculas rara vez se encuentran sin interacción entre ellas, salvo en gases enrarecidos y en los gases nobles. Así, pueden encontrarse en redes cristalinas, como el caso de las moléculas de H2O en el hielo o con interacciones intensas, pero que cambian rápidamente de direccionalidad, como en el agua líquida. En orden creciente de intensidad, las fuerzas intermoleculares más relevantes son: las fuerzas de Van der Waals y los puentes de hidrógeno. La dinámica molecular es un método de simulación por computadora que utiliza estas fuerzas para tratar de explicar las propiedades de las moléculas. No se puede definir una molécula típica para sales ni para cristales covalentes, aunque estos a menudo se componen de células unitarias repetidas que se extienden en un plano, por ejemplo, el grafeno ; o tridimensionalmente, por ejemplo, el diamante, el cuarzo, o el cloruro de sodio. El tema de la estructura celular unitaria repetida también se aplica a la mayoría de los metales que son fases condensadas con enlaces metálicos. Por tanto, los metales sólidos no están hechos de moléculas. En los vidrios, que son sólidos que existen en un estado vítreo desordenado, los átomos se mantienen unidos por enlaces químicos sin presencia de ninguna molécula definible, ni ninguna de la regularidad de la estructura celular unitaria repetida que caracteriza a las sales, cristales covalentes y rieles. Ciencia molecular La ciencia de las moléculas se denomina química molecular o física molecular, dependiendo de si se centra en la química o en la física. La química molecular se ocupa de las leyes que rigen la interacción entre las moléculas que da lugar a la formación y ruptura de enlaces químicos, mientras que la física molecular se ocupa de las leyes que rigen su estructura y propiedades. En la práctica, sin embargo, esta distinción es imprecisa. En las ciencias moleculares, una molécula consiste en un sistema estable (estado ligado) compuesto por dos o más átomos. Los iones poliatómicos pueden considerarse a veces como moléculas cargadas eléctricamente. El término molécula inestable se utiliza para especies muy reactivas, es decir, conjuntos de corta duración (resonancias) de electrones y núcleos, como radicales, iones moleculares, moléculas de Rydberg, estados de transición, complejos de van der Waals, o sistemas de átomos en colisión como en el condensado de Bose-Einstein. Historia y etimología Artículo principal: Historia de la teoría molecular Según la Real Academia Española el vocablo «molécula» deriva del latín moles 'mole' o 'masa' y el sufijo diminutivo -ula 'masa pequeña'.[14]​ Molécula (1794) - «partícula extremadamente diminuta», del francés molécule (1678), del Nuevo Latín molecula, diminutivo del latín moles masa, barrera. Un significado vago al principio; la moda de la palabra (utilizada hasta finales del siglo XVIII solo en forma latina) se remonta a la filosofía de Descartes.[15]​[16]​ La definición de molécula ha ido evolucionando a medida que ha aumentado el conocimiento de la estructura de las moléculas. Las definiciones anteriores eran menos precisas, y definían las moléculas como las partículas más pequeñas de sustancia químicas puras que aún conservan su composición y sus propiedades químicas.[17] Esta definición a menudo se rompe ya que muchas sustancias en la experiencia ordinaria, como rocas, sales, y metales, se componen de grandes redes cristalinas de átomos de enlace químico o iones, pero no están hechas de moléculas discretas. Definición y sus límites De manera menos general y precisa, se ha definido molécula como la parte más pequeña de una sustancia química que conserva sus propiedades químicas, y a partir de la cual se puede reconstituir la sustancia sin reacciones químicas. De acuerdo con esta definición, que resulta razonablemente útil para aquellas sustancias puras constituidas por moléculas, podrían existir las moléculas monoatómicas de gases nobles, mientras que las redes cristalinas, sales, metales y la mayoría de vidrios quedarían en una situación confusa. Las moléculas lábiles pueden perder su consistencia en tiempos relativamente cortos, pero si el tiempo de vida medio es del orden de unas pocas vibraciones moleculares, estamos ante un estado de transición que no se puede considerar molécula. Actualmente, es posible el uso de láser pulsado para el estudio de la química de estos sistemas. Las entidades que comparten la definición de las moléculas, pero tienen carga eléctrica se denominan iones poliatómicos, iones moleculares o moléculas ion. Las sales compuestas por iones poliatómicos se clasifican habitualmente dentro de los materiales de base molecular o materiales moleculares. Ejemplo de molécula poliatómica: el agua Las moléculas están formadas por partículas. Una molécula viene a ser la porción de materia más pequeña que aún conserva las propiedades de la materia original. Las moléculas se encuentran fuertemente enlazadas con la finalidad de formar materia. Las moléculas están formadas por átomos unidos por medio de enlaces químicos. Una molécula es una unidad de sustancia que puede ser monoatómica o poliatómica. La unidad de todas las sustancias gaseosas es la molécula.[18]​ Tipos de moléculas Las moléculas se pueden clasificar en: Moléculas discretas: constituidas por un número bien definido de átomos, sean estos del mismo elemento (moléculas homonucleares, como el dinitrógeno o el fullereno) o de elementos distintos (moléculas heteronucleares, como el agua). Molécula de dinitrógeno, el gas que es el componente mayoritario del aire Molécula de dinitrógeno, el gas que es el componente mayoritario del aire Molécula de fullereno, tercera forma estable del carbono tras el diamante y el grafito Molécula de fullereno, tercera forma estable del carbono tras el diamante y el grafito Molécula de agua, «disolvente universal», de importancia fundamental en innumerables procesos bioquímicos e industriales Molécula de agua, «disolvente universal», de importancia fundamental en innumerables procesos bioquímicos e industriales Representación poliédrica del anión de Keggin, un polianión molecular Representación poliédrica del anión de Keggin, un polianión molecular Macromoléculas o polímeros: constituidas por la repetición de una unidad comparativamente simple —o un conjunto limitado de dichas unidades— y que alcanzan pesos moleculares relativamente altos. Representación de un fragmento de ADN, un polímero de importancia fundamental en la genética Representación de un fragmento de ADN, un polímero de importancia fundamental en la genética Enlace peptídico que une los péptidos para formar proteínas Enlace peptídico que une los péptidos para formar proteínas Representación de un fragmento lineal de polietileno, el plástico más usado Representación de un fragmento lineal de polietileno, el plástico más usado Primera generación de un dendrímero, un tipo especial de polímero que crece de forma fractal Primera generación de un dendrímero, un tipo especial de polímero que crece de forma fractal Enlaces Los átomos que forman las moléculas se mantienen juntos mediante enlaces covalentes o enlaces iónicos. Varios tipos de elementos no metálicos existen solo como moléculas en el medio ambiente. Por ejemplo, el hidrógeno solo existe como molécula de hidrógeno. Una molécula de un compuesto está formada por dos o más elementos.[19] Una molécula homonuclear está formada por dos o más átomos de un solo elemento. Mientras que algunas personas dicen que un cristal metálico puede considerarse una sola molécula gigante unida por enlaces metálicos,[20] otros señalan que los metales actúan de manera muy diferente a las moléculas.[21]​ Covalente Artículo principal: Enlace covalente Un enlace covalente que forma H2 (derecha) donde dos átomos de hidrógeno comparten los dos electrones. Un enlace covalente es un enlace químico que implica el intercambio de pares de electrones entre átomos. Estos pares de electrones se denominan pares compartidos o pares de enlace, y el equilibrio estable de fuerzas atractivas y repulsivas entre átomos, cuando comparten electrones, se denomina enlace covalente.[22]​ Iónico Artículo principal: Enlace iónico El sodio y el flúor experimentan una reacción redox para formar fluoruro de sodio. El sodio pierde su electrón externo para adoptar una configuración electrónica estable, y este electrón entra en el átomo de flúor en forma exotérmica. El enlace iónico es un tipo de enlace químico que implica la atracción electrostática entre iones con carga eléctrica opuesta y es la interacción principal que se produce en los compuestos iónicos. Los iones son átomos que han perdido uno o más electrones (denominados cationes) y átomos que han ganado uno o más electrones (denominados aniones).[23] Esta transferencia de electrones se denomina electrovalencia en contraste con la covalencia. En el caso más simple, el catión es un átomo de metal y el anión es un átomo no metálico, pero estos iones pueden ser de naturaleza más complicada, por ejemplo, iones moleculares como NH4+ o SO4 2−. A temperaturas y presiones normales, la unión iónica crea principalmente sólidos (u ocasionalmente líquidos) sin moléculas identificables separadas, pero la vaporización/sublimación de tales materiales produce pequeñas moléculas separadas donde los electrones aún se transfieren lo suficiente como para que los enlaces se consideren iónicos en lugar de covalentes. Descripción La estructura molecular puede ser descrita de diferentes formas. La fórmula molecular es útil para moléculas sencillas, como H2O para el agua o NH3 para el amoniaco. Contiene los símbolos de los elementos presentes en la molécula, así como su proporción indicada por los subíndices. Para moléculas más complejas, como las que se encuentran comúnmente en química orgánica, la fórmula química no es suficiente, y vale la pena usar una fórmula estructural o una fórmula esqueletal, las que indican gráficamente la disposición espacial de los distintos grupos funcionales. Cuando se quieren mostrar variadas propiedades moleculares, o se trata de sistemas muy complejos como proteínas, ADN o polímeros, se utilizan representaciones especiales, como los modelos tridimensionales (físicos o representados por ordenador). En proteínas, por ejemplo, cabe distinguir entre estructura primaria (orden de los aminoácidos), secundaria (primer plegamiento en hélices, hojas, giros…), terciaria (plegamiento de las estructuras tipo hélice/hoja/giro para dar glóbulos) y cuaternaria (organización espacial entre los diferentes glóbulos). Figura 1. Representaciones de la terpenoide, atisano, 3D (centro izquierda) y 2D (derecha). En el modelo 3D de la izquierda, los átomos de carbono están representados por esferas azules; las blancas representan a los átomos de hidrógeno y los cilindros representan los enlaces. El modelo es una representación de la superficies molecular, coloreada por áreas de carga eléctrica positiva (rojo) o negativa (azul). En el modelo 3D del centro, las esferas azul claro representan átomos de carbono, las blancas de hidrógeno y los cilindros entre los átomos son los enlaces simples. Moléculas en la teoría cuántica La mecánica clásica y el electromagnetismo clásico no podían explicar la existencia y estabilidad de las moléculas, ya que de acuerdo con sus ecuaciones una carga eléctrica acelerada emitiría radiación por lo que los electrones necesariamente perderían energía cinética por radiación hasta caer sobre el núcleo atómico. La mecánica cuántica proveyó el primer modelo cualitativamente correcto que además predecía la existencia de átomos estables y proporcionaba explicación cuantitativa muy aproximada para fenómenos empíricos como los espectros de emisión característicos de cada elemento químico. En mecánica cuántica una molécula o un ion poliatómico se describe como un sistema formado por  (1) definido sobre el espacio de funciones antisimetrizadas de cuadrado integrable  (2) donde el primer término representa la interacción de los electrones entre sí, el segundo la interacción de los electrones con los núcleos atómicos, y el tercero las interacciones de los núcleos entre sí. En una molécula neutra se tendrá obviamente que: Si  Aproximación de Born-Oppenheimer Resolver el problema de autovalores y autofunciones para el hamiltoniano cuántico dado por (1) es un problema matemático difícil, por lo que es común simplificarlo de alguna manera. Así dado que los núcleos atómicos son mucho más pesados que los electrones (entre 103 y 105 veces más) puede suponerse que los núcleos atómicos apenas se mueven comparados con los electrones, por lo que se considera que están congelados en posiciones fijas, con lo cual se puede aproximar el hamiltoniano (1) por la aproximación de Born-Oppenheimer dada por: (3) definido sobre el espacio de funciones  Teorema de Kato Los operadores  Tosio Kato La propiedad de ser autoadjunto implicará que las energías son cantidades reales, y el que sean acotados inferiormente implicará que existe un estado fundamental de mínima energía por debajo del cual los electrones no pueden decaer, y por tanto, las moléculas serán estables, ya que los electrones no pueden perder y perder energía como parecían predecir las ecuaciones del electromagnetismo clásico. Dos resultados matemáticos adicionales nos dicen como son las energías permitidas de los electrones dentro de una molécula:[24]​ Teorema HVZ para átomos y moléculas BO El espectro esencial  inf W. Hunziker, C. Van Winter y G. M. Zhislin Además dentro de la mecánica cuántica puede demostrarse que pueden existir iones positivos (cationes, con carga positiva comparable al núcleo atómico), mientras que no es igual de fácil tener iones negativos (aniones), el siguiente resultado matemático implica tiene que ver con la posibilidad de cationes y aniones:["
ksampletext_wikipedia_chem_compuestoquimico: str = "Compuesto químico. Un compuesto químico es una sustancia formada por la combinación química de dos o más elementos de la tabla periódica.[1] Los compuestos son representados por una fórmula química. Por ejemplo, el agua (H2O) está constituida por dos átomos de hidrógeno y uno de oxígeno. Los elementos de un compuesto no se pueden dividir ni separar por procesos físicos (decantación, filtración, destilación), sino solo mediante procesos químicos. Los compuestos están formados por moléculas o iones con enlaces estables que no obedece a una selección humana arbitraria. Por lo tanto, no son mezclas o aleaciones como el bronce o el chocolate.[2]​[3] Un elemento químico unido a un elemento químico idéntico no es un compuesto químico, ya que solo está involucrado un elemento, no dos elementos diferentes. Hay cuatro tipos de compuestos, dependiendo de cómo se mantienen unidos los átomos constituyentes: Moléculas unidas por enlaces covalentes. Compuestos iónicos unidos por enlaces iónicos. Compuestos intermetálicos unidos por enlaces metálicos. Ciertos complejos que se mantienen unidos por enlaces covalentes coordinados. Muchos compuestos químicos tienen un identificador numérico único asignado por el Chemical Abstracts Service (CAS): su número CAS. Fórmula Artículo principal: Fórmula molecular En química inorgánica los compuestos se representan mediante fórmulas químicas.[4] Una fórmula química es una forma de expresar información sobre las proporciones de los átomos que constituyen un compuesto químico en particular, utilizando las abreviaturas normalizadas de los elementos químicos y subíndices para indicar el número de átomos involucrados. Por ejemplo, el agua se compone de dos átomos de hidrógeno unidos a uno de oxígeno átomo: la fórmula química es H2O. En el caso de compuestos no estequiométricos, las proporciones pueden ser reproducibles con respecto a su preparación y dar proporciones fijas de sus elementos componentes, pero proporciones que no son integrales [por ejemplo, para el hidruro de paladio, PdH x (0.02 <x <0.58 )].[5]​ El orden de los elementos en la fórmula de los compuestos inorgánicos comienza por la izquierda con el elemento menos electronegativo, hasta la derecha con el más electronegativo. Por ejemplo en el NaCl, el cloro que es más electronegativo que el sodio va en la parte derecha.[6] Para los compuestos orgánicos existen otras varias reglas y se utilizan fórmulas esqueletales o semidesarrolladas para su representación.[7]​ Definiciones Cualquier sustancia que consista en dos o más tipos diferentes de átomos (elementos químicos) en una proporción estequiométrica fija puede denominarse compuesto químico. El concepto se entiende mejor cuando se consideran sustancias químicas puras.[8]​[9]​[10] De la composición de proporciones fijas de dos o más tipos de átomos se desprende que los compuestos químicos se pueden convertir, mediante una reacción química, en compuestos o sustancias, cada uno con menos átomos. Los compuestos químicos tienen una estructura química única y definida que se mantiene unida en una disposición espacial concebida por enlaces químicos. Los compuestos químicos pueden ser compuestos moleculares, mantenidos juntos por enlaces covalentes, sales mantenidas entre sí por enlaces iónicos, compuestos intermetálicos mantenidos juntos por enlaces metálicos, o el subconjunto de complejos químicos que se mantienen unidos por enlaces covalentes coordinados .[11] Los elementos químicos puros generalmente no se consideran compuestos químicos, ya que no cumplen con el requisito de dos o más átomos, aunque a menudo consisten en moléculas compuestas de múltiples átomos (como en la molécula diatómica H2, o la molécula poliatómica S8, etc.)[11] Muchos compuestos químicos tienen un identificador numérico único asignado por el Chemical Abstracts Service (CAS): su número CAS.[12]​ Hay nomenclatura variable y a veces inconsistente para diferenciar sustancias, que incluyen ejemplos verdaderamente no estequiométricos de los compuestos químicos, que requieren que las proporciones sean fijas. Muchas sustancias químicas sólidas, por ejemplo muchos minerales de silicato, no tienen fórmulas simples que reflejen el enlace químico de los elementos entre sí en proporciones fijas; aun así, estas sustancias cristalinas a menudo se denominan compuestos no estequiométricos. Se puede argumentar que están relacionados con dichos productos, en lugar de ser compuestos químicos propiamente dichos, en la medida en que la variabilidad en sus composiciones a menudo se debe a la presencia de elementos extraños atrapados dentro de la estructura cristalina de un compuesto químico verdadero, o debido a perturbaciones en su estructura en relación con el compuesto conocido que surge debido a un exceso o déficit de los elementos constituyentes en lugares de su estructura; tales sustancias no estequiométricas forman la mayor parte de la corteza y el manto de la Tierra. Otros compuestos considerados químicamente idénticos pueden tener cantidades variables de isótopos pesados o ligeros de los elementos constituyentes, lo que cambia ligeramente la proporción en masa de los elementos. Clasificación Se pueden clasificar de acuerdo al tipo de enlace químico o a su composición. Atendiendo al tipo de enlace químico, se pueden dividir en: Moléculas Compuestos iónicos Compuestos intermetálicos Complejos Por su composición, se pueden dividir en dos grandes grupos:[13]​ Compuestos inorgánicos:[14]​ Óxidos básicos. También llamados óxidos metálicos, que están formados por un metal y oxígeno. Ejemplos: el óxido plúmbico, óxido de litio. Óxidos ácidos. También llamados óxidos no metálicos, formados por un no metal y oxígeno. Ejemplos: óxido hipocloroso, óxido selenioso. Hidruros, que pueden ser tanto metálicos como no metálicos. Están compuestos por un elemento e hidrógeno. Ejemplos: hidruro de aluminio, hidruro de sodio. Hidrácidos, son hidruros no metálicos que, cuando se disuelven en agua, adquieren carácter ácido. Por ejemplo, el ácido yodhídrico. Hidróxidos, compuestos formados por la reacción entre un óxido básico y el agua, que se caracterizan por presentar el grupo hidroxilo (OH). Por ejemplo, el hidróxido de sodio, o sosa cáustica. Oxácidos, compuestos obtenidos por la reacción de un óxido ácido y agua. Sus moléculas están formadas por hidrógeno, un no metal y oxígeno. Por ejemplo, ácido clórico. Sales binarias, compuestos formados por un hidrácido más un hidróxido. Por ejemplo, el cloruro de sodio. Oxisales, formadas por la reacción de un oxácido y un hidróxido, como por ejemplo el hipoclorito de sodio. Compuestos orgánicos:[15]​ Compuestos alifáticos, son compuestos orgánicos constituidos por carbono e hidrógeno cuyo carácter no es aromático. Compuestos aromáticos, es un compuesto orgánico cíclico conjugado que posee una mayor estabilidad debido a la deslocalización electrónica en enlaces π. Compuestos heterocíclicos, son compuestos orgánicos cíclicos en los que al menos uno de los componentes del ciclo es de un elemento diferente al carbono.[16]​ Compuestos organometálicos, es un compuesto en el que los átomos de carbono forman enlaces covalentes, es decir, comparten electrones, con un átomo metálico. Polímeros, son macromoléculas formadas por la unión de moléculas más pequeñas llamadas monómeros. Moléculas Artículo principal: Molécula Una molécula es un grupo eléctricamente neutro de dos o más átomos unidos por enlaces químicos.[17]​[18]​[19]​[20]​[21] Una molécula puede ser homonuclear, es decir, estar formada por átomos de un mismo elemento químico, como ocurre con dos átomos en la molécula de oxígeno (O2); o puede ser heteronuclear, es decir, un compuesto químico compuesto por más de un elemento, como el agua (dos átomos de hidrógeno y un átomo de oxígeno; H2O).[22] Los átomos y complejos unidos por enlaces no covalentes como los enlaces de hidrógeno no se suelen considerar como moléculas individuales. Compuestos iónicos Artículo principal: Compuesto iónico Un compuesto iónico es un compuesto químico compuesto de anion que se mantienen unidos por fuerzas electrostáticas denominadas enlace iónico. El compuesto es neutro en general, pero consta de iones cargados positivamente llamados cationes y iones cargados negativamente llamados aniones. Estos pueden ser iones simples como el sodio(Na+) y el cloruro (Cl−) en el cloruro de sodio, o especies poliatómicas como el amonio (NH+ 4) y carbonato (CO2− 3) en el carbonato de amonio.[23] Los iones individuales dentro de un compuesto iónico generalmente tienen múltiples vecinos más cercanos, por lo que no se consideran parte de moléculas, sino parte de una red tridimensional continua, generalmente en una estructura cristalina.[24]​ Los compuestos iónicos que contienen iones básicos hidróxido (OH−) u óxido(O2−) se clasifican como bases. Los compuestos iónicos sin estos iones también se conocen como sales y pueden formarse mediante reacciones ácido-base.[25] Los compuestos iónicos también se pueden producir a partir de sus iones constituyentes por evaporación de su disolvente, precipitación, congelación, una reacción en estado sólido o la reacción de transferencia de electrones de metales reactivos con no metales reactivos, como los gases halógenos. Los compuestos iónicos suelen tener altos puntos de fusión y ebullición, y son duros y quebradizos. Como sólidos, casi siempre son eléctricamente aislantes, pero cuando se funden o disuelven se vuelven altamente conductores, porque se movilizan los iones.[26]​ Compuestos intermetálicos Un compuesto intermetálico es un tipo de aleación metálica que forma un compuesto de estado sólido ordenado entre dos o más elementos metálicos. Los intermetálicos son generalmente duros y quebradizos, con buenas propiedades mecánicas a altas temperaturas.[27]​[28]​[29] Se pueden clasificar como compuestos intermetálicos estequiométricos o no estequiométricos.[27]​ Complejos químicos Artículo principal: Complejo (química) Un complejo de coordinación consiste en un átomo o ion central, que generalmente es metálico y se llama centro de coordinación, y una matriz circundante de moléculas o iones unidos, que a su vez se conocen como ligandos o agentes complejantes.[30]​[31]​[32] Muchos compuestos que contienen metales, especialmente los de metales de transición, son complejos de coordinación.[33] Un complejo de coordinación cuyo centro es un átomo metálico se denomina complejo metálico o elemento de bloque d.[34]​ Enlaces y fuerzas Los compuestos se mantienen unidos por medio de diferentes tipos de enlaces y fuerzas. Las diferencias entre los tipos de enlaces de los compuestos dependen del tipo de elemento presente en el compuesto. Las fuerzas de dispersión de London son las fuerzas más débiles entre las fuerzas intermoleculares. Son fuerzas de atracción temporales que se forman cuando los electrones en dos átomos adyacentes se colocan de manera que crean un dipolo temporal. Además, estas fuerzas son responsables de la condensación de sustancias no polares en líquidos y posterior congelación a un estado sólido dependiendo de la temperatura del ambiente.[35]​ Un enlace covalente, también conocido como enlace molecular, implica el intercambio de electrones entre dos átomos. Principalmente, este tipo de enlace se produce entre elementos que aparecen uno cerca del otro en la tabla periódica de elementos, aunque se observa entre algunos metales y no metales. Esto se debe al mecanismo de este tipo de enlace. Los elementos cercanos en la tabla periódica tienden a tener electronegatividades similares, lo que significa que tienen una afinidad similar por los electrones. Como ninguno de los elementos tiene una afinidad más fuerte para donar o ganar electrones, hace que los elementos compartan electrones de manera que ambos elementos tengan un octeto más estable. El enlace iónico se produce cuando los electrones de valencia se transfieren completamente entre los elementos. Al contrario que el covalente, este enlace químico crea dos iones de carga opuesta. Los metales en enlaces iónicos generalmente pierden sus electrones de valencia, convirtiéndose en cationes, cargados positivamente. El no metal ganará los electrones del metal, haciendo que el no metal sea un anión, es decir, cargado negativamente. Es decir, los enlaces iónicos se producen entre un donador de electrones, generalmente un metal, y un aceptor de electrones, que tiende a ser un no metal.[36]​ El enlace de hidrógeno se produce cuando un átomo de hidrógeno unido a un átomo electronegativo forma una conexión electrostática con otro átomo electronegativo a través de dipolos o cargas que interactúan.[37]​[38]​[39]​ Reacciones Un compuesto se puede convertir en una composición química diferente (productos) mediante la interacción con un segundo compuesto químico (reactivos) a través de una reacción química. En este proceso, los enlaces entre los átomos se rompen en ambos compuestos que interactúan, y luego los enlaces se reforman para obtener nuevas asociaciones entre los mismos átomos. Esquemáticamente, esta reacción podría describirse como AB + CD → AD + CB, donde A, B, C y D son cada uno átomos únicos; y AB, AD, CD y CB son cada uno compuestos ùnicos ."

ksampletext_wikipedia_biol_celula: str = "Célula. La célula (del latín cellula, diminutivo de cella, ‘celda’) es la unidad morfológica y funcional de todo ser vivo. De hecho, la célula es el elemento de menor tamaño que puede considerarse vivo.[2] De este modo, puede clasificarse a los organismos vivos según el número de células que posean: si solo tienen una, se les denomina unicelulares (como pueden ser los protozoos o las bacterias, organismos microscópicos); si poseen más, se les llama pluricelulares. En estos últimos el número de células es variable: de unos pocos cientos, como en algunos nematodos, a cientos de billones (1014), como en el caso del ser humano. Las células suelen poseer un tamaño de 10 µm y una masa de 1 ng, si bien existen células mucho mayores. Célula animal La teoría celular, propuesta en 1838 para los vegetales y en 1839 para los animales,[3] por Matthias Jakob Schleiden y Theodor Schwann, postula que todos los organismos están compuestos por células, y que todas las células derivan de otras precedentes. De este modo, todas las funciones vitales emanan de la maquinaria celular y de la interacción entre células adyacentes; además, la tenencia de la información genética, base de la herencia, en su ADN permite la transmisión de aquella de generación en generación.[4]​ La aparición del primer organismo vivo sobre la Tierra suele asociarse al nacimiento de la primera célula. Si bien existen muchas hipótesis que especulan cómo ocurrió, usualmente se describe que el proceso se inició gracias a la transformación de moléculas inorgánicas en orgánicas bajo unas condiciones ambientales adecuadas; tras esto, dichas biomoléculas se asociaron dando lugar a entes complejos capaces de autorreplicarse. Existen posibles evidencias fósiles de estructuras celulares en rocas datadas en torno a 4 o 3,5 miles de millones de años (gigaaños o Ga).[5]​[6]​[nota 1] Se han encontrado evidencias muy fuertes de formas de vida unicelulares fosilizadas en microestructuras en rocas de la formación Strelley Pool, en Australia Occidental, con una antigüedad de 3,4 Ga.[cita requerida] Se trataría de los fósiles de células más antiguos encontrados hasta la fecha. Evidencias adicionales muestran que su metabolismo sería anaerobio y basado en el sulfuro.[7]​ Tipos celulares Existen dos grandes tipos celulares: Célula procariota, propia de los procariontes, que comprende las células de arqueas y bacterias. Célula eucariota, propia de los eucariontes, tales como la célula animal, célula vegetal, y las células de hongos y protistas. Historia y teoría celular La historia de la biología celular ha estado ligada al desarrollo tecnológico que pudiera sustentar su estudio. De este modo, el primer acercamiento a su morfología se inicia con la popularización del microscopio rudimentario de lentes compuestas en el siglo XVII, se suplementa con diversas técnicas histológicas para microscopía óptica en los siglos XIX y XX y alcanza un mayor nivel resolutivo mediante los estudios de microscopía electrónica, de fluorescencia y confocal, entre otros, ya en el siglo XX. El desarrollo de herramientas moleculares, basadas en el manejo de ácidos nucleicos y enzimas permitieron un análisis más exhaustivo a lo largo del siglo XX.[8]​ Descubrimiento Robert Hooke, quien acuñó el término «célula». Las primeras aproximaciones al estudio de la célula surgieron en el siglo XVII;[9] tras el desarrollo a finales del siglo XVI de los primeros microscopios.[10] Estos permitieron realizar numerosas observaciones, que condujeron en apenas doscientos años a un conocimiento morfológico relativamente aceptable. A continuación se enumera una breve cronología de tales descubrimientos: 1665: Robert Hooke publicó los resultados de sus observaciones sobre tejidos vegetales, como el corcho, realizadas con un microscopio de 50 aumentos construido por él mismo. Este investigador fue el primero que, al ver en esos tejidos unidades que se repetían a modo de celdillas de un panal, las bautizó como elementos de repetición, «células» (del latín cellulae, celdillas). Pero Hooke solo pudo observar células muertas por lo que no pudo describir las estructuras de su interior.[11]​ Década de 1670: Anton van Leeuwenhoek observó diversas células eucariotas (como protozoos y espermatozoides) y procariotas (bacterias). 1745: John Needham describió la presencia de «animálculos» o «infusorios»; se trataba de organismos unicelulares. Dibujo de la estructura del corcho observado por Robert Hooke bajo su microscopio y tal como aparece publicado en Micrographia. Década de 1830: Theodor Schwann estudió la célula animal; junto con Matthias Schleiden postularon que las células son las unidades elementales en la formación de las plantas y animales, y que son la base fundamental del proceso vital. 1831: Robert Brown describió el núcleo celular. 1839: Purkinje observó el citoplasma celular. 1857: Kölliker identificó las mitocondrias. 1858: Rudolf Virchow postuló que todas las células provienen de otras células. 1860: Pasteur realizó multitud de estudios sobre el metabolismo de levaduras y sobre la asepsia. 1880: August Weismann descubrió que las células actuales comparten similitud estructural y molecular con células de tiempos remotos. 1931: Ernst Ruska construyó el primer microscopio electrónico de transmisión en la Universidad de Berlín. Cuatro años más tarde, obtuvo una resolución óptica doble a la del microscopio óptico. 1981: Lynn Margulis publica su hipótesis sobre la endosimbiosis serial, que explica el origen de la célula eucariota.[12]​ Teoría celular Artículo principal: Teoría celular El concepto de célula como unidad anatómica y funcional de los organismos surgió entre los años 1830 y 1880, aunque fue en el siglo XVII cuando Robert Hooke describió por vez primera la existencia de las mismas, al observar en una preparación vegetal la presencia de una estructura organizada que derivaba de la arquitectura de las paredes celulares vegetales. En 1830 se disponía ya de microscopios con una óptica más avanzada, lo que permitió a investigadores como Theodor Schwann y Matthias Schleiden definir los postulados de la teoría celular, la cual afirma, entre otras cosas: Que la célula es una unidad morfológica de todo ser vivo: es decir, que en los seres vivos todo está formado por células o por sus productos de secreción. Este primer postulado sería completado por Rudolf Virchow con la afirmación Omnis cellula ex cellula, la cual indica que toda célula deriva de una célula precedente (biogénesis). En otras palabras, este postulado constituye la refutación de la teoría de generación espontánea o ex novo, que hipotetizaba la posibilidad de que se generara vida a partir de elementos inanimados.[13]​ Un tercer postulado de la teoría celular indica que las funciones vitales de los organismos ocurren dentro de las células, o en su entorno inmediato, y son controladas por sustancias que ellas secretan. Cada célula es un sistema abierto, que intercambia materia y energía con su medio. En una célula ocurren todas las funciones vitales, de manera que basta una sola de ellas para que haya un ser vivo (que será un individuo unicelular). Así pues, la célula es la unidad fisiológica de la vida. El cuarto postulado expresa que cada célula contiene toda la información hereditaria necesaria para el control de su propio ciclo y del desarrollo y el funcionamiento de un organismo de su especie, así como para la transmisión de esa información a la siguiente generación celular.[14]​ Definición Se define a la célula como la unidad morfológica y funcional de todo ser vivo. De hecho, la célula es el elemento de menor tamaño que puede considerarse vivo. Como tal posee una membrana de fosfolípidos con permeabilidad selectiva que mantiene un medio interno altamente ordenado y diferenciado del medio externo en cuanto a su composición, sujeta a control homeostático, la cual consiste en biomoléculas y algunos metales y electrolitos. La estructura se automantiene activamente mediante el metabolismo, asegurándose la coordinación de todos los elementos celulares y su perpetuación por replicación a través de un genoma codificado por ácidos nucleicos. La parte de la biología que se ocupa de ella es la citología. Características Las células, como sistemas termodinámicos complejos, poseen una serie de elementos estructurales y funcionales comunes que posibilitan su supervivencia; no obstante, los distintos tipos celulares presentan modificaciones de estas características comunes que permiten su especialización funcional y, por ello, la ganancia de complejidad.[15] De este modo, las células permanecen altamente organizadas a costa de incrementar la entropía del entorno, uno de los requisitos de la vida.[16]​ Características estructurales La existencia de polímeros como la celulosa en la pared vegetal permite sustentar la estructura celular empleando un armazón externo. Individualidad: Todas las células están rodeadas de una envoltura (que puede ser una bicapa lipídica desnuda, en células animales; una pared de polisacárido, en hongos y vegetales; una membrana externa y otros elementos que definen una pared compleja, en bacterias Gram negativas; una pared de peptidoglicano, en bacterias Gram positivas; o una pared de variada composición, en arqueas)[9] que las separa y comunica con el exterior, que controla los movimientos celulares y que mantiene el potencial de membrana. Contienen un medio interno acuoso, el citosol, que forma la mayor parte del volumen celular y en el que están inmersos los orgánulos celulares. Poseen material genético en forma de ADN, el material hereditario de los genes, que contiene las instrucciones para el funcionamiento celular, así como ARN, a fin de que el primero se exprese.[17]​ Tienen enzimas y otras proteínas, que sustentan, junto con otras biomoléculas, un metabolismo activo. Características funcionales Estructura tridimensional de una enzima, un tipo de proteínas implicadas en el metabolismo celular. Las células vivas son un sistema bioquímico complejo. Las características que permiten diferenciar las células de los sistemas químicos no vivos son: Nutrición. Las células toman sustancias del medio, las transforman de una forma a otra, liberan energía y eliminan productos de desecho, mediante el metabolismo. Crecimiento y multiplicación. Las células son capaces de dirigir su propia síntesis. A consecuencia de los procesos nutricionales, una célula crece y se divide, formando dos células, en una célula idéntica a la célula original, mediante la división celular. Diferenciación. Muchas células pueden sufrir cambios de forma o función en un proceso llamado diferenciación celular. Cuando una célula se diferencia, se forman algunas sustancias o estructuras que no estaban previamente formadas y otras que lo estaban dejan de formarse. La diferenciación es a menudo parte del ciclo celular en que las células forman estructuras especializadas relacionadas con la reproducción, la dispersión o la supervivencia. Señalización. Las células responden a estímulos químicos y físicos tanto del medio externo como de su interior y, en el caso de células móviles, hacia determinados estímulos ambientales o en dirección opuesta mediante un proceso que se denomina quimiotaxis. Además, frecuentemente las células pueden interaccionar o comunicar con otras células, generalmente por medio de señales o mensajeros químicos, como hormonas, neurotransmisores, factores de crecimiento... en seres pluricelulares en complicados procesos de comunicación celular y transducción de señales. Evolución. A diferencia de las estructuras inanimadas, los organismos unicelulares y pluricelulares evolucionan. Esto significa que hay cambios hereditarios (que ocurren a baja frecuencia en todas las células de modo regular) que pueden influir en la adaptación global de la célula o del organismo superior de modo positivo o negativo. El resultado de la evolución es la selección de aquellos organismos mejor adaptados a vivir en un medio particular. Las propiedades celulares no tienen por qué ser constantes a lo largo del desarrollo de un organismo: evidentemente, el patrón de expresión de los genes varía en respuesta a estímulos externos, además de factores endógenos.[18] Un aspecto importante a controlar es la pluripotencialidad, característica de algunas células que les permite dirigir su desarrollo hacia un abanico de posibles tipos celulares. En metazoos, la genética subyacente a la determinación del destino de una célula consiste en la expresión de determinados factores de transcripción específicos del linaje celular al cual va a pertenecer, así como a modificaciones epigenéticas. Además, la introducción de otro tipo de factores de transcripción mediante ingeniería genética en células somáticas basta para inducir la mencionada pluripotencialidad, luego este es uno de sus fundamentos moleculares.[19]​ Tamaño, forma y función Comparativa de tamaño entre neutrófilos, células sanguíneas eucariotas (de mayor tamaño), y bacterias Bacillus anthracis, procariotas (de menor tamaño, con forma de bastón). El tamaño y la forma de las células depende de sus elementos más periféricos (por ejemplo, la pared, si la hubiere) y de su andamiaje interno (es decir, el citoesqueleto). Además, la competencia por el espacio tisular provoca una morfología característica: por ejemplo, las células vegetales, poliédricas in vivo, tienden a ser esféricas in vitro.[20] Incluso pueden existir parámetros químicos sencillos, como los gradientes de concentración de una sal, que determinen la aparición de una forma compleja.[21]​ En cuanto al tamaño, la mayoría de las células son microscópicas, es decir, no son observables a simple vista. (un milímetro cúbico de sangre puede contener unos cinco millones de células),[15] A pesar de ser muy pequeñas, el tamaño de las células es extremadamente variable. La célula más pequeña observada, en condiciones normales, corresponde a Mycoplasma genitalium, de 0,2 μm, encontrándose cerca del límite teórico de 0,17 μm.[22] Existen bacterias con 1 y 2 μm de longitud. Las células humanas son muy variables: hematíes de 7 micras, hepatocitos con 20 micras, espermatozoides de 53 μm, óvulos de 150 μm e, incluso, algunas neuronas de en torno a un metro de longitud. En las células vegetales los granos de polen pueden llegar a medir de 200 a 300 μm. Respecto a las células de mayor tamaño; por ejemplo, los xenofióforos,[23] son foraminíferos unicelulares que han desarrollado un gran tamaño, los cuales alcanzar tamaños macroscópicos (Syringammina fragilissima alcanza los 20 cm de diámetro).[24]​ Para la viabilidad de la célula y su correcto funcionamiento siempre se debe tener en cuenta la relación superficie-volumen.[16] Puede aumentar considerablemente el volumen de la célula y no así su superficie de intercambio de membrana, lo que dificultaría el nivel y regulación de los intercambios de sustancias vitales para la célula. Respecto de su forma, las células presentan una gran variabilidad, e, incluso, algunas no la poseen bien definida o permanente. Pueden ser: fusiformes (forma de huso), estrelladas, prismáticas, aplanadas, elípticas, globosas o redondeadas, etc. Algunas tienen una pared rígida y otras no, lo que les permite deformar la membrana y emitir prolongaciones citoplasmáticas (pseudópodos) para desplazarse o conseguir alimento. Hay células libres que no muestran esas estructuras de desplazamiento, pero poseen cilios o flagelos, que son estructuras derivadas de un orgánulo celular (el centrosoma) que dota a estas células de movimiento.[2] De este modo, existen multitud de tipos celulares, relacionados con la función que desempeñan; por ejemplo: Células contráctiles que suelen ser alargadas, como los miocitos esqueléticos. Células con finas prolongaciones, como las neuronas que transmiten el impulso nervioso. Células con microvellosidades o con pliegues, como las del intestino para ampliar la superficie de contacto y de intercambio de sustancias. Células cúbicas, prismáticas o aplanadas como las epiteliales que recubren superficies como las losas de un pavimento. Estudio de las células Los biólogos utilizan diversos instrumentos para lograr el conocimiento de las células. Obtienen información de sus formas, tamaños y componentes, que les sirve para comprender además las funciones que en ellas se realizan. Desde las primeras observaciones de células, hace más de 300 años, hasta la época actual, las técnicas y los aparatos se han ido perfeccionando, originando una rama más de la biología: la microscopía. Dado el pequeño tamaño de la gran mayoría de las células, el uso del microscopio es de enorme valor en la investigación biológica. En la actualidad, los biólogos utilizan dos tipos básicos de microscopio: los ópticos y los electrónicos. Un microscopio óptico utiliza la luz visible para el estudio de muestras. Obteniedo imágenes aumentadas a partir de la desviación de la luz con lentes de cristal. Es utilizado para la observación de tejidos y células desde su invención en el siglo XVII. Los microscopios electrónicos son aquellos que utilizan electrones a alta velocidad para el análisis de muestras. Lo cual ofrece mayores capacidades de aumento que los de tipo óptico. Utilizándose en ramas como la medicina, y el estudio de materiales a nivel atómico. Además de usarse para la observación de células, virus y tejidos a nivel subcelular. Sin embargo, estos presentan limitaciones, debido a una cámara de vacío y a la preparación que requieren las muestras para ser analizadas, no pueden observarse células vivas.[25]​ Escaneo de microscopio electrónico de barrido muestra el SARS-CoV-2 emergiendo de la superficie de las células cultivadas en el laboratorio. Imagen del virus SARS-CoV-2, tomada con un microscopio electrónico de barrido. La célula procariota Artículo principal: Célula procariota Las células procariotas son pequeñas y menos complejas que las eucariotas. Contienen ribosomas, pero carecen de sistemas de endomembranas (esto es, orgánulos delimitados por membranas biológicas, como puede ser el núcleo celular). Por ello poseen el material genético en el citosol. Sin embargo, existen excepciones: algunas bacterias fotosintéticas poseen sistemas de membranas internos.[26] También en el filo Planctomycetota existen organismos como Pirellula que rodean su material genético mediante una membrana intracitoplasmática y Gemmata obscuriglobus que lo rodea con doble membrana. Esta última posee además otros compartimentos internos de membrana, posiblemente conectados con la membrana externa del nucleoide y con la membrana plasmática, que no está asociada a peptidoglucano.[27]​[28]​[29]​ Estudios realizados en 2017, demuestran otra particularidad de Gemmata: presenta estructuras similares al poro nuclear, en la membrana que rodea su cuerpo nuclear.[30]​ Por lo general podría decirse que los procariotas carecen de citoesqueleto. Sin embargo se ha observado que algunas bacterias, como Bacillus subtilis, poseen proteínas tales como MreB y mbl que actúan de un modo similar a la actina y son importantes en la morfología celular.[31] Fusinita van den Ent, en Nature, va más allá, afirmando que los citoesqueletos de actina y tubulina tienen origen procariótico.[32]​ De gran diversidad, los procariotas sustentan un metabolismo extraordinariamente complejo, en algunos casos exclusivo de ciertos taxa, como algunos grupos de bacterias, lo que incide en su versatilidad ecológica.[13] Los procariotas se clasifican, según Carl Woese, en arqueas y bacterias.[33]​ Arqueas Artículo principal: Arquea Estructura bioquímica de la membrana de arqueas (arriba) comparada con la de bacterias y eucariotas (en medio): nótese la presencia de enlaces éter (2) en sustitución de los tipo éster (6) en los fosfolípidos. Las arqueas poseen un diámetro celular comprendido entre 0,1 y 15 μm, aunque las formas filamentosas pueden ser mayores por agregación de células. Presentan multitud de formas distintas: incluso las hay descritas cuadradas y planas.[34] Algunas arqueas tienen flagelos y son móviles. Las arqueas, al igual que las bacterias, no tienen membranas internas que delimiten orgánulos. Como todos los organismos presentan ribosomas, pero a diferencia de los encontrados en las bacterias que son sensibles a ciertos agentes antimicrobianos, los de las arqueas, más cercanos a los eucariotas, no lo son. La membrana celular tiene una estructura similar a la de las demás células, pero su composición química es única, con enlaces tipo éter en sus lípidos.[35] Casi todas las arqueas poseen una pared celular (algunos Thermoplasma son la excepción) de composición característica, por ejemplo, no contienen peptidoglicano (mureína), propio de bacterias. No obstante, pueden clasificarse bajo la tinción de Gram, de vital importancia en la taxonomía de bacterias; sin embargo, en arqueas, poseedoras de una estructura de pared en absoluto común a la bacteriana, dicha tinción es aplicable, pero carece de valor taxonómico. El orden Methanobacteriales tiene una capa de pseudomureína, que provoca que dichas arqueas respondan como positivas a la tinción de Gram.[36]​[37]​[38]​ Como en casi todos los procariotas, las células de las arqueas carecen de núcleo, y presentan un solo cromosoma circular. Existen elementos extracromosómicos, tales como plásmidos. Sus genomas son de pequeño tamaño, sobre 2-4 millones de pares de bases. También es característica la presencia de ARN polimerasas de constitución compleja y un gran número de nucleótidos modificados en los ácidos ribonucleicos ribosomales. Por otra parte, su ADN se empaqueta en forma de nucleosomas, como en los eucariotas, gracias a proteínas semejantes a las histonas y algunos genes poseen intrones.[39] Pueden reproducirse por fisión binaria o múltiple, fragmentación o gemación. Bacterias Artículo principal: Bacteria Estructura de la célula procariota. Las bacterias son organismos relativamente sencillos, de dimensiones muy reducidas, de apenas unas micras en la mayoría de los casos. Como otros procariotas, carecen de un núcleo delimitado por una membrana, aunque presentan un nucleoide, una estructura elemental que contiene una gran molécula generalmente circular de ADN.[17]​[40] Carecen de núcleo celular y demás orgánulos delimitados por membranas biológicas.[41] En el citoplasma se pueden apreciar plásmidos, pequeñas moléculas circulares de ADN que coexisten con el nucleoide y que contienen genes: son comúnmente usados por las bacterias en la parasexualidad (reproducción sexual bacteriana). El citoplasma también contiene ribosomas y diversos tipos de gránulos. En algunos casos, puede haber estructuras compuestas por membranas, generalmente relacionadas con la fotosíntesis.[9]​ Poseen una membrana celular compuesta de lípidos, en forma de una bicapa y sobre ella se encuentra una cubierta en la que existe un polisacárido complejo denominado peptidoglicano; dependiendo de su estructura y subsecuente su respuesta a la tinción de Gram, se clasifica a las bacterias en Gram positivas y Gram negativas. El espacio comprendido entre la membrana celular y la pared celular (o la membrana externa, si esta existe) se denomina espacio periplásmico. Algunas bacterias presentan una cápsula. Otras son capaces de generar endosporas (estadios latentes capaces de resistir condiciones extremas) en algún momento de su ciclo vital. Entre las formaciones exteriores propias de la célula bacteriana destacan los flagelos (de estructura completamente distinta a la de los flagelos eucariotas) y los pili (estructuras de adherencia y relacionadas con la parasexualidad).[9]​ La mayoría de las bacterias disponen de un único cromosoma circular y suelen poseer elementos genéticos adicionales, como distintos tipos de plásmidos. Su reproducción, binaria y muy eficiente en el tiempo, permite la rápida expansión de sus poblaciones, generándose un gran número de células que son virtualmente clones, esto es, idénticas entre sí.[39]​ La célula eucariota Artículo principal: Célula eucariota Las células eucariotas son el exponente de la complejidad celular actual.[15] Presentan una estructura básica relativamente estable caracterizada por la presencia de distintos tipos de orgánulos intracitoplasmáticos especializados, entre los cuales destaca el núcleo, que alberga el material genético. Especialmente en los organismos pluricelulares, las células pueden alcanzar un alto grado de especialización. Dicha especialización o diferenciación es tal que, en algunos casos, compromete la propia viabilidad del tipo celular en aislamiento. Así, por ejemplo, las neuronas dependen para su supervivencia de las células gliales.[13]​ Por otro lado, la estructura de la célula varía dependiendo de la situación taxonómica del ser vivo: de este modo, las células vegetales difieren de las animales, así como de las de los hongos. Por ejemplo, las células animales carecen de pared celular, son muy variables, no tiene plastos, puede tener vacuolas, pero no son muy grandes y presentan centríolos (que son agregados de microtúbulos cilíndricos que contribuyen a la formación de los cilios y los flagelos y facilitan la división celular). Las células de los vegetales, por su lado, presentan una pared celular compuesta principalmente de celulosa, disponen de plastos como cloroplastos (orgánulo capaz de realizar la fotosíntesis), cromoplastos (orgánulos que acumulan pigmentos) o leucoplastos (orgánulos que acumulan el almidón fabricado en la fotosíntesis), poseen vacuolas de gran tamaño que acumulan sustancias de reserva o de desecho producidas por la célula y finalmente cuentan también con plasmodesmos, que son conexiones citoplasmáticas que permiten la circulación directa de las sustancias del citoplasma de una célula a otra, con continuidad de sus membranas plasmáticas.[42]​ Diagrama de una célula animal. (1. Nucléolo, 2. Núcleo, 3. Ribosoma, 4. Vesícula, 5. Retículo endoplasmático rugoso, 6. Aparato de Golgi, 7. Citoesqueleto (microtúbulos), 8. Retículo endoplasmático liso, 9. Mitocondria, 10. Vacuola, 11. Citoplasma, 12. Lisosoma, 13. Centríolos). Diagrama de una célula vegetal Compartimentos Las células son entes dinámicos, con un metabolismo celular interno de gran actividad cuya estructura es un flujo entre rutas anastomosadas. Un fenómeno observado en todos los tipos celulares es la compartimentalización, que consiste en una heterogeneidad que da lugar a entornos más o menos definidos (rodeados o no mediante membranas biológicas) en las cuales existe un microentorno que aglutina a los elementos implicados en una ruta biológica.[43] Esta compartimentalización alcanza su máximo exponente en las células eucariotas, las cuales están formadas por diferentes estructuras y orgánulos que desarrollan funciones específicas, lo que supone un método de especialización espacial y temporal.[2] No obstante, células más sencillas, como los procariotas, ya poseen especializaciones semejantes.[44]​ Membrana plasmática y superficie celular Artículo principal: Membrana plasmática La composición de la membrana plasmática varía entre células dependiendo de la función o del tejido en la que se encuentre, pero posee elementos comunes. Está compuesta por una doble capa de fosfolípidos, por proteínas unidas no covalentemente a esa bicapa, y por glúcidos unidos covalentemente a lípidos o proteínas. Generalmente, las moléculas más numerosas son las de lípidos; sin embargo, las proteínas, debido a su mayor masa molecular, representan aproximadamente el 50 % de la masa de la membrana.[43]​ Un modelo que explica el funcionamiento de la membrana plasmática es el modelo del mosaico fluido, de J. S. Singer y Garth Nicolson (1972), que desarrolla un concepto de unidad termodinámica basada en las interacciones hidrófobas entre moléculas y otro tipo de enlaces no covalentes.[45]​ Esquema de una membrana celular. Se observa la bicapa de fosfolípidos, las proteínas y otras moléculas asociadas que permiten las funciones inherentes a este orgánulo. Dicha estructura de membrana sustenta un complejo mecanismo de transporte, que posibilita un fluido intercambio de masa y energía entre el entorno intracelular y el externo.[43] Además, la posibilidad de transporte e interacción entre moléculas de células aledañas o de una célula con su entorno faculta a estas poder comunicarse químicamente, esto es, permite la señalización celular. Neurotransmisores, hormonas, mediadores químicos locales afectan a células concretas modificando el patrón de expresión génica mediante mecanismos de transducción de señal.[46]​ Sobre la bicapa lipídica, independientemente de la presencia o no de una pared celular, existe una matriz que puede variar, de poco conspicua, como en los epitelios, a muy extensa, como en el tejido conjuntivo. Dicha matriz, denominada glucocalix (glicocáliz), rica en líquido tisular, glucoproteínas, proteoglicanos y fibras, también interviene en la generación de estructuras y funciones emergentes, derivadas de las interacciones célula-célula.[13]​ Estructura y expresión génica Artículo principal: Expresión génica El ADN y sus distintos niveles de empaquetamiento. Las células eucariotas poseen su material genético en, generalmente, un solo núcleo celular, delimitado por una envoltura consistente en dos bicapas lipídicas atravesadas por numerosos poros nucleares y en continuidad con el retículo endoplasmático. En su interior, se encuentra el material genético, el ADN, observable, en las células en interfase, como cromatina de distribución heterogénea. A esta cromatina se encuentran asociadas multitud de proteínas, entre las cuales destacan las histonas, así como ARN, otro ácido nucleico.[47]​ Dicho material genético se encuentra inmerso en una actividad continua de regulación de la expresión génica; las ARN polimerasas transcriben ARN mensajero continuamente, que, exportado al citosol, es traducido a proteína, de acuerdo a las necesidades fisiológicas. Asimismo, dependiendo del momento del ciclo celular, dicho ADN puede entrar en replicación, como paso previo a la mitosis.[39] No obstante, las células eucarióticas poseen material genético extranuclear: concretamente, en mitocondrias y plastos, si los hubiere; estos orgánulos conservan una independencia genética parcial del genoma nuclear.[48]​[49]​ Síntesis y degradación de macromoléculas Dentro del citosol, esto es, la matriz acuosa que alberga a los orgánulos y demás estructuras celulares, se encuentran inmersos multitud de tipos de maquinaria de metabolismo celular: orgánulos, inclusiones, elementos del citoesqueleto, enzimas... De hecho, estas últimas corresponden al 20 % de las enzimas totales de la célula.[13]​ Estructura de los ribosomas; 1) subunidad mayor, 2) subunidad menor. Imagen de un núcleo, el retículo endoplasmático y el aparato de Golgi; 1, Núcleo. 2, Poro nuclear.3, Retículo endoplasmático rugoso (REr).4, Retículo endoplasmático liso (REl). 5, Ribosoma en el RE rugoso. 6, Proteínas siendo transportadas.7, Vesícula (transporte). 8, Aparato de Golgi. 9, Lado cis del aparato de Golgi.10, Lado trans del aparato de Golgi.11, Cisternas del aparato de Golgi. Ribosoma: Los ribosomas, visibles al microscopio electrónico como partículas esféricas,[50] son complejos supramoleculares encargados de ensamblar proteínas a partir de la información genética que les llega del ADN transcrita en forma de ARN mensajero. Elaborados en el núcleo, desempeñan su función de síntesis de proteínas en el citoplasma. Están formados por ARN ribosómico y por diversos tipos de proteínas. Estructuralmente, tienen dos subunidades. En las células, estos orgánulos aparecen en diferentes estados de disociación. Cuando están completos, pueden estar aislados o formando grupos (polisomas). También pueden aparecer asociados al retículo endoplasmático rugoso o a la envoltura nuclear.[39]​ Retículo endoplasmático: El retículo endoplasmático es orgánulo vesicular interconectado que forma cisternas, tubos aplanados y sáculos comunicados entre sí. Intervienen en funciones relacionadas con la síntesis proteica, glicosilación de proteínas, metabolismo de lípidos y algunos esteroides, detoxificación, así como el tráfico de vesículas. En células especializadas, como las miofibrillas o células musculares, se diferencia en el retículo sarcoplásmico, orgánulo decisivo para que se produzca la contracción muscular.[15]​ Aparato de Golgi: El aparato de Golgi es un orgánulo formado por apilamientos de sáculos denominados dictiosomas, si bien, como ente dinámico, estos pueden interpretarse como estructuras puntuales fruto de la coalescencia de vesículas.[51]​[52] Recibe las vesículas del retículo endoplasmático rugoso que han de seguir siendo procesadas. Dentro de las funciones que posee el aparato de Golgi se encuentran la glicosilación de proteínas, selección, destinación, glicosilación de lípidos y la síntesis de polisacáridos de la matriz extracelular. Posee tres compartimientos; uno proximal al retículo endoplasmático, denominado «compartimento cis», donde se produce la fosforilación de las manosas de las enzimas que han de dirigirse al lisosoma; el «compartimento intermedio», con abundantes manosidasas y N-acetil-glucosamina transferasas; y el «compartimento o red trans», el más distal, donde se transfieren residuos de galactosa y ácido siálico, y del que emergen las vesículas con los diversos destinos celulares.[13]​ Lisosoma: Los lisosomas son orgánulos que albergan multitud de enzimas hidrolíticas. De morfología muy variable, no se ha demostrado su existencia en células vegetales.[13] Una característica que agrupa a todos los lisosomas es la posesión de hidrolasas ácidas: proteasas, nucleasas, glucosidasas, lisozima, arilsulfatasas, lipasas, fosfolipasas y fosfatasas. Procede de la fusión de vesículas procedentes del aparato de Golgi, que, a su vez, se fusionan en un tipo de orgánulo denominado endosoma temprano, el cual, al acidificarse y ganar en enzimas hidrolíticos, pasa a convertirse en el lisosoma funcional. Sus funciones abarcan desde la degradación de macromoléculas endógenas o procedentes de la fagocitosis a la intervención en procesos de apoptosis.[53]​ La vacuola regula el estado de turgencia de la célula vegetal. Vacuola vegetal: Las vacuolas vegetales, numerosas y pequeñas en células meristemáticas y escasas y grandes en células diferenciadas, son orgánulos exclusivos de los representantes del mundo vegetal. Inmersas en el citosol, están delimitadas por el tonoplasto, una membrana lipídica. Sus funciones son: facilitar el intercambio con el medio externo, mantener la turgencia celular, la digestión celular y la acumulación de sustancias de reserva y subproductos del metabolismo.[42]​ Inclusión citoplasmática: Las inclusiones son acúmulos nunca delimitados por membrana de sustancias de diversa índole, tanto en células vegetales como animales. Típicamente se trata de sustancias de reserva que se conservan como acervo metabólico: almidón, glucógeno, triglicéridos, proteínas..., aunque también existen de pigmentos.[13]​ Conversión energética El metabolismo celular está basado en la transformación de unas sustancias químicas, denominadas metabolitos, en otras; dichas reacciones químicas transcurren catalizadas mediante enzimas. Si bien buena parte del metabolismo sucede en el citosol, como la glucólisis, existen procesos específicos de orgánulos.[46]​ Modelo de una mitocondria: 1. Membrana interna; 2. Membrana externa; 3. Cresta mitocondrial; 4. Matriz mitocondrial. Mitocondria: Las mitocondrias son orgánulos de aspecto, número y tamaño variable que intervienen en el ciclo de Krebs, fosforilación oxidativa y en la cadena de transporte de electrones de la respiración. Presentan una doble membrana, externa e interna, que dejan entre ellas un espacio perimitocondrial; la membrana interna, plegada en crestas hacia el interior de la matriz mitocondrial, posee una gran superficie. En su interior posee generalmente una sola molécula de ADN, el genoma mitocondrial, típicamente circular, así como ribosomas más semejantes a los bacterianos que a los eucariotas.[13] Según la teoría endosimbiótica, se asume que la primera protomitocondria era un tipo de proteobacteria.[54]​ Estructura de un cloroplasto. Cloroplasto: Los cloroplastos son los orgánulos celulares que en los organismos eucariotas fotosintéticos se ocupan de la fotosíntesis. Están limitados por una envoltura formada por dos membranas concéntricas y contienen vesículas, los tilacoides, donde se encuentran organizados los pigmentos y demás moléculas implicadas en la conversión de la energía lumínica en energía química. Además de esta función, los plastidios intervienen en el metabolismo intermedio, produciendo energía y poder reductor, sintetizando bases púricas y pirimidínicas, algunos aminoácidos y todos los ácidos grasos. Además, en su interior es común la acumulación de sustancias de reserva, como el almidón.[13] Se considera que poseen analogía con las cianobacterias.[55]​ Modelo de la estructura de un peroxisoma. Peroxisoma: Los peroxisomas son orgánulos muy comunes en forma de vesículas que contienen abundantes enzimas de tipo oxidasa y catalasa; de tan abundantes, es común que cristalicen en su interior. Estas enzimas cumplen funciones de detoxificación celular. Otras funciones de los peroxisomas son: las oxidaciones flavínicas generales, el catabolismo de las purinas, la beta-oxidación de los ácidos grasos, el ciclo del glioxilato, el metabolismo del ácido glicólico y la detoxificación en general.[13] Se forman de vesículas procedentes del retículo endoplasmático.[56]​ Citoesqueleto Artículo principal: Citoesqueleto Las células poseen un andamiaje que permite el mantenimiento de su forma y estructura, pero más aún, este es un sistema dinámico que interactúa con el resto de componentes celulares generando un alto grado de orden interno. Dicho andamiaje está formado por una serie de proteínas que se agrupan dando lugar a estructuras filamentosas que, mediante otras proteínas, interactúan entre ellas dando lugar a una especie de retículo. El mencionado andamiaje recibe el nombre de citoesqueleto, y sus elementos mayoritarios son: los microtúbulos, los microfilamentos y los filamentos intermedios.[2]​[nota 2]​[57]​[58]​ Microfilamentos: Los microfilamentos o filamentos de actina están formados por una proteína globular, la actina, que puede polimerizar dando lugar a estructuras filiformes. Dicha actina se expresa en todas las células del cuerpo y especialmente en las musculares, ya que está implicada en la contracción muscular, por interacción con la miosina. Además, posee lugares de unión a ATP, lo que dota a sus filamentos de polaridad.[59] Puede encontrarse en forma libre o polimerizarse en microfilamentos, que son esenciales para funciones celulares tan importantes como la movilidad y la contracción de la célula durante la división celular.[51]​ Citoesqueleto eucariota: microfilamentos en rojo, microtúbulos en verde y núcleo en azul. Microtúbulos: Los microtúbulos son estructuras tubulares de 25 nm de diámetro exterior y unos 12 nm de diámetro interior, con longitudes que varían entre unos pocos nanómetros a micrómetros, que se originan en los centros organizadores de microtúbulos y que se extienden a lo largo de todo el citoplasma. Se hallan en las células eucariotas y están formadas por la polimerización de un dímero de dos proteínas globulares, la alfa y la beta tubulina. Las tubulinas poseen capacidad de unir GTP.[2]​[51] Los microtúbulos intervienen en diversos procesos celulares que involucran desplazamiento de vesículas de secreción, movimiento de orgánulos, transporte intracelular de sustancias, así como en la división celular (mitosis y meiosis) y que, junto con los microfilamentos y los filamentos intermedios, forman el citoesqueleto. Además, constituyen la estructura interna de los cilios y los flagelos.[2]​[51]​ Filamentos intermedios: Los filamentos intermedios son componentes del citoesqueleto. Formados por agrupaciones de proteínas fibrosas, su nombre deriva de su diámetro, de 10 nm, menor que el de los microtúbulos, de 24 nm, pero mayor que el de los microfilamentos, de 7 nm. Son ubicuos en las células animales, y no existen en plantas ni hongos. Forman un grupo heterogéneo, clasificado en cinco familias: las queratinas, en células epiteliales; los neurofilamentos, en neuronas; los gliofilamentos, en células gliales; la desmina, en músculo liso y estriado; y la vimentina, en células derivadas del mesénquima.[13]​ Micrografía al microscopio electrónico de barrido mostrando la superficie de células ciliadas del epitelio de los bronquiolos. Centríolos: Los centríolos son una pareja de estructuras que forman parte del citoesqueleto de células animales. Semejantes a cilindros huecos, están rodeados de un material proteico denso llamado material pericentriolar; todos ellos forman el centrosoma o centro organizador de microtúbulos que permiten la polimerización de microtúbulos de dímeros de tubulina que forman parte del citoesqueleto. Los centríolos se posicionan perpendicularmente entre sí. Sus funciones son participar en la mitosis, durante la cual generan el huso acromático, y en la citocinesis,[60] así como, se postula, intervenir en la nucleación de microtúbulos.[61]​[62]​ Cilios y flagelos: Se trata de especializaciones de la superficie celular con motilidad; con una estructura basada en agrupaciones de microtúbulos, ambos se diferencian en la mayor longitud y menor número de los flagelos, y en la mayor variabilidad de la estructura molecular de estos últimos.[13]​ Ciclo vital Artículo principal: Ciclo celular Diagrama del ciclo celular: la interfase, en naranja, alberga a las fases G1, S y G2; la fase M, en cambio, únicamente consta de la mitosis y citocinesis, si la hubiere. El ciclo celular es el proceso ordenado y repetitivo en el tiempo mediante el cual una célula madre crece y se divide en dos células hijas. Las células que no se están dividiendo se encuentran en una fase conocida como G0, paralela al ciclo. La regulación del ciclo celular es esencial para el correcto funcionamiento de las células sanas, está claramente estructurado en fases[51]​ El estado de no división o interfase. La célula realiza sus funciones específicas y, si está destinada a avanzar a la división celular, comienza por realizar la duplicación de su ADN. El estado de división, llamado fase M, situación que comprende la mitosis y citocinesis. En algunas células la citocinesis no se produce, obteniéndose como resultado de la división una masa celular plurinucleada denominada plasmodio.[nota 3]​ A diferencia de lo que sucede en la mitosis, donde la dotación genética se mantiene, existe una variante de la división celular, propia de las células de la línea germinal, denominada meiosis. En ella, se reduce la dotación genética diploide, común a todas las células somáticas del organismo, a una haploide, esto es, con una sola copia del genoma. De este modo, la fusión, durante la fecundación, de dos gametos haploides procedentes de dos parentales distintos da como resultado un zigoto, un nuevo individuo, diploide, equivalente en dotación genética a sus padres.[63]​ La interfase consta de tres estadios claramente definidos.[2]​[51]​ Fase G1: es la primera fase del ciclo celular, en la que existe crecimiento celular con síntesis de proteínas y de ARN. Es el período que trascurre entre el fin de una mitosis y el inicio de la síntesis de ADN. En él la célula dobla su tamaño y masa debido a la continua síntesis de todos sus componentes, como resultado de la expresión de los genes que codifican las proteínas responsables de su fenotipo particular. Fase S: es la segunda fase del ciclo, en la que se produce la replicación o síntesis del ADN. Como resultado cada cromosoma se duplica y queda formado por dos cromátidas idénticas. Con la duplicación del ADN, el núcleo contiene el doble de proteínas nucleares y de ADN que al principio. Fase G2: es la segunda fase de crecimiento del ciclo celular en la que continúa la síntesis de proteínas y ARN. Al final de este período se observa al microscopio cambios en la estructura celular, que indican el principio de la división celular. Termina cuando los cromosomas empiezan a condensarse al inicio de la mitosis. La fase M es la fase de la división celular en la cual una célula progenitora se divide en dos células hijas idénticas entre sí y a la madre. Esta fase incluye la mitosis, a su vez dividida en: profase, metafase, anafase, telofase; y la citocinesis, que se inicia ya en la telofase mitótica. La incorrecta regulación del ciclo celular puede conducir a la aparición de células precancerígenas que, si no son inducidas al suicidio mediante apoptosis, puede dar lugar a la aparición de cáncer. Los fallos conducentes a dicha desregulación están relacionados con la genética celular: lo más común son las alteraciones en oncogenes, genes supresores de tumores y genes de reparación del ADN.[64]​ Origen Artículos principales: Historia de la vida y Anexo:Cronología de la historia evolutiva de la vida. Origen de la primera célula Artículo principal: Abiogénesis La aparición de la vida, y, por ello, de la célula, probablemente se inició gracias a la transformación de moléculas inorgánicas en orgánicas bajo unas condiciones ambientales adecuadas, produciéndose más adelante la interacción de estas biomoléculas generando entes de mayor complejidad. El experimento de Miller y Urey, realizado en 1953, demostró que una mezcla de compuestos orgánicos sencillos puede transformarse en algunos aminoácidos, glúcidos y lípidos (componentes todos ellos de la materia viva) bajo unas condiciones ambientales que simulan las presentes hipotéticamente en la Tierra primigenia (en torno al eón Hádico).[65] Se ha sugerido que el último antepasado común universal vivió hace más de 4200 millones de años.[66]​ Se postula que dichos componentes orgánicos se agruparon generando estructuras complejas, los coacervados de Oparin, aún acelulares que, en cuanto alcanzaron la capacidad de autoorganizarse y perpetuarse, dieron lugar a un tipo de célula primitiva, el progenote de Carl Woese, antecesor de los tipos celulares actuales.[33] Una vez se diversificó este grupo celular, dando lugar a las variantes procariotas, arqueas y bacterias, pudieron aparecer nuevos tipos de células, más complejos, por endosimbiosis, esto es, captación permanente de unos tipos celulares en otros sin una pérdida total de autonomía de aquellos.[67] De este modo, algunos autores describen un modelo en el cual la primera célula eucariota surgió por introducción de una arquea en el interior de una bacteria, dando lugar esta primera a un primitivo núcleo celular.[68] No obstante, la imposibilidad de que una bacteria pueda efectuar una fagocitosis y, por ello, captar a otro tipo de célula, dio lugar a otra hipótesis, que sugiere que fue una célula denominada cronocito la que fagocitó a una bacteria y a una arquea, dando lugar al primer organismo eucariota. De este modo, y mediante un análisis de secuencias a nivel genómico de organismos modelo eucariotas, se ha conseguido describir a este cronocito original como un organismo con citoesqueleto y membrana plasmática, lo cual sustenta su capacidad fagocítica, y cuyo material genético era el ARN, lo que puede explicar, si la arquea fagocitada lo poseía en el ADN, la separación espacial en los eucariotas actuales entre la transcripción (nuclear), y la traducción (citoplasmática).[69]​ Una dificultad adicional es el hecho de que no se han encontrado organismos eucariotas primitivamente amitocondriados como exige la hipótesis endosimbionte. Además, el equipo de María Rivera, de la Universidad de California, comparando genomas completos de todos los dominios de la vida ha encontrado evidencias de que los eucariotas contienen dos genomas diferentes, uno más semejante a bacterias y otro a arqueas, apuntando en este último caso semejanzas a los metanógenos, en particular en el caso de las histonas.[70]​[71] Esto llevó a Bill Martin y Miklós Müller a plantear la hipótesis de que la célula eucariota surgiera no por endosimbiosis, sino por fusión quimérica y acoplamiento metabólico de un metanógeno y una α-proteobacteria simbiontes a través del hidrógeno (hipótesis del hidrógeno).[72] Esta hipótesis atrae hoy en día posiciones muy encontradas, con detractores como Christian de Duve.[73]​ Harold Morowitz, un físico de la Universidad Yale, ha calculado que las probabilidades de obtener la bacteria viva más sencilla mediante cambios al azar es de 1 sobre 1 seguido por 100 000 000 000 ceros. «Este número es tan grande —dijo Robert Shapiro— que para escribirlo en forma convencional necesitaríamos varios centenares de miles de libros en blanco». Presenta la acusación de que los científicos que han abrazado la evolución química de la vida pasan por alto la evidencia aumentante y «han optado por aceptarla como verdad que no puede ser cuestionada, consagrándola así como mitología».[74]​ Origen de la célula eucariota Artículo principal: Eucariogénesis En la teoría de la simbiogenesis, la fusión entre una arquéa y una bacteria aeróbia creo la célula eucariota, con mitochondrias aeróbicas, hace unos 2500 millones de años. Una segunda fusión, hace 2000 millones de años, añadió los cloroplastos, originando la célula vegetal.[75]​ Las células eucariotas se formaron hace 2500 millones de años en un proceso llamado eucariogénesis.[76] Se acepta ampliamente que esto implicó una simbiogénesis, en la que una arquea y una bacteria se unieron para crear el primer ancestro común eucariota.[77] Esta célula tenía un nuevo nivel de complejidad y capacidad, con un núcleo[78]​[79] y mitocondrias facultativamente aeróbicas.[75] Evolucionó hasta convertirse en una población de organismos unicelulares que incluía al último ancestro común eucariota, acumulando capacidades a lo largo del camino, aunque la secuencia de los pasos involucrados ha sido cuestionada y es posible que no haya comenzado con la simbiogénesis. Presentaba al menos un centriolo y cilio, sexo (meiosis y singamia), peroxisomas y un quiste latente con una pared celular de quitina y/o celulosa.[80]​[81] A su vez, el último ancestro común eucariota dio origen al grupo terminal de los eucariotas, que contiene los ancestros de animales, hongos, plantas y una amplia gama de organismos unicelulares.[82]​[83]​[84] Las células vegetales se formaron hace unos 2000 millones de años con un segundo episodio de simbiogénesis al que se añadieron cloroplastos, derivados de una cianobacteria."
ksampletext_wikipedia_biol_filogenia: str = "Filogenia. La filogenia es la relación de parentesco entre especies o taxones en general.[1] Aunque el término también aparece en lingüística histórica para referirse a la clasificación de las lenguas humanas según su origen común, el término se utiliza principalmente en su sentido biológico. La filogenética es una disciplina de la biología evolutiva[2] que se ocupa de comprender las relaciones históricas entre diferentes grupos de organismos a partir de la distribución en un árbol o cladograma dicotómico de los caracteres derivados (sinapomorfías) de un ancestro común a dos o más taxones que contiene aquellos caracteres plesiomórficos en común. Incluso en el campo del cáncer,[3] la filogenética permite estudiar la evolución clonal (evolución de los clones de la célula cancerosa original, debido a las mutaciones que ocurran)[4] de los tumores y la cronología molecular, viéndose como varían las poblaciones celulares a lo largo de la progresión de la enfermedad, incluso durante el tratamiento de la misma, mediante el empleo de técnicas de secuenciación del genoma completo en muestras de ADN circulante tumoral.[5]​ Para reconstruir la filogenia de un grupo taxonómico (familia, género, subgénero, etc.) es imprescindible construir matrices basadas en datos morfológicos y/o moleculares (ADN, ARN y proteínas).[1]​[6] Las matrices son analizadas con determinados algoritmos que permiten encontrar los árboles filogenéticos más cortos siguiendo el principio de parsimonia,[7] que supone la menor cantidad de cambios bajo el supuesto de que la evolución acontece de la manera más simple, esto es: los árboles que son considerados como la mejor opción filogenética son aquellos más cortos, es decir, más parsimoniosos. Interpretar los árboles obtenidos implica rastrear la historia del grupo bajo un paradigma evolutivo basado en el supuesto de un antecesor común del que van derivando cada uno de los clados, considerando que estos solo se sustentan por homologías. La condición de homología es resultante de la aceptación a priori de la existencia de monofilia. Explicar las relaciones de filogenéticas sobre la base del mapa de caracteres que ofrecen los cladogramas permite construir clasificaciones más naturales, uno de los propósitos centrales de la sistemática, una disciplina cuyos orígenes, en términos académicos, se remontan a los aportes de Linneo. No obstante, muchas clasificaciones han tenido diversos propósitos y responden a metodologías y criterios diferentes. Las primeras han sido artificiales y meramente utilitarias; otras se han basado en criterios que la ciencia ha depuesto en la actualidad, sustituyendo las categorías taxonómicas o los sistemas de clasificación creados bajo esas metodologías por otros que son legitimados por los científicos. Entre las corrientes más relevantes respecto de las clasificaciones biológicas mediadas por la metodología se encuentran en la actualidad dos programas de investigación que en sus inicios se presentaron como antagónicos: el feneticismo[8] y el cladismo[9] y que si bien comparten el propósito de encontrar un sistema que ordene a la diversidad de especies y de categorías taxonómicas se basan en postulados, supuestos y teorías auxiliares diversas y en metodologías diferentes. La sistemática filogenética se ha impuesto con el devenir de los años a causa de que la homología (que no constituye una premisa bajo la lógica feneticista) es consistente con el supuesto de un antecesor común y, por lo tanto, congruente con la evolución y, en consecuencia, con la posibilidad de definir arreglos taxonómicos más naturales. Esta necesidad de conocer la historia evolutiva de los seres vivos inicia con la publicación de El origen de las especies por Charles Darwin en 1859, aunque existen ideas previas que al menos desde Aristóteles[10] han intentado explicar la diversidad de las formas de vida y sus relaciones. No obstante, explicar las relaciones históricas entre especies en función de la evolución es una tarea interminable y provisoria tal como lo es el conocimiento científico, sujeto a marcos teóricos y a coyunturas políticas. Uno de los hitos en relación con la justificación de estas relaciones fueron las contribuciones de Willi Hennig[11] (entomólogo alemán, 1913-1976), Walter Zimmermann[12] (botánico alemán, 1892-1980), Warren H. Wagner, Jr.[13] (botánico estadounidense, 1920-2000) entre otros por la centralidad de sus aportes, tanto desde el punto de vista teórico como metodológico. Técnicas y uso Filogenética molecular Es la técnica de la filogenia que investiga las relaciones de los seres vivos mediante análisis moleculares de la secuencia de ADN, ARN y proteínas. Constituye la herramienta principal de la biología evolutiva moderna para inferir parentescos, especialmente en grupos donde los rasgos morfológicos son escasos o poco informativos, como los microorganismos. En este enfoque, las similitudes en la secuencia de nucleótidos y aminoácidos se interpretan como sinapomorfías en el análisis, es decir, características compartidas derivadas de un ancestro común. No obstante, dichas similitudes pueden variar considerablemente, ya que un organismo puede compartir secuencias idénticas con varios linajes cercanos, lo que puede generar hipótesis filogenéticas alternativas. La filogenética molecular ha permitido identificar numerosos clados evolutivos que no habían sido reconocidos mediante análisis morfológicos y ha corregido múltiples errores derivados del estudio exclusivo de características anatómicas. No obstante, esta aproximación también puede verse afectada por fenómenos como la atracción de ramas largas, en la que linajes con tasas de evolución acelerada se agrupan erróneamente. Por ello, se emplean modelos evolutivos más complejos y métodos estadísticos como la inferencia bayesiana o la máxima verosimilitud para minimizar dichos sesgos y obtener árboles más precisos. En la actualidad, la filogenética molecular se apoya en herramientas bioinformáticas y en bases de datos genómicas de gran escala, lo que permite analizar miles de genes o incluso genomas completos (filogenómica). Estas técnicas han revolucionado la clasificación biológica y la comprensión de la historia evolutiva de los organismos, contribuyendo a redefinir la sistemática moderna y a mejorar la identificación de especies, el estudio de la biodiversidad y la reconstrucción de eventos evolutivos profundos. Filogenética morfológica Es la técnica de la filogenia que investiga las relaciones de los seres vivos mediante análisis morfológicos como anatomía comparada, homología, embriología, alometría y fósiles. Las similitudes morfológicas entre organismos pueden ser un indicativo de parentesco, pero posteriormente se demostró que las similitudes morfológicas pueden evolucionar convergentemente en linajes diferentes. Actualmente, se usan ciertos caracteres morfológicos (sinapomorfías) que pueden emplearse para determinar las relaciones. La filogenética morfológica es empleada por los paleontólogos para determinar las relaciones entre los fósiles y los grupos existentes. También es usada por algunos zoólogos y botánicos evolutivos para determinar ciertos caracteres morfológicos válidos entre sus grupos de estudio (animales y plantas). Antiguamente, se usó para estudiar las relaciones entre los microorganismos, pero su uso quedó obsoleto debido a la ausencia de caracteres morfológicos en estos grupos. Filogenética molecular-estructural Es una técnica filogenética novedosa que investiga las relaciones mediante un tipo de biomolécula específico o similar que porten los organismos, sin tomar en cuenta la secuencia. Es similar a la filogenética morfológica en el hecho de que la sinapomorfía es una biomolécula única o similar que portan dichos organismos sin recurrir a la secuencia. Por ejemplo, las bacterias son un dominio que se caracteriza por tener una pared celular de peptidoglicanos. Las bacterias de Sphingobacteria se caracterizan por tener esfingolípidos. Los dominios de virus Riboviria, Duplodnaviria, Adnaviria y Varidnaviria se determinaron filogenéticamente mediante la presencia de una proteína única o similar estructuralmente. Inferencia de árboles filogenéticos La reconstrucción de árboles filogenéticos, a partir de datos moleculares o morfológicos, requiere de métodos que permitan inferir las relaciones evolutivas entre los taxones de interés. La filogenética computacional es una rama de la bioinformática, que aplica algoritmos y herramientas informáticas para reconstruir y analizar árboles filogenéticos. Entre los enfoques más empleados para estos propósito se encuentran la máxima parsimonia, los métodos basados en distancias, la máxima verosimilitud[14] y la inferencia bayesiana,[15] cada uno basado en distintos supuestos sobre la evolución de los caracteres. Método de máxima parsimonia: es un modelo de reconstrucción filogenética basado en el principio de parsimonia, según el cual se busca el árbol filogenético que implique la menor cantidad posible de cambios evolutivos o transiciones de un estado a otro. Aunque este método no se utiliza con tanta frecuencia en la actualidad, sigue siendo una herramienta importante para la reconstrucción de árboles, especialmente cuando no se cuenta con datos moleculares y se trabaja con caracteres morfológicos.[16]​ Métodos basados en distancias: estos métodos de reconstrucción estiman las distancias evolutivas entre pares de taxones. Esta distancia se calcula generalmente alineando las secuencias de ADN o de proteínas y evaluando cuánto difieren entre sí. Una vez obtenidas las distancias, se reconstruye el árbol filogenético mediante algoritmos de agrupamiento que unen primero a los taxones más similares. Algunos de los algoritmos más utilizados son Neighbor-Joining[17] y UPGMA.[18]​ Método de máxima verosimilitud: en este enfoque de reconstrucción filogenética se asume que el árbol representa un modelo de evolución, y se busca encontrar la topología y la longitud de las ramas que maximizan la probabilidad de que los datos observados hayan ocurrido bajo dicho modelo. Para realizar este proceso, el método requiere un modelo de sustitución que describa la probabilidad de que un nucleótido o un aminoácido cambie por otro a lo largo del tiempo. Aunque es computacionalmente más costoso, es uno de los métodos más utilizados actualmente.[14]​ Método de inferencia bayesiana: este método aplica la inferencia bayesiana para realizar la reconstrucción filogenética. En este método, el modelo de sustitución, la topología del árbol y las longitudes de las ramas se tratan como parámetros del modelo, y se buscan los valores que maximizan la probabilidad posterior, la cual combina la función de verosimilitud de los datos, la probabilidad previa de los parámetros y la probabilidad marginal de los datos. Debido a su alto costo computacional, normalmente se emplea el algoritmo de cadenas de Markov de Monte Carlo (MCMC) para realizar la inferencia de los parametros.[15]​ Caracteres y estados del carácter El primer paso para reconstruir la filogenia de los organismos es determinar cuanta similitud hay entre sí, ya sea en morfología, anatomía, embriología, biogeografía, moléculas de ADN, ARN o proteínas, ya que en última instancia estos parecidos pueden ser un indicador de su parecido genético, y, por lo tanto, de sus relaciones evolutivas. La evolución es un proceso muy lento, y en la gran mayoría de los casos nadie la ha visto suceder. Lo que se maneja es una serie de hipótesis acerca de cómo ocurrió la diversificación de los organismos, que desembocó en la aparición de las distintas especies variadamente relacionadas entre sí. Esas hipótesis son las que determinan cómo deberían analizarse los organismos para determinar su filogenia. Supongamos una única población ancestral de plantas. Para establecer que los organismos que componen esta población son morfológicamente similares entre sí, determinamos una serie de caracteres: color de pétalo, leñosidad del tallo, presencia o ausencia de tricomas en las hojas, cantidad de estambres, fruto seco o carnoso, y rugosidad de la semilla. Todas las plantas de esta población ancestral comparten los mismos estados del carácter para cada uno de ellos: los pétalos son blancos, el tallo herbáceo, las hojas sin tricomas, los estambres son 5, el fruto es seco, y la semilla lisa. Finalmente, mediante algún mecanismo de aislamiento reproductivo, la población se divide en dos subpoblaciones que no intercambian material genético entre sí. Al cabo de algunas generaciones, se va haciendo evidente que aparecen mutantes en las dos subpoblaciones nuevas. Algunos de ellos son más exitosos reproductivamente que el resto de la población y, por lo tanto, después de unas generaciones más, su genotipo se convierte en el dominante en esa población. Como las mutaciones ocurren al azar en cada subpoblación, y la probabilidad de que ocurra espontáneamente la misma mutación en cada subpoblación es muy baja, las dos subpoblaciones van acumulando diferentes mutaciones exitosas, generando diferentes genotipos, que se pueden ver reflejados en los cambios que ocurren en los estados de los caracteres. Así, por ejemplo, la subpoblación 1 pasó a poseer el tallo leñoso, y la subpoblación 2 pasó a poseer los pétalos rojos (pero conservando el tallo herbáceo ancestral). Como resultado, la última generación de plantas corresponde a dos poblaciones muy similares entre sí, con muchos caracteres compartidos, salvo la leñosidad del tallo y el color de los pétalos. Nosotros, en nuestra corta vida, solo vemos este resultado de la evolución, e hipotetizamos que lo que ocurrió fue el proceso que se indica más arriba. Esta hipótesis se puede reflejar en un árbol filogenético, un diagrama que resume las relaciones de parentesco entre los ancestros y sus descendientes, como el siguiente: Árbol filogenético que muestra cómo, después de un evento de aislamiento reproductivo entre dos poblaciones de la misma especie, apareció una mutación exitosa en cada población, que pasaron a diferenciarse entre ellas mediante la observación de los estados de sus caracteres. En el cladograma, la especie 1 comparte con su ancestro todos los estados de los caracteres salvo el tallo, que es leñoso. La especie 2, a su vez, comparte con su ancestro todos los caracteres salvo el color de los pétalos, que es rojo. Las dos especies comparten entre sí todos los caracteres salvo la leñosidad del tallo y el color de los pétalos. En este ejemplo, se han establecido 2 linajes: secuencias de poblaciones desde el ancestro hasta los descendientes. En los inicios de la sistemática, los caracteres utilizados para comparar a los grupos entre sí eran conspicuos, principalmente morfológicos. A medida que se acumuló más conocimiento se empezó a tomar cada vez más cantidad de caracteres crípticos, como los anatómicos, embriológicos, serológicos, químicos y finalmente caracteres del cariotipo y los derivados del análisis molecular. Los caracteres correspondientes al ancestro de un grupo de organismos que son retenidos por el grupo se dice que son plesiomórficos (ancestrales), mientras que los que fueron adquiridos exclusivamente por ese grupo (en el ejemplo, el tallo leñoso para la especie 1 o los pétalos rojos para la especie 2) se dice que son sinapomórficos o derivados (nuevos). Nótese que solo la presencia de sinapomorfías nos indica que se ha formado un nuevo linaje, nótese también que en árboles filogenéticos más extensos, como el siguiente: Árbol filogenético que muestra un ejemplo de diversificación de una especie ancestral en 5 especies presentes en la actualidad. el mismo carácter puede ser una sinapomorfía o una plesiomorfía, según desde qué porción del árbol se la observe. Por ejemplo, el tallo leñoso es una sinapomorfía de C (y de C+A+B) pero una plesiomorfía para A, ya que comparte ese estado del carácter con B a través de su ancestro común. Otra forma de decirlo es que el tallo leñoso es un carácter derivado desde el punto de vista de la población original, pero es ancestral para A y para B. El aspecto del árbol filogenético (su topología) solo está dado por las conexiones entre sus nodos, y no por el orden en que son diagramados. Así, [[A+B]+C] es el mismo árbol que [C+[A+B]]. La topología tampoco está dada por la posición en que el árbol es dibujado, a veces se los dibuja erectos (con el ancestro abajo y los grupos terminales arriba), a veces se los dibuja recostados (con el ancestro a la izquierda y los grupos terminales a la derecha). Las dos formas de dibujarlos son igualmente válidas. En los árboles filogenéticos como los aquí expuestos, el largo de las ramas tampoco da ninguna información acerca de cuánto diverge ese linaje en términos de sus caracteres ni acerca de en qué momento geológico ocurrió el aislamiento de ese linaje (pero hay árboles que sí dan esa información). Un cladograma es un árbol filogenético que solo muestra las relaciones evolutivas, sin darle un significado a sus ramas. Por el otro lado, hay dos tipos de árboles filogenéticos con significado en la longitud de sus ramas: el cronograma, donde la longitud de las ramas indica el tiempo transcurrido entre un nodo y otro, y la posición en el tiempo de cada nodo con respecto a los otros; y el filograma, donde la longitud de las ramas indica la cantidad de cambio evolutivo desde el ancestro común más cercano.[19]​ Monofilia, parafilia y polifilia Artículos principales: Monofilético, Parafilético y Polifilético. Grupos filogenéticos: monofilético, parafilético, polifilético. Un grupo formado por un ancestro y todos sus descendientes se denomina monofilético, también llamado clado. Al grupo al que se le ha excluido alguno de sus descendientes se lo llama parafilético. Los grupos formados por los descendientes de más de un ancestro se denominan polifiléticos.[cita requerida] Por ejemplo, se cree que las aves y los reptiles descienden de un único ancestro común, luego este grupo taxonómico (amarillo en el diagrama) es considerado monofilético. Los reptiles actuales como grupo también tienen un ancestro común a todos ellos, pero ese grupo (reptiles modernos) no incluye a todos los descendientes de tal ancestro porque se está dejando a las aves fuera (solo incluye los de color cian en el diagrama); por lo que un grupo así se considera como parafilético. Un grupo que incluyera a los vertebrados de sangre caliente contendría solo a los mamíferos y las aves (rojo/naranja en el diagrama) y sería polifilético, porque entre los miembros de este agrupamiento no está el más reciente ancestro común de ellos. Los animales de sangre caliente son todos descendientes de un ancestro de sangre fría. La condición endotérmica (sangre caliente) ha aparecido dos veces, independientemente, en el ancestro de los mamíferos, por un lado, y en el de las aves (y quizá algunos o todos los dinosaurios), por otro.[20]​ Algunos autores sostienen[21] que la diferencia entre grupos parafiléticos y polifiléticos es sutil, y prefieren llamar a estos dos tipos de asemblajes como no monofiléticos. Muchos taxones largamente reconocidos de plantas y animales resultaron ser no monofiléticos según los análisis de filogenia hechos en las últimas décadas, por lo que muchos científicos recomendaron abandonar su uso, ejemplos de estos taxones son Prokaryota, Protista, Pisces, Reptilia, Pteridophyta, Dicotyledoneae, y varios otros más. Como su uso está muy extendido por haber sido tradicionalmente reconocidos, y porque muchos científicos consideran a los taxones parafiléticos válidos (discusión que aún no está terminada en el ambiente científico; el ejemplo más claro de un taxón que muchos[22] desean conservar quizás sean Reptilia), a veces se indica el nombre del taxón, con la salvedad de que su nombre se pone entre comillas, para indicar que el taxón no se corresponde con un clado.[21] La diferencia entre un clado y un taxón es que un clado debe ser un grupo natural (monofilético), mientras que un taxón puede ser o no monofilético, pero los taxones no monofiléticos pierden su validez en la clasificación actual de los organismos.[19]​ El rol de las sinapomorfías en el análisis filogenético Las sinapomorfías que caracterizan a cada grupo monofilético son estados de los caracteres que se originaron en el ancestro común a todos los miembros del grupo, pero que no estaban presentes en los ancestros anteriores a estos, ancestros comunes tanto a los miembros del grupo como a otros grupos más. Hay que tener en cuenta que si bien una sinapomorfía es un estado del carácter que se hipotetiza que está presente en el ancestro del grupo, no necesariamente será encontrada en todos sus descendientes, debido a que la evolución puede modificarla y hasta revertirla a su estado anterior por azar (proceso que se conoce como reversión). Por lo tanto, no está garantizado que una lista de sinapomorfías vaya a encontrarse en todos los miembros de un grupo, y solo mediante un síndrome de caracteres podemos asegurarnos de que cada miembro pertenece a ese clado.[cita requerida] El concepto de sinapomorfía fue formalizado por primera vez por Hennig (1966) y Wagner (1980).[23] Mucho del análisis filogenético actual se basa en la búsqueda de sinapomorfías que permitan establecer grupos monofiléticos. En ese sentido, son revolucionarios los análisis moleculares de ADN que se están realizando desde hace algunos años, que entre otras técnicas determinan la secuencia de bases del mismo trozo de ADN en diferentes taxones, y comparan directamente sus secuencias de bases. En estos análisis, que se realizan con secuencias conservadas de genes concretos (como el ARNr), cada base es un carácter, y los posibles estados del carácter son las 4 posibles bases: adenina, timina, guanina y citosina. Si bien las sinapomorfías encontradas a través de los análisis moleculares de ADN son oscuras y no son útiles para identificar organismos en el campo o para plantear hipótesis acerca de la adaptación de los organismos a su ambiente, poseen ventajas (como la cantidad de caracteres medidos con poca cantidad de recursos, el establecimiento de caracteres menos subjetivos que los basados en fenotipos), que le otorgan a los análisis filogenéticos una precisión sin precedentes, obligando en muchas ocasiones a abandonar hipótesis evolutivas largamente reconocidas. Además, según la hipótesis del reloj molecular, la comparación de secuencias de ADN permite no solo determinar la distancia genética entre dos especies, sino además estimar el tiempo transcurrido desde el último antecesor común.[cita requerida] Sinapomorfías y especies La regla para construir los árboles filogenéticos es el reconocimiento de grupos monofiléticos (clados) a partir de sus sinapomorfías (estados de los caracteres comunes al grupo). Esto es cierto para todos los nodos del árbol salvo el terminal, a nivel de las especies. No se puede establecer monofilia a nivel de las especies debido a que la naturaleza de las relaciones entre los organismos cambia por encima y por debajo del nivel de especie: por encima del nivel de especie, organismos de dos clados diferentes no pueden cruzarse entre sí y dar descendencia fértil, por lo que sus bagajes genéticos se mantienen sin mezclarse. Por debajo del nivel de especie, existe interfertilidad entre los organismos, por lo que el genoma de cada organismo es el resultado del cruce de dos genomas diferentes. Esta diferencia se puede esquematizar como un árbol ramificado para representar a todas las agrupaciones de organismos por encima del nivel de especie, pero en los organismos que pertenecen a la misma especie, las ramas del árbol se entrecruzan entre sí creando una red interconectada de organismos. Como muchas poblaciones del planeta están en diferentes etapas del proceso de especiación, y a veces se reconocen dos poblaciones diferentes como especies diferentes a pesar de ser algo interfértiles, entonces no es fácil determinar si un estado de un carácter es exclusivo de una de las especies o pertenece también en una baja proporción no muestreada a la otra especie, o si pertenecerá en algún momento debido a una hibridación casual, a la otra especie. Avances recientes En los últimos años, el uso de datos genómicos a gran escala ha transformado la filogenia al permitir la reconstrucción de árboles evolutivos con miles de loci —lo que se conoce como filogenómica— y enfrentarse simultáneamente a nuevos retos como la inferencia de ortología/paralogs, la hibridación, el intercambio genético lateral y la incongruencia entre genes y especies (Zaharias et al., 2022). Además, se está reconociendo que en algunos linajes complejos, como ciertos grupos de angiospermas, el modelo clásico de árbol bifurcado podría no bastar y se requiere considerar redes evolutivas o grafos reticulados para reflejar procesos de introgresión y duplicación genómica (Li et al., 2025)."
ksampletext_wikipedia_biol_botanica: str = "Botánica. La botánica (del griego, ‘hierba’) es la rama de la biología que estudia las plantas, en sentido amplio, incluyendo a las algas, hongos y organismos fotosintéticos no necesariamente clasificados como plantas, bajo todos sus aspectos, incluyendo la descripción, clasificación, distribución, identificación, estudio de la reproducción, fisiología, morfología, relaciones recíprocas, relaciones con los otros seres vivos y efectos provocados sobre el medio en el que se encuentran.[1]​[2] Los términos para quien se dedica a esta disciplina son dos: botánico /a y botanista. La botánica estudia las plantas en sentido amplio, abarcando las categorías taxonómicas de las plantas sin flores (criptógamas), las plantas sin flores y sin vasos (briofitas), las plantas sin flores y con vasos (pteridofitas), las plantas con flores (espermatofitas), las plantas con flores y sin fruto (gimnospermas) y las plantas con flores y con fruto (angiospermas), dentro de la clasificación clásica de los organismos vegetales. No obstante, en términos históricos, el objeto de estudio de la botánica no se ha restringido estrictamente al Reino Plantae, sino que ha abarcado un grupo de organismos lejanamente emparentados entre sí, esto es, las cianobacterias, los hongos, las algas y las plantas, los que casi no poseen ningún carácter en común salvo la presencia de cloroplastos (a excepción de los hongos y cianobacterias) o el no poseer capacidad de desplazamiento.[3]​[4] En el campo de la botánica hay que distinguir entre la botánica pura, cuyo objeto es ampliar el conocimiento de la naturaleza, y la botánica aplicada, cuyas investigaciones están al servicio de la tecnología agraria, forestal y farmacéutica. Su conocimiento afecta a muchos aspectos de nuestra vida y por tanto es una disciplina estudiada por biólogos y ambientólogos, pero también por farmacéuticos, ingenieros agrónomos, ingenieros forestales, entre otros.[5]​ La botánica cubre una amplia gama de contenidos, que incluyen aspectos específicos propios de los vegetales, así como de las disciplinas biológicas que se ocupan de la composición química (fitoquímica), de la organización celular y tisular (histología vegetal), del metabolismo y el funcionamiento orgánico (fisiología vegetal), del crecimiento y el desarrollo, de la morfología (fitografía), de la reproducción, de la herencia (genética vegetal), de las enfermedades (fitopatología), de las adaptaciones al ambiente (ecología), de la distribución geográfica (fitogeografía o geobotánica), de los fósiles (paleobotánica) y de la evolución. Los organismos que estudia la botánica La idea de que la naturaleza puede ser dividida en tres reinos (mineral, vegetal y animal) fue propuesta por Nicolás Lemery (1675)[6] y popularizada por Carlos Linneo en el siglo XVIII.[7] Carlos Linneo,[8] a finales del siglo XVIII, introdujo el actual sistema de clasificación. Este incluye los conocimientos sobre las diversas especies vegetales dentro de un sistema más amplio, ofreciendo una versión sintética y enriquecedora. No en vano se ha dicho que el sistema de clasificación de Linneo prefigura lo que después serían las teorías evolutivas. A pesar de que con posterioridad fueron determinados como reinos separados para los hongos (en 1783),[9] protozoarios (en 1858)[10] y bacterias (en 1925)[11] la concepción del siglo XVII de que solo existían dos reinos de organismos dominó la biología por tres siglos. El descubrimiento de los protozoarios en 1675, y de las bacterias en 1683, ambos realizados por Leeuwenhoek,[12]​[13] finalmente comenzó a minar el sistema de dos reinos. No obstante, un acuerdo general entre los científicos acerca de que el mundo viviente debería ser clasificado en al menos cinco reinos,[14]​[15]​[16] solo fue logrado luego de los descubrimientos realizados por la microscopía electrónica en la segunda mitad del siglo XX. Tales hallazgos confirmaron que existían diferencias fundamentales entre las bacterias y los eucariotas y, además, revelaron la tremenda diversidad ultraestructural de los protistas. La aceptación generalizada de la necesidad de utilizar varios reinos para incluir a todos los seres vivos también debe mucho a la síntesis sistemática de Herbert Copeland (1956)[17] y a los influyentes trabajos de Roger Y. Stanier (1961-1962)[18]​[19] y Robert H. Whittaker (1969).[20]​[7]​En el sistema de seis reinos, propuesto por Thomas Cavalier-Smith en 1983[21] y modificado en 1998,[7] las bacterias son tratadas en un único reino (Bacteria) y los eucariotas se dividen en 5 reinos: protozoarios (Protozoa), animales (Animalia), hongos (Fungi), plantas (Plantae) y Chromista (algas cuyos cloroplastos contienen clorofilas a y c, así como otros organismos sin clorofila relacionados con ellas). La nomenclatura de estos tres últimos reinos, clásico objeto de estudio de la botánica, está sujeta a las reglas y recomendaciones del Código Internacional de Nomenclatura Botánica.[22]​ Divisiones de la botánica Familias botánicas Las plantas pueden estudiarse desde varios puntos de vista, así, pueden diferenciarse distintas líneas de trabajo de acuerdo con los niveles de organización que se estudien: desde las moléculas y las células, pasando por los tejidos y los órganos, hasta los individuos, las poblaciones y las comunidades vegetales. Otras posibilidades se refieren al estudio de las plantas que vivieron en épocas geológicas pasadas o al de las que viven en la actualidad, al examen de los distintos grupos sistemáticos y a la investigación de cómo pueden ser utilizados los vegetales por el ser humano.[23]​[24]​ Una de las metas más importantes para la botánica, es que junto a la biotecnología e ingeniería genética puedan llegar a crear vida. En general, todas esas direcciones de trabajo se basan en el análisis comparativo de los fenómenos particulares y de su variabilidad, para llegar a una generalización y al reconocimiento de las relaciones regulares que unen dichos fenómenos entre sí. Siempre deben asociarse los métodos estático y dinámico: por un lado el reconocimiento y la interpretación de las estructuras y formas y, por el otro, el análisis de los procesos vitales, de funciones y de fenómenos de desarrollo. El objetivo final de ambos métodos debe ser en todo caso la comprensión de las formas y de las funciones en su dependencia recíproca y en su evolución. Los distintos puntos de vista descritos y el empleo de diferentes métodos de trabajo han conducido a que dentro de la botánica se hayan desarrollado numerosas disciplinas. En primer lugar, se puede citar a la Morfología, la cual, en sentido amplio, es la teoría general de la estructura y forma de las plantas, e incluye la Citología y la Histología. La primera se ocupa del estudio de la fina constitución de las células y se asocia, en los aspectos relacionados con las moléculas, con algunas partes de la Biología Molecular. La Histología es el estudio de los tejidos de las plantas. Citología e Histología, conjuntamente, son necesarias para comprender la Anatomía de las plantas, o sea, su constitución interna.[25]​[26]​[27]​ Al ocuparse de los procesos de adaptación, la morfología se relaciona con la ecología, disciplina que investiga las relaciones entre la planta y su ambiente. Tales relaciones están basadas en los estudios de la fisiología vegetal, que se ocupa —de modo general— al estudio del modo en que se realizan las funciones de la planta en los campos del metabolismo, del cambio de forma (que incluye el crecimiento y desarrollo de la planta) y de los movimientos. La reproducción de las plantas y el modo en que se heredan y cambian las características a través de las generaciones es el campo de la Genética.[26]​ La botánica sistemática trata de averiguar las afinidades que existen entre los diversos tipos de plantas, basándose en los resultados de todas las disciplinas mencionadas previamente, entre las que, al lado de la morfología, son importantes la citología, la anatomía, el estudio de las esporas y del polen (Palinología), el estudio de la generación sexual y del embrión (Embriología), las sustancias producidas y contenidas en las plantas (fitoquímica), la Genética y la Geobotánica. Como parte de la sistemática, se encuentra principalmente la taxonomía, que se ocupa de la descripción, nomenclatura y ordenación de las especies de plantas existentes, las cuales sobrepasan el número de 330 000. A ella se añade el estudio de la historia evolutiva de las plantas (Filogenia), que se apoya especialmente en la Paleobotánica, el estudio de las plantas que vivieron en otras eras geológicas y en la evolución, que ilustra sobre las leyes y las causas que rigen la formación de las estirpes vegetales.[26]​[28]​ Finalmente, dentro de la botánica existen ramas de estudio que se ocupan de modo especial de grupos particulares de organismos, como la Microbiología (que estudia los microorganismos en general, incluyendo muchos de los que se consideran organismos vegetales), la Bacteriología (que se ocupa de las bacterias), la Micología (que estudia los hongos), la Ficología (que estudia las algas), la Liquenología (estudio de los líquenes), la Briología (estudio de los briófitos: los musgos y las hepáticas), la Pteridología (estudio de los helechos).[29]​[3]​También existen distintas disciplinas aplicadas, que estudian el valor práctico de las plantas para los seres humanos y con ello establecen el enlace con la Agricultura, la Silvicultura y la Farmacia, entre otras. Como ejemplo de estas disciplinas se pueden mencionar el Mejoramiento Genético de Plantas —o fitomejoramiento— (estudia la variabilidad genética y la selección de plantas), la Fitopatología (se ocupa de las enfermedades de las plantas y de los métodos de control de las mismas), la Farmacognosia (estudia las plantas medicinales y sus principios activos).[26]​[30]​ Historia Esta sección es un extracto de Historia de la botánica.[editar] Busto de Teofrasto, considerado como el padre de la botánica. La historia de la botánica es la exposición y narración de las ideas, investigaciones y obras relacionadas con la descripción, clasificación, funcionamiento, distribución y relaciones de los organismos pertenecientes a los reinos Fungi, Chromista y Plantae a través de los diferentes períodos históricos.[n 1]​[n 2]​ Desde la antigüedad, el estudio de los vegetales se ha abordado con dos aproximaciones bastante diferentes: la teórica y la utilitaria. Desde el primer punto de vista, al que se denomina botánica pura, la ciencia de las plantas se erigió por sus propios méritos como una parte integral de la biología. Desde una concepción utilitaria, por otro lado, la denominada botánica aplicada era concebida como una disciplina subsidiaria de la medicina o de la agronomía. En los diferentes períodos de su evolución una u otra aproximación ha predominado, si bien en sus orígenes —que datan del siglo VIII a. C.— la aproximación aplicada fue la preponderante.[33]​ La botánica, como muchas otras ciencias, alcanzó la primera expresión definida de sus principios y problemas en la Grecia clásica y, posteriormente, continuó su desarrollo durante la época del Imperio romano.[34] Teofrasto, discípulo de Aristóteles y considerado el «padre de la botánica», legó dos obras importantes que se suelen señalar como el origen de esta ciencia: De historia plantarum [Historia de las plantas] y De causis plantarum [Sobre las causas de las plantas].[35] Los romanos contribuyeron poco a los fundamentos de la botánica, pero hicieron una gran contribución al conocimiento de la botánica aplicada a la agricultura.[36] El enciclopedista romano Plinio el Viejo aborda las plantas en los libros XII a XXVI de sus 37 volúmenes de Naturalis Historia.[37]​ Se estima que en la época del imperio romano entre 1300 y 1400 plantas se habían registrado en el oeste.[38] Tras la caída del Imperio en el siglo V, todas las conquistas alcanzadas en la antigüedad clásica tuvieron que redescubrirse a partir del siglo XII, por perderse o ignorarse buena parte de ellas durante la alta Edad Media. La tradición conservadora de la Iglesia y la labor de contadas personalidades hicieron avanzar, aunque muy lentamente, el conocimiento de los vegetales durante este período.[39]​ En los siglos XV y XVI la botánica se desarrolló como una disciplina científica, separada de la herboristería y de la Medicina, si bien continuó contribuyendo a ambas. Diversos factores permitieron el desarrollo y progreso de la botánica durante esos siglos: la invención de la imprenta, la aparición del papel para la elaboración de los herbarios, y el desarrollo de los jardines botánicos, todo ello unido al desarrollo del arte y ciencia de la navegación que permitió la realización de expediciones botánicas. Todos estos factores conjuntamente supusieron un incremento notable en el número de las especies conocidas y permitieron la difusión del conocimiento local o regional a una escala internacional.[40]​[41]​ Impulsada por las obras de Galileo, Kepler, Bacon y Descartes, en el siglo XVII se originó la ciencia moderna. Debido a la creciente necesidad de los naturalistas europeos de intercambiar ideas e información, se comenzaron a fundar las primeras academias científicas.[42] Joachim Jungius fue el primer científico que combinó una mentalidad entrenada en la filosofía con observaciones exactas de las plantas. Tenía la habilidad de definir los términos con exactitud y, por ende, de reducir el uso de términos vagos o arbitrarios en la sistemática. Se lo considera el fundador del lenguaje científico, el que fue desarrollado más tarde por el inglés John Ray y perfeccionado por el sueco Carlos Linneo.[42]​ A Linneo se le atribuyen varias innovaciones centrales en la taxonomía. En primer lugar, la utilización de la nomenclatura binomial de las especies en conexión con una rigurosa caracterización morfológica de las mismas. En segundo lugar, el uso de una terminología exacta. Basado en el trabajo de Jungius, Linneo definió con precisión varios términos morfológicos que serían utilizados en sus descripciones de cada especie o género, en particular aquellos relacionados con la morfología floral y con la morfología del fruto. No obstante, el mismo Linneo notó las fallas de su sistema y buscó en vano nuevas alternativas. Su concepto de la constancia de cada especie fue un obstáculo obvio para lograr establecer un sistema natural ya que esa concepción de la especie negaba la existencia de las variaciones naturales, las cuales son esenciales para el desarrollo de un sistema natural. Esta contradicción permaneció durante mucho tiempo y no fue resuelta hasta 1859 con la obra de Charles Darwin.[42] Durante los siglos XVII y XVIII también se originaron dos disciplinas científicas que, a partir de ese momento, iban a tener una profunda influencia en el desarrollo de todos los ámbitos de la botánica: la anatomía y la fisiología vegetal. Las ideas esenciales de la teoría de la evolución por selección natural de Darwin influirían notablemente en la concepción de la clasificación de los vegetales. De ese modo, aparecieron las clasificaciones filogenéticas, basadas primordialmente en las relaciones de proximidad evolutiva entre las distintas especies, reconstruyendo la historia de su diversificación desde el origen de la vida en la Tierra hasta la actualidad. El primer sistema admitido como filogenético fue el contenido en el Syllabus der Planzenfamilien (1892) de Adolf Engler y conocido más tarde como sistema de Engler cuyas numerosas adaptaciones posteriores han sido la base de un marco universal de referencia según el cual se han ordenado (y se siguen ordenando) muchos tratados de floras y herbarios de todo el mundo, si bien algunos de sus principios para interpretar el proceso evolutivo en las plantas han sido abandonados por la ciencia moderna.[43]​ Los siglos XIX y XX han sido particularmente fecundos en las investigaciones botánicas, las que han llevado a la creación de numerosas disciplinas como la ecología, la geobotánica, la citogenética y la biología molecular y, en las últimas décadas, a una concepción de la taxonomía basada en la filogenia y en los análisis moleculares de ADN y a la primera publicación de la secuencia del genoma de una angiosperma: Arabidopsis thaliana.[44]​[45]​ La botánica moderna (desde 1945) Esta sección es un extracto de Botánica moderna.[editar] La botánica moderna es una ciencia que considera una gran cantidad de nuevos conocimientos en la actualidad que han sido generados por el estudio de las plantas modelo y sobre la botánica actual, en concreto, ésta comenzó desde 1945. Arabidopsis thaliana motivó a los biólogos actuales a estudiar a fondo este tipo de plantas, esta mala hierba fue una de las primeras plantas en ver su genoma secuenciado. Otros más importantes comercialmente como alimentos básicos como el arroz, trigo, maíz, cebada, centeno, mijo y la soja están teniendo también sus secuencias del genoma. Algunas de éstas son un reto puesto que tienen en sus secuencias más de dos juegos de cromosomas haploides, una condición conocida como poliploidía, común en el reino vegetal. Un alga verde Chlamydomonas reinhardtii (un célula, sola, verde alga) es otro organismo modelo importante que ha sido extensivamente estudiado y provee importantes conocimientos a la biología celular. Significado de la botánica como ciencia Los distintos grupos de vegetales participan de manera fundamental en los ciclos de la biosfera. Las plantas y algas son los productores primarios, responsables de la captación de energía solar de la que depende la mayoría de la vida terrestre, de la creación de materia orgánica y también, como subproducto, de la generación del oxígeno que inunda la atmósfera y causa que casi todos los organismos saquen ventaja del metabolismo aerobio.[46]​ Alimentación humana Casi todo lo que comemos proviene de las plantas, ya sea consumiéndolas directamente (frutas, verduras hortalizas), como indirectamente a través del ganado que se alimenta con plantas que componen el forrajeras. Por lo tanto, las plantas son la base de toda la cadena alimentaria, o lo que los ecólogos llaman el primer nivel trófico. El estudio de las plantas y las técnicas de mejoramiento para producir alimentos son claves para ser capaces de alimentar al mundo y proporcionar una seguridad alimentaria para las generaciones futuras.[47] No obstante, como todas las plantas no son beneficiosas para este fin, la botánica también estudia las especies consideradas nocivas para la agricultura. También estudia los patógenos (fitopatología) que afectan al reino vegetal y la interacción de los humanos con este reino (etnobotánica). Procesos biológicos fundamentales Las plantas son susceptibles de ser estudiadas en sus procesos fundamentales (como la división celular y síntesis proteica por ejemplo), pero sin los problemas éticos que supone estudiar animales o seres humanos. Las leyes de la herencia fueron descubiertas de esta manera por Gregor Mendel, que estudió cómo se hereda la morfología del guisante. Las leyes descubiertas por Mendel a partir del estudio de plantas han conocido desarrollos posteriores, y se han aplicado sobre las propias plantas para conseguir nuevas variedades beneficiosas. Otro estudio clásico efectuado en plantas fue el realizado por Bárbara McClintock, quien descubrió los 'genes saltarines' (o transposones) estudiando el maíz. Son ejemplos que muestran cómo la botánica ha tenido una importancia capital para el entendimiento de los procesos biológicos fundamentales. Aplicaciones de las plantas Muchas de nuestras medicinas y drogas, como el cannabis, vienen directamente del reino vegetal. Otros productos medicinales se derivan de sustancias de origen vegetal; así, la aspirina es un derivado del ácido salicílico, que originalmente se obtenía de la corteza de sauce. La investigación sobre productos farmacéuticamente útiles en las plantas es un campo activo de trabajo que rinde buenos resultados. Estimulantes populares como el café (por su contenido en cafeína), el chocolate, el tabaco (por la nicotina), y el té tienen origen vegetal. Muchas bebidas alcohólicas derivan de la fermentación de plantas como la cebada, el maíz y la uva. Las plantas también nos proveen de muchos materiales, como el algodón, la madera, el papel, el lino, el aceite vegetal, algunos tipos de cuerdas y plásticos. La producción de seda no sería posible sin el cultivo de los árboles de morera. La caña de azúcar y otras plantas han sido recientemente usadas como biomasa para producir una energía renovable alternativa al combustible fósil. Entendimiento de cambios ambientales Las plantas también pueden ayudar al entendimiento de los cambios del medio ambiente de muchas formas. Entendimiento de la destrucción de hábitat y de especies en extinción depende de un catálogo completo y exacto de plantas, de la sistemática y taxonomía. Respuesta de las plantas a radiación ultravioleta puede monitorear problemas como los agujeros en la capa de ozono. El análisis de polen depositado por plantas en miles de millones de años atrás puede ayudar a los científicos a reconstruir los climas del pasado y pronosticar el futuro, una parte esencial de investigaciones sobre cambios climáticos. Recopilar y analizar el tiempo del ciclo de vida es importante para la fenología usado para la investigación de cambios climáticos. Líquenes, sensibles a las condiciones atmosféricas, tienen un uso extensivo como indicadores de contaminación. Las plantas pueden servir como ‘sensores’, una especie de “señales tempranas de aviso” que den la alerta sobre cambios importantes en el ambiente. Por último, las plantas son sumamente valoradas en el aspecto recreativo para millones de personas que disfrutan de su uso en la jardinería, la horticultura y el arte culinario. Disciplinas Subdisciplinas de la botánica Anatomía vegetal u organografía Botánica aplicada Botánica marina Botánica pura o general Botánica sistemática Dendrología Ecología vegetal Ficología Fisiología vegetal geobotánica Histología vegetal Morfología vegetal Paleobotánica Palinología Sistemática vegetal Disciplinas relacionadas Agricultura Agronomía Bioquímica y fitoquímica Ecología Etnobotánica fitoterapia Fitopatología Fitosociología Genética Horticultura Micología Microbiología Métodos de la botánica Herbario Artículo principal: Herbario Secado de especímenes en un herbario de Burkina Faso. Un herbario (del latín herbarium) es una colección de plantas o partes de plantas, preservadas, casi siempre a través de la desecación, procesadas para su conservación, e identificadas, y acompañadas de información importante, como nombre científico y nombre común, utilidad, características de la planta en vivo y del sitio de muestreo, así como la ubicación del punto donde se colectó. Estas plantas se conservan indefinidamente, y constituyen un banco de información que representa la flora o vegetación de una región determinada en un espacio reducido. Estos especímenes se usan con frecuencia como material de referencia para definir el taxón de una planta; pues contienen los holotipos para estas plantas. El tipo nomenclatural o, simplemente, tipo es un ejemplar de una dada especie sobre el que se ha realizado la descripción de la misma y que, de ese modo, valida la publicación de un nombre científico basado en él. El tipo del nombre de una especie es por lo general el espécimen de herbario (o pliego de herbario) a partir del cual se ha perfilado la descripción que valida el nombre. El tipo del nombre de un género es la especie sobre la cual se basó la descripción original que validaba el nombre. El tipo del nombre de una familia es el género sobre el cual fue basada la descripción original válida. En los nombres de taxones de rango superior al de familia no se aplica el principio de tipificación.[48]​ Jardín botánico Artículo principal: Jardín botánico Jardín Botánico de Curitiba. Los jardines botánicos (del latín hortus botanicus) son instituciones habilitadas por un organismo público, privado o asociativo (en ocasiones la gestión es mixta) cuyo objetivo es el estudio, la conservación y divulgación de la diversidad vegetal. Se caracterizan por exhibir colecciones científicas de plantas vivas, que se cultivan para conseguir alguno de estos objetivos: su conservación, investigación, divulgación y enseñanza. En los jardines botánicos se exponen plantas originarias de todo el mundo, generalmente con el objetivo de fomentar el interés de los visitantes hacia el mundo vegetal, aunque algunos de estos jardines se dedican, exclusivamente, a determinadas plantas y a especies concretas. Código Internacional de Nomenclatura para algas, hongos y plantas Estos párrafos son un extracto de Código Internacional de Nomenclatura para algas, hongos y plantas.[editar] El Código Internacional de Nomenclatura para algas, hongos y plantas (ICN)[49] es el compendio de reglas que rigen la nomenclatura taxonómica de los organismos tradicionalmente estudiados por la botánica (plantas, algas y hongos) a efectos de determinar, para cada taxón, un único nombre válido internacionalmente. Hasta el año 2011, con la celebración del XVIII Congreso Internacional de Botánica en Melbourne (Australia), se denominaba Código Internacional de Nomenclatura Botánica (en inglés, ICBN, en español CINB)."
ksampletext_wikipedia_biol_bioquimica: str = "Bioquímica. La bioquímica es una rama de la ciencia que estudia la composición química de los seres vivos, especialmente las proteínas, carbohidratos, lípidos y ácidos nucleicos, además de otras pequeñas moléculas presentes en las células y las reacciones químicas que sufren estos compuestos, como en el metabolismo que les permiten obtener energía (catabolismo) y generar biomoléculas propias (anabolismo). La bioquímica se basa en el concepto de que todo ser vivo contiene carbono y en general las moléculas biológicas están compuestas principalmente de carbono, hidrógeno, oxígeno, nitrógeno, fósforo y azufre. Es la rama de la ciencia que estudia la base química de las moléculas que componen algunas células y los tejidos, que catalizan las reacciones químicas del metabolismo celular como la digestión, la fotosíntesis y la inmunidad, entre otras muchas cosas. Podemos entender la bioquímica como una disciplina científica integradora que elabora el estudio de los biomas y biosistemas. Integra de esta forma las leyes químico-físicas y la evolución biológica que afectan a los biosistemas y a sus componentes. Lo hace desde un punto de vista molecular y trata de entender y aplicar su conocimiento a amplios sectores de la medicina (terapia genética y biomedicina), la agroalimentación, la farmacología. Constituye un pilar fundamental de la biotecnología, y se ha consolidado como una disciplina esencial para abordar los grandes problemas y enfermedades actuales y del futuro, tales como el cambio climático, la escasez de recursos agroalimentarios ante el aumento de población mundial, el agotamiento de las reservas de combustibles fósiles, la aparición de nuevas alergias, el aumento del cáncer, las enfermedades genéticas, la obesidad, etc. La bioquímica es una ciencia experimental y por ello recurrirá al uso de numerosas técnicas instrumentales propias y de otros campos, pero la base de su desarrollo parte del hecho de que lo que ocurre en vivo a nivel subcelular se mantiene o se conserva tras el fraccionamiento subcelular, y a partir de ahí, podemos estudiarlo. Historia Siglo XIX y primera mitad del XX La historia de la bioquímica como la conocemos hoy en día es prácticamente moderna; desde el siglo XIX se comenzó a direccionar una buena parte de la biología y la química a la creación de una nueva disciplina integradora: la química fisiológica o la bioquímica. Pero la aplicación de la bioquímica y su conocimiento probablemente comenzó hace 5000 años, con la producción de pan usando levaduras, en un proceso conocido como fermentación. Es difícil abordar la historia de la bioquímica, en cuanto que, es una mezcla compleja de química orgánica y biología, y en ocasiones, se hace complicado discernir entre lo exclusivamente biológico y lo exclusivamente químico orgánico y es evidente que la contribución a esta disciplina ha sido muy extensa. Aunque es cierto que existen datos experimentales que son básicos en la bioquímica. Se suele situar el inicio de la bioquímica en los descubrimientos en 1828 de Friedrich Wöhler que publicó un artículo acerca de la síntesis de urea, probando que los compuestos orgánicos pueden ser creados artificialmente, en contraste con la creencia comúnmente aceptada durante mucho tiempo, de que la generación de estos compuestos era posible solo en el interior de los seres vivos. La diastasa fue la primera enzima descubierta. En 1833 se extrajo de la solución de malta por Anselme Payen y Jean-François Persoz, dos químicos de una fábrica de azúcar francesa.[1]​ A mediados del siglo XIX, Louis Pasteur demostró los fenómenos de isomería química existente entre las moléculas de ácido tartárico provenientes de los seres vivos y las sintetizadas químicamente en el laboratorio. También estudió el fenómeno de la fermentación y descubrió que intervenían ciertas levaduras, y por tanto no era exclusivamente un fenómeno químico como se había defendido hasta ahora (entre ellos el propio Liebig); así Pasteur escribió: «la fermentación del alcohol es un acto relacionado con la vida y la organización de las células de las levaduras, y no con la muerte y la putrefacción de las células». Además desarrolló un método de esterilización de la leche, el vino y la cerveza (pasteurización) y contribuyó enormemente a refutar la idea de la generación espontánea de los seres vivos. En 1869 se descubre la nucleína y se observa que es una sustancia muy rica en fósforo. Dos años más tarde, Albrecht Kossel concluye que la nucleína es rica en proteínas y contiene las bases púricas adenina y guanina y las pirimidínicas citosina y timina. En 1889 se aíslan los dos componentes mayoritarios de la nucleína: Proteínas (70 %) Sustancias de carácter ácido: ácidos nucleicos (30 %) En 1878 el fisiólogo Wilhelm Kühne acuñó el término enzima para referirse a los componentes biológicos desconocidos que producían la fermentación. La palabra enzima fue usada después para referirse a sustancias inertes tales como la pepsina. En 1897 Eduard Buchner comenzó a estudiar la capacidad de los extractos de levadura para fermentar azúcar a pesar de la ausencia de células vivientes de levadura. En una serie de experimentos en la Universidad Humboldt de Berlín, encontró que el azúcar era fermentado incluso cuando no había elementos vivos en los cultivos de células de levaduras. Llamó a la enzima que causa la fermentación de la sacarosa, “zimasa”. Al demostrar que las enzimas podrían funcionar fuera de una célula viva, el siguiente paso fue demostrar cuál era la naturaleza bioquímica de esos biocatalizadores. El debate fue extenso; muchos, como el bioquímico alemán Richard Willstätter, discrepaban de que la proteína fuera el catalizador enzimático, hasta que en 1926, James B. Sumner demostró que la enzima ureasa era una proteína pura y la cristalizó. La conclusión de que las proteínas puras podían ser enzimas fue definitivamente probada en torno a 1930 por John Howard Northrop y Wendell Meredith Stanley, quienes trabajaron con diversas enzimas digestivas como la pepsina, la tripsina y la quimotripsina. En 1903 Mijaíl Tswett inicia los estudios de cromatografía para separación de pigmentos. En torno a 1915 Gustav Embden y Otto Meyerhof realizan sus estudios sobre la glucólisis. En 1920 se descubre que en las células hay ADN y ARN y que difieren en el azúcar que forma parte de su composición: desoxirribosa o ribosa. El ADN reside en el núcleo. Unos años más tarde, se descubre que en los espermatozoides hay fundamentalmente ADN y proteínas, y posteriormente Feulgen descubre que hay ADN en los cromosomas con su tinción específica para este compuesto. En 1925 Theodor Svedberg demuestra que las proteínas son macromoléculas y desarrolla la técnica de ultracentrifugación analítica. En 1928, Alexander Fleming descubre la penicilina y desarrolla estudios sobre la lisozima. Richard Willstätter (en torno 1910) estudia la clorofila y comprueba la similitud que hay con la hemoglobina. Posteriormente Hans Fischer en torno a 1930, investiga la química de las porfirinas de las que derivan la clorofila o el grupo porfirínico de la hemoglobina. Consiguió sintetizar hemina y bilirrubina. Paralelamente Heinrich Otto Wieland formula teorías sobre las deshidrogenaciones y explica la constitución de muchas otras sustancias de naturaleza compleja, como la pteridina, las hormonas sexuales o los ácidos biliares. En la década de 1940, Melvin Calvin concluye el estudio del ciclo de Calvin en la fotosíntesis y Albert Claude la síntesis del ATP en las mitocondrias. En torno a 1945 Gerty Cori, Carl Cori, y Bernardo Houssay completan sus estudios sobre el ciclo de Cori. En 1953 James Dewey Watson y Francis Crick, gracias a los estudios previos con cristalografía de rayos X de ADN de Rosalind Franklin y Maurice Wilkins, y los estudios de Erwin Chargaff sobre apareamiento de bases nitrogenadas, deducen la estructura de doble hélice del ADN. En 1957, Matthew Meselson y Franklin Stahl demuestran que la replicación del ADN es semiconservativa. Segunda mitad del siglo XX En la segunda mitad del siglo XX, comienza la auténtica revolución de la bioquímica y la biología molecular moderna, especialmente gracias al desarrollo de las técnicas experimentales más básicas como la cromatografía, la centrifugación, la electroforesis, las técnicas radioisotópicas y la microscopía electrónica, y las técnicas más complejas como la cristalografía de rayos X, la resonancia magnética nuclear, la PCR (Kary Mullis), el desarrollo de la inmuno-técnicas. Desde 1950 a 1975 , se conocen en profundidad y detalle aspectos del metabolismo celular inimaginables hasta ahora (fosforilación oxidativa (Peter Dennis Mitchell), ciclo de la urea y ciclo de Krebs (Hans Adolf Krebs), así como otras rutas metabólicas), se produce toda una revolución en el estudio de los genes y su expresión; se descifra el código genético (Francis Crick, Severo Ochoa, Har Gobind Khorana, Robert W. Holley y Marshall Warren Nirenberg), se descubren las enzimas de restricción (finales de 1960, Werner Arber, Daniel Nathans y Hamilton Smith), la ADN ligasa (en 1972, Mertz y Davis) y finalmente en 1973 Stanley Cohen y Herbert Boyer producen el primer ser vivo recombinante, naciendo así la ingeniería genética, convertida en una herramienta poderosísima con la que se supera la frontera entre especies y con la que podemos obtener un beneficio hasta ahora impensable. En 1970, un argentino, Luis Federico Leloir, médico, bioquímico y farmacéutico recibió el Premio Nobel de Química por sus investigaciones sobre los nucleótidos de azúcar, y el rol que cumplen en la fabricación de los hidratos de carbono.[2]​ En 1984, otro argentino, César Milstein, oriundo de la ciudad de Bahía Blanca, recibe el Premio Nobel de Medicina por sus investigaciones sobre anticuerpos monoclonales, hoy utilizados para tratar muchas enfermedades, incluidos algunos tipos de cáncer.[3]​ De 1975 hasta principios del siglo XXI, comienza a secuenciarse el ADN (Allan Maxam, Walter Gilbert y Frederick Sanger), comienzan a crearse las primeras industrias biotecnológicas (Genentech), se aumenta la creación de fármacos y vacunas más eficaces, se eleva el interés por las inmunología y las células madres y se descubre la enzima telomerasa (Elizabeth Blackburn y Carol Greider). En 1989 se utiliza la biorremediación a gran escala en el derrame del petrolero Exxon Valdez en Alaska. Se clonan los primeros seres vivos, se secuencia el ADN de decenas de especies y se publica el genoma completo del hombre (Craig Venter, Celera Genomics y Proyecto Genoma Humano), se resuelven decenas de miles de estructuras proteicas y se publican en PDB, así como genes, en GenBank. Comienza el desarrollo de la bioinformática y la computación de sistemas complejos, que se constituyen como herramientas muy poderosas en el estudio de los sistemas biológicos. Se crea el primer cromosoma artificial y se logra la primera bacteria con genoma sintético (2007, 2009, Craig Venter). Se fabrican las nucleasas con dedos de zinc. Se inducen artificialmente células, que inicialmente no eran pluripotenciales, a células madre pluripotenciales (Shin'ya Yamanaka). Comienzan a darse los primeros pasos. Ramas de la bioquímica Esquema de una célula típica animal con sus orgánulos y estructuras. El pilar fundamental de la investigación bioquímica clásica se centra en las propiedades de las proteínas, muchas de las cuales son enzimas. Sin embargo, existen otras disciplinas que se centran en las propiedades biológicas de carbohidratos (glucobiología)[4] y lípidos (lipobiología).[5]​ Por razones históricas la bioquímica del metabolismo de la célula ha sido intensamente investigada, en importantes líneas de investigación actuales (como el Proyecto Genoma, cuya función es la de identificar y registrar todo el material genético humano), se dirigen hacia la investigación del ADN, el ARN, la síntesis de proteínas, la dinámica de la membrana celular y los ciclos energéticos. Las ramas de la bioquímica son muy amplias y diversas, y han ido variando con el tiempo y los avances de la biología, la química y la física. Bioquímica estructural: es un área de la bioquímica que pretende comprender la arquitectura química de las macromoléculas biológicas, especialmente de las proteínas y de los ácidos nucleicos (ADN y ARN). Así se intenta conocer las secuencias peptídicas, su estructura y conformación tridimensional, y las interacciones físico-químicas atómicas que posibilitan a dichas estructuras. Uno de sus máximos retos es determinar la estructura de una proteína conociendo solo la secuencia de aminoácidos, que supondría la base esencial para el diseño racional de proteínas (ingeniería de proteínas).[6]​ Ciencia que estudia la estructura, propiedades físicas, la reactividad y transformación de los compuestos orgánicos. Química Orgánica Química orgánica: es un área de la química que se encarga del estudio de los compuestos orgánicos (es decir, aquellos que tienen enlaces covalentes carbono-carbono o carbono-hidrógeno) que provienen específicamente de seres vivos. Se trata de una ciencia íntimamente relacionada con la bioquímica clásica,[7] ya que en la mayoría de los compuestos biológicos[8] participa el carbono[9] Mientras que la bioquímica clásica ayuda a comprender los procesos biológicos con base en conocimientos de estructura, enlace químico, interacciones moleculares y reactividad de las moléculas orgánicas, la química bioorgánica intenta integrar los conocimientos de síntesis orgánica, mecanismos de reacción, análisis estructural y métodos analíticos con las reacciones metabólicas primarias y secundarias, la biosíntesis, el reconocimiento celular y la diversidad química de los organismos vivos. De allí surge la Química de Productos Naturales (V. Metabolismo secundario).[10]​ Enzimología: estudia el comportamiento de los catalizadores biológicos o enzimas, como son algunas proteínas y ciertos ARN catalíticos, así como las coenzimas y cofactores como metales y vitaminas. Así se cuestiona los mecanismos de catálisis, los procesos de interacción de las enzimas-sustrato, los estados de transición catalíticos, las actividades enzimáticas, la cinética de la reacción y los mecanismos de regulación y expresión enzimáticas, todo ello desde un punto de vista bioquímico. Estudia y trata de comprender los elementos esenciales del centro activo y de aquellos que no participan, así como los efectos catalíticos que ocurren en la modificación de dichos elementos; en este sentido, utilizan frecuentemente técnicas como la mutagénesis dirigida.[11]​ Bioquímica metabólica: es un área de la bioquímica que pretende conocer los diferentes tipos de rutas metabólicas a nivel celular, y su contexto orgánico. De esta forma son esenciales conocimientos de enzimología y biología celular. Estudia todas las reacciones bioquímicas celulares que posibilitan la vida, y así como los índices bioquímicos orgánicos saludables, las bases moleculares de las enfermedades metabólicas o los flujos de intermediarios metabólicos a nivel global. De aquí surgen disciplinas académicas como la bioenergética (estudio del flujo de energía en los organismos vivos), la bioquímica nutricional (estudio de los procesos de nutrición asociados a| rutas metabólicas)[12] y la bioquímica clínica (estudio de las alteraciones bioquímicas en estado de enfermedad o traumatismo). La metabolómica es el conjunto de ciencias y técnicas dedicadas al estudio completo del sistema constituido por el conjunto de moléculas que constituyen los intermediarios metabólicos, metabolitos primarios y secundarios, que se pueden encontrar en un sistema biológico. Xenobioquímica: es la disciplina que estudia el comportamiento metabólico de los compuestos cuya estructura química no es propia en el metabolismo regular de un organismo determinado. Pueden ser metabolitos secundarios de otros organismos (por ejemplo las micotoxinas, los venenos de serpientes y los fitoquímicos cuando ingresan al organismo humano) o compuestos poco frecuentes o inexistentes en la naturaleza.[13] La farmacología es una disciplina que estudia a los xenobióticos que benefician al funcionamiento celular en el organismo debido a sus efectos terapéuticos o preventivos (fármacos). La farmacología tiene aplicaciones clínicas cuando las sustancias son utilizadas en el diagnóstico, prevención, tratamiento y alivio de síntomas de una enfermedad así como el desarrollo racional de sustancias menos invasivas y más eficaces contra dianas biomoleculares concretas. Por otro lado, la toxicología es el estudio que identifica, estudia y describe, la dosis, la naturaleza, la incidencia, la severidad, la reversibilidad y, generalmente, los mecanismos de los efectos adversos (efectos tóxicos) que producen los xenobióticos. Actualmente la toxicología también estudia el mecanismo de los componentes endógenos, como los radicales libres de oxígeno y otros intermediarios reactivos, generados por xenobióticos y endobióticos. Inmunología: área de la biología, la cual se interesa por la reacción del organismo frente a otros organismos como las bacterias y virus. Todo esto tomando en cuenta la reacción y funcionamiento del sistema inmune de los seres vivos. Es esencial en esta área el desarrollo de los estudios de producción y comportamiento de los anticuerpos.[14]​ Endocrinología: es el estudio de las secreciones internas llamadas hormonas, las cuales son sustancias producidas por células especializadas cuyo fin es de afectar la función de otras células. La endocrinología trata la biosíntesis, el almacenamiento y la función de las hormonas, las células y los tejidos que las secretan, así como los mecanismos de señalización hormonal. Existen subdisciplinas como la endocrinología médica, la endocrinología vegetal y la endocrinología animal.[15]​ Neuroquímica: es el estudio de las moléculas orgánicas que participan en la actividad neuronal. Este término es empleado con frecuencia para referir a los neurotransmisores y otras moléculas como las drogas neuro-activas que influencian la función neuronal. Quimiotaxonomía: es el estudio de la clasificación e identificación de organismos de acuerdo a sus diferencias y similitudes demostrables en su composición química. Los compuestos estudiados pueden ser fosfolípidos, proteínas, péptidos, heterósidos, alcaloides y terpenos. John Griffith Vaughan fue uno de los pioneros de la quimiotaxonomía. Entre los ejemplos de las aplicaciones de la quimiotaxonomía pueden citarse la diferenciación de las familias Asclepiadaceae y Apocynaceae según el criterio de la presencia de látex; la presencia de agarofuranos en la familia Celastraceae; las sesquiterpenlactonas con esqueleto de germacrano que son características de la familia Asteraceae o la presencia de abietanos en las partes aéreas de plantas del género Salvia del viejo Mundo a diferencia de las del Nuevo Mundo que presentan principalmente neo-clerodanos.[16]​ Ecología química: es el estudio de los compuestos químicos de origen biológico implicados en las interacciones de organismos vivos. Se centra en la producción y respuesta de moléculas señalizadoras (semioquímicos), así como los compuestos que influyen en el crecimiento, supervivencia y reproducción de otros organismos (aleloquímicos). Virología: área de la biología, que se dedica al estudio de los biosistemas más elementales: los virus. Tanto en su clasificación y reconocimiento, como en su funcionamiento y estructura molecular. Pretende reconocer dianas para la actuación de posibles de fármacos y vacunas que eviten su directa o preventivamente su expansión. También se analizan y predicen, en términos evolutivos, la variación y la combinación de los genomas víricos, que podrían hacerlos finalmente, más peligrosos. Finalmente suponen una herramienta con mucha proyección como vectores recombinantes, y han sido ya utilizados en terapia génica.[17]​ Imagen: Proteína mioglobina Genética molecular e ingeniería genética: es un área de la bioquímica y la biología molecular que estudia los genes, su herencia y su expresión. Molecularmente, se dedica al estudio del ADN y del ARN principalmente, y utiliza herramientas y técnicas potentes en su estudio, tales como la PCR y sus variantes, los secuenciadores masivos, los kits comerciales de extracción de ADN y ARN, procesos de transcripción-traducción in vitro e in vivo, enzimas de restricción, ADN ligasas… Es esencial conocer como el ADN se replica, se transcribe y se traduce a proteínas (Dogma Central de la Biología Molecular), así como los mecanismos de expresión basal e inducible de genes en el genoma. También estudia la inserción de genes, el silenciamiento génico y la expresión diferencial de genes y sus efectos. Superando así las barreras y fronteras entre especies en el sentido que el genoma de una especie podemos insertarlo en otro y generar nuevas especies. Uno de sus máximos objetivos actuales es conocer los mecanismos de regulación y expresión genética, es decir, obtener un código epigenético. Constituye un pilar esencial en todas las disciplinas biocientíficas, especialmente en biotecnología. La biotecnología moderna tiene múltiples aplicaciones y variadas e incluyen, además de la fabricación de medicamentos, alimentos, papel, entre otros, el mejoramiento de animales y plantas de interés agronómico.[18]​ Biología Molecular: es la disciplina científica que tiene como objetivo el estudio de los procesos que se desarrollan en los seres vivos desde un punto de vista molecular. Así como la bioquímica clásica investiga detalladamente los ciclos metabólicos y la integración y desintegración de las moléculas que componen los seres vivos, la biología molecular pretende fijarse con preferencia en el comportamiento biológico de las macromoléculas (ADN, ARN, enzimas, hormonas, etc.) dentro de la célula y explicar las funciones biológicas del ser vivo por estas propiedades a nivel molecular.[19]​ Biología celular: (antiguamente citología, de citos=célula y logos=Estudio o Tratado ) es un área de la biología que se dedica al estudio de la morfología y fisiología de las células procariotas y eucariotas. Trata de conocer sus propiedades, estructura, composición bioquímica, funciones, orgánulos que contienen, su interacción con el ambiente y su ciclo vital. Es esencial en esta área conocer los procesos intrínsecos a la vida celular durante el ciclo celular, como la nutrición, la respiración, la síntesis de componentes, los mecanismos de defensa, la división celular y la muerte celular. También se deben conocer los mecanismos de comunicación de células (especialmente en organismos pluricelulares) o las uniones intercelulares. Es un área esencialmente de observación y experimentación en cultivos celulares, que, frecuentemente, tienen como objetivo la identificación y separación de poblaciones celulares y el reconocimiento de orgánulos celulares. Algunas técnicas utilizadas en biología celular tienen que ver con el empleo de técnicas de citoquímica, siembra de cultivos celulares, observación por microscopía óptica y electrónica, inmunocitoquímica, inmunohistoquímica, ELISA o citometría de flujo."


###############################################################################################


ksampletext_wikipedia_bacilo: str = "Bacilo. En bacteriología: la palabra bacilo se usa para describir cualquier bacteria con forma de barra o vara, y pueden encontrarse en muchos grupos taxonómicos diferentes tipos de bacterias. Sin embargo el nombre Bacillus, se refiere a un género específico de bacteria. El otro nombre Bacilli; hace referencia a una clase de bacilos que incluyen dos órdenes, uno de los cuales contiene al género Bacillus. Los bacilos son bacterias que se encuentran en diferentes ambientes y solo se pueden observar con un microscopio. Los bacilos suelen dividirse en el mismo plano y son solitarios, pero pueden combinarse para formar diplobacilos, estreptobacilos y cocobacilos: Diplobacilos: Dos bacilos dispuestos uno al lado del otro. Estreptobacilos: Bacilos dispuestos en cadenas. Cocobacilos: Ovalados y en forma de bastoncillo. Por tipo de bacteria los bacilos pueden ser: Bacilos Gram positivos: fijan el cristal violeta (tinción de Gram) en la pared celular porque tienen una gruesa capa de peptidoglucano. Bacilos Gram negativos: no fijan el cristal violeta y se tiñen con el colorante de contraste usado en la tinción de Gram que es la safranina, debido a que tienen una fina capa de péptidoglucano en medio de dos bicapas lipídicas en la cual se encuentran los lipopolisacáridos o también llamados endotoxinas (principalmente en la membrana externa). Aunque muchos bacilos son patógenos para el ser humano, algunos no hacen daño, pues producen algunos productos lácteos como el yogur (lactobacilos). A lo largo de la historia de la medicina y de la microbiología, varias de estas bacterias han producido enfermedad en los humanos y por lo general se han adoptado el nombre del científico que los descubría, por ejemplo: Bacilo de Aertrycke: Salmonela. Bacilo de Bang: Brucella abortus. Bacilo de Ducrey: Haemophilus ducreyi. Bacilo de Eberth: Salmonella typhi. Bacilo de Nicolaier: Tétano. Bacilo de Hansen: Mycobacterium leprae. Bacilo de Klebs-Löffler: Corynebacterium diphtheriae. Bacilo de Koch: Mycobacterium tuberculosis. Bacilo de Morex: Género Moraxella. Bacilo de Yersin: Yersinia pestis."

