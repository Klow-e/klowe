

# klowe/sampletexts.py


###############################################################################################


def KSampleTexts():
    """
    Prints every variable that starts with 'ksampletext_', that's to say, every sample text in KlowE.
    """
    for i in globals(): print(i) if i.startswith("ksampletext_") else None


###############################################################################################


ksampletext_wikipedia_math_matematicas: str = "Matemáticas. Las matemáticas o, también, la matemática (del latín mathematĭca, y a la vez del griego, transliterado como, derivado de, lo que se comprende) es una ciencia formal que estudia los patrones, propiedades, estructuras y relaciones presentes en sistemas lógicos y abstractos creados por los humanos, conceptos tales como cantidad, forma, espacio y número se podrían considerar como el objeto de estudio de la matemática. Descripción Las ciencias naturales han hecho un uso extensivo de la matemática para explicar diversos fenómenos observables, tal como lo expresó Eugene Paul Wigner (Premio Nobel de Física en 1963): «El primer punto es que la enorme utilidad de las matemáticas en las ciencias naturales es algo que roza lo misterioso y que no tiene una explicación racional. En segundo lugar, es precisamente esta extraña utilidad de los conceptos matemáticos lo que plantea la cuestión de la unicidad de nuestras teorías físicas.» «El milagro de la adecuación del lenguaje de las matemáticas para la formulación de las leyes de la física es un don maravilloso que no comprendemos ni merecemos.» Galileo Galilei, en la misma línea, lo había expresado así: «La filosofía está escrita en este enorme libro, que está continuamente abierto ante nuestros ojos (digo en el nuevo idioma), pero uno no puede entenderlo primero, uno no aprende a entender el idioma y a conocer los caracteres en que está escrito. Está escrito en lenguaje matemático, y los caracteres son triángulos, círculos y otras figuras geométricas, sin las cuales es imposible entender una palabra; sin éstos es un vano vagar por un oscuro laberinto.» Mediante la abstracción y el uso de la lógica en el razonamiento, la matemática ha evolucionado basándose en el cálculo y las mediciones, junto con el estudio sistemático de la forma y el movimiento de los objetos físicos. Las matemáticas, desde sus comienzos, han tenido un fin práctico. Las explicaciones que se apoyaban en la lógica aparecieron por primera vez con la matemática helénica, especialmente con los Elementos de Euclides. La matemática siguió desarrollándose, con continuas interrupciones, hasta que en el Renacimiento las innovaciones matemáticas interactuaron con los nuevos descubrimientos científicos. Como consecuencia, hubo una aceleración en la investigación que continúa hasta la actualidad. Hoy día, la matemática se usa en todo el mundo como una herramienta esencial en muchos campos, entre los que se encuentran las ciencias naturales, las ciencias aplicadas, las humanidades, la medicina y las ciencias sociales, e incluso disciplinas que, aparentemente, no están vinculadas con ella, como la música (por ejemplo, en cuestiones de resonancia armónica, Cuerda vibrante, etc.) y la literatura. Las matemáticas aplicadas, rama de la matemática destinada a la aplicación del conocimiento matemático a otros ámbitos, inspiran y hacen uso de los nuevos descubrimientos matemáticos y, en ocasiones, conducen al desarrollo de nuevas disciplinas. Los matemáticos también participan en la matemática pura, sin tener en cuenta sus aplicaciones, aunque estas suelen ser descubiertas con el paso del tiempo. Historia Artículo principal: Historia de las matemáticas Las matemáticas son una de las ciencias más antiguas. Floreció primero antes de la antigüedad en Mesopotamia, en cuanto a la geometría India y China, y más tarde en la antigüedad en Grecia y el helenismo. De ahí data la orientación hacia la tarea de «demostración puramente lógica» y la primera axiomatización, a saber, la geometría euclidiana. En la Edad Media sobrevivió de forma independiente en el primer humanismo de las universidades y en el mundo árabe. A principios de la era moderna, François Viète introdujo variables y René Descartes inauguró un enfoque computacional de la geometría mediante el uso de coordenadas. La consideración de las tasas de cambio (fluxión) así como la descripción de las tangentes y la determinación de los contenidos de las superficies (cuadratura) condujeron al cálculo infinitesimal de Gottfried Wilhelm Leibniz e Isaac Newton. La mecánica de Newton y su ley de la gravitación fueron también una fuente de orientación de problemas matemáticos como el problema de los tres cuerpos en los siglos siguientes. Otro de los principales problemas de la primera época moderna fue la solución de ecuaciones algebraicas cada vez más complicadas. Para hacer frente a esto, Niels Henrik Abel y Évariste Galois desarrollaron el concepto de grupo, que describe las relaciones entre las simetrías de un objeto. El álgebra más reciente y, en particular, la geometría algebraica pueden considerarse como una profundización de estas investigaciones. Una idea entonces nueva en el intercambio de cartas entre Blaise Pascal y Pierre de Fermat en 1654 acerca del problema de los juegos de azar, aunque existían otras soluciones discutibles como las de Cardano, quien intentó matematizarlas. Pierre-Simon Laplace hace un recuento de los diferentes logros hasta 1812 cuando publica su Ensayo filosófico sobre las posibilidades. Las nuevas ideas y métodos conquistaron muchos campos. Pero durante siglos, la teoría clásica de la probabilidad se dividió en escuelas separadas. Los intentos de definir explícitamente el término «probabilidad» solo tuvieron éxito para casos especiales. Solo la publicación del libro de texto de Andrei Kolmogorov en 1933 Los fundamentos de la Teoría de la Probabilidad completó el desarrollo de los fundamentos de la teoría moderna de la probabilidad. En el transcurso del siglo XIX, el cálculo infinitesimal encontró su forma actual de rigor gracias a los trabajos de Augustin-Louis Cauchy y Karl Weierstrass. La teoría de conjuntos desarrollada por Georg Cantor hacia finales del siglo XIX es también indispensable en la matemática actual, aunque las paradojas del concepto ingenuo de conjuntos dejaron claro, en un primer momento, la incierta base sobre la que se asentaban las matemáticas. El desarrollo de la primera mitad del siglo XX estuvo influenciado por la publicación de los problemas de Hilbert. Uno de los problemas intentaba axiomatizar completamente las matemáticas; al mismo tiempo, se hicieron grandes esfuerzos de abstracción, es decir, el intento de reducir los objetos a sus propiedades esenciales. Así, Emmy Noether desarrolló los fundamentos del álgebra moderna, Felix Hausdorff desarrolló la topología general como el estudio de los espacios topológicos, Stefan Banach desarrolló probablemente el concepto más importante del análisis funcional, el espacio de Banach que lleva su nombre. Un nivel de abstracción aún mayor, un marco común para la consideración de construcciones similares de diferentes áreas de las matemáticas, fue finalmente creado por la introducción de la teoría de categorías por Samuel Eilenberg y Saunders Mac Lane. Introducción Etimología La palabra «matemática» (del griego μαθηματικά mathēmatiká, «cosas que se aprenden») viene del griego antiguo μάθημα (máthēma), que quiere decir «campo de estudio o instrucción». Las matemáticas requieren un esfuerzo de instrucción o aprendizaje, refiriéndose a áreas del conocimiento que solo pueden entenderse tras haber sido instruido en las mismas, como la astronomía. «El arte matemática» (μαθηματική τέχνη, mathēmatikḗ tékhnē) se contrapondría en esto a la música, «el arte de las musas» (μουσική τέχνη, mousikē téchnē), que sería un arte, como la poesía, retórica y similares, que se puede apreciar directamente, «que se puede entender sin haber sido instruido». Aunque el término ya era usado por los pitagóricos (matematikoi) en el siglo VI a. C., alcanzó su significado más técnico y reducido de «estudio matemático» en los tiempos de Aristóteles (siglo IV a. C.). Su adjetivo es μαθηματικός (mathēmatikós), «relacionado con el aprendizaje», lo cual, de manera similar, vino a significar «matemático». En particular, μαθηματική τέχνη (mathēmatikḗ tékhnē; en latín ars mathematica), significa «el arte matemática». La forma más usada es el plural matemáticas (cuyo acortamiento, en algunos países, es «mates»), que tiene el mismo significado que el singular y viene de la forma latina mathematica (Cicerón), basada en el plural en griego τα μαθηματικά (ta mathēmatiká), usada por Aristóteles y que significa, a grandes rasgos, «todas las cosas matemáticas». Algunos autores, sin embargo, hacen uso de la forma singular del término; tal es el caso de Bourbaki, en el tratado Elementos de matemática (Élements de mathématique, 1940), destaca la uniformidad de este campo aportada por la visión axiomática moderna, aunque también hace uso de la forma plural como en Éléments dhistoire des mathématiques (1969), posiblemente sugiriendo que es Bourbaki quien finalmente realiza la unificación de las matemáticas. Así mismo, en el escrito LArchitecture des mathématiques (1948) plantea el tema en la sección «¿Matemáticas, singular o plural?» donde defiende la unicidad conceptual de la matemática aunque hace uso de la forma plural en dicho escrito. Algunas definiciones de matemática Establecer definiciones claras y precisas es el fundamento de la matemática, aunque encontrar una definición única para ella es improbable. Se muestran algunas reflexiones de reconocidos autores: René Descartes: «Y considerando esto más atentamente al cabo se nota que solamente aquellas en las que se estudia cierto orden y medida hacen referencia a la Mathesis, y que no importa si tal medida ha de buscarse en los números, en las figuras, en los astros, en los sonidos o en cualquier otro objeto;» Carl Friedrich Gauss: «El matemático se abstrae totalmente de la naturaleza de los objetos y el contenido de sus relaciones; se preocupa únicamente por la enumeración y la comparación de las relaciones entre ellos [...]» David Hilbert: «[...] nos lleva a una concepción de las matemáticas que considera a éstas como un inventario de fórmulas a las que corresponden, en primer lugar, expresiones concretas de enunciados finitistas y a las que se añaden, en segundo lugar, otras fórmulas que carecen de todo significado y que constituyen los objetos ideales de nuestra teoría.» Benjamin Peirce: «La matemática es la ciencia que extrae conclusiones necesarias.» Bertrand Russell: Trató de probar «que toda la Matemática pura trabaja exclusivamente con conceptos definibles en función de un número muy pequeño de conceptos lógicos fundamentales, y de que todas las proposiciones se pueden deducir de un número muy pequeño de principios lógicos fundamentales.» John David Barrow: «En el fondo, matemáticas es el nombre que le damos al conjunto de todos los patrones e interrelaciones posibles. Algunos de esos patrones están entre formas, otros están en secuencias de números, mientras que otros son relaciones más abstractas entre estructuras. La esencia de las matemáticas radica en las relaciones entre cantidades y cualidades. Por lo tanto, son las relaciones entre los números, no los números en sí mismos, las que constituyen el foco de interés de los matemáticos modernos.» Epistemología y controversia sobre la matemática como ciencia El carácter epistemológico y científico de la matemática ha sido ampliamente discutido. En la práctica, la matemática se emplea para estudiar relaciones cuantitativas, estructuras, relaciones geométricas y las magnitudes variables. Los matemáticos buscan patrones, formulan nuevas conjeturas e intentan alcanzar la verdad matemática mediante deducciones rigurosas. Estas les permiten establecer los axiomas y las definiciones apropiados para dicho fin. Algunas definiciones clásicas restringen las matemáticas al razonamiento sobre cantidades, aunque solo una parte de la matemática actual usa números, predominando el análisis lógico de construcciones abstractas no cuantitativas. Existe cierta discusión acerca de si los objetos matemáticos, como los números y puntos, realmente existen o simplemente provienen de la imaginación humana. El matemático Benjamin Peirce definió las matemáticas como «la ciencia que señala las conclusiones necesarias». Por otro lado: «cuando las leyes de la matemática se refieren a la realidad, no son exactas; cuando son exactas, no se refieren a la realidad». Albert Einstein Se ha discutido el carácter científico de las matemáticas debido a que sus procedimientos y resultados poseen una firmeza e inevitabilidad inexistentes en otras disciplinas como pueden ser la física, la química o la biología. Así, la matemática sería tautológica, infalible y a priori, mientras que otras, como la geología o la fisiología, serían falibles y a posteriori. Son estas características lo que hace dudar de colocarse en el mismo rango que las disciplinas antes citadas pese a las afirmaciones como las de John Stuart Mill quien sostenía en 1843: «En realidad, las leyes de los números son verdades físicas provenientes de la observación.» Así, los matemáticos pueden descubrir nuevos procedimientos para resolver integrales o teoremas, pero se muestran incapaces de descubrir un suceso que ponga en duda el Teorema de Pitágoras o cualquier otro, como sí sucede constantemente con las ciencias de la naturaleza. El teorema de Pitágoras es uno de los enunciados más conocidos y antiguos de las matemáticas. Un ábaco, instrumento para efectuar operaciones aritméticas sencillas (sumas, restas y también multiplicaciones), fue muy utilizado en otros tiempos. La matemática puede ser entendida como ciencia; si es así debiera señalarse su objeto y su método. Sin embargo, algunos plantean que la matemática es un lenguaje formal, seguro, eficiente, aplicable al entendimiento de la naturaleza, tal como indicó Galileo; además muchos fenómenos de carácter social, otros de carácter biológico o geológico, pueden ser estudiados mediante la aplicación de ecuaciones diferenciales, cálculo de probabilidades o teoría de conjunto. Precisamente, el avance de la física y de la química ha exigido la invención de nuevos conceptos, instrumentos y métodos en la matemática, sobre todo en el análisis real, análisis complejo y el análisis matricial. Aspectos formales, metodológicos y estéticos La inspiración, las matemáticas puras, aplicadas y la estética Isaac Newton (1643-1727), comparte con Leibniz la autoría del desarrollo del cálculo integral y diferencial. Es muy posible que el arte de calcular haya sido desarrollado antes incluso que la escritura, relacionado fundamentalmente con la contabilidad y la administración de bienes, el comercio, en la agrimensura y, posteriormente, en la astronomía. Actualmente, todas las ciencias aportan problemas que son estudiados por matemáticos, al mismo tiempo que aparecen nuevos problemas dentro de las propias matemáticas. Por ejemplo, el físico Richard Feynman propuso la integral de caminos como fundamento de la mecánica cuántica, combinando el razonamiento matemático y el enfoque de la física, pero todavía, no se ha logrado una definición plenamente satisfactoria en términos matemáticos. Igualmente, la teoría de cuerdas, una teoría científica en desarrollo que trata de unificar las cuatro fuerzas fundamentales de la física, sigue inspirando a las más modernas matemáticas. Algunas matemáticas solo son relevantes en el área en la que estaban inspiradas y son aplicadas para otros problemas en ese campo. Sin embargo, a menudo las matemáticas inspiradas en un área concreta resultan útiles en muchos ámbitos, y se incluyen dentro de los conceptos matemáticos generales aceptados. El notable hecho de que incluso la matemática más pura habitualmente tiene aplicaciones prácticas es lo que Eugene Paul Wigner ha definido como «la irrazonable eficacia de las matemáticas en las Ciencias Naturales». Como en la mayoría de las áreas de estudio, la explosión de los conocimientos en la era científica ha llevado a la especialización de las matemáticas. Hay una importante distinción entre las matemáticas puras y las matemáticas aplicadas. La mayoría de los matemáticos que se dedican a la investigación se centran únicamente en una de estas áreas y, a veces, la elección se realiza cuando comienzan su licenciatura. Varias áreas de las matemáticas aplicadas se han fusionado con otras áreas tradicionalmente fuera de las matemáticas y se han convertido en disciplinas independientes, como pueden ser la estadística, la investigación de operaciones o la informática. Aquellos que sienten predilección por las matemáticas, consideran que prevalece un aspecto estético que define a la mayoría de las matemáticas. Muchos matemáticos hablan de la elegancia de la matemática, su intrínseca estética y su belleza interna. En general, uno de sus aspectos más valorados es la simplicidad. Hay belleza en una simple y contundente demostración, como la demostración de Euclides de la existencia de infinitos números primos, y en un elegante análisis numérico que acelera el cálculo, así como en la transformada rápida de Fourier. Godfrey Harold Hardy en A Mathematicians Apology (Apología de un matemático) expresó la convicción de que estas consideraciones estéticas son, en sí mismas, suficientes para justificar el estudio de las matemáticas puras. Los matemáticos con frecuencia se esfuerzan por encontrar demostraciones de los teoremas que son especialmente elegantes, el excéntrico matemático Paul Erdős se refiere a este hecho como la búsqueda de pruebas de El Libro en el que Dios ha escrito sus demostraciones favoritas. La popularidad de la matemática recreativa es otra señal que nos indica el placer que produce resolver las preguntas matemáticas. Notación, lenguaje y rigor Artículo principal: Notación matemática Leonhard Euler. Probablemente el más prolífico matemático de todos los tiempos. La mayor parte de la notación matemática que se utiliza hoy en día no se inventó hasta el siglo XVIII. Antes de eso, las matemáticas eran escritas con palabras, un minucioso proceso que limitaba el avance matemático. En el siglo XVIII, Euler, fue responsable de muchas de las notaciones empleadas en la actualidad. La notación moderna hace que las matemáticas sean mucho más fácil para los profesionales, pero para los principiantes resulta complicada. La notación reduce las matemáticas al máximo, hace que algunos símbolos contengan una gran cantidad de información. Al igual que la notación musical, la notación matemática moderna tiene una sintaxis estricta y codifica la información que sería difícil de escribir de otra manera. El símbolo de infinito en diferentes tipografías. El lenguaje matemático también puede ser difícil para los principiantes. Palabras tales como o y solo si tienen significados más precisos que en lenguaje cotidiano. Además, palabras como abierto y cuerpo tienen significados matemáticos muy concretos. La jerga matemática, o lenguaje matemático, incluye términos técnicos como homeomorfismo o integrabilidad. La razón que explica la necesidad de utilizar la notación y la jerga es que el lenguaje matemático requiere más precisión que el lenguaje cotidiano. Los matemáticos se refieren a esta precisión en el lenguaje y en la lógica como el «rigor». El rigor es una condición indispensable que debe tener una demostración matemática. Los matemáticos quieren que sus teoremas a partir de los axiomas sigan un razonamiento sistemático. Esto sirve para evitar teoremas erróneos, basados en intuiciones falibles, que se han dado varias veces en la historia de esta ciencia. El nivel de rigor previsto en las matemáticas ha variado con el tiempo: los griegos buscaban argumentos detallados, pero en tiempos de Isaac Newton los métodos empleados eran menos rigurosos. Los problemas inherentes de las definiciones que Newton utilizaba dieron lugar a un resurgimiento de un análisis cuidadoso y a las demostraciones oficiales del siglo XIX. Ahora, los matemáticos continúan apoyándose entre ellos mediante demostraciones asistidas por ordenador. Un axioma se interpreta tradicionalmente como una «verdad evidente», pero esta concepción es problemática. En el ámbito formal, un axioma no es más que una cadena de símbolos, que tiene un significado intrínseco solo en el contexto de todas las fórmulas derivadas de un sistema axiomático. La matemática como ciencia Carl Friedrich Gauss, apodado el «príncipe de los matemáticos», se refería a la matemática como «la reina de las ciencias». Carl Friedrich Gauss se refería a la matemática como «la reina de las ciencias». Tanto en el latín original Scientiārum Regīna, así como en alemán Königin der Wissenschaften, la palabra ciencia debe ser interpretada como (campo de) conocimiento. Si se considera que la ciencia es el estudio del mundo físico, entonces las matemáticas, o por lo menos las matemáticas puras, no son una ciencia. Muchos filósofos creen que las matemáticas no son experimentalmente falsables y, por ende, no son una ciencia según la definición de Karl Popper. No obstante, en la década de 1930 una importante labor en la lógica matemática demuestra que las matemáticas no pueden reducirse a la lógica y Karl Popper llegó a la conclusión de que «la mayoría de las teorías matemáticas son, como las de física y biología, hipotético-deductivas. Por lo tanto, las matemáticas puras se han vuelto más cercanas a las ciencias naturales cuyas hipótesis son conjeturas, así ha sido hasta ahora». Otros pensadores, en particular Imre Lakatos, han solicitado una versión de Falsacionismo para las propias matemáticas. Una visión alternativa es que determinados campos científicos (como la física teórica) son matemáticas con axiomas que pretenden corresponder a la realidad. De hecho, el físico teórico, John Michael Ziman, propone que la ciencia es «conocimiento público» y, por tanto, incluye a las matemáticas. En cualquier caso, las matemáticas tienen mucho en común con distintos campos de las ciencias físicas, especialmente la exploración de las consecuencias lógicas de las hipótesis. La intuición y la experimentación también desempeñan un papel importante en la formulación de conjeturas tanto en las matemáticas como en las otras ciencias. Las matemáticas experimentales siguen ganando representación dentro de las matemáticas. El cálculo y simulación están jugando un papel cada vez mayor tanto en las ciencias como en las matemáticas, atenuando la objeción de que las matemáticas no se sirven del método científico. En 2002 Stephen Wolfram propuso, en su libro Un nuevo tipo de ciencia, que la matemática computacional merece ser explorada empíricamente como un campo científico. Las opiniones de los matemáticos sobre este asunto son muy variadas. Muchos matemáticos consideran que llamar a su campo ciencia es minimizar la importancia de su perfil estético, además supone negar su historia dentro de las siete artes liberales. Otros consideran que hacer caso omiso de su conexión con las ciencias supone ignorar la evidente conexión entre las matemáticas y sus aplicaciones en la ciencia y la ingeniería, que ha impulsado considerablemente el desarrollo de las matemáticas. Otro asunto de debate, que guarda cierta relación con el anterior, es si la matemática fue creada (como el arte) o descubierta (como la ciencia). Este es uno de los muchos temas de incumbencia de la filosofía de las matemáticas. Los premios matemáticos se mantienen generalmente separados de sus equivalentes en la ciencia. El más prestigioso premio dentro de las matemáticas es la Medalla Fields, fue instaurado en 1936 y se concede cada cuatro años. A menudo se le considera el equivalente del Premio Nobel para la ciencia. Otros premios son el Premio Wolf en matemática, creado en 1978, que reconoce los logros en vida de los matemáticos, y el Premio Abel, otro gran premio internacional, que se introdujo en 2003. Estos dos últimos se conceden por un excelente trabajo, que puede ser una investigación innovadora o la solución de un problema pendiente en un campo determinado. Una famosa lista de esos 23 problemas sin resolver, denominada los «Problemas de Hilbert», fue recopilada en 1900 por el matemático alemán David Hilbert. Esta lista ha alcanzado gran popularidad entre los matemáticos y, al menos, nueve de los problemas ya han sido resueltos. Una nueva lista de siete problemas fundamentales, titulada «Problemas del milenio», se publicó en 2000. La solución de cada uno de los problemas será recompensada con 1 millón de dólares. Curiosamente, tan solo uno (la hipótesis de Riemann) aparece en ambas listas. Ramas de estudio de las matemáticas Artículo principal: Áreas de las matemáticas La Sociedad Matemática Americana distingue unas 5.000 ramas distintas de matemática. En una subdivisión escolarizada de la matemática se distinguen cinco áreas de estudio básicas: la cantidad, la estructura, el espacio, el cambio y la variabilidad que se corresponden con la aritmética, el álgebra, la geometría, el cálculo, la probabilidad y estadística. Como señalaba Richard Courant «Es posible seguir una ruta directa a partir de los elementos fundamentales hasta puntos avanzados» para que puedan divisarse las directrices de la matemática como ciencia. Además, hay ramas de las matemáticas conectadas a otros campos, por ejemplo la lógica, teoría de conjuntos y las matemáticas aplicadas entre muchas otras tal como indica la Sociedad Matemática Americana. Véase también: Categoría:Áreas de las matemáticas Matemática pura Artículo principal: Matemáticas puras Cantidad Números naturales Enteros Números racionales Números reales Números complejos Estructura Combinatoria Teoría de números Teoría de grupos Teoría de grafos Teoría del orden Álgebra Espacio Geometría Trigonometría Geometría diferencial Topología Geometría fractal Teoría de la medida Cambio Cálculo Cálculo vectorial Ecuaciones diferenciales Sistemas dinámicos Teoría del caos Análisis complejo Matemática aplicada Artículo principal: Matemáticas aplicadas El concepto «matemática aplicada» se refiere a aquellos métodos y herramientas matemáticas que pueden ser utilizados en el análisis o resolución de problemas pertenecientes al área de las ciencias básicas o aplicadas. Muchos métodos matemáticos han resultado efectivos en el estudio de problemas en física, química, biología, medicina, ciencias sociales, ingeniería, economía, finanzas, ecología entre otras. Sin embargo, una posible diferencia es que en matemática aplicada se procura el desarrollo de la matemática «hacia afuera», es decir su aplicación o transferencia hacia el resto de las áreas. Y en menor grado «hacia dentro» o sea, hacia el desarrollo de la matemática misma. Este último sería el caso de la matemática pura o matemática elemental. La matemática aplicada se usa con frecuencia en distintas áreas tecnológicas para modelado, simulación y optimización de procesos o fenómenos, como el túnel de viento o el diseño de experimentos. Estadística y ciencias de la decisión La estadística es la rama de la matemática que estudia la variabilidad, así como el proceso aleatorio que la genera siguiendo leyes de probabilidad. Es un conocimiento fundamental para la investigación científica en algunos campos de la tecnología, como informática e ingeniería, y de las ciencias fácticas, como economía, genética, sociología, psicología, medicina, contabilidad, etc. En ocasiones, estas áreas de conocimiento necesitan aplicar técnicas estadísticas durante su proceso de investigación factual, con el fin de obtener nuevos conocimientos basados en la experimentación y en la observación, precisando para ello recolectar, organizar, presentar y analizar un conjunto de datos numéricos y, a partir de ellos y de un marco teórico, hacer las inferencias apropiadas. Se consagra en forma directa al gran problema universal de cómo tomar decisiones inteligentes y acertadas en condiciones de incertidumbre. La estadística descriptiva sirve como fuente de instrucción en los niveles básicos de estadística aplicada a las ciencias fácticas y, por tanto, los conceptos manejados y las técnicas empleadas suelen ser presentadas de la forma más simple y clara posibles. Matemática computacional."
ksampletext_wikipedia_math_calculo: str = "Cálculo. En general el término cálculo (del latín calculus, piedrecita, usado para contar o como ayuda al calcular) hace referencia al resultado correspondiente a la acción de calcular. Calcular, por su parte, consiste en realizar las operaciones necesarias para prever el resultado de una acción previamente concebida, o conocer las consecuencias que se pueden derivar de unos datos previamente conocidos. No obstante, el uso más común del término «cálculo» es el lógico-matemático. Desde esta perspectiva, el cálculo consiste en un procedimiento mecánico o algoritmo, mediante el cual podemos conocer las consecuencias que se derivan de las variables previamente conocidas debidamente formalizadas y simbolizadas. Cálculo como razonamiento y cálculo lógico-matemático Ejemplo de aplicación de un cálculo algebraico a la resolución de un problema, según la interpretación de una teoría física. La expresión del cálculo algebraico Pero si interpretamos Al mismo tiempo, según dicha teoría, sirve para resolver el problema de calcular cuántos kilómetros ha recorrido un coche que circula de Madrid a Barcelona a una velocidad constante de 60 km/h durante 4 horas de recorrido. 240 kilómetros recorridos = 60 km/h x 4 h Las dos acepciones del cálculo (la general y la restringida) arriba definidas están íntimamente ligadas. El cálculo es una actividad natural y primordial en el hombre, que comienza en el mismo momento en que empieza a relacionar unas cosas con otras en un pensamiento o discurso. El cálculo lógico natural como razonamiento es el primer cálculo elemental del ser humano. El cálculo en sentido lógico-matemático aparece cuando se toma conciencia de esta capacidad de razonar y trata de formalizarse. Por lo tanto, podemos distinguir dos tipos de operaciones: Operaciones orientadas hacia la consecución de un fin, como prever, programar, conjeturar, estimar, precaver, prevenir, proyectar, configurar, etc. que incluyen en cada caso una serie de complejas actividades y habilidades tanto de pensamiento como de conducta. En su conjunto dichas actividades adquieren la forma de argumento o razones que justifican una finalidad práctica o cognoscitiva. Operaciones formales como algoritmo que se aplica bien directamente a los datos conocidos o a los esquemas simbólicos de la interpretación lógico-matemática de dichos datos; las posibles conclusiones, inferencias o deducciones de dicho algoritmo son el resultado de la aplicación de reglas estrictamente establecidas de antemano. Resultado que es: Conclusión de un proceso de razonamiento. Resultado aplicable directamente a los datos iniciales (resolución de problemas). Modelo de relaciones previamente establecido como teoría científica y significativo respecto a determinadas realidades (Creación de modelos científicos). Mero juego formal simbólico de fundamentación, creación y aplicación de las reglas que constituyen el sistema formal del algoritmo (Cálculo lógico-matemático, propiamente dicho). Dada la importancia que históricamente ha adquirido la actividad lógico-matemática en la cultura humana el presente artículo se refiere a este último sentido. De hecho la palabra, en su uso habitual, casi queda restringida a este ámbito de aplicación; para algunos, incluso, queda reducida a un solo tipo de cálculo matemático, pues en algunas universidades se llamaba «Cálculo» a una asignatura específica de cálculo matemático (como puede ser el cálculo infinitesimal, análisis matemático, cálculo diferencial e integral, etc.). En un artículo general sobre el tema no puede desarrollarse el contenido de lo que supone el cálculo lógico-matemático en la actualidad. Aquí se expone solamente el fundamento de sus elementos más simples, teniendo en cuenta que sobre estas estructuras simples se construyen los cálculos más complejos tanto en el aspecto lógico como en el matemático. Historia del cálculo Artículo principal: Historia del cálculo De la Antigüedad Reconstrucción de un ábaco romano. Un ábaco moderno. El término «cálculo» procede del latín calculus, piedrecita que se mete en el calzado y que produce molestia. Precisamente, tales piedrecitas ensartadas en tiras constituían el ábaco romano que, junto con el suanpan chino, constituyen las primeras máquinas de calcular en el sentido de contar. Los antecedentes de procedimiento de cálculo, como algoritmo, se encuentran en los que utilizaron los geómetras griegos, Eudoxo en particular, en el sentido de llegar por aproximación de restos cada vez más pequeños, a una medida de figuras curvas; así como Diofanto precursor del álgebra. Se considera que Arquímedes fue uno de los matemáticos más grandes de la antigüedad y, en general, de toda la historia. Usó el método exhaustivo para calcular el área bajo el arco de una parábola con el sumatorio de una serie infinita, y dio una aproximación extremadamente precisa del número Pi. También definió la espiral que lleva su nombre, fórmulas para los volúmenes de las superficies de revolución y un ingenioso sistema para expresar números muy largos. La consideración del cálculo como una forma de razonamiento abstracto aplicado en todos los ámbitos del conocimiento se debe a Aristóteles, quien en sus escritos lógicos fue el primero en formalizar y simbolizar los tipos de razonamientos categóricos (silogismos). Este trabajo sería completado más tarde por los estoicos, los megáricos, la Escolástica. Los algoritmos actuales del cálculo aritmético, utilizados universalmente, son fruto de un largo proceso histórico. De vital importancia son las aportaciones de Muhammad ibn al-Juarismi en el siglo IX; En el siglo XIII, Fibonacci introduce en Europa la representación de los números arábigos del sistema decimal. Se introdujo el 0, ya de antiguo conocido en la India y se construye definitivamente el sistema decimal de diez cifras con valor posicional. La escritura antigua de números en Babilonia, en Egipto, en Grecia o en Roma, hacía muy difícil un procedimiento mecánico de cálculo. El sistema decimal fue muy importante para el desarrollo de la contabilidad de los comerciantes de la Baja Edad Media, en los inicios del capitalismo. El concepto de función por tablas ya era practicado de antiguo pero adquirió especial importancia en la Universidad de Oxford en el siglo XIV. La idea de un lenguaje o algoritmo capaz de determinar todas las verdades, incluidas las de la fe, aparecen en el intento de Raimundo Lulio en su Ars Magna A fin de lograr una operatividad mecánica se confeccionaban unas tablas a partir de las cuales se podía generar un algoritmo prácticamente mecánico. Este sistema de tablas ha perdurado en algunas operaciones durante siglos, como las tablas de logaritmos, o las funciones trigonométricas; las tablas venían a ser como la calculadora de hoy día; un instrumento imprescindible de cálculo. Las amortizaciones de los créditos en los bancos, por ejemplo, se calculaban a partir de tablas elementales hasta que se produjo la aplicación de la informática en el tercer tercio del siglo XX. A finales de la Edad Media la discusión entre los partidarios del ábaco y los partidarios del algoritmo se decantó claramente por estos últimos. De especial importancia es la creación del sistema contable por partida doble recomendado por Luca Pacioli fundamental para el progreso del capitalismo en el Renacimiento. Renacimiento El sistema que usamos actualmente fue introducido por Luca Pacioli en 1494, el cual fue creado y desarrollado para responder a la necesidad de la contabilidad en los negocios de la burguesía renacentista. El desarrollo del álgebra (con la introducción de un sistema de símbolos por un lado, y la resolución de problemas por medio de las ecuaciones) vino de la mano de los grandes matemáticos de la época renacentista como Tartaglia, Stevin, Cardano o Vieta y fue esencial para el planteamiento y solución de los más diversos problemas que surgieron en la época, que dieron como consecuencia los grandes descubrimientos que hicieron posible el progreso científico que surgiría en el siglo XVII. Siglos XVII y XVIII Página del artículo de Leibniz Explication de lArithmétique Binaire, 1703/1705 En el siglo XVII el cálculo conoció un enorme desarrollo siendo los autores más destacados Descartes, Pascal y, finalmente, Leibniz y Newton con el cálculo infinitesimal que en muchas ocasiones ha recibido simplemente, por absorción, el nombre de cálculo. El concepto de cálculo formal en el sentido de algoritmo reglado para el desarrollo de un razonamiento y su aplicación al mundo de lo real, adquiere una importancia y desarrollo enorme respondiendo a una necesidad de establecer relaciones matemáticas entre diversas medidas, esencial para el progreso de la ciencia física que, debido a esto, es tomada como nuevo modelo de Ciencia frente a la especulación tradicional filosófica, por el rigor y seguridad que ofrece el cálculo matemático. Cambia así el sentido tradicional de la Física como filosofía de la naturaleza y toma el sentido de ciencia que estudia los cuerpos materiales, en cuanto materiales. A partir de entonces el propio sistema de cálculo permite establecer modelos sobre la realidad física, cuya comprobación experimental supone la confirmación de la teoría como sistema. Es el momento de la consolidación del llamado método científico cuyo mejor exponente es en aquel momento la Teoría de la Gravitación Universal y las leyes de la Mecánica de Newton. Siglos XIX y XX George Boole Durante el siglo XIX y XX el desarrollo científico y la creación de modelos teóricos fundados en sistemas de cálculo aplicables tanto en mecánica como en electromagnetismo y radioactividad, etc., así como en astronomía fue impresionante. Las geometrías no euclidianas encuentran aplicación en modelos teóricos de astronomía y física. El mundo deja de ser un conjunto de infinitas partículas que se mueven en un espacio-tiempo absoluto y se convierte en un espacio de configuración o espacio de fases de La lógica asimismo sufrió una transformación radical. La formalización simbólica fue capaz de integrar las leyes lógicas en un cálculo matemático, hasta el punto que la distinción entre razonamiento lógico-formal y cálculo matemático viene a considerarse como meramente utilitaria. En la segunda mitad del siglo XIX y primer tercio del XX, a partir del intento de formalización de todo el sistema matemático, Frege, y de matematización de la lógica, (Bolzano, Boole, Whitehead, Russell) fue posible la generalización del concepto como cálculo lógico. Se lograron métodos muy potentes de cálculo, sobre todo a partir de la posibilidad de tratar como «objeto» conjuntos de infinitos elementos, dando lugar a los números transfinitos de Cantor. Mediante el cálculo la lógica encuentra nuevos desarrollos como lógicas modales y lógicas polivalentes. Los intentos de axiomatizar el cálculo como cálculo perfecto por parte de Hilbert y Poincaré, llevaron, como consecuencia de diversas paradojas (Cantor, Russell, etc.) a nuevos intentos de axiomatización, Axiomas de Zermelo-Fraenkel y a la demostración de Gödel de la imposibilidad de un sistema de cálculo perfecto: consistente, decidible y completo en 1931, de grandes implicaciones lógicas, matemáticas y científicas. Actualidad En la actualidad, el cálculo en su sentido más general, en tanto que cálculo lógico interpretado matemáticamente como sistema binario, y físicamente hecho material mediante la lógica de circuitos electrónicos, ha adquirido una dimensión y desarrollo impresionante por la potencia de cálculo conseguida por los ordenadores, propiamente máquinas computadoras. La capacidad y velocidad de cálculo de estas máquinas hace lo que humanamente sería imposible: millones de operaciones por segundo. El cálculo así utilizado se convierte en un instrumento fundamental de la investigación científica por las posibilidades que ofrece para la modelización de las teorías científicas, adquiriendo especial relevancia en ello el cálculo numérico. Cálculo infinitesimal: breve reseña Artículo principal: Cálculo infinitesimal El cálculo infinitesimal, llamado por brevedad «cálculo», tiene su origen en la antigua geometría griega. Demócrito calculó el volumen de pirámides y conos considerándolos formados por un número infinito de secciones de grosor infinitesimal (infinitamente pequeño). Eudoxo y Arquímedes utilizaron el «método de agotamiento» o exhaución para encontrar el área de un círculo con la exactitud finita requerida mediante el uso de polígonos regulares inscritos de cada vez mayor número de lados. En el periodo tardío de Grecia, el neoplatónico Pappus de Alejandría hizo contribuciones sobresalientes en este ámbito. Sin embargo, las dificultades para trabajar con números irracionales y las paradojas de Zenón de Elea impidieron formular una teoría sistemática del cálculo en el periodo antiguo. En el siglo XVII, Cavalieri y Torricelli ampliaron el uso de los infinitesimales, Descartes y Fermat utilizaron el álgebra para encontrar el área y las tangentes (integración y derivación en términos modernos). Fermat e Isaac Barrow tenían la certeza de que ambos cálculos estaban relacionados, aunque fueron Newton (hacia 1660), en Inglaterra y Leibniz en Alemania (hacia 1670) quienes demostraron que los problemas del área y la tangente son inversos, lo que se conoce como teorema fundamental del cálculo. Leibniz es el creador del simbolismo de la derivada, diferencial y la ∫ estilizada para la integración, en vez de la I de Bernoulli. Usó el nombre de cálculo diferencial y el nombre de cálculo integral propuso Juan Bernoulli, que sustituyó al nombre de cálculo sumatorio de Leibniz. La simbología de Leibniz impulsó el avance del cálculo en Europa continental. El descubrimiento de Newton, a partir de su teoría de la gravitación universal, fue anterior al de Leibniz, pero el retraso en su publicación aún provoca controversias sobre quién de los dos fue el primero. Newton utilizó el cálculo en mecánica en el marco de su tratado «Principios matemáticos de filosofía natural», obra científica por excelencia, llamando a su método de «fluxiones». Leibniz utilizó el cálculo en el problema de la tangente a una curva en un punto, como límite de aproximaciones sucesivas, dando un carácter más filosófico a su discurso. Sin embargo, terminó por adoptarse la notación de Leibniz por su versatilidad. En el siglo XVIII aumentó considerablemente el número de aplicaciones del cálculo, pero el uso impreciso de las cantidades infinitas e infinitesimales, así como la intuición geométrica, causaban todavía confusión y duda sobre sus fundamentos. De hecho, la noción de límite, central en el estudio del cálculo, era aún vaga e imprecisa en ese entonces. Uno de sus críticos más notables fue el filósofo George Berkeley. En el siglo XIX el trabajo de los analistas matemáticos sustituyeron esas vaguedades por fundamentos sólidos basados en cantidades finitas: Bolzano y Cauchy definieron con precisión los conceptos de límite en términos de épsilon-delta y de derivada, Cauchy y Riemann hicieron lo propio con las integrales, y Dedekind y Weierstrass con los números reales. Fue el periodo de la fundamentación del cálculo. Por ejemplo, se supo que las funciones diferenciables son continuas y que las funciones continuas son integrables, aunque los recíprocos son falsos. En el siglo XX, el análisis no convencional, legitimó el uso de los infinitesimales, al mismo tiempo que la aparición de las computadoras ha incrementado las aplicaciones y velocidad del cálculo. Actualmente, el cálculo infinitesimal tiene un doble aspecto: por un lado, se ha consolidado su carácter disciplinario en la formación de la sociedad culta del conocimiento, destacando en este ámbito textos propios de la disciplina como el de Louis Leithold, el de Earl W. Swokowski, el de Denis G. Zill o el de James Stewart, entre muchos otros; por otro su desarrollo como disciplina científica que ha desembocado en ámbitos tan especializados como el cálculo fraccional, la teoría de funciones analíticas de variable compleja o el análisis matemático. El éxito del cálculo ha sido extendido con el tiempo a las ecuaciones diferenciales, al cálculo de vectores, al cálculo de variaciones, al análisis complejo y a las topología algebraica y topología diferencial entre muchas otras ramas. El desarrollo y uso del cálculo ha tenido efectos muy importantes en casi todas las áreas de la vida moderna: es fundamento para el cálculo numérico aplicado en casi todos los campos técnicos y/o científicos cuya principal característica es la continuidad de sus elementos, en especial en la física. Prácticamente todos los desarrollos técnicos modernos como la construcción, aviación, transporte, meteorología, etc., hacen uso del cálculo. Muchas fórmulas algebraicas se usan hoy en día en balística, calefacción, refrigeración, etc. Como complemento del cálculo, en relación con sistemas teóricos o físicos cuyos elementos carecen de continuidad, se ha desarrollado una rama especial conocida como Matemática discreta. Recientemente, se ha desarrollado el Cálculo Fraccional de Conjuntos (en inglés, Fractional Calculus of Sets o FCS) como una metodología derivada del Cálculo Fraccional. Esta metodología, mencionada por primera vez en el artículo Sets of Fractional Operators and Numerical Estimation of the Order of Convergence of a Family of Fractional Fixed-Point Methods, tiene como objetivo caracterizar y organizar los elementos del cálculo fraccional mediante el uso de conjuntos, aprovechando la variedad de operadores fraccionales disponibles en la literatura. Actualmente, el cálculo fraccional carece de una definición unificada de lo que constituye una derivada fraccional. En consecuencia, cuando no es necesario especificar explícitamente la forma de una derivada fraccional, típicamente se denota de la siguiente manera: Los operadores fraccionales tienen varias representaciones, pero una de sus propiedades fundamentales es que recuperan los resultados del cálculo tradicional a medida que Denotando y lim Cálculo lógico Artículo principal: Cálculo lógico El cálculo lógico es un sistema de reglas de inferencia o deducción de un enunciado a partir de otro u otros. El cálculo lógico requiere un conjunto consistente de axiomas y unas reglas de inferencia; su propósito es poder deducir algorítmicamente proposiciones lógicas verdaderas a partir de dichos axiomas. La inferencia es una operación lógica que consiste en obtener una proposición lógica como conclusión a partir de otra(s) (premisas) mediante la aplicación de reglas de inferencia. Informalmente interpretamos que alguien infiere ,o deduce, T de R si acepta que si R tiene valor de verdad V, entonces, necesariamente, T tiene valor de verdad V. Sin embargo, en el enfoque moderno del cálculo lógico no es necesario acudir al concepto de verdad, para construir el cálculo lógico. Los hombres en nuestra tarea diaria, utilizamos constantemente el razonamiento deductivo. Partimos de enunciados empíricos ,supuestamente verdaderos y válidos, para concluir en otro enunciado que se deriva de aquellos, según las leyes de la lógica natural. La lógica, como ciencia formal, se ocupa de analizar y sistematizar dichas leyes, fundamentarlas y convertirlas en las reglas que permiten la transformación de unos enunciados ,premisas- en otros -conclusiones, con objeto de convertir las operaciones en un algoritmo riguroso y eficaz, que garantiza que dada la verdad de las premisas, la conclusión es necesariamente verdadera. Al aplicar las reglas de un cálculo lógico a los enunciados de un argumento mediante la simbolización adecuada como fórmulas o expresiones bien formadas (EBF) del cálculo, construimos un modelo o sistema deductivo. En ese contexto, las reglas de formación de fórmulas definen la sintaxis de un lenguaje formal de símbolos no interpretados, es decir, sin significado alguno; y las reglas de transformación del sistema permiten transformar dichas expresiones en otras equivalentes; entendiendo por equivalentes que ambas tienen siempre y de forma necesaria el mismo valor de verdad. Dichas transformaciones son meramente tautologías. Un lenguaje formal que sirve de base para el cálculo lógico está formado por varias clases de entidades: Un conjunto de elementos primitivos. Dichos elementos pueden establecerse por enumeración, o definidos por una propiedad tal que permita discernir sin duda alguna cuándo un elemento pertenece o no pertenece al sistema. Un conjunto de reglas de formación de «expresiones bien formadas» (EBF) que permitan en todo momento establecer, sin forma de duda, cuándo una expresión pertenece al sistema y cuándo no. Un conjunto de reglas de transformación de expresiones, mediante las cuales partiendo de una expresión bien formada del cálculo podremos obtener una nueva expresión equivalente y bien formada que pertenece al cálculo. Cuando en un cálculo así definido se establecen algunas expresiones determinadas como verdades primitivas o axiomas, decimos que es un sistema formal axiomático. Un cálculo así definido si cumple al mismo tiempo estas tres condiciones decimos que es un Cálculo Perfecto: Es consistente: No es posible que dada una expresión bien formada del sistema, ƒ, y su negación, no – ƒ, sean ambas teoremas del sistema. No puede haber contradicción entre las expresiones del sistema. Decidible: Dada cualquier expresión bien formada del sistema podemos encontrar un método que nos permita decidir mediante una serie finita de operaciones si dicha expresión es o no es un teorema del sistema. Completo: Cuando dada cualquier expresión bien formada del sistema, podemos establecer la demostración matemática o prueba de que es un teorema del sistema. La misma lógica-matemática ha demostrado que tal sistema de cálculo perfecto «no es posible» (véase el Teorema de Gödel). Sistematización de un cálculo de deducción natural Reglas de formación de fórmulas I. Una letra enunciativa (con o sin subíndice) es una EBF. II. Si A es una EBF, ¬ A también lo es. III. Si A es una EBF y B también, entonces A ∧ B; A ∨ B; A → B; A ↔ B, también lo son. IV. Ninguna expresión es una fórmula del Cálculo sino en virtud de I, II, III. Notas: A, B, … con mayúsculas están utilizadas como metalenguaje en el que cada variable expresa cualquier proposición, atómica (p,q,r,s, …) o molecular (p ∧ q), (p ∨ q), …309>100 A, B, … son símbolos que significan variables; ¬, ∧, ∨, →, ↔, son símbolos constantes. Existen diversas formas de simbolización. Utilizamos aquí la de uso más frecuente en España. Reglas de transformación de fórmulas 1) Regla de sustitución (R.T.1): Dada una tesis EBF del cálculo, en la que aparecen variables de enunciados, el resultado de sustituir una, algunas o todas esas variables por expresiones bien formadas (EBF) del cálculo, será también una tesis EBF del cálculo. Y ello con una única restricción, si bien muy importante: cada variable ha de ser sustituida siempre que aparece y siempre por el mismo sustituto. Veamos el ejemplo: 1 [(p ∧ q) ∨ r] → t ∨ s Transformación 2 A ∨ r → B Donde A = (p ∧ q); y donde B = (t ∨ s) 3 C → B Donde C = A ∨ r O viceversa 1 C → B Transformación 2 A ∨ r → B Donde A ∨ r = C 3 [(p ∧ q) ∨ r] → t ∨ s Donde (p ∧ q) = A; y donde (t ∨ s) = B 2) Regla de separación (R.T.2): Si X es una tesis EBF del sistema y lo es también X → Y, entonces Y es una tesis EBF del sistema. Esquemas de inferencia Sobre la base de estas dos reglas, siempre podremos reducir un argumento cualquiera a la forma: [A ∧ B ∧ C … ∧ N] → Y lo que constituye un esquema de inferencia en el que una vez conocida la verdad de cada una de las premisas A, B, … N y, por tanto, de su producto, podemos obtener la conclusión Y con valor de verdad V, siempre y cuando dicho esquema de inferencia sea una ley lógica, es decir su tabla de verdad nos muestre que es una tautología. Por la regla de separación podremos concluir Y, de forma independiente como verdad. Dada la poca operatividad de las tablas de verdad, el cálculo se construye como una cadena deductiva aplicando a las premisas o a los teoremas deducidos las leyes lógicas utilizadas como reglas de transformación, como se expone en cálculo lógico. El lenguaje natural como modelo de un cálculo lógico Naturalmente el cálculo lógico es útil porque puede tener aplicaciones, pero ¿en qué consisten o cómo se hacen tales aplicaciones? Podemos considerar que el lenguaje natural es un modelo de C si podemos someterlo, es decir, aplicarle una correspondencia en C. Para ello es necesario someter al lenguaje natural a un proceso de formalización de tal forma que podamos reducir las expresiones lingüísticas del lenguaje natural a EBF de un cálculo mediante reglas estrictas manteniendo el sentido de verdad lógica de dichas expresiones del lenguaje natural. Esto es lo que se expone en cálculo lógico. Las diversas formas en que tratemos las expresiones lingüísticas formalizadas como proposiciones lógicas dan lugar a sistemas diversos de formalización y cálculo: Cálculo proposicional o cálculo de enunciados Cuando se toma la oración simple significativa del lenguaje natural con posible valor de verdad o falsedad como una proposición atómica, como un todo sin analizar. Cálculo como lógica de clases Cuando se toma la oración simple significativa del lenguaje natural con posible valor de verdad o falsedad como resultado del análisis de la oración como una relación de individuos o posibles individuos que poseen o no poseen una propiedad común determinada como pertenecientes o no pertenecientes a una clase natural o a un conjunto como individuos. Cálculo de predicados o cuantificacional Cuando se toma la oración simple significativa del lenguaje natural con posible valor de verdad o falsedad como resultado del análisis de la misma de forma que una posible función predicativa (P), se predica de unos posibles sujetos variables (x) [tomados en toda su posible extensión: (Todos los x); o referente a algunos indeterminados: (algunos x)], o de una constante individual existente (a). Cálculo como lógica de relaciones Cuando se toma la oración simple significativa con posible valor de verdad propio, verdadero o falso, como resultado del análisis de la oración como una relación R que se establece entre un sujeto y un predicado. La simbolización y formación de EBFs en cada uno de esos cálculos, así como las reglas de cálculo se trata en cálculo lógico."
ksampletext_wikipedia_math_algebra: str = "Álgebra. El álgebra (del árabe) es la rama de la matemática que estudia la combinación de elementos de estructuras abstractas acorde a ciertas reglas. Originalmente esos elementos podían ser interpretados como números o cantidades, por lo que el álgebra en cierto modo fue originalmente una generalización y extensión de la aritmética. En el álgebra moderna existen áreas del álgebra que en modo alguno pueden considerarse extensiones de la aritmética (álgebra abstracta, álgebra homológica, álgebra exterior, etc.). El álgebra elemental difiere de la aritmética en el uso de abstracciones, como el empleo de letras para representar números que son desconocidos o que pueden tomar muchos valores. Por ejemplo, en La palabra álgebra también se utiliza en ciertas formas especializadas. Un tipo especial de objeto matemático en el álgebra abstracta se llama álgebra, y la palabra se usa, por ejemplo, en las frases álgebra lineal y topología algebraica. Etimología La palabra álgebra proviene del y cálculo de datos del título del libro de principios del siglo, La ciencia del restablecimiento y el equilibrio por el matemático y astrónomo persa Muḥammad ibn Mūsā al-Khwārizmī. En su obra, el término al-jabr se refería a la operación de mover un término de un lado de una ecuación al otro, المقابلة al-muqābala equilibrar se refería a añadir términos iguales a ambos lados. Acortada a simplemente algeber o álgebra en latín, la palabra acabó entrando en la lengua inglesa durante el siglo XV, ya sea desde el español, el italiano o el latín medieval. Originalmente se refería al procedimiento quirúrgico de fijar huesos rotos o dislocados. El significado matemático se registró por primera vez (en inglés) en el siglo XVI. Introducción A diferencia de la aritmética elemental, que trata de los números y las operaciones fundamentales, en álgebra -para lograr la generalización- se introducen además símbolos (usualmente letras) para representar parámetros (variables o coeficientes), o cantidades desconocidas (incógnitas); las expresiones así formadas son llamadas «fórmulas algebraicas» y expresan una regla o un principio general. El álgebra conforma una de las grandes áreas de las matemáticas, junto a la teoría de números, la geometría y el análisis. Página del libro Kitāb al-mukhtaṣar fī ḥisāb al-ŷabr wa-l-muqābala, de Al-Juarismi La palabra «álgebra» proviene del vocablo árabe الجبر al-ŷabar (en árabe dialectal por asimilación progresiva se pronunciaba [alŷɛbɾ], de donde derivan los términos de las lenguas europeas), que se traduce como restauración o reposición, reintegración. Deriva del tratado escrito alrededor del año 820 e. c. por el matemático y astrónomo persa Muhammad ibn Musa al-Jwarizmi (conocido como Al Juarismi), titulado Al-kitāb al-mukhtaṣar fī ḥisāb al-ŷarabi waˀl-muqābala (Compendio de cálculo por reintegración y comparación), el cual proporcionaba operaciones simbólicas para la solución sistemática de ecuaciones lineales y cuadráticas. Muchos de sus métodos derivan del desarrollo de la matemática en el islam medieval, destacando la independencia del álgebra como una disciplina matemática independiente de la geometría y de la aritmética. Puede considerarse al álgebra como el arte de hacer cálculos del mismo modo que en aritmética, pero con objetos matemáticos no-numéricos. El adjetivo «algebraico» denota usualmente una relación con el álgebra, como por ejemplo en estructura algebraica. Por razones históricas, también puede indicar una relación con las soluciones de ecuaciones polinomiales, números algebraicos, extensión algebraica o expresión algebraica. Conviene distinguir entre: Álgebra elemental es la parte del álgebra que se enseña generalmente en los cursos de matemáticas. Álgebra abstracta es el nombre dado al estudio de las «estructuras algebraicas» propiamente. El álgebra usualmente se basa en estudiar las combinaciones de cadenas finitas de signos y, mientras que análisis matemático requiere estudiar límites y sucesiones de una cantidad infinita de elementos. Historia del álgebra Véase también: Historia de la matemática El álgebra en la antigüedad Las raíces del álgebra pueden rastrearse hasta la antigua matemática babilónica, que había desarrollado un avanzado sistema aritmético con el que fueron capaces de hacer cálculos en una forma algorítmica. Con el uso de este sistema lograron encontrar fórmulas y soluciones para resolver problemas que, en la actualidad, suelen resolverse mediante ecuaciones lineales, ecuaciones de segundo grado y ecuaciones indeterminadas. En contraste, la mayoría de los egipcios de esta época, y la mayoría de los matemáticos griegos y chinos del primer milenio antes de Cristo, normalmente resolvían tales ecuaciones por métodos geométricos, tales como los descritos en el Papiro de Rhind, Los Elementos de Euclides y Los nueve capítulos sobre el arte matemático. Papiro de Ahmes; datado entre 2000 al 1800 a. e. c. Papiro de Ahmes; datado entre 2000 al 1800 a. e. c. Elementos de Euclides, ca. Elementos de Euclides, ca. 300 a. e. c. Las nueve lecciones del arte matemático; compilado durante siglos II y III a. e. c. Las nueve lecciones del arte matemático; compilado durante siglos II y III a. e. c. Arithmetica; escrito por Diofanto alrededor de 280 de nuestra era Véase también: Matemática helénica Los matemáticos de la Antigua Grecia introdujeron una importante transformación al crear un álgebra de tipo geométrico, en donde los «términos» eran representados mediante los «lados de objetos geométricos», usualmente líneas a las cuales asociaban letras. Los matemáticos helénicos Herón de Alejandría y Diofanto así como también los matemáticos indios como Brahmagupta, siguieron las tradiciones de Egipto y Babilonia, si bien la Arithmetica de Diofanto y el Brahmasphutasiddhanta de Brahmagupta se hallan a un nivel de desarrollo mucho más alto. Por ejemplo, la primera solución aritmética completa (incluyendo al cero y soluciones negativas) para las ecuaciones cuadráticas fue descrita por Brahmagupta en su libro Brahmasphutasiddhanta. Más tarde, los matemáticos árabes y musulmanes desarrollarían métodos algebraicos a un grado mucho mayor de sofisticación. Diofanto (siglo III), algunas veces llamado «el pádre del álgebra», fue un matemático alejandrino, autor de una serie de libros intitulados Arithmetica. Estos textos tratan de las soluciones a las ecuaciones algebraicas. Influencia árabe Véase también: Matemática en el islam medieval Los babilonios y Diofanto utilizaron sobre todo métodos especiales ad hoc para resolver ecuaciones, la contribución de Al-Khwarizmi fue fundamental; resuelve ecuaciones lineales y cuadráticas sin el simbolismo algebraico, números negativos o el cero, por lo que debe distinguir varios tipos de >jab. El matemático persa Omar Khayyam desarrolló la geometría algebraica y encontró la solución geométrica de la ecuación cúbica. Otro matemático persa, Sharaf Al-Din al-Tusi, encontró la solución numérica y algebraica a diversos casos de ecuaciones cúbicas; también desarrolló el concepto de función. Los matemáticos indios Mahavirá y Bhaskara II, el matemático persa Al-Karaji, y el matemático chino Zhu Shijie, resolvieron varios casos de ecuaciones de grado tres, cuatro y cinco, así como ecuaciones polinómicas de orden superior mediante métodos numéricos. Edad Moderna Durante la Edad Moderna europea tienen lugar numerosas innovaciones, y se alcanzan resultados que claramente superan los resultados obtenidos por los matemáticos árabes, persas, indios o griegos. Parte de este estímulo viene del estudio de las ecuaciones polinómicas de tercer y cuarto grado. Las soluciones para ecuaciones polinómicas de segundo grado ya era conocida por los matemáticos babilónicos cuyos resultados se difundieron por todo el mundo antiguo. El descrubrimiento del procedimiento para encontrar soluciones algebraicas de tercer y cuarto orden se dieron en la Italia del siglo XVI. También es notable que la noción de determinante fue descubierta por el matemático japonés Kowa Seki en el siglo XVII, seguido por Gottfried Leibniz diez años más tarde, con el fin de resolver sistemas de ecuaciones lineales simultáneas utilizando matrices. Entre los siglos XVI y XVII se consolidó la noción de número complejo, con lo cual la noción de álgebra empezaba a apartarse de cantidades medibles. Gabriel Cramer también hizo un trabajo sobre matrices y determinantes en el siglo XVIII. También Leonhard Euler, Joseph-Louis Lagrange, Adrien-Marie Legendre y numerosos matemáticos del siglo XVIII hicieron avances notables en álgebra. Siglo XIX El álgebra abstracta se desarrolló en el siglo XIX, inicialmente centrada en lo que hoy se conoce como teoría de Galois y en temas de la constructibilidad. Los trabajos de Gauss generalizaron numerosas estructuras algebraicas. La búsqueda de una fundamentación matemática rigurosa y una clasificación de los diferentes tipos de construcciones matemáticas llevó a crear áreas del álgebra abstracta durante el siglo XIX absolutamente independientes de nociones aritméticas o geométricas (algo que no había sucedido con el álgebra de los siglos anteriores). Áreas de matemáticas con la palabra álgebra en su nombre Algunas áreas de las matemáticas que entran en la clasificación de álgebra abstracta tienen la palabra álgebra en su nombre; el álgebra lineal es un ejemplo. Otras no: La teoría de grupos, la teoría de anillos y la teoría de campos son ejemplos. En esta sección, enumeramos algunas áreas de las matemáticas con la palabra álgebra en el nombre. Álgebra elemental, la parte del álgebra que se suele enseñar en los cursos elementales de matemáticas. Álgebra abstracta, en la que se definen e investigan las estructuras algebraicas como grupos, anillos y campos. Álgebra lineal, en la que se estudian las propiedades específicas de las ecuaciones lineales, los espacios vectoriales y las matrices. Álgebra de Boole, una rama del álgebra que abstrae el cálculo con los valor de verdades falso y verdadero. Álgebra conmutativa, el estudio de los anillos conmutativos. Álgebra computacional, la implementación de métodos algebraicos como algoritmos y programas de ordenador. Álgebra homológica, el estudio de las estructuras algebraicas fundamentales para el estudio de los espacios topológicos. Álgebra universal, en la que se estudian propiedades comunes a todas las estructuras algebraicas. Teoría de números algebraicos, en la que se estudian las propiedades de los números desde un punto de vista algebraico. Geometría algebraica, una rama de la geometría, que en su forma primitiva especifica las curvas y superficies como soluciones de ecuaciones polinómicas. Combinatoria algebraica, en la que se utilizan métodos algebraicos para estudiar cuestiones combinatorias. Álgebra relacional: conjunto de relaciones finitas que son cerradas bajo ciertos operadores. Muchas estructuras matemáticas se llaman álgebras: Álgebra sobre un cuerpo o más generalmente álgebra sobre un anillo. Muchas clases de álgebras sobre un campo o sobre un anillo tienen un nombre específico: Álgebra asociativa Álgebra no asociativa Álgebra de Lie Álgebra de Hopf C*-álgebra Álgebra simétrica Álgebra exterior Álgebra tensorial En teoría de la medida, σ-álgebra Álgebra sobre un conjunto En teoría de categorías Álgebra F y co-álgebra F Álgebra T En lógica, Álgebra de relación, un álgebra booleana residuada expandida con una involución llamada conversa. Álgebra booleana, un complementado del Retículo distributivo. Álgebra de Heyting Notación algebraica Notación matemática de raíz cuadrada de x Consiste en que los números se emplean para representar cantidades conocidas y determinadas. Las letras se emplean para representar toda clase de cantidades, ya sean conocidas o desconocidas. Las cantidades conocidas se expresan por las primeras letras del alfabeto: a, b, c, d, … Las cantidades desconocidas se representan por las últimas letras del alfabeto: u, v, w, x, y, z. Los signos empleados en álgebra son tres clases: Signos de operación, signos de relación y signos de agrupación. Signos de operación En álgebra se verifican con las cantidades las mismas operaciones que en aritmética: suma, resta, multiplicación, elevación a potencias y extracción de raíces, que se indican con los principales signos de aritmética excepto el signo de multiplicación. En lugar del signo × suele emplearse un punto entre los factores y también se indica a la multiplicación colocando los factores entre paréntesis. Así a⋅b y (a)(b) equivale a a × b. Signos de relación Se emplean estos signos para indicar la relación que existe entre dos cantidades. Los principales son: =, que se lee igual a. Así, a=b se lee “a igual a b”. >, que se lee mayor que. Así, x + y > m se lee “x + y mayor que m”. <, que se lee menor que. Así, a < b + c se lee “a menor que b + c”. Signos de agrupación Signos y símbolos más comunes Los signos y símbolos son utilizados en el álgebra ,y en general en teoría de conjuntos y álgebra de conjuntos, con los que se constituyen ecuaciones, matrices, series, etc. Sus letras son llamadas variables, ya que se usa esa misma letra en otros problemas y su valor va variando. Aquí algunos ejemplos: Signos y símbolos Expresión Uso + Además de expresar adición, también es usada para expresar operaciones binarias c o k Expresan términos constantes Primeras letras del abecedario a, b, c, … Se utilizan para expresar cantidades conocidas Últimas letras del abecedario …, x, y, z Se utilizan para expresar incógnitas n Expresa cualquier número (1, 2, 3, 4, …, n) Exponentes y subíndices Expresar cantidades de la misma especie, de diferente magnitud. Simbología de Conjuntos Símbolo Descripción ∈ Es un elemento del conjunto o pertenece al conjunto. ∉ No es un elemento del conjunto o no pertenece al conjunto. ⎜ Tal que n (C) Cardinalidad del conjunto C U Conjunto Universo Φ Conjunto vacío ⊆ Subconjunto de ⊂ Subconjunto propio de ⊄ No es subconjunto propio de > Mayor que < Menor que ≥ Mayor o igual que ≤ Menor o igual que ∩ Intersección de conjuntos ∪ Unión de Conjuntos A Complemento del conjunto A = Símbolo de igualdad ≠ No es igual a … El conjunto continúa ⇔ Si y solo si ¬ (en algunos ocasiones ∼) No, negación lógica (es falso que) ∧ Y ∨ O Lenguaje algebraico Lenguaje algebraico Lenguaje común Lenguaje algebraico Un número cualquiera m Un número cualquiera aumentado en siete m + 7 La diferencia de dos números cualesquiera f - q El doble de un número excedido en cinco 2x + 5 La división de un número entero entre su antecesor x/(x-1) La mitad de un número d/2 El cuadrado de un número y^2 La media de la suma de dos números (b+c)/2 Las dos terceras partes de un número disminuidos en cinco es igual a 12. 2/3 (x-5) = 12 Tres números naturales consecutivos. x, x + 1, x + 2. La parte mayor de 1200, si la menor es w 1200 - w El cuadrado de un número aumentado en siete b2 + 7 Las tres quintas partes de un número más la mitad de su consecutivo equivalen a tres. 3/5 p + 1/2 (p+1) = 3 El producto de un número con su antecesor equivalen a 30. x(x-1) = 30 El cubo de un número más el triple del cuadrado de dicho número x3 + 3x2 Estructura algebraica Artículo principal: Estructura algebraica En matemáticas, una estructura algebraica es un conjunto de elementos con unas propiedades operacionales determinadas; es decir, lo que define a la estructura del conjunto son las operaciones que se pueden realizar con los elementos de dicho conjunto y las propiedades matemáticas que dichas operaciones poseen. Un objeto matemático constituido por un conjunto no vacío y algunas leyes de composición interna definida en él es una estructura algebraica. Las estructuras algebraicas más importantes son."
ksampletext_wikipedia_math_numero: str = "Número. Un número es un objeto matemático utilizado para contar (cantidades), medir (magnitudes) y etiquetar. Los números más sencillos, que utilizamos todos en la vida cotidiana, son los números naturales: 1, 2, 3, etc. Se denotan mediante Los números desempeñan un papel esencial en las ciencias empíricas, ya que permiten cuantificar y describir fenómenos observables. No solo los números naturales son utilizados, sino también diversos conjuntos numéricos desarrollados a lo largo de la historia de las matemáticas. El conjunto de los números enteros (representado por Desde la antigüedad, se conoce la existencia de números que no pueden expresarse como fracciones de enteros. Por ejemplo, la longitud de la diagonal de un cuadrado de lado unidad es Con el tiempo, se introdujeron otros tipos de números para ampliar el alcance del análisis matemático, como los imaginarios, complejos y trascendentes, que permiten describir y resolver fenómenos de creciente complejidad en diversas ramas de la ciencia y la ingeniería. Nótese que la teoría de números es una rama de las matemáticas que se ocupa de los enteros (no de números en general). El origen de los números es que los primeros números que el hombre inventó fueron los números naturales, los cuales se utilizaban y se utilizan para contar elementos de un conjunto finito, ya que se procede a enumerar dichos elementos de una manera ordenada seleccionándolos uno tras otro a la vez que se le atribuye a cada uno un número. Tipos de números Clasificación de los números. Los números más conocidos son los números naturales. Denotados mediante Otro tipo de números ampliamente usados son números fraccionarios, los cuales representan tanto cantidades inferiores a una unidad, como números mixtos (un conjunto de unidades más una parte inferior a la unidad). Los números fraccionarios pueden ser expresados siempre como cocientes de enteros. El conjunto de todos los números fraccionarios es el conjunto de los números racionales (que usualmente se define para que incluya tanto a los racionales positivos, como a los racionales negativos y el cero). Este conjunto de números se designa como Los números racionales permiten resolver gran cantidad de problemas prácticos, pero desde los antiguos griegos se conoce que ciertas relaciones geométricas (la diagonal de un cuadrado de lado unidad) son números no enteros que tampoco son racionales. Igualmente, la solución numérica de una ecuación polinómica cuyos coeficientes son números racionales, usualmente es un número no racional. Puede demostrarse que cualquier número irracional puede representarse como una sucesión de Cauchy de números racionales que se aproximan a un límite numérico. El conjunto de todos los números racionales y los irracionales (obtenidos como límites de sucesiones de Cauchy de números racionales) es el conjunto de los números reales Uno de los problemas de los números reales es que no forman un cuerpo algebraicamente cerrado, por lo que ciertos problemas no tienen solución planteados en términos de números reales. Esa es una de las razones por las cuales se introdujeron los números complejos Al margen de los números reales y complejos, claramente conectados con problemas de las ciencias naturales, existen otros tipos de números que generalizan aún más y extienden el concepto de número de una manera más abstracta y responden más a creaciones deliberadas de matemáticos. La mayoría de estas generalizaciones del concepto de número se usan solo en matemáticas, aunque algunos de ellos han encontrado aplicaciones para resolver ciertos problemas físicos. Entre ellos están los números hipercomplejos, que incluyen a los cuaterniones, útiles para representar rotaciones en un espacio de tres dimensiones, y generalizaciones de estos, como octoniones y los sedeniones. A un nivel un poco más abstracto también se han ideado conjuntos de números capaces de tratar con cantidades infinitas e infinitesimales, como los hiperreales Lista de los tipos de números existentes La teoría de los números trata básicamente de las propiedades de los números naturales y los enteros, mientras que las operaciones del álgebra y el cálculo permiten definir la mayor parte de los sistemas numéricos, entre los cuales están: Números naturales Número primo Números compuestos Números perfectos Números enteros Números negativos Números pares Números impares Números racionales Números reales Números irracionales Números algebraicos Números trascendentes Extensiones de los números reales Números complejos Números hipercomplejos Cuaterniones Octoniones Números hiperreales Números superreales Números surreales Números usados en teoría de conjuntos Números ordinales Números cardinales Números transfinitos Estructura de los sistemas numéricos En álgebra abstracta y análisis matemático un sistema numérico se caracteriza por una: Estructura algebraica, usualmente un anillo conmutativo o cuerpo matemático (en el caso no conmutativo son un álgebra sobre un cuerpo y en el caso de los números naturales solo un monoide conmutativo). Estructura de orden, usualmente un conjunto ordenado, en el caso de los números naturales, enteros, racionales y reales se trata de conjuntos totalmente ordenados, aunque los números complejos e hipercomplejos solo son conjuntos parcialmente ordenados. Los reales además son un conjunto bien ordenado y con un orden denso. Estructura topológica, los conjuntos numéricos numerables usualmente son conjuntos disconexos, sobre los que se considera la topología discreta, mientras que sobre los conjuntos no numerables se considera una topología que los hace adecuados para el análisis matemático. Otra propiedad interesante de muchos conjuntos numéricos es que son representables mediante diagramas de Hasse, diagramas de Euler y diagramas de Venn, pudiéndose tomar una combinación de ambos en un diagrama de Euler-Venn con la forma característica de cuadrilátero y además pudiéndose representar internamente un diagrama de Hasse (es una recta). Tanto históricamente como conceptualmente, los diversos conjuntos numéricos, desde el más simple de los números naturales, hasta extensiones transcendentes de los números reales y complejos, elaboradas mediante la teoría de modelos durante el siglo XX, se construyen desde una estructura más simple hasta otra más compleja. Números naturales especiales El estudio de ciertas propiedades que cumplen los números ha producido una enorme cantidad de tipos de números, la mayoría sin un interés matemático específico. Se pueden encuadrar dentro de la matemática recreativa. A continuación se indican algunos: Perfecto: número igual a la suma de sus divisores (incluyendo el 1). Ejemplo: 6 = 1 + 2 + 3. Sheldon: el número 73, es el 21° número primo, que al multiplicar 7 × 3 = 21; Y al dar la vuelta a sus dígitos da 37 que es el 12° número primo. Narcisista: número de n dígitos que resulta ser igual a la suma de las potencias de orden n de sus dígitos. Ejemplo: 153 = 1³ + 5³ + 3³. Omirp: número primo que al invertir sus dígitos da otro número primo. Ejemplo: 1597 y 7951 son primos. Vampiro: número que es el producto de dos números obtenidos a partir de sus dígitos. Ejemplo: 2187 = 27 × 81. Hamsteriano: Su estructura aritmética N= (a×b)2-1, donde a y b son primos los dos, la suma de sus divisores sobrepasa N, y la cantidad de sus divisores es > a×b/2; va como ejemplo: 1224 = (5×7)2-1 Pitagórico: una terna pitagórica son tres números que cumplen las siguientes condiciones: el cuadrado de uno de ellos, más el cuadrado de otro, es igual al cuadrado del tercero, por ejemplo: (3, 4, 5) ya que 32 + 42 = 9 + 16 = 25 = 52 Una vez entendido el problema de la naturaleza y la clasificación de los números, surge otro, más práctico, pero que condiciona todo lo que se va a hacer con ellos: la manera de escribirlos. El sistema que se ha impuesto universalmente es la numeración posicional, gracias al invento del cero, con una base constante. Más formalmente, en Los fundamentos de la aritmética, Gottlob Frege (1848-1925) realiza una definición de «número», la cual fue tomada como referencia por muchos matemáticos (entre ellos Bertrand Russell [1872-1870], cocreador de Principia mathematica): «n» es un número, es entonces la definición de «que existe un concepto “F” para el cual “n” aplica», que a su vez se ve explicado como que «n» es la extensión del concepto «equinumerable con» para «F», y dos conceptos son equinumerables si existe una relación «uno a uno» (véase que no se utiliza el símbolo «1» porque no está definido aún) entre los elementos que lo componen (es decir, una biyección en otros términos). Véase también que Frege, tanto como cualquier otro matemático, se ve inhabilitado para definir al número como la expresión de una cantidad, porque la simbología matemática no hace referencia necesaria a la numerabilidad, y el hecho de «cantidad» referiría a algo numerable, mientras que números se adoptan para definir la cardinalidad de, por ejemplo, los elementos que se encuentran en el intervalo abierto (0, 1), que contiene innumerables elementos (el continuo). Peano, antes de establecer sus cinco proposiciones sobre los números naturales, explícita que supone sabida una definición (quizás debido a su «obviedad») de las palabras o conceptos cero, sucesor y número. De esta manera postula: 0 es un número natural el sucesor de todo número es un número dos números diferentes no tienen el mismo sucesor 0 no es el sucesor de ningún número y la propiedad inductiva Sin embargo, si uno define el concepto cero como el número 100, y el concepto número como los números mayores a 100, entonces las cinco proposiciones mencionadas anteriormente aplican, no a la idea que Peano habría querido comunicar, sino a su formalización. La definición de número se encuentra por ende no totalmente formalizada, aunque se encuentre un acuerdo mayoritario en adoptar la definición enunciada por Frege. Historia del concepto de número Hueso de Ishango. Cognitivamente el concepto de número está asociado a la habilidad de contar y comparar cuál de dos conjuntos de entidades similares tiene mayor cantidad de elementos. Las primeras sociedades humanas se encontraron muy pronto con el problema de determinar cuál de dos conjuntos era «mayor» que otro, o de conocer con precisión cuántos elementos formaban una colección de cosas. Esos problemas podían ser resueltos simplemente contando. La habilidad de contar del ser humano, no es un fenómeno simple, aunque la mayoría de culturas tienen sistemas de cuenta que llegan como mínimo a centenares, algunos pueblos con una cultura material simple, solo disponen de términos para los números 1, 2 y 3 y usualmente usan el término «muchos» para cantidades mayores, aunque cuando es necesario usan recursivamente expresiones traducibles como «3 más 3 y otros 3». El conteo se debió iniciar mediante el uso de objetos físicos (tales como montones de piedras) y de marcas de cuenta, como las encontradas en huesos tallados: el de Lebombo, con 29 muescas grabadas en un hueso de babuino, tiene unos 37 000 años de antigüedad y otro hueso de lobo encontrado en la antigua Checoslovaquia, con 57 marcas dispuestas en cinco grupos de 11 y dos sueltas, se ha estimado en unos 30 000 años de antigüedad. Ambos casos constituyen una de las más antiguas marcas de cuenta conocidas habiéndose sugerido que pudieran estar relacionadas con registros de fases lunares. En cuanto al origen ordinal algunas teorías lo sitúan en rituales religiosos. Los sistemas numerales de la mayoría de familias lingüísticas reflejan que la operación de contar estuvo asociado al conteo de dedos (razón por la cual los sistemas de base decimal y vigesimal son los más abundantes), aunque está testimoniado el empleo de otras bases numéricas. El paso hacia los símbolos numerales, al igual que la escritura, se ha asociado a la aparición de sociedades complejas con instituciones centralizadas constituyendo artificios burocráticos de contabilidad en registros impositivos y de propiedades. Su origen estaría en primitivos símbolos con diferentes formas para el recuento de diferentes tipos de bienes como los que se han encontrado en Mesopotamia inscritos en tablillas de arcilla que a su vez habían venido a sustituir progresivamente el conteo de diferentes bienes mediante fichas de arcilla (constatadas al menos desde el 8000 a. C.) Los símbolos numerales más antiguos encontrados se sitúan en las civilizaciones mesopotámicas usándose como sistema de numeración ya no solo para la contabilidad o el comercio sino también para la agrimensura o la astronomía como, por ejemplo, registros de movimientos planetarios. En conjunto, desde hace 5000 años la mayoría de las civilizaciones han contado como lo hacemos hoy aunque la forma de escribir los números (si bien todos representan con exactitud los naturales) ha sido muy diversa. Básicamente la podemos clasificar en tres categorías: Sistemas de notación aditiva. Acumulan los símbolos de todas las unidades, decenas, centenas, …, necesarios hasta completar el número. Aunque los símbolos pueden ir en cualquier orden, adoptaron siempre una determinada posición (de más a menos). De este tipo son los sistemas de numeración: egipcio, hitita, cretense, romano, griego, armenio y judío. Sistemas de notación híbrida. Combinan el principio aditivo con el multiplicativo. En los anteriores 500 se representa con 5 símbolos de 100, en estos se utiliza la combinación del 5 y el 100. El orden de las cifras es ahora fundamental (estamos a un paso del sistema posicional). De este tipo son los sistemas de numeración: chino clásico, asirio, armenio, etíope y maya. Este último utilizaba símbolos para el 1, el 5 y el 0. Siendo este el primer uso documentado del cero tal como lo conocemos hoy (año 36 a. C.) ya que el de los babilonios solo se utilizaba entre otros dígitos. Sistemas de notación posicional. La posición de las cifras nos indica si son unidades, decenas, centenas, …, o en general la potencia de la base. Solo tres culturas además de la india lograron desarrollar un sistema de este tipo: el sistema chino (300 a. C.) que no disponía de 0, el sistema babilónico (2000 a. C.) con dos símbolos, de base 10 aditivo hasta el 60 y posicional (de base 60) en adelante, sin 0 hasta el 300 a. C. Las fracciones unitarias egipcias (Papiro de Ahmes/Rhind) Artículo principal: Fracción egipcia En este papiro adquirido por Alexander Henry Rhind (1833-1863) en 1858, cuyo contenido data del 2000 al 1800 a. C. además del sistema de numeración antes descrito nos encontramos con su tratamiento de las fracciones. No consideran las fracciones en general, solo las fracciones unitarias (inversas de los naturales 1/20) que se representan con un signo oval encima del número, la fracción 2/3 que se representa con un signo especial y en algunos casos fracciones del tipo Al ser un sistema sumativo la notación es: 1+1/2+1/4. La operación fundamental es la suma y nuestras multiplicaciones y divisiones se hacían por «duplicaciones» y «mediaciones», por ejemplo 69×19=69×(16+2+1), donde 16 representa 4 duplicaciones y 2 una duplicación. Fracciones sexagesimales babilónicas (documentos cuneiformes) En las tablillas cuneiformes de la dinastía Hammurabi (1800-1600 a. C.) aparece el sistema posicional, antes referido, extendido a las fracciones, pero XXX vale para Para calcular recurrían, como nosotros antes de disponer de máquinas, a las numerosas tablas que disponían: De multiplicar, de inversos, de cuadrados y cubos, de raíces cuadradas y cúbicas, de potencias sucesivas de un número dado no fijo, etc. Por ejemplo, para calcular Realizaban las operaciones de forma parecida a hoy, la división multiplicando por el inverso (para lo que utilizan sus tablas de inversos). En la tabla de inversos faltan los de 7 y 11 que tienen una expresión sexagesimal infinitamente larga. Sí están, pero no se percataron del desarrollo periódico. Descubrimiento de los inconmensurables Las circunstancias y la fecha de este descubrimiento son inciertas, aunque se atribuye a la escuela pitagórica (se utiliza el teorema de Pitágoras). Aristóteles (384-322 a. C.) menciona una demostración de la inconmensurabilidad de la diagonal de un cuadrado con respecto a su lado basada en la distinción entre lo par y lo impar. La reconstrucción que realiza C. Boyer es: Sean d:diagonal, s:lado y d/s racional, que podremos escribirlo como La teoría pitagórica de todo es número quedó seriamente dañada. El problema lo resolvería Eudoxo de Cnido (408-355 a. C.) tal como nos indica Euclides en el libro V de Los elementos. Para ello estableció el axioma de Arquímedes: «Dos magnitudes tienen una razón si se puede encontrar un múltiplo de una de ellas que supere a la otra» (excluye el 0). Después, en la definición 5, da la famosa formulación de Eudoxo: «Dos magnitudes están en la misma razón En el libro Historia de la matemática (1985), de J. P. Colette, se hace la observación de que esta definición está muy próxima a la de número real que dará Dedekind (1831-1916), divide las fracciones en las Creación del cero Artículo principal: Cero En cualquier sistema de numeración posicional surge el problema de la falta de unidades de determinado orden. Por ejemplo, en el sistema babilónico el número Hacia el siglo III a. C., en Grecia, se comenzó a representar la nada mediante una o que significa oudos vacío, y que no dio origen al concepto de cero como existe hoy en día. La idea del cero como concepto matemático parece haber surgido en la India antes que en ningún otro lugar. La única notación ordinal del Viejo Mundo fue la sumeria, donde el cero se representaba por un vacío. En América, la primera expresión conocida del sistema de numeración vigesimal prehispánico data del siglo III a. C. Se trata de una estela olmeca tardía, la cual ya contaba tanto con el concepto de orden como el de cero. Los mayas inventaron cuatro signos para el cero; los principales eran: el corte de un caracol para el cero matemático, y una flor para el cero calendárico (que implicaba no la ausencia de cantidad, sino el cumplimiento de un ciclo). Números negativos Brahmagupta, en el 628 de nuestra era, considera las dos raíces de las ecuaciones cuadráticas, aunque una de ellas sea negativa o irracional. De hecho en su obra es la primera vez que aparece sistematizada la aritmética (+, -, *, /, potencias y raíces) de los números positivos, negativos y el cero, que él llamaba los bienes, las deudas y la nada. Así, por ejemplo, para el cociente, establece: Positivo dividido por positivo, o negativo dividido por negativo, es afirmativo. Cifra dividido por cifra es nada (0/0=0). Positivo dividido por negativo es negativo. Negativo dividido por afirmativo es negativo. Positivo o negativo dividido por cifra es una fracción que la tiene por denominador (a/0=¿?) No solo utilizó los negativos en los cálculos, sino que los consideró como entidades aisladas, sin hacer referencia a la geometría. Todo esto se consiguió gracias a su despreocupación por el rigor y la fundamentación lógica, y su mezcla de lo práctico con lo formal. Sin embargo, el tratamiento que hicieron de los negativos cayó en el vacío, y fue necesario que transcurrieran varios siglos (hasta el Renacimiento) para que fuese recuperado. Al parecer, los chinos también poseían la idea de número negativo, y estaban acostumbrados a calcular con ellos utilizando varillas negras para los negativos y rojas para los positivos. Transmisión del sistema indo-arábigo a Occidente Varios autores del siglo XIII contribuyeron a esta difusión, destacan Alexandre de Villedieu (1225), Sacrobosco (circa 1195, o 1200-1256) y sobre todo Leonardo de Pisa (1180-1250). Este último, conocido como Fibonacci, viajó por Oriente y aprendió de los árabes el sistema posicional hindú. Escribió un libro, El Liber abaci, que trata en el capítulo I la numeración posicional, en los cuatro siguientes las operaciones elementales, en los capítulos VI y VII las fracciones: comunes, sexagesimales y unitarias (¡no usa los decimales, principal ventaja del sistema!), y en el capítulo XIV los radicales cuadrados y cúbicos. También contiene el problema de los conejos que da la serie: No aparecen los números negativos, que tampoco consideraron los árabes, debido a la identificación de número con magnitud (¡obstáculo que duraría siglos!). A pesar de la ventaja de sus algoritmos de cálculo, se desataría por diversas causas una lucha encarnizada entre abacistas y algoristas, hasta el triunfo final de estos últimos. Las fracciones continuas Pietro Antonio Cataldi (1548-1626), aunque con ejemplos numéricos, desarrolla una raíz cuadrada en fracciones continuas como hoy: Queremos calcular Siendo así los números irracionales aceptados con toda normalidad, pues se les podía aproximar fácilmente mediante números racionales. Primera formulación de los números complejos Los números complejos eran en pocos casos aceptados como raíces o soluciones de ecuaciones (M. Stifel (1487-1567), S. Stevin (1548-1620)) y por casi ninguno como coeficientes). Estos números se llamaron inicialmente ficticii ficticios (el término imaginario usado actualmente es reminiscente de estas reticencias a considerarlos números respetables). A pesar de esto G. Cardano (1501-1576) conoce la regla de los signos y R. Bombelli (1526-1573) las reglas aditivas a través de haberes y débitos, pero se consideran manipulaciones formales para resolver ecuaciones, sin entidad al no provenir de la medida o el conteo. Cardano en la resolución del problema dividir 10 en dos partes tales que su producto valga 40 obtiene como soluciones En la resolución de ecuaciones cúbicas con la fórmula de Cardano-Tartaglia, aunque las raíces sean reales, aparecen en los pasos intermedios raíces de números negativos. En esta situación Bombelli dice en su Álgebra que tuvo lo que llamó una idea loca, esta era que los radicales podían tener la misma relación que los radicandos y operar con ellos, tratando de eliminarlos después. En un texto posterior en 20 años utiliza p.d.m. Generalización de las fracciones decimales Aunque se encuentra un uso más que casual de las fracciones decimales en la Arabia medieval y en la Europa renacentista, y ya en 1579 Vieta (1540-1603) proclamaba su apoyo a éstas frente a las sexagesimales, y las aceptaban los matemáticos que se dedicaban a la investigación, su uso se generalizó con la obra que Simon Stevin publicó en 1585 De Thiende (La Disme). En su definición primera dice que la Disme es una especie de aritmética que permite efectuar todas las cuentas y medidas utilizando únicamente números naturales. En las siguientes define nuestra parte entera: cualquier número que vaya el primero se dice comienzo y su signo es (0), (primera posición decimal 1/10). El siguiente se dice primera y su signo es (1) (segunda posición decimal 1/100). El siguiente se dice segunda (2). Es decir, los números decimales que escribe: 0,375 como 3(1)7(2)5(3), o 372,43 como 372(0)4(1)3(2). Añade que no se utiliza ningún número roto (fracciones), y el número de los signos, exceptuando el 0, no excede nunca a 9. Esta notación la simplificó Joost Bürgi (1552-1632) eliminando la mención al orden de las cifras y sustituyéndolo por un «.» en la parte superior de las unidades 372·43, poco después Magini (1555-1617) usó el «.» entre las unidades y las décimas: 372.43, uso que se generalizaría al aparecer en la Constructio de Napier (1550-1617) de 1619. La «,» también fue usada a comienzos del siglo XVII por el holandés Willebrord Snellius: 372,43. El principio de inducción matemática Artículo principal: Inducción matemática Su antecedente es un método de demostración, llamado inducción completa, por aplicación reiterada de un mismo silogismo que se extiende indefinidamente y que usó Maurolyco (1494-1575) para demostrar que la suma de los primeros Si El hecho de que entonces De manera intuitiva se entiende la inducción como un efecto dominó. Suponiendo que se tiene una fila infinita de fichas de dominó, el paso base equivale a tirar la primera ficha; por otro lado, el paso inductivo equivale a demostrar que si alguna ficha se cae, entonces la ficha siguiente también se caerá. La conclusión es que se pueden tirar todas las fichas de esa fila. La interpretación geométrica de los números complejos Esta interpretación suele ser atribuida a Gauss (1777-1855) que hizo su tesis doctoral sobre el teorema fundamental del álgebra, enunciado por primera vez por Harriot y Girard en 1631, con intentos de demostración realizados por DAlembert, Euler y Lagrange, demostrando que las pruebas anteriores eran falsas y dando una demostración correcta primero para el caso de coeficientes, y después de complejos. También trabajó con los números enteros complejos que adoptan la forma La representación gráfica de los números complejos había sido descubierta ya por Caspar Wessel (1745-1818) pero pasó desapercibida, y así el plano de los números complejos se llama «plano de Gauss» a pesar de no publicar sus ideas hasta 30 años después. Desde la época de Girard (mitad siglo XVII) se conocía que los números reales se pueden representar en correspondencia con los puntos de una recta. Al identificar ahora los complejos con los puntos del plano los matemáticos se sentirán cómodos con estos números, ver es creer. Descubrimiento de los números trascendentes La distinción entre números irracionales algebraicos y trascendentes data del siglo XVIII, en la época en que Euler demostró que Liouville (1809-1882) demostró en 1844 que todos los números de la forma Hermite (1822-1901) en una memoria Sobre la función exponencial de 1873 demostró la trascendencia de Lindeman (1852-1939) en la memoria Sobre el número El problema 7 de Hilbert (1862-1943) que plantea si Teorías de los irracionales Hasta mediados del siglo XIX los matemáticos se contentaban con una comprensión intuitiva de los números y sus sencillas propiedades no son establecidas lógicamente hasta el siglo XIX. La introducción del rigor en el análisis puso de manifiesto la falta de claridad y la imprecisión del sistema de los números reales, y exigía su estructuración lógica sobre bases aritméticas. Bolzano había hecho un intento de construir los números reales basándose en sucesiones de números racionales, pero su teoría pasó desapercibida y no se publicó hasta 1962. Hamilton hizo un intento, haciendo referencia a la magnitud tiempo, a partir de particiones de números racionales: si cuando y si cuando pero no desarrolló más su teoría. Pero en el mismo año 1872 cinco matemáticos, un francés y cuatro alemanes, publicaron sus trabajos sobre la aritmetización de los números reales: Charles Meray (1835-1911) en su obra Nouveau précis danalyse infinitesimale define el número irracional como un límite de sucesiones de números racionales, sin tener en cuenta que la existencia misma del límite presupone una definición del número real. Hermann Heine (1821-1881) publicó, en el Journal de Crelle en 1872, su artículo «Los elementos de la teoría de funciones», donde proponía ideas similares a las de Cantor, teoría que en conjunto se llama actualmente «teorema de Heine-Cantor». Richard Dedekind (1831-1916) publica su Stetigkeit und irrationale zahlen. Su idea se basa en la continuidad de la recta real y en los agujeros que hay si solo consideramos los números racionales. En la sección dedicada al «dominio R» enuncia un axioma por el que se establece la continuidad de la recta: «cada punto de la recta divide los puntos de ésta en dos clases tales que cada punto de la primera se encuentra a la izquierda de cada punto de la segunda clase, entonces existe un único punto que produce esta división». Esta misma idea la utiliza en la sección «creación de los números irracionales» para introducir su concepto de «cortadura». Bertrand Russell apuntaría después que es suficiente con una clase, pues esta define a la otra. Georg Cantor (1845-1918). Define los conceptos de: sucesión fundamental, sucesión elemental, y límite de una sucesión fundamental, y partiendo de ellos define el número real. Karl Weierstrass (1815-1897). No llegó a publicar su trabajo, continuación de los de Bolzano, Abel y Cauchy, pero fue conocido por sus enseñanzas en la Universidad de Berlín. Su caracterización basada en los «intervalos encajados», que pueden contraerse a un número racional pero no necesariamente lo hacen, no es tan generalizable como las anteriores, pero proporciona fácil acceso a la representación decimal de los números reales. Álgebras hipercomplejas La construcción de obtención de los números complejos a partir de los números reales, y su conexión con el grupo de transformaciones afines en el plano sugirió a algunos matemáticos otras generalizaciones similares conocidas como números hipercomplejos. En todas estas generalizaciones los números complejos son un subconjunto de estos nuevos sistemas numéricos, aunque estas generalizaciones tienen la estructura matemática de álgebra sobre un cuerpo, pero en ellos la operación de multiplicación no es conmutativa. Teoría de conjuntos Artículo principal: Teoría de conjuntos La teoría de conjuntos sugirió muchas y variadas formas de extender los números naturales y los números reales de formas diferentes a como los números complejos extendían al conjunto de los números reales. El intento de capturar la idea de conjunto con un número no finito de elementos llevó a la aritmética de números transfinitos que generalizan a los naturales, pero no a los números enteros. Los números transfinitos fueron introducidos por Georg Cantor hacia 1873. Los números hiperreales usados en el análisis no estándar generalizan a los reales pero no a los números complejos (aunque admiten una complejificación que generalizaría también a los números complejos). Aunque parece los números hiperreales no proporcionan resultados matemáticos interesantes que vayan más allá de los obtenibles en el análisis real, algunas demostraciones y pruebas matemáticas parecen más simples en el formalismo de los números hiperreales, por lo que no están exentos de importancia práctica. Socialmente Los números naturales por la necesidad de contar. Los números fraccionarios por la necesidad de medir partes de un todo, y compartir. Los enteros negativos por fenómenos de doble sentido: izquierda-derecha, arriba-abajo, pérdida-ganancia. Los números reales por la necesidad de medir segmentos. Los números complejos por exigencias de resolver ecuaciones algebraicas, como el caso de la cúbicas o de x2 + 1 = 0. Sistemas de representación de los números Cifra, dígito y numeral Artículo principal: Cifra (matemática) Una de las formas más frecuentes de representar números por escrito consiste en un «conjunto finito de símbolos» o dígitos que, adecuadamente combinados, permiten formar cifras que funcionan como representaciones de números (cuando una secuencia específica de signos se emplea para representar un número se la llama numeral, aunque una cifra también puede representar simplemente un código identificativo). Base numérica Artículo principal: Base (aritmética) Tanto las lenguas naturales como la mayor parte de sistemas de representación de números mediante cifras, usan un inventario finito de unidades para expresar una cantidad mucho mayor de números. Una manera importante de lograr eso es el uso de una base aritmética en esos sistemas un número se expresa en general mediante suma o multiplicación de números. Los sistemas puramente aritméticos recurren a bases donde cada signo recibe una interpretación diferente según su posición. Así en el siguiente numeral arábigo (base 10): 13568 El <8> por estar en última posición representa unidades, el <6> representa decenas, el <5> centenas, el <3> millares y el <1> decenas de millares. Es decir, ese numeral representara el número: 13568 13568 Muchas lenguas del mundo usan una base decimal, igual que el sistema arábigo, aunque también es frecuente que las lenguas usen sistemas vigesimales (base 20). De hecho la idea de usar un número finito de dígitos o signos para representar números arbitrariamente grandes funciona para cualquier base b, donde b es un número entero mayor o igual que 2. Los ordenadores frecuentemente usan para sus operaciones la base binaria (b = 2), y para ciertos usos también se emplea la base octal (b = 8 ) o hexadecimal (b = 16). La base coincide con el número de signos primarios, si un sistema posicional tiene b símbolos primarios que designaremos por Designará al número: Números en las lenguas naturales Artículo principal: Numeral (lingüística) Las lenguas naturales usan nombres o numerales para los números frecuentemente basados en el contaje mediante dedos, razón por la cual la mayoría de las lenguas usan sistemas de numeración en base 10 (dedos de las manos) o base 20 (dedos de manos y pies), aunque también existen algunos sistemas exóticos que emplean otras bases."

ksampletext_wikipedia_phys_mecanicacuantica: str = "Mecánica cuántica. La mecánica cuántica es la rama de la física que estudia la naturaleza a escalas espaciales pequeñas, los sistemas atómicos, subatómicos, sus interacciones con la radiación electromagnética y otras fuerzas, en términos de cantidades observables. Se basa en la observación de que todas las formas de energía se liberan en unidades discretas o paquetes llamados cuantos. Las partículas con esta propiedad pueden pertenecer a dos tipos distintos: fermiones o bosones. Algunos de estos últimos están ligados a una -interacción fundamental (por ejemplo, el fotón pertenece a la electromagnética). Sorprendentemente, la teoría cuántica solo permite normalmente cálculos probabilísticos o estadísticos de las características observadas de las partículas elementales, entendidos en términos de funciones de onda. La ecuación de Schrödinger desempeña, en la mecánica cuántica, el papel que las leyes de Newton y la conservación de la energía desempeñan en la mecánica clásica. Es decir, la predicción del comportamiento futuro de un sistema dinámico y es una ecuación de onda en términos de una función de onda la que predice analíticamente la probabilidad precisa de los eventos o resultados. En teorías anteriores de la física clásica, la energía era tratada únicamente como un fenómeno continuo, en tanto que la materia se supone que ocupa una región muy concreta del espacio y que se mueve de manera continua. Según la teoría cuántica, la energía se emite y se absorbe en cantidades discretas y minúsculas. Un paquete individual de energía, llamado cuanto, en algunas situaciones se comporta como una partícula de materia. Por otro lado, se encontró que las partículas exponen algunas propiedades ondulatorias cuando están en movimiento y ya no son vistas como localizadas en una región determinada, sino más bien extendidas en cierta medida. La luz u otra radiación emitida o absorbida por un átomo solo tiene ciertas frecuencias (o longitudes de onda), como puede verse en la línea del espectro asociado al elemento químico representado por tal átomo. La teoría cuántica demuestra que tales frecuencias corresponden a niveles definidos de los cuantos de luz, o fotones, y es el resultado del hecho de que los electrones del átomo solo pueden tener ciertos valores de energía permitidos. Cuando un electrón pasa de un nivel permitido a otro, una cantidad de energía es emitida o absorbida, cuya frecuencia es directamente proporcional a la diferencia de energía entre los dos niveles. La mecánica cuántica surge tímidamente en los inicios del siglo XX dentro de las tradiciones más profundas de la física para dar una solución a problemas para los que las teorías conocidas hasta el momento habían agotado su capacidad de explicar, como la llamada catástrofe ultravioleta en la radiación de cuerpo negro predicha por la física estadística clásica y la inestabilidad de los átomos en el modelo atómico de Rutherford. La primera propuesta de un principio propiamente cuántico se debe a Max Planck en 1900, para resolver el problema de la radiación de cuerpo negro, que fue duramente cuestionado, hasta que Albert Einstein lo convierte en el principio que exitosamente pueda explicar el efecto fotoeléctrico. Las primeras formulaciones matemáticas completas de la mecánica cuántica no se alcanzan hasta mediados de la década de 1920, sin que hasta el día de hoy se tenga una interpretación coherente de la teoría, en particular del problema de la medición. El formalismo de la mecánica cuántica se desarrolló durante la década de 1920. En 1924, Louis de Broglie propuso que, al igual que las ondas de luz presentan propiedades de partículas, como ocurre en el efecto fotoeléctrico, las partículas, también presentan propiedades ondulatorias. Dos formulaciones diferentes de la mecánica cuántica se presentaron después de la sugerencia de Broglie. En 1926, la mecánica ondulatoria de Erwin Schrödinger implica la utilización de una entidad matemática, la función de onda, que está relacionada con la probabilidad de encontrar una partícula en un punto dado en el espacio. En 1925, la mecánica matricial de Werner Heisenberg no hace mención alguna de las funciones de onda o conceptos similares, pero ha demostrado ser matemáticamente equivalente a la teoría de Schrödinger. Un descubrimiento importante de la teoría cuántica es el principio de incertidumbre, enunciado por Heisenberg en 1927, que pone un límite teórico absoluto en la precisión de ciertas mediciones. Como resultado de ello, la asunción clásica de los científicos de que el estado físico de un sistema podría medirse exactamente y utilizarse para predecir los estados futuros tuvo que ser abandonada. Esto supuso una revolución filosófica y dio pie a numerosas discusiones entre los más grandes físicos de la época. La mecánica cuántica propiamente dicha no incorpora a la relatividad en su formulación matemática. La parte de la mecánica cuántica que incorpora elementos relativistas de manera formal para abordar diversos problemas se conoce como mecánica cuántica relativista o ya, en forma más correcta y acabada, teoría cuántica de campos (que incluye a su vez a la electrodinámica cuántica, cromodinámica cuántica y teoría electrodébil dentro del modelo estándar) y más generalmente, la teoría cuántica de campos en espacio-tiempo curvo. La única interacción elemental que no se ha podido cuantizar hasta el momento ha sido la interacción gravitatoria. Este problema constituye entonces uno de los mayores desafíos de la física del siglo XXI. La mecánica cuántica se combinó con la teoría de la relatividad en la formulación de Paul Dirac de 1928, lo que, además, predijo la existencia de antipartículas. Otros desarrollos de la teoría incluyen la estadística cuántica, presentada en una forma por Einstein y Bose (la estadística de Bose-Einstein) y en otra forma por Dirac y Enrico Fermi (la estadística de Fermi-Dirac); la electrodinámica cuántica, interesada en la interacción entre partículas cargadas y los campos electromagnéticos, su generalización, la teoría cuántica de campos y la electrónica cuántica. La mecánica cuántica proporciona el fundamento de la fenomenología del átomo, de su núcleo y de las partículas elementales (lo cual requiere necesariamente el enfoque relativista). También su impacto en teoría de la información, criptografía y química ha sido decisivo entre esta misma. Contexto histórico La mecánica cuántica es, cronológicamente hablando, la última de las grandes ramas de la física. Se formuló a principios del siglo XX, casi al mismo tiempo que la teoría de la relatividad, aunque el grueso de la mecánica cuántica se desarrolló a partir de 1920 (siendo la teoría de la relatividad especial de 1905 y la teoría general de la relatividad de 1915). Además al advenimiento de la mecánica cuántica existían diversos problemas no resueltos en la electrodinámica clásica. El primero de estos problemas era la emisión de radiación de cualquier objeto en equilibrio, llamada radiación térmica, que es la que proviene de la vibración microscópica de las partículas que lo componen. Usando las ecuaciones de la electrodinámica clásica, la energía que emitía esta radiación térmica tendía al infinito, si se suman todas las frecuencias que emitía el objeto, con ilógico resultado para los físicos. También la estabilidad de los átomos no podía ser explicada por el electromagnetismo clásico, y la noción de que el electrón fuera o bien una partícula clásica puntual o bien una cáscara esférica de dimensiones finitas resultaban igualmente problemáticas para esto. Radiación electromagnética El problema de la radiación electromagnética de un cuerpo negro fue uno de los primeros problemas resueltos en el seno de la mecánica cuántica. Es en el seno de la mecánica estadística donde surgen por primera vez las ideas cuánticas en 1900. Al físico alemán Max Planck se le ocurrió un artificio matemático: si en el proceso aritmético se sustituía la integral de esas frecuencias por una suma no continua (discreta), se dejaba de obtener infinito como resultado, con lo que se eliminaba el problema; además, el resultado obtenido concordaba con lo que después era medido. Fue Max Planck quien entonces enunció la hipótesis de que la radiación electromagnética es absorbida y emitida por la materia en forma de «cuantos» de luz o fotones de energía cuantizados introduciendo una constante estadística, que se denominó constante de Planck. Su historia es inherente al siglo XX, ya que la primera formulación cuántica de un fenómeno fue dada a conocer por el mismo Planck el 14 de diciembre de 1900 en una sesión de la Sociedad Física de la Academia de Ciencias de Berlín. La idea de Planck habría permanecido muchos años solo como hipótesis sin verificar por completo si Albert Einstein no la hubiera retomado, proponiendo que la luz, en ciertas circunstancias, se comporta como partículas de energía (los cuantos de luz o fotones) en su explicación del efecto fotoeléctrico. Fue Albert Einstein quien completó en 1905 las correspondientes leyes del movimiento su teoría especial de la relatividad, demostrando que el electromagnetismo era una teoría esencialmente no mecánica. Culminaba así lo que se ha dado en llamar física clásica, es decir, la física no-cuántica. Usó este punto de vista llamado por él «heurístico», para desarrollar su teoría del efecto fotoeléctrico, publicando esta hipótesis en 1905, lo que le valió el Premio Nobel de Física de 1921. Esta hipótesis fue aplicada también para proponer una teoría sobre el calor específico, es decir, la que resuelve cuál es la cantidad de calor necesaria para aumentar en una unidad la temperatura de la unidad de masa de un cuerpo. El siguiente paso importante se dio hacia 1925, cuando Louis De Broglie propuso que cada partícula material tiene una longitud de onda asociada, inversamente proporcional a su masa, y a su velocidad. Así quedaba establecida la dualidad onda/materia. Poco tiempo después Erwin Schrödinger formuló una ecuación de movimiento para las «ondas de materia», cuya existencia había propuesto De Broglie y varios experimentos sugerían que eran reales. La mecánica cuántica introduce una serie de hechos contraintuitivos que no aparecían en los paradigmas físicos anteriores; con ella se descubre que el mundo atómico no se comporta como esperaríamos. Los conceptos de incertidumbre o cuantización son introducidos por primera vez aquí. Además la mecánica cuántica es la teoría científica que ha proporcionado las predicciones experimentales más exactas hasta el momento, a pesar de estar sujeta a las probabilidades. Inestabilidad de los átomos clásicos El segundo problema importante que la mecánica cuántica resolvió a través del modelo de Bohr, fue el de la estabilidad de los átomos. De acuerdo con la teoría clásica un electrón orbitando alrededor de un núcleo cargado positivamente debería emitir energía electromagnética perdiendo así velocidad hasta caer sobre el núcleo. La evidencia empírica era que esto no sucedía, y sería la mecánica cuántica la que resolvería este hecho primero mediante postulados ad hoc formulados por Bohr y más tarde mediante modelos como el modelo atómico de Schrödinger basados en supuestos más generales. A continuación se explica el fracaso del modelo clásico. En mecánica clásica, un átomo de hidrógeno es un tipo de problema de los dos cuerpos en que el protón sería el primer cuerpo que tiene más del 99% de la masa del sistema y el electrón es el segundo cuerpo que es mucho más ligero. Para resolver el problema de los dos cuerpos es conveniente hacer la descripción del sistema, colocando el origen del sistema de referencia en el centro de masa de la partícula de mayor masa, esta descripción es correcta considerando como masa de la otra partícula la masa reducida que viene dada por 999 Siendo watt Ese proceso acabaría con el colapso del átomo sobre el núcleo en un tiempo muy corto dadas las grandes aceleraciones existentes. A partir de los datos de la ecuación anterior el tiempo de colapso sería de 10-8 s, es decir, de acuerdo con la física clásica los átomos de hidrógeno no serían estables y no podrían existir más de una cienmillonésima de segundo. Esa incompatibilidad entre las predicciones del modelo clásico y la realidad observada llevó a buscar un modelo que explicara fenomenológicamente el átomo. El modelo atómico de Bohr era un modelo fenomenológico y provisorio que explicaba satisfactoriamente aunque de manera heurística algunos datos, como el orden de magnitud del radio atómico y los espectros de absorción del átomo, pero no explicaba cómo era posible que el electrón no emitiera radiación perdiendo energía. La búsqueda de un modelo más adecuado llevó a la formulación del modelo atómico de Schrödinger en el cual puede probarse que el valor esperado de la aceleración es nulo, y sobre esa base puede decirse que la energía electromagnética emitida debería ser también nula. Sin embargo, al contrario del modelo de Bohr, la representación cuántica de Schrödinger es difícil de entender en términos intuitivos. Desarrollo histórico Artículo principal: Historia de la mecánica cuántica La teoría cuántica fue desarrollada en su forma básica a lo largo de la primera mitad del siglo XX. El hecho de que la energía se intercambie de forma discreta se puso de relieve por hechos experimentales como los siguientes, inexplicables con las herramientas teóricas anteriores de la mecánica clásica o la electrodinámica: Fig. 1: La función de onda del electrón de un átomo de hidrógeno posee niveles de energía definidos y discretos denotados por un número cuántico n=1, 2, 3,... y valores definidos de momento angular caracterizados por la notación: s, p, d,... Las áreas brillantes en la figura corresponden a densidades elevadas de probabilidad de encontrar el electrón en dicha posición. Espectro de la radiación del cuerpo negro, resuelto por Max Planck con la cuantización de la energía. La energía total del cuerpo negro resultó que tomaba valores discretos más que continuos. Este fenómeno se llamó cuantización, y los intervalos posibles más pequeños entre los valores discretos son llamados quanta (singular: quantum, de la palabra latina para «cantidad», de ahí el nombre de mecánica cuántica). La magnitud de un cuanto es un valor fijo llamado constante de Planck, y que vale: 6,626 ×10-34 J·s. Bajo ciertas condiciones experimentales, los objetos microscópicos como los átomos o los electrones exhiben un comportamiento ondulatorio, como en la interferencia. Bajo otras condiciones, las mismas especies de objetos exhiben un comportamiento corpuscular, de partícula, («partícula» quiere decir un objeto que puede ser localizado en una región concreta del espacio), como en la dispersión de partículas. Este fenómeno se conoce como dualidad onda-partícula. Las propiedades físicas de objetos con historias asociadas pueden ser correlacionadas, en una amplitud prohibida para cualquier teoría clásica, solo pueden ser descritos con precisión si se hace referencia a ambos a la vez. Este fenómeno es llamado entrelazamiento cuántico y la desigualdad de Bell describe su diferencia con la correlación ordinaria. Las medidas de las violaciones de la desigualdad de Bell fueron algunas de las mayores comprobaciones de la mecánica cuántica. Explicación del efecto fotoeléctrico, dada por Albert Einstein, en que volvió a aparecer esa misteriosa necesidad de cuantizar la energía. Efecto Compton. El desarrollo formal de la teoría fue obra de los esfuerzos conjuntos de varios físicos y matemáticos de la época como Schrödinger, Heisenberg, Einstein, Dirac, Bohr y Von Neumann entre otros (la lista es larga). Algunos de los aspectos fundamentales de la teoría están siendo aún estudiados activamente. La mecánica cuántica ha sido también adoptada como la teoría subyacente a muchos campos de la física y la química, incluyendo la física de la materia condensada, la química cuántica y la física de partículas. La región de origen de la mecánica cuántica puede localizarse en la Europa central, en Alemania y Austria, y en el contexto histórico del primer tercio del siglo XX. Suposiciones más importantes Artículo principal: Interpretaciones de la mecánica cuántica Las suposiciones más importantes de esta teoría son las siguientes: Al ser imposible fijar a la vez la posición y el momento de una partícula, se renuncia al concepto de trayectoria, vital en mecánica clásica. En vez de eso, el movimiento de una partícula puede ser explicado por una función matemática que asigna, a cada punto del espacio y a cada instante, la probabilidad de que la partícula descrita se halle en tal posición en ese instante (al menos, en la interpretación de la Mecánica cuántica más usual, la probabilista o interpretación de Copenhague). A partir de esa función, o función de ondas, se extraen teóricamente todas las magnitudes del movimiento necesarias. Existen dos tipos de evolución temporal, si no ocurre ninguna medida el estado del sistema o función de onda evolucionan de acuerdo con la ecuación de Schrödinger, sin embargo, si se realiza una medida sobre el sistema, este sufre un «salto cuántico» hacia un estado compatible con los valores de la medida obtenida (formalmente el nuevo estado será una proyección ortogonal del estado original). Existen diferencias notorias entre los estados ligados y los que no lo están. La energía no se intercambia de forma continua en un estado ligado, sino en forma discreta lo cual implica la existencia de paquetes mínimos de energía llamados cuantos, mientras en los estados no ligados la energía se comporta como un continuo. Descripción de la teoría Interpretación de Copenhague Artículo principal: Interpretación de Copenhague Para describir la teoría de forma general es necesario un tratamiento matemático riguroso, pero aceptando una de las tres interpretaciones de la mecánica cuántica (a partir de ahora la Interpretación de Copenhague), el marco se relaja. La mecánica cuántica describe el estado instantáneo de un sistema (estado cuántico) con una función de onda que codifica la distribución de probabilidad de todas las propiedades medibles, u observables. Algunos observables posibles sobre un sistema dado son la energía, posición, momento y momento angular. La mecánica cuántica no asigna valores definidos a los observables, sino que hace predicciones sobre sus distribuciones de probabilidad. Las propiedades ondulatorias de la materia son explicadas por la interferencia de las funciones de onda. Estas funciones de onda pueden variar con el transcurso del tiempo. Esta evolución es determinista si sobre el sistema no se realiza ninguna medida aunque esta evolución es estocástica y se produce mediante colapso de la función de onda cuando se realiza una medida sobre el sistema (Postulado IV de la MC). Por ejemplo, una partícula moviéndose sin interferencia en el espacio vacío puede ser descrita mediante una función de onda que es un paquete de ondas centrado alrededor de alguna posición media. Según pasa el tiempo, el centro del paquete puede trasladarse, cambiar, de modo que la partícula parece estar localizada más precisamente en otro lugar. La evolución temporal determinista de las funciones de onda es descrita por la ecuación de Schrödinger. Algunas funciones de onda describen estados físicos con distribuciones de probabilidad que son constantes en el tiempo, estos estados se llaman estacionarios, son estados propios del operador hamiltoniano y tienen energía bien definida. Muchos sistemas que eran tratados dinámicamente en mecánica clásica son descritos mediante tales funciones de onda estáticas. Por ejemplo, un electrón en un átomo sin excitar se dibuja clásicamente como una partícula que rodea el núcleo, mientras que en mecánica cuántica es descrito por una nube de probabilidad estática que rodea al núcleo. Cuando se realiza una medición en un observable del sistema, la función de ondas se convierte en una del conjunto de las funciones llamadas funciones propias o estados propios del observable en cuestión. Este proceso es conocido como colapso de la función de onda. Las probabilidades relativas de ese colapso sobre alguno de los estados propios posibles son descritas por la función de onda instantánea justo antes de la reducción. Considerando el ejemplo anterior sobre la partícula en el vacío, si se mide la posición de la misma, se obtendrá un valor impredecible x. En general, es imposible predecir con precisión qué valor de x se obtendrá, aunque es probable que se obtenga uno cercano al centro del paquete de ondas, donde la amplitud de la función de onda es grande. Después de que se ha hecho la medida, la función de onda de la partícula colapsa y se reduce a una que esté muy concentrada en torno a la posición observada x. La ecuación de Schrödinger es determinista en el sentido de que, dada una función de onda a un tiempo inicial dado, la ecuación suministra una predicción concreta de qué función tendremos en cualquier tiempo posterior. Durante una medida, el eigen-estado al cual colapsa la función es probabilista y en este aspecto la mecánica cuántica es no determinista. Así que la naturaleza probabilista de la mecánica cuántica nace del acto de la medida. Esto conduce al problema de definir objetivamente en qué momento se produce la medida y la evolución pasa de lineal y determinista, a no-lineal y estocástica/aleatoria, cuestión que se conoce como problema de la medida y que, además de la interpretación de Copenhague, ha dado lugar a un número elevado de propuestas de resolución, conocidas como interpretaciones de la mecánica cuántica. Formulación matemática Artículos principales: Postulados de la mecánica cuántica y Notación braket. En la formulación matemática rigurosa, desarrollada por Dirac y von Neumann, los estados posibles de un sistema cuántico están representados por vectores unitarios (llamados estados) que pertenecen a un Espacio de Hilbert complejo separable (llamado el espacio de estados). Qué tipo de espacio de Hilbert es necesario en cada caso depende del sistema; por ejemplo, el espacio de estados para los estados de posición y momento es el espacio de funciones de cuadrado integrable Cada magnitud observable queda representada por un operador lineal hermítico definido sobre un dominio denso del espacio de estados. Cada estado propio de un observable corresponde a un eigenvector del operador, y el valor propio o eigenvalor asociado corresponde al valor del observable en aquel estado propio. El espectro de un operador puede ser continuo o discreto. La medida de un observable representado por un operador con espectro discreto solo puede tomar un conjunto numerable de posibles valores, mientras que los operadores con espectro continuo presentan medidas posibles en intervalos reales completos. Durante una medida, la probabilidad de que un sistema colapse a uno de los eigenestados viene dada por el cuadrado del valor absoluto del producto interno entre el estado propio o auto-estado (que podemos conocer teóricamente antes de medir) y el vector estado del sistema antes de la medida. Podemos así encontrar la distribución de probabilidad de un observable en un estado dado computando la descomposición espectral del operador correspondiente. El principio de incertidumbre de Heisenberg se representa por la aseveración de que los operadores correspondientes a ciertos observables no conmutan. Principio de Incertidumbre Una de las consecuencias del formalismo cuántico es el principio de incertidumbre. En su forma más familiar, establece que ninguna medición de una partícula cuántica puede implicar simultáneamente predicciones precisas para la medición de su posición y la medición de su momento. Tanto posición como momento son observables, esto significa que son representados por operadores hermíticos. El operador posición Dado un estado cuántico, la regla de Born nos permite encontrar valores para y de la misma manera para el momento: El principio de incertidumbre establece que En principio, cualquiera de las desviaciones estándar puede hacerse arbitrariamente pequeña, pero no ambas simultáneamente . Esta desigualdad se generaliza a pares arbitrarios de operadores autoadjuntos y proporciona el límite inferior en el producto de las desviaciones estándar: Otra consecuencia de la relación de conmutación canónica es que los operadores posición y momento son la transformada de Fourier del otro, de modo que una descripción de un objeto según su momento es la transformada de Fourier de su descripción según su posición. El hecho de que la dependencia en cantidad de movimiento sea la transformada de Fourier de la dependencia en posición significa que el operador de cantidad de movimiento es equivalente (hasta un factor de Aplicaciones En muchos aspectos, la tecnología moderna opera a una escala en la que los efectos cuánticos son significativos. Las aplicaciones importantes de la teoría cuántica incluyen la química cuántica, la óptica cuántica, la computación cuántica, los imanes superconductores, los diodos emisores de luz, el amplificador óptico y el láser, el transistor y semiconductores como el microprocesador, imágenes médicas y de investigación como la resonancia magnética y el microscopio electrónico. Las explicaciones de muchos fenómenos biológicos y físicos tienen su origen en la naturaleza del enlace químico, sobre todo la macromolécula del ADN. Relatividad y la mecánica cuántica Artículos principales: Teoría cuántica de campos y Segunda cuantización. El mundo moderno de la física se funda notablemente en dos teorías principales, la relatividad general y la mecánica cuántica, aunque ambas teorías usan principios aparentemente incompatibles. Los postulados que definen la teoría de la relatividad de Einstein y la teoría del quántum están apoyados por rigurosa y repetida evidencia empírica. Sin embargo, ambas se resisten a ser incorporadas dentro de un mismo modelo coherente. Desde mediados del siglo XX, aparecieron teorías cuánticas relativistas del campo electromagnético (electrodinámica cuántica) y las fuerzas nucleares (modelo electrodébil, cromodinámica cuántica), pero no se tiene una teoría cuántica relativista del campo gravitatorio que sea plenamente consistente y válida para campos gravitatorios intensos (existen aproximaciones en espacios asintóticamente planos). Todas las teorías cuánticas relativistas consistentes usan los métodos de la teoría cuántica de campos. En su forma ordinaria, la teoría cuántica abandona algunos de los supuestos básicos de la teoría de la relatividad, como por ejemplo el principio de localidad usado en la descripción relativista de la causalidad. El mismo Einstein había considerado absurda la violación del principio de localidad a la que parecía abocar la mecánica cuántica. La postura de Einstein fue postular que la mecánica cuántica si bien era consistente era incompleta. Para justificar su argumento y su rechazo a la falta de localidad y la falta de determinismo, Einstein y varios de sus colaboradores postularon la llamada paradoja de Einstein-Podolsky-Rosen (EPR), la cual demuestra que medir el estado de una partícula puede instantáneamente cambiar el estado de su socio enlazado, aunque las dos partículas pueden estar a una distancia arbitrariamente grande. Modernamente el paradójico resultado de la paradoja EPR se sabe es una consecuencia perfectamente consistente del llamado entrelazamiento cuántico. Es un hecho conocido que si bien la existencia del entrelazamiento cuántico efectivamente viola el principio de localidad, en cambio no viola la causalidad definido en términos de información, puesto que no hay transferencia posible de información. Si bien en su tiempo, parecía que la paradoja EPR suponía una dificultad empírica para la mecánica cuántica, y Einstein consideró que la mecánica cuántica en la interpretación de Copenhague podría ser descartada por experimento, décadas más tarde los experimentos de Alain Aspect (1981) revelaron que efectivamente la evidencia experimental parece apuntar en contra del principio de localidad. Y por tanto, el resultado paradójico que Einstein rechazaba como «sin sentido» parece ser lo que sucede precisamente en el mundo real."
ksampletext_wikipedia_phys_teoriadecuerdas: str = "Teoría de cuerdas. Las teorías de cuerdas son una serie de hipótesis científicas y modelos fundamentales de física teórica que asumen que las partículas subatómicas, aparentemente puntuales, son en realidad estados vibracionales de un objeto extendido más básico llamado cuerda o filamento. De acuerdo con estas teorías, un electrón no sería un punto sin estructura interna y de dimensión cero, sino una cuerda minúscula en forma de lazo vibrando en un espacio-tiempo de más de cuatro dimensiones; de hecho, el planteamiento matemático de esta teoría no funciona a menos que el universo tenga diez dimensiones. Mientras que un punto simplemente se movería por el espacio, una cuerda podría hacer algo más: vibrar de diferentes maneras. Si vibrase de cierto modo, veríamos un electrón; pero si lo hiciese de otro, veríamos un fotón, un cuark o cualquier otra partícula del modelo estándar dependiendo de la forma concreta en que estuviese vibrando. Estas teorías, ampliadas con otras como la de las supercuerdas o la teoría M, pretende alejarse de la concepción del punto-partícula. La siguiente formulación de una teoría de cuerdas se debe a Jöel Scherk y John Henry Schwarz, que en 1974 publicaron un artículo en el que mostraban que una teoría basada en objetos unidimensionales o cuerdas en lugar de partículas puntuales podía describir la fuerza gravitatoria, aunque estas ideas no recibieron en ese momento mucha atención hasta la primera revolución de supercuerdas de 1984. De acuerdo con la formulación de la teoría de cuerdas surgida de esta revolución, las teorías de cuerdas pueden considerarse de hecho un caso general de teoría de Kaluza-Klein cuantizada. Las ideas fundamentales son dos: Los objetos básicos de la teoría no serían partículas puntuales, sino objetos unidimensionales extendidos (en las cinco teorías de supercuerdas convencionales estos objetos eran unidimensionales o cuerdas; actualmente en la teoría-M se admiten también de dimensión superior o p-branas). Esto renormaliza algunos infinitos de los cálculos perturbativos. El espacio-tiempo en el que se mueven las cuerdas y p-branas de la teoría no sería el espacio-tiempo ordinario de cuatro dimensiones, sino un espacio de tipo Kaluza-Klein, en el que a las cuatro dimensiones convencionales se añaden seis dimensiones compactadas en forma de variedad de Calabi-Yau. Por tanto convencionalmente en la teoría de cuerdas existe una dimensión temporal, tres dimensiones espaciales ordinarias y seis dimensiones compactadas e inobservables en la práctica. La inobservabilidad de las dimensiones adicionales está relacionada con el hecho de que estas estarían compactadas, y solo serían relevantes a escalas pequeñas comparables con la longitud de Planck. Igualmente, con la precisión de medida convencional las cuerdas cerradas con una longitud similar a la longitud de Planck se asemejarían a partículas puntuales. Desarrollos posteriores Tras la introducción de la teoría de cuerdas, se consideró la conveniencia de introducir el principio de que la teoría fuera supersimétrica; es decir, que admitiera una simetría abstracta que relacionara fermiones y bosones. Actualmente la mayoría de teóricos de cuerdas trabajan en teorías supersimétricas; de ahí que la teoría de cuerdas actualmente se llame teoría de supercuerdas. Esta última teoría es básicamente una teoría de cuerdas supersimétrica; es decir, que es invariante bajo transformaciones de supersimetría. Actualmente existen cinco teorías de supercuerdas relacionadas con los cinco modos que se conocen de implementar la supersimetría en el modelo de cuerdas. Aunque dicha multiplicidad de teorías desconcertó a los especialistas durante más de una década, el saber convencional actual sugiere que las cinco teorías son casos límites de una teoría única sobre un espacio de 10 dimensiones (las tres del espacio y una temporal serían las 4 que ya conocemos más otras seis adicionales resabiadas o compactadas) y una que las engloba formando membranas de las cuales se podría escapar parte de la gravedad de ellas en forma de gravitones. Esta teoría única, llamada teoría M, de la que solo se conocerían algunos aspectos, fue conjeturada en 1995. Variantes de la teoría La teoría de supercuerdas es algo actual. En sus principios (mediados de los años 1980) aparecieron unas cinco teorías de cuerdas, las cuales después fueron identificadas como límites particulares de una sola teoría: la teoría M. Las cinco versiones de la teoría actualmente existentes, entre las que pueden establecerse varias relaciones de dualidad, son: La Teoría de cuerdas de Tipo I, donde aparecen tanto cuerdas y D-branas abiertas como cerradas, que se mueven sobre un espacio-tiempo de diez dimensiones. Las D-branas tienen una, cinco y nueve dimensiones espaciales. La Teoría de cuerdas de Tipo IIA. Es también una teoría de diez dimensiones, pero que emplea solo cuerdas y D-branas cerradas. Incorpora los gravitinos (partículas teóricas asociadas al gravitón mediante relaciones de supersimetría). Usa D-branas de dimensión 0, 2, 4, 6 y 8. La Teoría de cuerdas de Tipo IIB. Difiere de la teoría de tipo IIA principalmente en el hecho de que esta última es no quiral (conservando la paridad). La Teoría de cuerda heterótica SO(32) (Heterótica-O), basada en el grupo de simetría O(32). La Teoría de cuerda heterótica E8xE8 (Heterótica-E), basada en el grupo de Lie excepcional E8. Fue propuesta en 1987 por Gross, Harvey, Martinec y Rohm. El término teoría de cuerdas se refiere en realidad a las teorías de cuerdas bosónicas de 26 dimensiones y la teoría de supercuerdas de diez dimensiones, esta última descubierta al añadir supersimetría a la teoría de cuerdas bosónica. Hoy en día la teoría de cuerdas se suele referir a la variante supersimétrica, mientras que la antigua se conoce por el nombre completo de teoría de cuerdas bosónicas. En 1995, Edward Witten conjeturó que las cinco diferentes teorías de supercuerdas son casos límite de una desconocida teoría de once dimensiones llamada teoría-M. La conferencia donde Witten mostró algunos de sus resultados inició la llamada segunda revolución de supercuerdas. En esta teoría M intervienen como objetos animados físicos fundamentales no solo cuerdas unidimensionales, sino toda una variedad de objetos no perturbativos, extendidos en varias dimensiones, que se llaman colectivamente p-branas (este nombre es una aféresis de membrana). Controversia sobre la teoría Aunque la teoría de cuerdas, según sus defensores, pudiera llegar a convertirse en una de las teorías físicas más predictivas, capaz de explicar algunas de las propiedades más fundamentales de la naturaleza en términos geométricos, los físicos que han trabajado en ese campo hasta la fecha no han podido hacer predicciones concretas con la precisión necesaria para confrontarlas con datos experimentales, al parecer se necesita de una tecnología más avanzada para visualizar las partículas, lo cual llevara muchos años. Dichos problemas de predicción se deberían, según el autor, a que el modelo no es falsable, y por tanto, no es científico, o bien a que «la teoría de las supercuerdas es tan ambiciosa que solo puede ser del todo correcta o del todo equivocada. El único problema es que sus matemáticas son tan nuevas y tan difíciles que durante varias décadas no sabremos cuáles son», dicho esto en 1990. D. Gross, premio Nobel de física por su trabajo en el modelo estándar, se convirtió en un formidable luchador de la teoría de cuerdas, pero recientemente ha dicho: «No sabemos de qué estamos hablando». Si los teóricos de cuerdas se equivocan, no pueden equivocarse solo un poco. Si las nuevas dimensiones y las simetrías no existen, consideraremos a los teóricos de cuerdas unos de los mayores fracasados de la ciencia (...). Su historia constituirá una leyenda moral de cómo no hacer ciencia, de cómo no permitir que se sobrepasen tanto los límites, hasta el punto de convertir la conjetura teórica en fantasía. Lee Smolin Otras teorías En 1997, el físico teórico argentino Juan Maldacena propuso un sorprendente modelo del universo según el cual la gravedad surge de cuerdas infinitesimales, delgadas y vibrantes y puede ser reinterpretada en términos físicos. Así, este mundo de cuerdas matemáticamente intrincado, que existe en diez dimensiones espaciales, no sería más que un holograma: la acción real se desarrollaría en un cosmos plano, más simple y en el que no hay gravedad. La idea de Maldacena entusiasmó a los físicos, entre otras razones porque resolvía aparentes inconsistencias entre la física cuántica y la teoría de la gravedad de Einstein. Así, el argentino proporcionó a los científicos una piedra Rosetta matemática, una dualidad, que les permitía resolver los problemas de un modelo que parecían no tener respuesta en el otro, y viceversa. Pero a pesar de la validez de sus ideas aún no se había logrado hallar ninguna prueba rigurosa de su teoría. Según un artículo publicado en la revista científica Nature, Yoshifumi Hyakutake, de la Universidad de Ibaraki (Japón), y sus colegas, proporcionaron en dos de sus estudios, sino una prueba real, al menos una muestra convincente de que la conjetura de Maldacena es cierta. En uno de los estudios, Hyakutake calculó la energía interna de un agujero negro, la posición de su horizonte de sucesos (el límite entre el agujero negro y el resto del universo), su entropía y otras propiedades a partir de las predicciones de la teoría de cuerdas y de los efectos asociados a las partículas virtuales, que aparecen continuamente dentro y fuera de la existencia. En el otro, él y sus colaboradores calcularon la energía interna del correspondiente universo de dimensión inferior sin gravedad. Los dos cálculos informáticos coinciden. Parece que es un cálculo correcto, dice Maldacena, al tiempo que subraya que los hallazgos son una forma interesante de demostrar muchas ideas de la gravedad cuántica y la teoría de cuerdas. Numéricamente han confirmado, tal vez por primera vez, algo de lo que estábamos bastante seguros pero era todavía una conjetura: que la termodinámica de ciertos agujeros negros puede ser reproducida desde un universo dimensional inferior, explica Leonard Susskind, físico teórico de la Universidad de Stanford, en California, quien fue uno de los primeros teóricos en explorar la idea de universos holográficos. Falsacionismo y teoría de cuerdas Artículo principal: Criterio de demarcación La teoría de cuerdas o la teoría M podrían no ser falsables, según sus críticos. Diversos autores han declarado su preocupación de que la teoría de cuerdas no sea falsable y como tal, siguiendo las tesis del filósofo de la ciencia Karl Popper, la teoría de cuerdas sería equivalente a una pseudociencia. El filósofo de la ciencia Mario Bunge ha manifestado lo siguiente: La consistencia, la sofisticación y la belleza nunca son suficientes en la investigación científica. La teoría de cuerdas es sospechosa (de pseudociencia). Parece científica porque aborda un problema abierto que es a la vez importante y difícil, el de construir una teoría cuántica de la gravitación. Pero la teoría postula que el espacio físico tiene seis o siete dimensiones, en lugar de tres, simplemente para asegurarse consistencia matemática. Puesto que estas dimensiones extra son inobservables, y puesto que la teoría se ha resistido a la confirmación experimental durante más de tres décadas, parece ciencia ficción, o al menos, ciencia fallida. La física de partículas está inflada con sofisticadas teorías matemáticas que postulan la existencia de entidades extrañas que no interactúan de forma apreciable, o para nada en absoluto, con la materia ordinaria, y como consecuencia, quedan a salvo al ser indetectables. Puesto que estas teorías se encuentran en discrepancia con el conjunto de la Física, y violan el requerimiento de falsacionismo, pueden calificarse de pseudocientíficas, incluso aunque lleven pululando un cuarto de siglo y se sigan publicando en las revistas científicas más prestigiosas. Mario Bunge, 2006. Impacto de la promoción de la teoría en el mundo académico Smolin indica que la teoría de cuerdas se ha convertido en el principal camino de exploración de «las grandes cuestiones de la física» debido a una agresiva promoción, considerando que resulta prácticamente un «suicidio profesional» para cualquier joven físico teórico no ingresar en sus filas. Expone además que «a pesar de la escasa inversión en [...] otros campos de investigación, algunos de ellos han avanzado más que el de la teoría de cuerdas» e identifica los siguientes rasgos en las «comunidades de supercuerdas»: Tremenda autosuficiencia y conciencia de pertenecer a una élite. Comunidades monolíticas con gran uniformidad de opiniones sobre cuestiones abiertas, generalmente impuestas por los que constituyen la jerarquía de la comunidad. Sentido de identificación con el grupo parecido a la pertenencia a una comunidad religiosa o partido político. Sentido de frontera entre el grupo y otros expertos. Gran desinterés por las ideas y personas que no son del grupo. Una confianza excesiva en interpretar positivamente los resultados e incluso aceptarlos exclusivamente porque son creídos por la mayoría. Una falta de percepción del riesgo que conlleva una nueva teoría."
ksampletext_wikipedia_phys_electromagnetismo: str = "Electromagnetismo. El electromagnetismo es la rama de la física que estudia y unifica los fenómenos eléctricos y magnéticos en una sola teoría. El electromagnetismo describe la interacción de partículas cargadas con campos eléctricos y magnéticos. La interacción electromagnética es una de las cuatro fuerzas fundamentales del universo conocido. El electromagnetismo es una rama de la física que estudia los efectos producidos por el magnetismo, lo cual surge a partir de la corriente eléctrica. Por su parte, el magnetismo es la disciplina que examina los fenómenos asociados a los imanes. Su nombre proviene de Magnesia, un distrito en Asia Menor (actual Turquía), donde se descubrieron por primera vez las piedras llamadas magnetitas, que tienen la capacidad de atraer ciertos metales. El electromagnetismo abarca diversos fenómenos del mundo real, como por ejemplo la luz. La luz es un campo electromagnético oscilante que se irradia desde partículas cargadas aceleradas. Aparte de la gravedad, la mayoría de las fuerzas en la experiencia cotidiana son consecuencia del electromagnetismo. Los principios del electromagnetismo encuentran aplicaciones en diversas disciplinas afines, tales como las microondas, antenas, máquinas eléctricas, comunicaciones por satélite, bioelectromagnetismo, plasmas, investigación nuclear, la fibra óptica, la interferencia y la compatibilidad electromagnéticas, la conversión de energía electromecánica, la meteorología por radar, y la observación remota. Los dispositivos electromagnéticos incluyen transformadores, relés, radio/TV, teléfonos, motores eléctricos, líneas de transmisión, guías de onda y láseres. Los fundamentos de la teoría electromagnética fueron presentados por Michael Faraday y formulados por primera vez de modo completo por James Clerk Maxwell en 1865. La formulación consiste en cuatro ecuaciones diferenciales vectoriales que relacionan el campo eléctrico, el campo magnético y sus respectivas fuentes materiales (corriente eléctrica, polarización eléctrica y polarización magnética), conocidas como ecuaciones de Maxwell, lo que ha sido considerada como la «segunda gran unificación de la física», siendo la primera realizada por Isaac Newton. El estudio de los campos electromagnéticos se puede dividir en electrostática ,el estudio de las interacciones entre cargas en reposo, y la electrodinámica ,el estudio de las interacciones entre cargas en movimiento y la radiación,. La teoría clásica del electromagnetismo se basa en la fuerza de Lorentz y en las ecuaciones de Maxwell. Muchas propiedades ópticas y físicas de la materia también son explicados por la teoría electromagnética. El electromagnetismo es una teoría de campos; es decir, las explicaciones y predicciones que provee se basan en magnitudes físicas vectoriales o tensoriales dependientes de la posición en el espacio y del tiempo. El electromagnetismo describe los fenómenos físicos macroscópicos en los cuales intervienen cargas eléctricas en reposo y en movimiento, usando para ello campos eléctricos y magnéticos y sus efectos sobre las sustancias sólidas, líquidas y gaseosas. Por ser una teoría macroscópica, es decir, aplicable a un número muy grande de partículas y a distancias grandes respecto de las dimensiones de estas, el electromagnetismo no describe los fenómenos atómicos y moleculares. La electrodinámica cuántica proporciona la descripción cuántica de esta interacción, que puede ser unificada con la interacción nuclear débil según el modelo electrodébil. Espectro electromagnético. Historia Esta sección es un extracto de Historia del electromagnetismo.[editar] El físico danés Hans Christian Ørsted, realizando el experimento que le permitió descubrir la relación entre la electricidad y el magnetismo en 1820. La historia del electromagnetismo, considerada como el conocimiento y el uso registrado de las fuerzas electromagnéticas, data de hace más de dos mil años. En la antigüedad ya estaban familiarizados con los efectos de la electricidad atmosférica, en particular del rayo ya que las tormentas son comunes en las latitudes más meridionales, ya que también se conocía el fuego de San Telmo. Sin embargo, se comprendía poco la electricidad y no eran capaces de producir estos fenómenos. Durante los siglos XVII y XVIII, William Gilbert, Otto von Guericke, Stephen Gray, Benjamin Franklin, Alessandro Volta entre otros investigaron estos dos fenómenos de manera separada y llegaron a conclusiones coherentes con sus experimentos. A principios del siglo XIX, Hans Christian Ørsted encontró evidencia empírica de que los fenómenos magnéticos y eléctricos estaban relacionados. De ahí es que los trabajos de físicos como André-Marie Ampère, William Sturgeon, Joseph Henry, Georg Simon Ohm, Michael Faraday en ese siglo, son unificados por James Clerk Maxwell en 1861 con un conjunto de ecuaciones que describían ambos fenómenos como uno solo, como un fenómeno electromagnético. Las ahora llamadas ecuaciones de Maxwell demostraban que los campos eléctricos y los campos magnéticos eran manifestaciones de un solo campo electromagnético. Además, describía la naturaleza ondulatoria de la luz, mostrándola como una onda electromagnética. Con una sola teoría consistente que describía estos dos fenómenos antes separados, los físicos pudieron realizar varios experimentos prodigiosos e inventos muy útiles como la bombilla eléctrica por Thomas Alva Edison o el generador de corriente alterna por Nikola Tesla. El éxito predictivo de la teoría de Maxwell y la búsqueda de una interpretación coherente de sus implicaciones, fue lo que llevó a Albert Einstein a formular su teoría de la relatividad que se apoyaba en algunos resultados previos de Hendrik Antoon Lorentz y Henri Poincaré. En la primera mitad del siglo XX, con el advenimiento de la mecánica cuántica, el electromagnetismo tuvo que mejorar su formulación para que fuera coherente con la nueva teoría. Esto se logró en la década de 1940 cuando se completó una teoría cuántica electromagnética conocida como electrodinámica cuántica Historia de la teoría Hans Christian Oersted Originalmente, la electricidad y el magnetismo se consideraban dos fenómenos independientes entre sí. Este punto de vista cambió, sin embargo, con la publicación en 1873 del Tratado de electricidad y magnetismo de James Maxwell , que mostró que la interacción de cargas positivas y negativas está gobernada por una sola fuerza. Hay cuatro efectos principales, resultantes de estas interacciones, que han sido claramente demostrados por experimentos: Las cargas eléctricas son atraídas o repelidas entre sí con una fuerza inversamente proporcional al cuadrado de la distancia entre ellas: las cargas diferentes se atraen, las cargas iguales se repelen. Los polos magnéticos (o estados de polarización en puntos separados) se atraen o repelen entre sí de manera similar y siempre van en pares: cada polo norte no existe por separado del polo sur. La corriente eléctrica en un cable crea un campo magnético circular alrededor del cable, dirigido (en sentido horario o antihorario) según el flujo de corriente. Se induce una corriente en el bucle del cable cuando se acerca o aleja con relación al campo magnético, o cuando el imán se acerca o aleja del bucle del cable; la dirección de la corriente depende de la dirección de estos movimientos. André-Marie Ampere En preparación para la conferencia, la noche del 21 de abril de 1820, Hans Christian Oersted hizo una observación asombrosa. Cuando estaba compilando el material, notó que la aguja de la brújula se desviaba del polo norte magnético cuando se encendía y apagaba la corriente eléctrica de la batería que estaba usando. Esta desviación lo llevó a creer que los campos magnéticos emanan de todos los lados de un cable a través del cual fluye una corriente eléctrica, al igual que la luz y el calor se propagan en el espacio, y esa experiencia indica una conexión directa entre la electricidad y el magnetismo. Michael Faraday En el momento del descubrimiento, Oersted no ofreció una explicación satisfactoria de este fenómeno y no intentó presentar el fenómeno en cálculos matemáticos. Sin embargo, tres meses después, comenzó a realizar investigaciones más intensivas. Poco después, publicó los resultados de su investigación, demostrando que una corriente eléctrica crea un campo magnético cuando fluye a través de cables. En el sistema CGS , la unidad de inducción electromagnética, Oe, recibió su nombre de su contribución al campo del electromagnetismo. James Clerk Maxwell Las conclusiones de Oersted llevaron a un estudio intensivo de electrodinámica por parte de la comunidad científica mundial. Las obras de Dominique François Arago también se remontan a 1820 , quien advirtió que un cable por el que fluye una corriente eléctrica atrae limaduras de hierro . También magnetizó por primera vez alambres de hierro y acero, colocándolos dentro de una bobina de alambres de cobre por donde pasaba la corriente. También logró magnetizar la aguja colocándola en una bobina y descargando la Botella de Leyden a través de la bobina. Independientemente de Arago, Davy descubrió la magnetización del acero y el hierro por la corriente . Las primeras definiciones cuantitativas de la acción de una corriente sobre un imán de la misma forma se remontan a 1820 y pertenecen a científicos franceses Jean-Baptiste Biot y Félix Savart. Los experimentos de Oersted también influyeron en el físico francés André-Marie Ampere , quien presentó la ley electromagnética entre un conductor y una corriente en forma matemática. El descubrimiento de Oersted también representa un paso importante hacia un concepto de campo unificado. Esta unidad, que fue descubierta por Michael Faraday , completada por James Clerk Maxwell , y también refinada por Oliver Heaviside y Heinrich Hertz, es uno de los logros clave del siglo XIX en física matemática . Este descubrimiento tuvo implicaciones de gran alcance, una de las cuales fue comprender la naturaleza de la luz. La luz y otras ondas electromagnéticas toman la forma de fenómenos oscilatorios autopropagantes cuantificados del campo electromagnético llamados fotones. Diferentes frecuencias de vibración conducen a diferentes formas de radiación electromagnética: desde ondas de radio a bajas frecuencias, a luz visible a frecuencias medias, a rayos gamma a altas frecuencias. Oersted no fue la única persona que descubrió la conexión entre la electricidad y el magnetismo. En 1802, Giovanni Domenico Romagnosi , un jurista italiano, desvió una aguja magnética con descargas electrostáticas. Pero, de hecho, la investigación de Romagnosi no utilizó una celda galvánica y no había corriente continua como tal. El informe del descubrimiento se publicó en 1802 en un periódico italiano, pero la comunidad científica apenas lo notó en ese momento. Ramas Electrostática Artículo principal: Electrostática La electrostática es el estudio de los fenómenos asociados a los cuerpos cargados en reposo. Como describe la ley de Coulomb, estos cuerpos ejercen fuerzas entre sí. Su comportamiento se puede analizar en términos de la idea de un campo eléctrico que rodea cualquier cuerpo cargado, de manera que otro cuerpo cargado colocado dentro del campo estará sujeto a una fuerza proporcional a la magnitud de su carga y de la magnitud del campo en su ubicación. El que la fuerza sea atractiva o repulsiva depende de la polaridad de la carga. La electrostática tiene muchas aplicaciones, que van desde el análisis de fenómenos como tormentas eléctricas hasta el estudio del comportamiento de los tubos electrónicos. Un electroscopio usado para medir la carga eléctrica de un objeto. Cuando hablamos de electrostática nos referimos a los fenómenos que ocurren debido a una propiedad intrínseca y discreta de la materia, la carga, cuando es estacionaria o no depende del tiempo. La unidad de carga elemental, es decir, la más pequeña observable, es la carga que tiene el electrón. Se dice que un cuerpo está cargado eléctricamente cuando tiene exceso o falta de electrones en los átomos que lo componen. Por definición el defecto de electrones se la denomina carga positiva y al exceso carga negativa. La relación entre los dos tipos de carga es de atracción cuando son diferentes y de repulsión cuando son iguales. La carga elemental es una unidad muy pequeña para cálculos prácticos, por eso en el Sistema Internacional la unidad de carga eléctrica, el culombio, se define como la cantidad de carga transportada en un segundo por una corriente de un amperio de intensidad de corriente eléctrica. que equivale a la carga de 6,25 x 1018 electrones. El movimiento de electrones por un conductor se denomina corriente eléctrica y la cantidad de carga eléctrica que pasa por unidad de tiempo se define como la intensidad de corriente. Se pueden introducir más conceptos como el de diferencia de potencial o el de resistencia, que nos conducirían ineludiblemente al área de circuitos eléctricos, y todo eso se puede ver con más detalle en el artículo principal. El nombre de la unidad de carga se debe a Coulomb, quien en 1785 llegó a una relación matemática de la fuerza eléctrica entre cargas puntuales, que ahora se la conoce como ley de Coulomb: Entre dos cargas puntuales Las cargas elementales al no encontrarse solas se las debe tratar como una distribución de ellas. Por eso debe implementarse el concepto de campo, definido como una región del espacio donde existe una magnitud escalar o vectorial dependiente o independiente del tiempo. Así el campo eléctrico Campo eléctrico de cargas puntuales. lim Y así finalmente llegamos a la expresión matemática que define el campo eléctrico: Es importante conocer el alcance de este concepto de campo eléctrico: nos brinda la oportunidad de conocer cuál es su intensidad y qué ocurre con una carga en cualquier parte de dicho campo sin importar el conocimiento de qué lo provoca. Una forma de obtener qué cantidad de fuerza eléctrica pasa por cierto punto o superficie del campo eléctrico es usar el concepto de flujo eléctrico. Este flujo eléctrico El matemático y físico, Carl Friedrich Gauss, demostró que la cantidad de flujo eléctrico en un campo es igual al cociente entre la carga encerrada por la superficie en la que se calcula el flujo, (1) Véanse también: Carga eléctrica, Ley de Coulomb, Campo eléctrico, Potencial eléctrico y Ley de Gauss. Magnetostática Artículo principal: Magnetostática Líneas de fuerza de una barra magnética. La magnetósfera de la Tierra, empujada por el viento solar. No fue sino hasta el año de 1820, cuando Hans Christian Ørsted descubrió que el fenómeno magnético estaba ligado al eléctrico, que se obtuvo una teoría científica para el magnetismo. La presencia de una corriente eléctrica, o sea, de un flujo de carga debido a una diferencia de potencial, genera una fuerza magnética que no varía en el tiempo. Si tenemos una carga q a una velocidad Para determinar el valor de ese campo magnético, Jean Baptiste Biot en 1820, dedujo una relación para corrientes estacionarias, ahora conocida como ley de Biot-Savart: Donde (2) Además en la magnetostática existe una ley comparable a la de Gauss en la electrostática, la ley de Ampère. Esta ley nos dice que la circulación en un campo magnético es igual a la densidad de corriente que exista en una superficie cerrada: Cabe indicar que esta ley de Gauss es una generalización de la ley de Biot-Savart. Además que las fórmulas expresadas aquí son para cargas en el vacío, para más información consúltese los artículos principales. Véanse también: Ley de Ampère, Corriente eléctrica, Campo magnético, Ley de Biot-Savart y Momento magnético dipolar. Electrodinámica clásica Artículo principal: Electrodinámica La electrodinámica es el estudio de los fenómenos asociados a los cuerpos cargados en movimiento y a los campos eléctricos y magnéticos variables. Dado que una carga en movimiento produce un campo magnético, la electrodinámica se refiere a efectos tales como el magnetismo, la radiación electromagnética, y la inducción electromagnética, incluyendo las aplicaciones prácticas, tales como el generador eléctrico y el motor eléctrico. Esta área de la electrodinámica, conocida como electrodinámica clásica, fue sistemáticamente explicada por James Clerk Maxwell, y las ecuaciones de Maxwell describen los fenómenos de esta área con gran generalidad. Una novedad desarrollada más reciente es la electrodinámica cuántica, que incorpora las leyes de la teoría cuántica a fin de explicar la interacción de la radiación electromagnética con la materia. Paul Dirac, Heisenberg y Wolfgang Pauli fueron pioneros en la formulación de la electrodinámica cuántica. La electrodinámica es inherentemente relativista y da unas correcciones que se introducen en la descripción de los movimientos de las partículas cargadas cuando sus velocidades se acercan a la velocidad de la luz. Se aplica a los fenómenos involucrados con aceleradores de partículas y con tubos electrónicos funcionando a altas tensiones y corrientes. En las secciones anteriores se han descrito campos eléctricos y magnéticos que no variaban con el tiempo. Pero los físicos a finales del siglo XIX descubrieron que ambos campos estaban ligados y así un campo eléctrico en movimiento, una corriente eléctrica que varíe, genera un campo magnético y un campo magnético de por sí implica la presencia de un campo eléctrico. Entonces, lo primero que debemos definir es la fuerza que tendría una partícula cargada que se mueva en un campo magnético y así llegamos a la unión de las dos fuerzas anteriores, lo que hoy conocemos como la fuerza de Lorentz: (3) Entre 1890 y 1900 Liénard y Wiechert calcularon el campo electromagnético asociado a cargas en movimiento arbitrario, resultado que se conoce hoy como potenciales de Liénard-Wiechert. Por otro lado, para generar una corriente eléctrica en un circuito cerrado debe existir una diferencia de potencial entre dos puntos del circuito, a esta diferencia de potencial se la conoce como fuerza electromotriz o «fem». Esta fuerza electromotriz es proporcional a la rapidez con que el flujo magnético varía en el tiempo, esta ley fue encontrada por Michael Faraday y es la interpretación de la inducción electromagnética, así un campo magnético que varía en el tiempo induce a un campo eléctrico, a una fuerza electromotriz. Matemáticamente se representa como: (4) El físico James Clerk Maxwell de 1861 relacionó las anteriormente citadas ecuaciones para la ley de Gauss ((1)), ley de Gauss para el campo magnético ((2)), ley de Faraday ((4)) e introdujo el concepto de una corriente de desplazamiento como una densidad de corriente efectiva para llegar a la ley de Ampère generalizada (5): (5) Las cuatro ecuaciones, tanto en su forma diferencial como en la integral aquí descritas, son fruto de la reformulación del trabajo de Maxwell realizada por Oliver Heaviside y Heinrich Rudolf Hertz. Pero el verdadero poder de estas ecuaciones, más la fuerza de Lorentz (3), se centra en que juntas son capaces de describir cualquier fenómeno electromagnético, además de las consecuencias físicas que posteriormente se describirán. Esquema de una onda electromagnética. La genialidad del trabajo de Maxwell es que sus ecuaciones describen un campo eléctrico que va ligado inequívocamente a un campo magnético perpendicular a este y a la dirección de su propagación, este campo es ahora llamado campo electromagnético. Dichos campos podían ser derivados de un potencial escalar. La solución de las ecuaciones de Maxwell implicaba la existencia de una onda que se propagaba a la velocidad de la luz, con lo que además de unificar los fenómenos eléctricos y magnéticos la teoría formulada por Maxwell predecía con absoluta certeza los fenómenos ópticos. Así la teoría predecía a una onda que, contraria a las ideas de la época, no necesitaba un medio de propagación; la onda electromagnética se podía propagar en el vacío debido a la generación mutua de los campos magnéticos y eléctricos. Esta onda a pesar de tener una velocidad constante, la velocidad de la luz c, puede tener diferente longitud de onda y consecuentemente dicha onda transporta energía. La radiación electromagnética recibe diferentes nombres al variar su longitud de onda, como rayos gamma, rayos X, espectro visible, etc.; pero en su conjunto recibe el nombre de espectro electromagnético. Espectro electromagnético. Véanse también: Fuerza de Lorentz, Fuerza electromotriz, Ley de Ampère, Ecuaciones de Maxwell y Campo electromagnético. Electrodinámica relativista Artículo principal: Tensor de campo electromagnético Clásicamente, al fijar un sistema de referencia, se puede descomponer los campos eléctricos y magnéticos del campo electromagnético. Pero, en la teoría de la relatividad especial, al tener a un observador con movimiento relativo respecto al sistema de referencia, este medirá efectos eléctricos y magnéticos diferentes de un mismo fenómeno electromagnético. El campo eléctrico y la inducción magnética a pesar de ser elementos vectoriales no se comportan como magnitudes físicas vectoriales, por el contrario la unión de ambos constituye otro ente físico llamado tensor y en este caso el tensor de campo electromagnético. Así, la expresión para el campo electromagnético es: Esta representación se conoce como formulación covariante tetradimensional del electromagnetismo. Las expresiones covariantes para las ecuaciones de Maxwell (7) y la fuerza de Lorentz (6) se reducen a. Dada la forma de las ecuaciones anteriores, si el dominio sobre el que se extiende el campo electromagnético es simplemente conexo el campo electromagnético puede expresarse como la derivada exterior de un cuadrivector llamado potencial vector, relacionado con los potenciales del electromagnetismo clásico de la siguiente manera: Donde: La relación entre el cuadrivector potencial y el tensor de campo electromanético resulta ser: El hecho de que la interacción electromagnética pueda representarse por un (cuadri)vector que define completamente el campo electromagnético es la razón por la que se afirma en el tratamiento moderno que la interacción electromagnética es un campo vectorial. En relatividad general el tratamiento del campo electromagnético en un espacio-tiempo curvo es similar al presentado aquí para el espacio-tiempo de Minkowski, solo que las derivadas parciales respecto a las coordenadas deben substituirse por derivadas covariantes. Electrodinámica cuántica Diagrama de Feynman mostrando la fuerza electromagnética entre dos electrones por medio del intercambio de un fotón virtual. Artículo principal: Electrodinámica cuántica Posteriormente a la revolución cuántica de inicios del siglo XX, los físicos se vieron forzados a buscar una teoría cuántica de la interacción electromagnética. El trabajo de Einstein con el efecto fotoeléctrico y la posterior formulación de la mecánica cuántica sugerían que la interacción electromagnética se producía mediante el intercambio de partículas elementales llamadas fotones. La nueva formulación cuántica lograda en la década de 1940 describe la interacción entre los bosones, o partículas portadoras de la interacción, y las otras partículas portadoras de materia (los fermiones). La electrodinámica cuántica es principalmente una teoría cuántica de campos renormalizada. Su desarrollo fue obra de Sinitiro Tomonaga, Julian Schwinger, Richard Feynman y Freeman Dyson alrededor de los años 1947 a 1949. En la electrodinámica cuántica, la interacción entre partículas viene descrita por un lagrangiano que posee simetría local, concretamente simetría de gauge. Para la electrodinámica cuántica, el campo de gauge donde los fermiones interactúan es el campo electromagnético, descrito en esta teoría como los estados de bosones (fotones, en este caso) portadores de la interacción. Matemáticamente, el lagrangiano para la interacción entre fermiones mediante intercambio de fotones viene dado por: Donde el significado de los términos son: Véanse también: Teoría cuántica de campos, Ecuación de Dirac y Modelo estándar."
ksampletext_wikipedia_phys_teoriadelarelatividad: str = "Teoría de la relatividad. La teoría de la relatividad incluye tanto a la teoría de la relatividad especial como la de la relatividad general, formuladas principalmente por Albert Einstein a principios del siglo XX, que pretendían resolver la incompatibilidad existente entre la mecánica newtoniana y el electromagnetismo. La teoría de la relatividad especial, publicada en 1905, trata de la física del movimiento de los cuerpos en ausencia de fuerzas gravitatorias, en el que se hacían compatibles las ecuaciones de Maxwell del electromagnetismo con una reformulación de las leyes del movimiento. En la teoría de la relatividad especial, Einstein, Lorentz y Minkowski, entre otros, unificaron los conceptos de espacio y tiempo, en un tramado tetradimensional al que se le denominó espacio-tiempo. La relatividad especial fue una teoría revolucionaria para su época, con la que el tiempo absoluto de Newton quedó relegado y conceptos como la invariabilidad en la velocidad de la luz, la dilatación del tiempo, la contracción de la longitud y la equivalencia entre masa y energía fueron introducidos. Además, con las formulaciones de la relatividad especial, las leyes de la Física son invariantes en todos los sistemas de referencia inerciales; como consecuencia matemática, se encuentra como límite superior de velocidad a la de la luz y se elimina la causalidad determinista que tenía la física hasta entonces. Hay que indicar que las leyes del movimiento de Newton son un caso particular de esta teoría donde la masa, al viajar a velocidades muy pequeñas, no experimenta variación alguna en longitud ni se transforma en energía y al tiempo se le puede considerar absoluto. La teoría de la relatividad general, publicada en 1915, es una teoría de la gravedad que reemplaza a la gravedad newtoniana, aunque coincide numéricamente con ella para campos gravitatorios débiles y velocidades «pequeñas». La teoría general se reduce a la teoría especial en presencia de campos gravitatorios. La relatividad general estudia la interacción gravitatoria como una deformación en la geometría del espacio-tiempo. En esta teoría se introducen los conceptos de la curvatura del espacio-tiempo como la causa de la interacción gravitatoria, el principio de equivalencia que dice que para todos los observadores locales inerciales las leyes de la relatividad especial son invariantes y la introducción del movimiento de una partícula por líneas geodésicas. La relatividad general no es la única teoría que describe la atracción gravitatoria, pero es la que más datos relevantes comprobables ha encontrado. Anteriormente, a la interacción gravitatoria se la describía matemáticamente por medio de una distribución de masas, pero en esta teoría no solo la masa percibe esta interacción, sino también la energía, mediante la curvatura del espacio-tiempo y por eso se necesita otro lenguaje matemático para poder describirla, el cálculo tensorial. Muchos fenómenos, como la curvatura de la luz por acción de la gravedad y la desviación en la órbita de Mercurio, son perfectamente predichos por esta formulación. La relatividad general también abrió otro campo de investigación en la física, conocido como cosmología y es ampliamente utilizado en la astrofísica. El 7 de marzo de 2010, la Academia Israelí de Ciencias exhibió públicamente los manuscritos originales de Einstein (redactados en 1905). El documento, que contiene 46 páginas de textos y fórmulas matemáticas escritas a mano, fue donado por Einstein a la Universidad Hebrea de Jerusalén en 1925 con motivo de su inauguración. Conceptos principales Artículo principal: Anexo:Glosario de relatividad El supuesto básico de la teoría de la relatividad es que la localización de los sucesos físicos, tanto en el tiempo como en el espacio, son relativos al estado de movimiento del observador: así, la longitud de un objeto en movimiento o el instante en que algo sucede, a diferencia de lo que sucede en mecánica newtoniana, no son invariantes absolutos, y diferentes observadores en movimiento relativo entre sí diferirán respecto a ellos (las longitudes y los intervalos temporales, en relatividad son relativos y no absolutos). Relatividad especial Artículo principal: Teoría de la relatividad especial La teoría de la relatividad especial, también llamada teoría de la relatividad restringida, fue publicada por Albert Einstein en 1905 y describe la física del movimiento en el marco de un espacio-tiempo plano. Esta teoría describe correctamente el movimiento de los cuerpos incluso a grandes velocidades y sus interacciones electromagnéticas, se usa básicamente para estudiar sistemas de referencia inerciales (no es aplicable para problemas astrofísicos donde el campo gravitatorio desempeña un papel importante). Estos conceptos fueron presentados anteriormente por Poincaré y Lorentz, que son considerados como precursores de la teoría. Si bien la teoría resolvía un buen número de problemas del electromagnetismo y daba una explicación del experimento de Michelson y Morley, no proporciona una descripción relativista adecuada del campo gravitatorio. Tras la publicación del artículo de Einstein, la nueva teoría de la relatividad especial fue aceptada en unos pocos años por prácticamente la totalidad de los físicos y los matemáticos. De hecho, Poincaré o Lorentz habían estado muy cerca de llegar al mismo resultado que Einstein. La forma geométrica definitiva de la teoría se debe a Hermann Minkowski, antiguo profesor de Einstein en la Politécnica de Zúrich; acuñó el término «espacio-tiempo» (Raumzeit) y le dio la forma matemática adecuada.[nota 1] El espacio-tiempo de Minkowski es una variedad tetradimensional en la que se entrelazaban de una manera indisoluble las tres dimensiones espaciales y el tiempo. En este espacio-tiempo de Minkowski, el movimiento de una partícula se representa mediante su línea de universo (Weltlinie), una curva cuyos puntos vienen determinados por cuatro variables distintas: las tres dimensiones espaciales ( Relatividad general Esta sección es un extracto de Relatividad general.[editar] Representación artística de la explosión de la supernova SN 2006gy, situada a 238 millones de años luz. De ser válido el principio de acción a distancia, las perturbaciones de origen gravitatorio de este estallido nos afectarían inmediatamente y más tarde nos llegarían las de origen electromagnético, que se transmiten a la velocidad de la luz. Esquema bidimensional de la curvatura del espacio-tiempo (cuatro dimensiones) generada por una masa esférica. La teoría general de la relatividad o relatividad general es una teoría del campo gravitatorio y de los sistemas de referencia generales, publicada por Albert Einstein en 1915 y 1916. El nombre de la teoría se debe a que generaliza la llamada teoría especial de la relatividad y el principio de relatividad para un observador arbitrario. Los principios fundamentales introducidos en esta generalización son el principio de equivalencia, que describe la aceleración y la gravedad como aspectos distintos de la misma realidad, la noción de la curvatura del espacio-tiempo y el principio de covariancia generalizado. La teoría de la relatividad general propone que la propia geometría del espacio-tiempo se ve afectada por la presencia de materia, de lo cual resulta una teoría relativista del campo gravitatorio. De hecho la teoría de la relatividad general predice que el espacio-tiempo no será plano en presencia de materia y que la curvatura del espacio-tiempo será percibida como un campo gravitatorio. La intuición básica de Einstein fue postular que en un punto concreto no se puede distinguir experimentalmente entre un cuerpo acelerado uniformemente y un campo gravitatorio uniforme. La teoría general de la relatividad permitió también reformular el campo de la cosmología. Einstein expresó el propósito de la teoría de la relatividad general para aplicar plenamente el programa de Ernst Mach de la relativización de todos los efectos de inercia, incluso añadiendo la llamada constante cosmológica a sus ecuaciones de campo para este propósito. Este punto de contacto real de la influencia de Ernst Mach fue claramente identificado en 1918, cuando Einstein distingue lo que él bautizó como el principio de Mach (los efectos inerciales se derivan de la interacción de los cuerpos) del principio de la relatividad general, que se interpreta ahora como el principio de covariancia general. El matemático alemán David Hilbert escribió e hizo públicas las ecuaciones de la covariancia antes que Einstein, ello resultó en no pocas acusaciones de plagio contra Einstein, pero probablemente sea más porque es una teoría (o perspectiva) geométrica. La misma postula que la presencia de masa o energía «curva» el espacio-tiempo, y esta curvatura afecta la trayectoria de los cuerpos móviles e incluso la trayectoria de la luz. Formalismo de la teoría de la relatividad Representación de la línea de universo de una partícula. Como no es posible reproducir un espacio-tiempo de cuatro dimensiones, en la figura se representa solo la proyección sobre 2 dimensiones espaciales y una temporal. Partículas En la teoría de la relatividad una partícula puntual queda representada por un par Campos Cuando se consideran campos o distribuciones continuas de masa, se necesita algún tipo de generalización para la noción de partícula. Un campo físico posee momentum y energía distribuidos en el espacio-tiempo, el concepto de cuadrimomento se generaliza mediante el llamado tensor de energía-impulso que representa la distribución en el espacio-tiempo tanto de energía como de momento lineal. A su vez un campo dependiendo de su naturaleza puede representarse por un escalar, un vector o un tensor. Por ejemplo el campo electromagnético se representa por un tensor de segundo orden totalmente antisimétrico o 2-forma. Si se conoce la variación de un campo o una distribución de materia, en el espacio y en el tiempo entonces existen procedimientos para construir su tensor de energía-impulso. Magnitudes físicas En relatividad, estas magnitudes físicas son representadas por vectores 4-dimensionales o bien por objetos matemáticos llamados tensores, que generalizan los vectores, definidos sobre un espacio de cuatro dimensiones. Matemáticamente estos 4-vectores y 4-tensores son elementos definidos del espacio vectorial tangente al espacio-tiempo (y los tensores se definen y se construyen a partir del fibrado tangente o cotangente de la variedad que representa el espacio-tiempo). Correspondencia entre E3[nota 2] y M4[nota 3] Espacio tridimensional euclídeo Espacio-tiempo de Minkowski Punto Suceso Longitud Intervalo Velocidad Cuadrivelocidad Momentum Cuadrimomentum Igualmente además de cuadrivectores, se definen cuadritensores (tensores ordinarios definidos sobre el fibrado tangente del espacio-tiempo concebido como variedad lorentziana). La curvatura del espacio-tiempo se representa por un 4-tensor (tensor de cuarto orden), mientras que la energía y el momento de un medio continuo o el campo electromagnético se representan mediante 2-tensores (simétrico el tensor de energía-impulso, antisimétrico el de campo electromagnético). Los cuadrivectores son, de hecho, 1-tensores, en esta terminología. En este contexto se dice que una magnitud es un invariante relativista si tiene el mismo valor para todos los observadores, obviamente todos los invariantes relativistas son escalares (0-tensores), frecuentemente formados por la contracción de magnitudes tensoriales. El intervalo relativista El intervalo relativista puede definirse en cualquier espacio-tiempo, sea este plano como en la relatividad especial, o curvo como en relatividad general. Sin embargo, por simplicidad, discutiremos inicialmente el concepto de intervalo para el caso de un espacio-tiempo plano. El tensor métrico del espacio-tiempo plano de Minkowski se designa con la letra El intervalo, la distancia tetradimensional, se representa mediante la expresión Reproducción de un cono de luz, en el que se representan dos dimensiones espaciales y una temporal (eje de ordenadas). El observador se sitúa en el origen, mientras que el futuro y el pasado absolutos vienen representados por las partes inferior y superior del eje temporal. El plano correspondiente a t = 0 se denomina plano de simultaneidad o hipersuperficie de presente (también llamado «diagrama de Minkowski»). Los sucesos situados dentro de los conos están vinculados al observador por intervalos temporales. Los que se sitúan fuera, por intervalos espaciales. Los intervalos pueden ser clasificados en tres categorías: Intervalos espaciales (cuando Los intervalos nulos pueden ser representados en forma de cono de luz, popularizados por el celebérrimo libro de Stephen Hawking, Breve Historia del Tiempo. Sea un observador situado en el origen, el futuro absoluto (los sucesos que serán percibidos por el individuo) se despliega en la parte superior del eje de ordenadas, el pasado absoluto (los sucesos que ya han sido percibidos por el individuo) en la parte inferior, y el presente percibido por el observador en el punto 0. Los sucesos que están fuera del cono de luz no nos afectan, y por lo tanto se dice de ellos que están situados en zonas del espacio-tiempo que no tienen relación de causalidad con la nuestra. Imaginemos, por un momento, que en la galaxia Andrómeda, situada a 2.5 millones de años luz de nosotros, sucedió un cataclismo cósmico hace 100 000 años. Dado que, primero: la luz de Andrómeda tarda 2 millones de años en llegar hasta nosotros y segundo: nada puede viajar a una velocidad superior a la de los fotones, es evidente, que no tenemos manera de enterarnos de lo que sucedió en dicha Galaxia hace tan solo 100 000 años. Se dice, por lo tanto, que el intervalo existente entre dicha hipotética catástrofe cósmica y nosotros, observadores del presente, es un intervalo espacial ( Imagen de la galaxia Andrómeda, tomada por el telescopio Spitzer, tal como era hace 2.5 millones de años (por estar situada a 2.5 millones de años luz). Los sucesos acaecidos 1 000 000 de años atrás se observarán desde la Tierra dentro de un millón y medio de años. Se dice, por tanto, que entre tales eventos y nosotros existe un intervalo espacial. Análisis El único problema con esta hipótesis, es que al entrar en un agujero negro, se anula el espacio-tiempo, y como ya sabemos, algo que contenga algún volumen o masa, debe tener como mínimo un espacio donde ubicarse, el tiempo en ese caso, no tiene mayor importancia, pero el espacio juega un rol muy importante en la ubicación de volúmenes, por lo que esto resulta muy improbable, pero no imposible para la tecnología. Podemos escoger otro episodio histórico todavía más ilustrativo: El de la estrella de Belén, tal y como fue interpretada por Johannes Kepler. Este astrónomo alemán consideraba que dicha estrella se identificaba con una supernova que tuvo lugar el año 5 a. C., cuya luz fue observada por los astrónomos chinos contemporáneos, y que vino precedida en los años anteriores por varias conjunciones planetarias en la constelación de Piscis. Esa supernova probablemente estalló miles de años atrás, pero su luz no llegó a la Tierra sino hasta el año 5 a. C. De ahí que el intervalo existente entre dicho evento y las observaciones de los astrónomos egipcios y megalíticos (que tuvieron lugar varios siglos antes de Cristo) sea un intervalo espacial, pues la radiación de la supernova nunca pudo llegarles. Por el contrario, la explosión de la supernova por un lado, y las observaciones realizadas por los tres magos en Babilonia y por los astrónomos chinos en el año 5 a. C. por el otro, están unidas entre sí por un intervalo temporal, ya que la luz sí pudo alcanzar a dichos observadores. El tiempo propio y el intervalo se relacionan mediante la siguiente equivalencia: Esta invarianza se expresa a través de la llamada geometría hiperbólica: La ecuación del intervalo Cuadrivelocidad, aceleración y cuadrimomentum Artículos principales: Cuadrivelocidad y Cuadrimomento. En el espacio-tiempo de Minkowski, las propiedades cinemáticas de las partículas se representan fundamentalmente por tres magnitudes: La cuadrivelocidad (o tetravelocidad), la cuadriaceleración y el cuadrimomentum (o tetramomentum). La cuadrivelocidad es un cuadrivector tangente a la línea de universo de la partícula, relacionada con la velocidad coordenada de un cuerpo medida por un observador en reposo cualquiera, esta velocidad coordenada se define con la expresión newtoniana La velocidad coordenada de un cuerpo con masa depende caprichosamente del sistema de referencia que escojamos, mientras que la cuadrivelocidad propia es una magnitud que se transforma de acuerdo con el principio de covariancia y tiene un valor siempre constante equivalente al intervalo dividido entre el tiempo propio ( La cuadriaceleración puede ser definida como la derivada temporal de la cuadrivelocidad ( Junto con los principios de invarianza del intervalo y la cuadrivelocidad, juega un papel fundamental la ley de conservación del cuadrimomentum. Es aplicable aquí la definición newtoniana del momentum Como tanto la velocidad de la luz como el cuadrimomentum son magnitudes conservadas, también lo es su producto, al que se le da el nombre de energía conservada Componentes Magnitud del cuadrimomentum Magnitud en cuerpos con masa Magnitud en fotones (masa = 0) Energía Energía en cuerpos con masa (cuerpos en reposo, p=0) Energía en fotones (masa en reposo = 0) La aparición de la Relatividad Especial puso fin a la secular disputa que mantenían en el seno de la mecánica clásica las escuelas de los mecanicistas y los energetistas. Los primeros sostenían, siguiendo a Descartes y Huygens, que la magnitud conservada en todo movimiento venía constituida por el momentum total del sistema, mientras que los energetistas ,que tomaban por base los estudios de Leibniz, consideraban que la magnitud conservada venía conformada por la suma de dos cantidades: La fuerza viva, equivalente a la mitad de la masa multiplicada por la velocidad al cuadrado ( La mecánica newtoniana dio la razón a ambos postulados, afirmando que tanto el momentum como la energía son magnitudes conservadas en todo movimiento sometido a fuerzas conservativas. Sin embargo, la Relatividad Especial dio un paso más allá, por cuanto a partir de los trabajos de Einstein y Minkowski el momentum y la energía dejaron de ser considerados como entidades independientes y se les pasó a considerar como dos aspectos, dos facetas de una única magnitud conservada: el cuadrimomentum. Componentes y magnitud de los diferentes conceptos cinemáticos Concepto Componentes Expresión algebraica Partículas con masa Fotones Intervalo ot =0} Cuadrivelocidad no definida Aceleración (sistemas inerciales) ot =0} (sistemas no inerciales) Aceleración no definida Cuadrimomentum El tensor de energía-impulso (Tab) Artículo principal: Tensor de energía-impulso Tensor de tensión-energía Tres son las ecuaciones fundamentales que en física newtoniana describen el fenómeno de la gravitación universal: la primera, afirma que la fuerza gravitatoria entre dos cuerpos es proporcional al producto de sus masas e inversamente proporcional al cuadrado de su distancia (1); la segunda, que el potencial gravitatorio ( Sin embargo, estas ecuaciones no son compatibles con la Relatividad Especial por dos razones: En primer lugar la masa no es una magnitud absoluta, sino que su medición deriva en resultados diferentes dependiendo de la velocidad relativa del observador. De ahí que la densidad de masa En segundo lugar, si el concepto de espacio es relativo, también lo es la noción de densidad. Es evidente que la contracción del espacio producida por el incremento de la velocidad de un observador, impide la existencia de densidades que permanezcan invariables ante las transformaciones de Lorentz. Por todo ello, resulta necesario prescindir del término O lo que es lo mismo: El componente donde Además, si los componentes del tensor se miden por un observador en reposo relativo respecto al fluido, entonces, el tensor métrico viene constituido simplemente por la métrica de Minkowski: diag diag Puesto que además la tetravelocidad del fluido respecto al observador en reposo es: como consecuencia de ello, los coeficientes del tensor de tensión-energía son los siguientes: Parte de la materia que cae en el disco de acreción de un agujero negro es expulsada a gran velocidad en forma de chorros. En supuestos como este, los efectos gravitomagnéticos pueden llegar a alcanzar cierta importancia. Donde Podemos, a partir del tensor de tensión-energía, calcular cuánta masa contiene un determinado volumen del fluido: Retomando la definición de este tensor expuesta unas líneas más arriba, se puede definir al coeficiente Del mismo modo, es posible deducir matemáticamente a partir del tensor de tensión-energía la definición newtoniana de presión, introduciendo en la mentada ecuación cualquier par de índices que sean diferentes de cero: La hipersuperficie Finalmente, derivamos parcialmente ambos miembros de la ecuación respecto al tiempo, y teniendo en cuenta que la fuerza no es más que la tasa de incremento temporal del momentum obtenemos el resultado siguiente: Que contiene la definición newtoniana de la presión como fuerza ejercida por unidad de superficie. El tensor electromagnético (Fab) Artículo principal: Tensor de campo electromagnético Las ecuaciones deducidas por el físico escocés James Clerk Maxwell demostraron que electricidad y magnetismo no son más que dos manifestaciones de un mismo fenómeno físico: el campo electromagnético. Ahora bien, para describir las propiedades de este campo los físicos de finales del siglo XIX debían utilizar dos vectores diferentes, los correspondientes los campos eléctrico y magnético. Fue la llegada de la relatividad especial la que permitió describir las propiedades del electromagnetismo con un solo objeto geométrico, el vector cuadripotencial, cuyo componente temporal se correspondía con el potencial eléctrico, mientras que sus componentes espaciales eran los mismos que los del potencial magnético. De este modo, el campo eléctrico puede ser entendido como la suma del gradiente del potencial eléctrico más la derivada temporal del potencial magnético: y el campo magnético, como el rotacional del potencial magnético: abla imes A} Las propiedades del campo electromagnético pueden también expresarse utilizando un tensor de segundo orden denominado tensor de Faraday y que se obtiene diferenciando exteriormente al vector cuadripotencial La fuerza de Lorentz puede deducirse a partir de la siguiente expresión."

ksampletext_wikipedia_chem_valenciaquimica: str = "Valencia (química). La valencia es el número de electrones que le faltan o debe ceder un elemento químico para completar su último nivel de energía. Estos electrones son los que pone en juego durante una reacción química o para establecer un enlace químico con otro elemento. Hay elementos con más de una valencia, por ello fue reemplazado este concepto con el de números de oxidación que finalmente representa lo mismo. A través del siglo XX, el concepto de valencia ha evolucionado en una amplia gama de aproximaciones para describir el enlace químico, incluyendo la estructura de Lewis (1916), la teoría del enlace de valencia (1927), la teoría de los orbitales moleculares (1928), la teoría de repulsión de pares electrónicos de la capa de valencia (1958) y todos los métodos avanzados de química cuántica. En química, históricamente se ha considerado la valencia de un elemento como su capacidad de combinarse con otros elementos, para formar compuestos moleculares. Diferentes autores utilizan distintas definiciones de valencia, lo que crea un debate sobre cuál es la correcta. Por ejemplo, algunos autores confunden número de coordinación con valencia o estado de oxidación con valencia, a pesar de que esos tres términos son diferentes entre sí. Historia La etimología de la palabra «valencia» proviene de 1543, significando molde, del latín valentía poder, capacidad, y el significado químico refiriéndose al «poder combinante de un elemento» está registrado desde 1884, del alemán Valenz. En 1890, William Higgins publicó bocetos sobre lo que él llamó combinaciones de partículas últimas, que esbozaban el concepto de enlaces de valencia. Si, por ejemplo, de acuerdo a Higgins, la fuerza entre la partícula última de oxígeno y la partícula última de nitrógeno era 6, luego la fuerza del enlace debería ser dividida acordemente, y de modo similar para las otras combinaciones de partículas últimas: estas son las de la tabla periódica. Combinaciones de partículas últimas de William Higgins (1789). Sin embargo, el origen no exacto de la teoría de las valencias químicas puede ser rastreado a una publicación de Edward Frankland, en la que combinó las viejas teorías de los radicales libres y «teoría de tipos» con conceptos sobre afinidad química para mostrar que ciertos elementos tienen la tendencia a combinarse con otros elementos para formar compuestos conteniendo tres equivalentes del átomo unido, por ejemplo, en los grupos de tres átomos (vg. NO3, NH3, NI3, etc.) o cinco, por ejemplo en los grupos de cinco átomos (vg. N2O5, NH4O, P2O5, etc.) Es en este modo, según Franklin, que sus afinidades están mejor satisfechas. Siguiendo estos ejemplos y postulados, Franklin declaró cuán obvio esto es que: Una tendencia o ley prevalece (aquí), y que, no importa qué puedan ser los caracteres de los átomos que se unen, el poder combinante de los elementos atrayentes, si me puedo permitir el término, se satisface siempre por el mismo número de estos átomos. Descripción La capacidad combinatoria o afinidad de un átomo de un elemento dado viene determinada por el número de átomos de hidrógeno con los que se combina. En el metano, el carbono tiene una valencia de 4; en el amoníaco, el nitrógeno tiene una valencia de 3; en el agua, el oxígeno tiene una valencia de 2; y en el cloruro de hidrógeno, el cloro tiene una valencia de 1. El cloro, al tener una valencia de uno, puede sustituir al hidrógeno. El fósforo tiene una valencia de 5 en el pentacloruro de fósforo, PCl 5. Los diagramas de valencia de un compuesto representan la conectividad de los elementos, con líneas dibujadas entre dos elementos, a veces llamadas enlaces, que representan una valencia saturada para cada elemento. Las dos tablas siguientes muestran algunos ejemplos de diferentes compuestos, sus diagramas de valencia y las valencias para cada elemento del compuesto. Compuesto H Hidrógeno CH Metano C Propano C Propileno C Acetileno Diagrama  Valencias Hidrógeno: 1 Carbono: 4 Hidrógeno: 1 Carbono: 4 Hidrogeno: 1 Carbono: 4 Hidrógeno: 1 Carbono: 4 Hidrógeno: 1 Compuesto NH Amoníaco NaCN Cianuro de sodio PSCl Thiophosphoryl chloride H Ácido sulfhídrico H 2SO Ácido sulfúrico H Dithionic acid Cl Óxido perclórico XeO Tetraóxido de xenón Diagrama   Valencias Nitrógeno: 3 Hidrógeno: 1 Sodio: 1 Carbono: 4 Nitrógeno: 3 Fósforo: 5 Azufre: 2 Cloro: 1 Azufre: 2 Hidrógeno: 1 Azufre: 6 Oxígeno: 2 Hidrógeno: 1 Azufre: 6 Oxígeno: 2 Hidrógeno: 1 Cloro: 7 Oxígeno: 2 Xenón: 8 Oxígeno: 2 Definiciones modernas La valencia es definida por la IUPAC como: El número máximo de átomos univalentes (originalmente átomos de hidrógeno o de cloro) que pueden combinarse con un átomo del elemento considerado, o con un fragmento, o por el que puede sustituirse un átomo de este elemento. Una descripción moderna alternativa es: El número de átomos de hidrógeno que pueden combinarse con un elemento en un hidruro binario o el doble del número de átomos de oxígeno que se combinan con un elemento en su óxido u óxidos. Esta definición difiere de la definición de la IUPAC, ya que se puede decir que un elemento tiene más de una valencia. Una definición moderna muy similar dada en un artículo reciente define la valencia de un átomo particular en una molécula como el número de electrones que un átomo utiliza en la unión, con dos fórmulas equivalentes para calcular la valencia: valencia = número de electrones en la capa de valencia del átomo libre - número de electrones no enlazantes del átomo en la molécula valencia = número de enlaces + carga formal. Sin embargo esta definición de valencia es incorrecta. Por esta definición, el átomo de nitrógeno en el ion de amonio [NH4]+ es pentavalente, y en el ion de amida [NH2]- es monovalente, que obviamente es falso, porque el átomo de nitrógeno en los iones de amonio y amida es trivalente. Por lo tanto, esta definición es engañosa porque puede dar resultados falsos. Desarrollo histórico La etimología de la palabra valencia se remonta a 1425, con el significado de extracto, preparado, del latín valentia fuerza, capacidad, del anterior valor valía, valor, y el significado químico referido al poder combinatorio de un elemento se registra a partir de 1884, del alemán Valenz. William Higgins combinaciones de partículas últimas (1789) El concepto de valencia se desarrolló en la segunda mitad del siglo XIX y ayudó a explicar con éxito la estructura molecular de los compuestos inorgánicos y orgánicos.La búsqueda de las causas subyacentes de la valencia condujo a las teorías modernas del enlace químico, incluyendo el átomo cúbico (1902), estructura de Lewiss (1916), teoría del enlace de valencia (1927), orbitales molecularess (1928), teoría de repulsión de pares de electrones de la corteza de valencia (1958), y todos los métodos avanzados de la química cuántica. En 1789, William Higgins publicó opiniones sobre lo que denominó combinaciones de partículas últimas, que prefiguraron el concepto de enlaces de valencia. Si, por ejemplo, según Higgins, la fuerza entre la partícula última de oxígeno y la partícula última de nitrógeno fuera 6, entonces la intensidad de la fuerza se dividiría en consecuencia, y lo mismo para las demás combinaciones de partículas últimas (véase la ilustración). Sin embargo, el origen exacto de la teoría de las valencias químicas se remonta a un trabajo de Edward Frankland de 1852, en el que combinó la antigua teoría de los radicales con ideas sobre la afinidad química para demostrar que ciertos elementos tienen tendencia a combinarse con otros elementos para formar compuestos que contienen 3, es decir, en los grupos de 3 átomos (por ejemplo, NO 3, NH 3, NI 3, etc.) o 5, es decir, en los grupos de 5 átomos (por ejemplo, NO 5, NH 4O, PO 5, etc.), equivalentes de los elementos unidos. Según él, ésta es la manera en que mejor se satisfacen sus afinidades, y siguiendo estos ejemplos y postulados, declara lo obvio que es que Una tendencia o ley prevalece (aquí), y es que, cualesquiera que sean los caracteres de los átomos que se unen, el poder combinatorio del elemento que atrae, si se me permite el término, se satisface siempre con el mismo número de estos átomos. En 1857 August Kekulé propuso valencias fijas para muchos elementos, como 4 para el carbono, y las utilizó para proponer fórmulas estructurales para muchas moléculas de orgánica, que todavía se aceptan hoy en día. Lothar Meyer en su libro de 1864, Die modernen Theorien der Chemie, que contenía una primera versión de la tabla periódica con 28 elementos, clasificó por primera vez los elementos en seis familias según su valencia. Los trabajos sobre la organización de los elementos por peso atómico, hasta entonces se habían visto obstaculizados por el uso generalizado de peso equivalentes para los elementos, en lugar de pesos atómicos. La mayoría de los químicos del siglo XIX definían la valencia de un elemento como el número de sus enlaces sin distinguir diferentes tipos de valencia o de enlace. Sin embargo, en 1893 Alfred Werner describió metal de transición complejos de coordinaciónes como [Co(NH 6]Cl 3, en los que distinguió valencias principales y subsidiarias (en alemán: Hauptvalenz y Nebenvalenz), correspondientes a los conceptos modernos de estado de oxidación y número de coordinación respectivamente. Para los elementos del grupo principal, en 1904 Richard Abegg consideró valencias positivas y negativas (estados de oxidación máximo y mínimo), y propuso la regla de Abegg según la cual su diferencia es a menudo 8. Electrones y valencia El modelo de Rutherford del átomo nuclear (1911) demostró que el exterior de un átomo está ocupado por electrones, lo que sugiere que los electrones son responsables de la interacción de los átomos y de la formación de enlaces químicos. En 1916, Gilbert N. Lewis explicó la valencia y el enlace químico en términos de una tendencia de los átomos (del grupo principal) a alcanzar una octeto estable de 8 electrones de valencia. Según Lewis, el enlace covalente conduce a octetos por la compartición de electrones, y el enlace iónico conduce a octetos por la transferencia de electrones de un átomo a otro. El término covalencia se atribuye a Irving Langmuir, quien afirmó en 1919 que el número de pares de electrones que un átomo dado comparte con los átomos adyacentes se denomina covalencia de ese átomo. El prefijo co- significa juntos, de modo que un enlace covalente significa que los átomos comparten una valencia. Posteriormente, ahora es más común hablar de enlaces covalentes en lugar de valencia, que ha caído en desuso en trabajos de nivel superior a partir de los avances en la teoría del enlace químico, pero sigue siendo ampliamente utilizado en estudios elementales, donde proporciona una introducción heurística al tema. En la década de 1930, Linus Pauling propuso que también existen enlaces covalentes polares, que son intermedios entre los covalentes y los iónicos, y que el grado de carácter iónico depende de la diferencia de electronegatividad de los dos átomos enlazados. Pauling también consideró las moléculas hipervalentes, en las que los elementos del grupo principal tienen valencias aparentes superiores a la máxima de 4 permitida por la regla del octeto. Por ejemplo, en la molécula de hexafluoruro de azufre (SF 6), Pauling consideró que el azufre forma 6 enlaces verdaderos de dos electrones utilizando orbitales atómicos híbridos sp3d2, que combinan un orbital s, tres orbitales p y dos orbitales d. Sin embargo, más recientemente, cálculos cuántico-mecánicos sobre esta molécula y otras similares han demostrado que el papel de los orbitales d en el enlace es mínimo, y que la molécula SF 6 debería describirse como una molécula con 6 enlaces covalentes polares (en parte iónicos) formados por sólo cuatro orbitales en el azufre (un s y tres p) de acuerdo con la regla del octeto, junto con seis orbitales en los fluorinos. Cálculos similares en moléculas de metales de transición muestran que el papel de los orbitales p es menor, de modo que un orbital s y cinco orbitales d en el metal son suficientes para describir el enlace. Tipos de valencia Valencia positiva máxima: es el número positivo que refleja la máxima capacidad de combinación de un átomo. Este número coincide con el grupo de la tabla periódica de los elementos al cual pertenece. Por ejemplo, el cloro (Cl) pertenece al grupo 7, por lo que su valencia positiva máxima es 7. Valencia negativa solo para el grupo A no para el grupo B: es el número negativo que refleja la capacidad que tiene un átomo de combinarse con otro pero que esté actuando con valencia positiva. Este número negativo se puede determinar contando lo que le falta a la valencia positiva máxima para llegar a 8, pero con signo -. Por ejemplo: a la valencia máxima positiva del átomo de cloro es 7, por lo que le falta un electrón para cumplir el octeto, entonces su valencia negativa será -1. Vista general El concepto fue desarrollado a mediados del siglo XIX, en un intento por racionalizar la fórmula química de compuestos químicos diferentes. En 1919, Irving Langmuir, tomó prestado el término para explicar el modelo del átomo cúbico de Gilbert N. Lewis al enunciar que el número de pares de electrones que cualquier átomo dado comparte con el átomo adyacente es denominado la covalencia del átomo. El prefijo co- significa «junto», así que un enlace covalente significa que los átomos comparten valencia. De ahí, si un átomo, por ejemplo, tiene una valencia +1, significa que perdió un electrón, y otro con una valencia de -1, significa que tiene un electrón adicional. Luego, un enlace entre estos dos átomos resultaría porque se complementarían o compartirían sus tendencias en el balance de la valencia. Subsecuentemente, actualmente es más común hablar de enlace covalente en vez de valencia, que ha caído en desuso del nivel más alto de trabajo, con los avances en la teoría del enlace químico, pero aún es usado ampliamente en estudios elementales donde provee una introducción heurística a la materia. Definición del número de enlaces Se creía originalmente que el número de enlaces formados por un elemento dado era una propiedad química fija y, en efecto, en muchos casos, es una buena aproximación. Por ejemplo, en muchos de sus compuestos, el carbono forma cuatro enlaces, el oxígeno dos y el hidrógeno uno. Sin embargo, pronto se hizo evidente que, para muchos elementos, la valencia podría variar entre compuestos diferentes. Uno de los primeros ejemplos en ser identificado era el fósforo, que algunas veces se comporta como si tuviera una valencia de tres, y otras como si tuviera una valencia de cinco. Un método para resolver este problema consiste en especificar la valencia para cada compuesto individual: aunque elimina mucho de la generalidad del concepto, esto ha dado origen a la idea de número de oxidación (usado en la nomenclatura Stock y a la notación lambda en la nomenclatura IUPAC de química inorgánica). Definición de IUPAC La Unión Internacional de Química Pura y Aplicada (IUPAC) ha hecho algunos intentos de llegar a una definición desambigua de valencia. La versión actual, adoptada en 1994, es la siguiente: La valencia es el máximo número de átomos univalentes (originalmente átomos de hidrógeno o cloro) que pueden combinarse con un átomo del elemento en consideración, o con un fragmento, o para el cual un átomo de este elemento puede ser sustituido. Esta definición reimpone una valencia única para cada elemento a expensas de despreciar, en muchos casos, una gran parte de su química. La mención del hidrógeno y el cloro es por razones históricas, aunque ambos en la práctica forman compuestos principalmente en los que sus átomos forman un enlace simple. Las excepciones en el caso del hidrógeno incluyen el ion bifluoruro, [HF2]−, y los diversos hidruros de boro tales como el diborano: estos son ejemplos de enlace de tres centros. El cloro forma un número de fluoruro,ClF, ClF3 y ClF5,y su valencia, de acuerdo a la definición de la IUPAC, es cinco. El flúor es el elemento para el que el mayor número de átomos se combinan con átomos de otros elementos: es univalente en todos sus compuestos, excepto en el ion [H2F]+. En efecto, la definición IUPAC sólo puede ser resuelta al fijar las valencias del hidrógeno y el flúor como uno, convención que ha sido seguida acá. Valencias de los elementos. Estados de oxidación de los elementos Las valencias de la mayoría de los elementos se basan en el fluoruro más alto conocido."
ksampletext_wikipedia_chem_quimicaorganica: str = "Química orgánica. La química orgánica es la rama de la química que estudia una clase numerosa de moléculas, que, en su mayoría contienen carbono formando enlaces covalentes: carbono-carbono o carbono-hidrógeno y otros heteroátomos, también conocidos como compuestos orgánicos. Debido a la omnipresencia del carbono en los compuestos que esta rama de la química estudia, esta disciplina también es llamada química del carbono. Historia El trabajo de Friedrich Wöhler sobre la síntesis de la urea es considerado por muchos como el inicio de la química orgánica, y en particular de la síntesis orgánica. La química orgánica constituyó o se instituyó como disciplina en los años treinta. El desarrollo de nuevos métodos de análisis de las sustancias de origen animal y vegetal, basados en el empleo de disolventes, como el éter o el alcohol, permitió el aislamiento de un gran número de sustancias orgánicas que recibieron el nombre de principios inmediatos. La aparición de la química orgánica se asocia a menudo al descubrimiento, en 1828, por el químico alemán Friedrich Wöhler, de que la sustancia inorgánica cianato de amonio podía convertirse en urea, una sustancia orgánica que se encuentra en la orina de muchos animales. Antes de este descubrimiento, los químicos creían que para sintetizar sustancias orgánicas, era necesaria la intervención de lo que llamaban la fuerza vital, es decir, los organismos vivos. El experimento de Wöhler rompió la barrera entre sustancias orgánicas e inorgánicas. De esta manera, los químicos modernos consideran compuestos orgánicos a aquellos que contienen carbono e hidrógeno, y otros elementos (que pueden ser uno o más), siendo los más comunes: oxígeno, nitrógeno, azufre y los halógenos. En 1856, sir William Henry Perkin, mientras trataba de estudiar la quinina, accidentalmente fabricó el primer colorante orgánico ahora conocido como malva de Perkin. La diferencia entre la química orgánica y la química biológica,es que en la segunda las moléculas de ADN tienen una historia y, por ende, en su estructura nos hablan de su historia, del pasado en el que se han constituido, mientras que una molécula orgánica, creada hoy, es solo testigo de su presente, sin pasado y sin evolución histórica. Cronología Artículo principal: Cronología de la Química orgánica 1675: Lémery clasifica los productos químicos naturales, según su origen en minerales, vegetales y animales 1784: Antoine Lavoisier demuestra que todos los productos vegetales y animales están formados básicamente por carbono e hidrógeno y, en menor proporción, nitrógeno, oxígeno y azufre 1807: Jöns Jacob Berzelius clasifica los productos químicos en: Orgánicos: los que proceden de organismos vivos. Inorgánicos: los que proceden de la materia inanimada. 1816: Michel Eugène Chevreul prepara distintos jabones a partir de diferentes fuentes de ácidos grasos y diversas bases, produciendo así distintas sales de ácidos grasos (o jabones), que no resultaron ser más que productos orgánicos nuevos derivados de productos naturales (grasas animales y vegetales). 1828: Friedrich Wöhler, a partir de sustancias inorgánicas y con técnicas normales de laboratorio, sintetizó la sustancia urea, la segunda sustancia orgánica obtenida artificialmente, luego del oxalato de amonio. Fórmula desarrollada urea 1856: Sir William Perkin sintetiza el primer colorante orgánico por accidente. 1865: August Kekulé propuso que los átomos de carbono que forman el benceno se unen formando cadenas cerradas o anillos. Primeros compendios La tarea de presentar la química orgánica de manera sistemática y global se realizó mediante una publicación surgida en Alemania, fundada por el químico Friedrich Konrad Beilstein (1838-1906). Su Handbuch der organischen Chemie (Manual de la química orgánica) comenzó a publicarse en Hamburgo en 1880 y consistió en dos volúmenes que recogían información de unos quince mil compuestos orgánicos conocidos. Cuando la Deutsche chemische Gesellschaft (Sociedad Alemana de Química) trató de elaborar la cuarta reedición, en la segunda década del siglo XX, la cifra de compuestos orgánicos se había multiplicado por diez. Treinta y siete volúmenes fueron necesarios para la edición básica, que aparecieron entre 1916 y 1937. Un suplemento de 27 volúmenes se publicó en 1938, recogiendo información aparecida entre 1910 y 1919. En la actualidad, se está editando el Fünftes Ergänzungswerk (quinta serie complementaria), que recoge la documentación publicada entre 1960 y 1979. Para ofrecer con más prontitud sus últimos trabajos, el Beilstein Institut ha creado el servicio Beilstein On line, que funciona desde 1988. Recientemente, se ha comenzado a editar periódicamente un CD-ROM, Beilstein Current Facts in Chemistry, que selecciona la información química procedente de importantes revistas. Actualmente, la citada información está disponible a través de internet. El alma de la química orgánica: el carbono Estructura tetraédrica del metano. La gran cantidad de compuestos orgánicos que existen tiene su explicación en las características del átomo de carbono, que tiene cuatro electrones en su capa de valencia: según la regla del octeto necesita ocho para completarla, por lo que forma cuatro enlaces (valencia = 4) con otros átomos. Esta especial configuración electrónica da lugar a una variedad de posibilidades de hibridación orbital del átomo de carbono (hibridación química). La molécula orgánica más sencilla que existe es el metano. En esta molécula, el carbono presenta hibridación sp3, con los átomos de hidrógeno formando un tetraedro. El carbono forma enlaces covalentes con facilidad para alcanzar una configuración estable, estos enlaces los forma con facilidad con otros carbonos, lo que permite formar frecuentemente cadenas abiertas (lineales o ramificadas) y cerradas (anillos). Clasificación de compuestos orgánicos La clasificación de los compuestos orgánicos puede realizarse de diversas maneras: atendiendo a su origen (natural o sintético), a su estructura (p. ej.: alifático o aromático), a su funcionalidad (p. ej.: alcoholes o cetonas), o a su peso molecular (p. ej.: monómeros o polímeros). Clasificación según su origen La clasificación de los compuestos orgánicos según el origen es de dos tipos: naturales o sintéticos. A menudo, los de origen natural se entiende que son los presentes en los seres vivos, pero no siempre es así, ya que algunas moléculas orgánicas también se sintetizan ex-vivo, es decir en ambientes inertes, como por ejemplo el ácido fórmico en el cometa Halle-Bopp. Natural In-vivo Los compuestos orgánicos presentes en los seres vivos o biosintetizados constituyen una gran familia de compuestos orgánicos. Su estudio tiene interés en medicina, farmacia, perfumería, cocina y muchos otros campos más. Carbohidratos Los carbohidratos están compuestos fundamentalmente de carbono (C), oxígeno (O) e hidrógeno (H). Son a menudo llamados azúcares, pero esta nomenclatura no es del todo correcta. Tienen una gran presencia en el reino vegetal (fructosa, celulosa, almidón, alginatos), pero también en el animal (glucógeno, glucosa). Se suelen clasificar según su grado de polimerización en: Monosacáridos (glucosa, fructosa, ribosa y desoxirribosa) Disacáridos (sacarosa, lactosa, maltosa) Trisacáridos (maltotriosa, rafinosa) Polisacáridos (alginatos, ácido algínico, celulosa, almidón, etc.) Lípidos Los lípidos son un conjunto de moléculas orgánicas, la mayoría biomoléculas, compuestas principalmente por carbono e hidrógeno y en menor medida oxígeno, aunque también pueden contener fósforo, azufre y nitrógeno. Tienen como característica principal el ser hidrófobas (insolubles en agua) y solubles en disolventes orgánicos como la bencina, el benceno y el cloroformo. En el uso coloquial, a los lípidos se les llama incorrectamente grasas, ya que las grasas son solo un tipo de lípidos procedentes de animales. Los lípidos cumplen funciones diversas en los organismos vivientes, entre ellas la de reserva energética (como los triglicéridos), la estructural (como los fosfolípidos de las bicapas) y la reguladora (como las hormonas esteroides). Proteínas fórmula química de un aminoácido. Las proteínas son polipéptidos, es decir están formados por la polimerización de péptidos, y estos por la unión de aminoácidos. Pueden considerarse así poliamidas naturales, ya que el enlace peptídico es análogo al enlace amida. Comprenden una familia muy importante de moléculas en los seres vivos, pero en especial en el reino animal. Por otra parte, son producto de la expresión de genes contenidos en el ADN. Algunos ejemplos de proteínas son el colágeno, las fibroínas, o la seda de araña. Ácidos nucleicos Los ácidos nucleicos son polímeros formados por la repetición de monómeros denominados nucleótidos, unidos mediante enlaces fosfodiéster. Se forman, así, largas cadenas; algunas moléculas de ácidos nucleicos llegan a alcanzar pesos moleculares gigantescos, con millones de nucleótidos encadenados. Están formados por la moléculas de carbono, hidrógeno, oxígeno, nitrógeno y fosfato. Los ácidos nucleicos almacenan la información genética de los organismos vivos y son los responsables de la transmisión hereditaria. Existen dos tipos básicos, el ADN y el ARN. Moléculas pequeñas Estructura de la testosterona. Una hormona, que se puede clasificar como molécula pequeña en el argot-químico-orgánico. Las moléculas pequeñas son compuestos orgánicos de peso molecular moderado (generalmente se consideran pequeñas aquellas con peso molecular menor a 1000 g/mol) y que aparecen en pequeñas cantidades en los seres vivos, pero no por ello su importancia es menor. A ellas pertenecen distintos grupos de hormonas como la testosterona, el estrógeno u otros grupos como los alcaloides. Las moléculas pequeñas tienen gran interés en la industria farmacéutica por su relevancia en el campo de la medicina. Ex-vivo Son compuestos orgánicos que han sido sintetizados sin la intervención de ningún ser vivo, en ambientes extracelulares y extravirales. Procesos geológicos Sello alemán de 1964 conmemorativo de la descripción de la estructura del benceno por Friedrich August Kekulé en 1865. El petróleo es una sustancia clasificada como mineral en la cual se presentan una gran cantidad de compuestos orgánicos. Muchos de ellos, como el benceno, son empleados por el hombre tal cual, pero muchos otros son tratados o derivados para conseguir una gran cantidad de compuestos orgánicos, como por ejemplo los monómeros para la síntesis de materiales poliméricos o plásticos. Procesos atmosféricos El sistema climático está constituido por la atmósfera, la hidrósfera, la biosfera, la geosfera y sus interacciones. Las variaciones en el equilibrio climático pueden generar diversos procesos como el calentamiento global, el efecto invernadero o la disminución de la capa de ozono. Procesos de síntesis planetaria En el año 2000 el ácido fórmico, un compuesto orgánico sencillo, también fue hallado en la cola del cometa Hale-Bopp. Puesto que la síntesis orgánica de estas moléculas es inviable bajo las condiciones espaciales, este hallazgo parece sugerir que a la formación del sistema solar debió anteceder un periodo de calentamiento durante su colapso final. Sintético Desde la síntesis de Wöhler de la urea un altísimo número de compuestos orgánicos han sido sintetizados químicamente para beneficio humano. Estos incluyen fármacos, desodorantes, perfumes, detergentes, jabones, fibras textiles sintéticas, materiales plásticos, polímeros en general, o colorantes orgánicos. Cadenas hidrocarbonadas sencillas Hidrocarburos El compuesto más simple es el metano, un átomo de carbono con cuatro de hidrógeno (valencia = 1), pero también puede darse la unión carbono-carbono, formando cadenas de distintos tipos, ya que pueden darse enlaces simples, dobles o triples. Cuando el resto de enlaces de estas cadenas son con hidrógeno, se habla de hidrocarburos, que pueden ser: Saturados: con enlaces covalentes simples, alcanos. Insaturados: con dobles enlaces covalentes (alquenos) o triples (alquinos). Hidrocarburos cíclicos: Hidrocarburos saturados con cadena cerrada, como el ciclohexano. Aromáticos: estructura cíclica. Radicales y ramificaciones de cadena Estructura de un hidrocarburo ramificado nombrado 5-butil-3,9-dimetil-undecano. Los radicales son fragmentos de cadenas de carbonos que cuelgan de la cadena principal. Su nomenclatura se hace con la raíz correspondiente (en el caso de un carbono met-, dos carbonos et-, tres carbonos prop-, cuatro carbonos but-, cinco carbonos pent-, seis carbonos hex-, y así sucesivamente) y el sufijo -il. Además, se indica con un número, colocado delante, la posición que ocupan. El compuesto más simple que se puede hacer con radicales es el 2-metilpropano. En caso de que haya más de un radical, se nombrarán por orden alfabético de las raíces. Por ejemplo, el 2-etil, 5-metil, 8-butil, 10-docoseno. Clasificación según los grupos funcionales Los compuestos orgánicos también pueden contener otros elementos, también otros grupos de átomos además del carbono e hidrógeno, llamados grupos funcionales. Un ejemplo es el grupo hidroxilo, que forma los alcoholes: un átomo de oxígeno enlazado a uno de hidrógeno (-OH), al que le queda una valencia libre. Asimismo también existen funciones alqueno (dobles enlaces), éteres, ésteres, aldehídos, cetonas, carboxílicos, carbamoilos, azo, nitro o sulfóxido, entre otros. Alquino Alquino Hidroxilo Hidroxilo Éter Éter Amina Amina Aldehído Aldehído Cetona Cetona Carboxilo Carboxilo Éster Éster Amida Amida Azo Azo Nitro Nitro Sulfóxido Sulfóxido Monómero de la celulosa. Oxigenados Son cadenas de carbonos con uno o varios átomos de oxígeno. Pueden ser: Alcoholes: Las propiedades físicas de un alcohol se basan principalmente en su estructura. El alcohol está compuesto por un alcano y agua. Contiene un grupo hidrofóbico (sin afinidad por el agua) del tipo de un alcano, y un grupo hidroxilo que es hidrófilo (con afinidad por el agua), similar al agua. De estas dos unidades estructurales, el grupo –OH da a los alcoholes sus propiedades físicas características, y el alquilo es el que las modifica, dependiendo de su tamaño y forma. El grupo –OH es muy polar y, lo que es más importante, es capaz de establecer puentes de hidrógeno: con sus moléculas compañeras o con otras moléculas neutras. Dependiendo de la cantidad de grupos -OH que forman parte del alcohol, el mismo puede ser clasificado como monohidroxilado (presencia de un hidroxilo) o polihidroxilado (dos o más grupos hidroxilos en la molécula). Aldehídos: Los aldehídos son compuestos orgánicos caracterizados por poseer el grupo funcional -CHO. Se denominan como los alcoholes correspondientes, cambiando la terminación -ol por -al: Es decir, el grupo carbonilo H-C=O está unido a un solo radical orgánico. Cetonas: Una cetona es un compuesto orgánico caracterizado por poseer un grupo funcional carbonilo unido a dos átomos de carbono, a diferencia de un aldehído, en donde el grupo carbonilo se encuentra unido al menos a un átomo de hidrógeno. Cuando el grupo funcional carbonilo es el de mayor relevancia en dicho compuesto orgánico, las cetonas se nombran agregando el sufijo -ona al hidrocarburo del cual provienen (hexano, hexanona; heptano, heptanona; etc). También se puede nombrar posponiendo cetona a los radicales a los cuales está unido (por ejemplo: metilfenil cetona). Cuando el grupo carbonilo no es el grupo prioritario, se utiliza el prefijo oxo- (ejemplo: 2-oxopropanal). El grupo funcional carbonilo consiste en un átomo de carbono unido con un doble enlace covalente a un átomo de oxígeno. El tener dos átomos de carbono unidos al grupo carbonilo, es lo que lo diferencia de los ácidos carboxílicos, aldehídos, ésteres. El doble enlace con el oxígeno, es lo que lo diferencia de los alcoholes y éteres. Las cetonas suelen ser menos reactivas que los aldehídos dado que los grupos alquílicos actúan como dadores de electrones por efecto inductivo. Ácidos carboxílicos: Los ácidos carboxílicos constituyen un grupo de compuestos que se caracterizan porque poseen un grupo funcional llamado grupo carboxilo o grupo carboxi (–COOH); se produce cuando coinciden sobre el mismo carbono un grupo hidroxilo (-OH) y carbonilo (C=O). Se puede representar como COOH o CO2H... Ésteres: Los ésteres presentan el grupo éster (-O-CO-) en su estructura. Algunos ejemplos de sustancias con este grupo incluyen el ácido acetil salicílico, componente de la aspirina, o algunos compuestos aromáticos como el acetato de isoamilo, con característico olor a plátano. Los aceites también son ésteres de ácidos grasos con glicerol. Éteres: Los éteres presentan el grupo éter(-O-) en su estructura. Suelen tener bajo punto de ebullición y son fácilmente descomponibles. Por ambos motivos, los éteres de baja masa molecular suelen ser peligrosos ya que sus vapores pueden ser explosivos. Nitrogenados Aminas: Las aminas son compuestos orgánicos caracterizados por la presencia del grupo amina (-N<). Las aminas pueden ser primarias (R-NH2), secundarias (R-NH-R) o terciarias (R-NR´-R). Las aminas suelen dar compuestos ligeramente amarillentos y con olores que recuerdan a pescado u orina. Amidas: Las amidas son compuestos orgánicos caracterizados por la presencia del grupo amida (-NH-CO-) en su estructura. Las proteínas o polipéptidos son poliamidas naturales formadas por enlaces peptídicos entre distintos aminoácidos. Isocianatos: Los isocianatos tienen el grupo isocianato (-N=C=O). Este grupo es muy electrófilo, reaccionando fácilmente con el agua para descomponerse mediante la transposición de Hofmann dar una amina y anhídrico carbónico, con los hidroxilos para dar uretanos, y con las aminas primarias o secundarias para dar ureas. Cíclicos Son compuestos que contienen un ciclo saturado. Un ejemplo de estos son los norbornanos, que en realidad son compuestos bicíclicos, los terpenos, u hormonas como el estrógeno, progesterona, testosterona u otras biomoléculas como el colesterol. Aromáticos El furano (C4H4O) es un ejemplo de compuesto aromático. Estructura tridimensional del furano mostrando la nube electrónica de electrones π. Los compuestos aromáticos tienen estructuras cíclicas insaturadas. El benceno es el claro ejemplo de un compuesto aromático, entre cuyos derivados están el tolueno, el fenol o el ácido benzoico. En general se define un compuesto aromático aquel que tiene anillos que cumplen la regla de Hückel, es decir que tienen 4n+2 electrones en orbitales π (n=0,1,2,...). A los compuestos orgánicos que tienen otro grupo distinto al carbono en sus cilos (normalmente N, O u S) se denominan compuestos aromáticos heterocíclicos. Así los compuestos aromáticos se suelen dividir en: Derivados del benceno: Policíclicos (antraceno, naftaleno, fenantreno, etc.), fenoles, aminas aromáticas, fulerenos, etc. Compuestos heterocíclicos: Piridina, furano, tiofeno, pirrol, porfirina, etc. Isómeros Isómeros del C6H12. Ya que el carbono puede enlazarse de diferentes maneras, una cadena puede tener diferentes configuraciones de enlace dando lugar a los llamados isómeros, moléculas tienen la misma fórmula química, pero distintas estructuras y propiedades. Existen distintos tipos de isomería: isomería de cadena, isomería de función, tautomería, estereoisomería, y estereoisomería configuracional. El ejemplo mostrado a la izquierda es un caso de isometría de cadena en la que el compuesto con fórmula C6H12 puede ser un ciclo (ciclohexano) o un alqueno lineal, el 1-hexeno. Un ejemplo de isomería de función sería el caso del propanal y la acetona, ambos con fórmula C3H6O. Compuestos orgánicos Artículo principal: Compuesto orgánico Los compuestos orgánicos pueden dividirse de manera muy general en: Compuestos alifáticos Compuestos aromáticos Compuestos heterocíclicos Compuestos organometálicos Polímeros Relación con la biología Una de las principales relaciones entre la química orgánica y la biología es el estudio de la síntesis y estructura de moléculas orgánicas de importancia en los procesos moleculares realizados por los organismos vivos, es decir en el metabolismo. La bioquímica es el campo interdisciplinar científico que estudia los seres vivos, y ya que estos usan compuestos que contienen carbono, la química orgánica es imprescindible para comprender los procesos metabólicos. En términos biológicos la química orgánica es de gran importancia sobre todo en un contexto celular y esto lo podemos ejemplificar con moléculas como los carbohidratos, presentes desde la membrana plasmática así como en la estructura química del ADN, los lípidos quienes son la base principal de la membrana plasmática, las proteínas que ayudan a dar sostén a un organismo o sus funciones como enzimas y el ADN, molécula encargada de resguardar la información genética de los organismos vivos."
ksampletext_wikipedia_chem_molecula: str = "Molécula. En química, una molécula (del nuevo latín molecula, que es un diminutivo de la palabra moles, masa) es un grupo eléctricamente neutro y suficientemente estable de al menos dos átomos en una configuración definida, unidos por enlaces químicos fuertes covalentes. En este estricto sentido, las moléculas se diferencian de los iones poliatómicos. En la química orgánica y la bioquímica, el término molécula se utiliza de manera menos estricta y se aplica también a los compuestos orgánicos (moléculas orgánicas) y en las biomoléculas. Antes, se definía la molécula de forma menos general y precisa, como la más pequeña parte de una sustancia que podía tener existencia independiente y estable conservando aún sus propiedades fisicoquímicas. De acuerdo con esta definición, podían existir moléculas monoatómicas. En la teoría cinética de los gases, el término molécula se aplica a cualquier partícula gaseosa con independencia de su composición. De acuerdo con esta definición, los átomos de un gas noble se considerarían moléculas aunque se componen de átomos no enlazados. Una molécula puede consistir en varios átomos de un único elemento químico, como en el caso del oxígeno diatómico (O2), o de diferentes elementos, como en el caso del agua (H2O). Los átomos y complejos unidos por enlaces no covalentes como los enlaces de hidrógeno o los enlaces iónicos no se suelen considerar como moléculas individuales. Las moléculas como componentes de la materia son comunes en las sustancias orgánicas (y por tanto en la bioquímica). También conforman la mayor parte de los océanos y de la atmósfera. Sin embargo, un gran número de sustancias sólidas familiares, que incluyen la mayor parte de los minerales que componen la corteza, el manto y el núcleo de la Tierra, contienen muchos enlaces químicos, pero no están formados por moléculas. Además, ninguna molécula típica puede ser definida en los cristales iónicos (sales) o en cristales covalentes, aunque estén compuestos por celdas unitarias que se repiten, ya sea en un plano (como en el grafito) o en tres dimensiones (como en el diamante o el cloruro de sodio). Este sistema de repetir una estructura unitaria varias veces también es válida para la mayoría de las fases condensadas de la materia con enlaces metálicos, lo que significa que los metales sólidos tampoco están compuestos por moléculas. En el vidrio (sólidos que presentan un estado vítreo desordenado), los átomos también pueden estar unidos por enlaces químicos sin que se pueda identificar ningún tipo de molécula, pero tampoco existe la regularidad de la repetición de unidades que caracteriza a los cristales. Casi toda la química orgánica y buena parte de la química inorgánica se ocupan de la síntesis y reactividad de moléculas y compuestos moleculares. La química física y, especialmente, la química cuántica también estudian, cuantitativamente, en su caso, las propiedades y reactividad de las moléculas. La bioquímica está íntimamente relacionada con la biología molecular, ya que ambas estudian a los seres vivos a nivel molecular. El estudio de las interacciones específicas entre moléculas, incluyendo el reconocimiento molecular es el campo de estudio de la química supramolecular. Estas fuerzas explican las propiedades físicas como la solubilidad o el punto de ebullición de un compuesto molecular. Las moléculas rara vez se encuentran sin interacción entre ellas, salvo en gases enrarecidos y en los gases nobles. Así, pueden encontrarse en redes cristalinas, como el caso de las moléculas de H2O en el hielo o con interacciones intensas, pero que cambian rápidamente de direccionalidad, como en el agua líquida. En orden creciente de intensidad, las fuerzas intermoleculares más relevantes son: las fuerzas de Van der Waals y los puentes de hidrógeno. La dinámica molecular es un método de simulación por computadora que utiliza estas fuerzas para tratar de explicar las propiedades de las moléculas. No se puede definir una molécula típica para sales ni para cristales covalentes, aunque estos a menudo se componen de células unitarias repetidas que se extienden en un plano, por ejemplo, el grafeno ; o tridimensionalmente, por ejemplo, el diamante, el cuarzo, o el cloruro de sodio. El tema de la estructura celular unitaria repetida también se aplica a la mayoría de los metales que son fases condensadas con enlaces metálicos. Por tanto, los metales sólidos no están hechos de moléculas. En los vidrios, que son sólidos que existen en un estado vítreo desordenado, los átomos se mantienen unidos por enlaces químicos sin presencia de ninguna molécula definible, ni ninguna de la regularidad de la estructura celular unitaria repetida que caracteriza a las sales, cristales covalentes y rieles. Ciencia molecular La ciencia de las moléculas se denomina química molecular o física molecular, dependiendo de si se centra en la química o en la física. La química molecular se ocupa de las leyes que rigen la interacción entre las moléculas que da lugar a la formación y ruptura de enlaces químicos, mientras que la física molecular se ocupa de las leyes que rigen su estructura y propiedades. En la práctica, sin embargo, esta distinción es imprecisa. En las ciencias moleculares, una molécula consiste en un sistema estable (estado ligado) compuesto por dos o más átomos. Los iones poliatómicos pueden considerarse a veces como moléculas cargadas eléctricamente. El término molécula inestable se utiliza para especies muy reactivas, es decir, conjuntos de corta duración (resonancias) de electrones y núcleos, como radicales, iones moleculares, moléculas de Rydberg, estados de transición, complejos de van der Waals, o sistemas de átomos en colisión como en el condensado de Bose-Einstein. Historia y etimología Artículo principal: Historia de la teoría molecular Según la Real Academia Española el vocablo «molécula» deriva del latín moles mole o masa y el sufijo diminutivo -ula masa pequeña. Molécula (1794) - «partícula extremadamente diminuta», del francés molécule (1678), del Nuevo Latín molecula, diminutivo del latín moles masa, barrera. Un significado vago al principio; la moda de la palabra (utilizada hasta finales del siglo XVIII solo en forma latina) se remonta a la filosofía de Descartes. La definición de molécula ha ido evolucionando a medida que ha aumentado el conocimiento de la estructura de las moléculas. Las definiciones anteriores eran menos precisas, y definían las moléculas como las partículas más pequeñas de sustancia químicas puras que aún conservan su composición y sus propiedades químicas. Esta definición a menudo se rompe ya que muchas sustancias en la experiencia ordinaria, como rocas, sales, y metales, se componen de grandes redes cristalinas de átomos de enlace químico o iones, pero no están hechas de moléculas discretas. Definición y sus límites De manera menos general y precisa, se ha definido molécula como la parte más pequeña de una sustancia química que conserva sus propiedades químicas, y a partir de la cual se puede reconstituir la sustancia sin reacciones químicas. De acuerdo con esta definición, que resulta razonablemente útil para aquellas sustancias puras constituidas por moléculas, podrían existir las moléculas monoatómicas de gases nobles, mientras que las redes cristalinas, sales, metales y la mayoría de vidrios quedarían en una situación confusa. Las moléculas lábiles pueden perder su consistencia en tiempos relativamente cortos, pero si el tiempo de vida medio es del orden de unas pocas vibraciones moleculares, estamos ante un estado de transición que no se puede considerar molécula. Actualmente, es posible el uso de láser pulsado para el estudio de la química de estos sistemas. Las entidades que comparten la definición de las moléculas, pero tienen carga eléctrica se denominan iones poliatómicos, iones moleculares o moléculas ion. Las sales compuestas por iones poliatómicos se clasifican habitualmente dentro de los materiales de base molecular o materiales moleculares. Ejemplo de molécula poliatómica: el agua Las moléculas están formadas por partículas. Una molécula viene a ser la porción de materia más pequeña que aún conserva las propiedades de la materia original. Las moléculas se encuentran fuertemente enlazadas con la finalidad de formar materia. Las moléculas están formadas por átomos unidos por medio de enlaces químicos. Una molécula es una unidad de sustancia que puede ser monoatómica o poliatómica. La unidad de todas las sustancias gaseosas es la molécula. Tipos de moléculas Las moléculas se pueden clasificar en: Moléculas discretas: constituidas por un número bien definido de átomos, sean estos del mismo elemento (moléculas homonucleares, como el dinitrógeno o el fullereno) o de elementos distintos (moléculas heteronucleares, como el agua). Molécula de dinitrógeno, el gas que es el componente mayoritario del aire Molécula de dinitrógeno, el gas que es el componente mayoritario del aire Molécula de fullereno, tercera forma estable del carbono tras el diamante y el grafito Molécula de fullereno, tercera forma estable del carbono tras el diamante y el grafito Molécula de agua, «disolvente universal», de importancia fundamental en innumerables procesos bioquímicos e industriales Molécula de agua, «disolvente universal», de importancia fundamental en innumerables procesos bioquímicos e industriales Representación poliédrica del anión de Keggin, un polianión molecular Representación poliédrica del anión de Keggin, un polianión molecular Macromoléculas o polímeros: constituidas por la repetición de una unidad comparativamente simple ,o un conjunto limitado de dichas unidades, y que alcanzan pesos moleculares relativamente altos. Representación de un fragmento de ADN, un polímero de importancia fundamental en la genética Representación de un fragmento de ADN, un polímero de importancia fundamental en la genética Enlace peptídico que une los péptidos para formar proteínas Enlace peptídico que une los péptidos para formar proteínas Representación de un fragmento lineal de polietileno, el plástico más usado Representación de un fragmento lineal de polietileno, el plástico más usado Primera generación de un dendrímero, un tipo especial de polímero que crece de forma fractal Primera generación de un dendrímero, un tipo especial de polímero que crece de forma fractal Enlaces Los átomos que forman las moléculas se mantienen juntos mediante enlaces covalentes o enlaces iónicos. Varios tipos de elementos no metálicos existen solo como moléculas en el medio ambiente. Por ejemplo, el hidrógeno solo existe como molécula de hidrógeno. Una molécula de un compuesto está formada por dos o más elementos. Una molécula homonuclear está formada por dos o más átomos de un solo elemento. Mientras que algunas personas dicen que un cristal metálico puede considerarse una sola molécula gigante unida por enlaces metálicos, otros señalan que los metales actúan de manera muy diferente a las moléculas. Covalente Artículo principal: Enlace covalente Un enlace covalente que forma H2 (derecha) donde dos átomos de hidrógeno comparten los dos electrones. Un enlace covalente es un enlace químico que implica el intercambio de pares de electrones entre átomos. Estos pares de electrones se denominan pares compartidos o pares de enlace, y el equilibrio estable de fuerzas atractivas y repulsivas entre átomos, cuando comparten electrones, se denomina enlace covalente. Iónico Artículo principal: Enlace iónico El sodio y el flúor experimentan una reacción redox para formar fluoruro de sodio. El sodio pierde su electrón externo para adoptar una configuración electrónica estable, y este electrón entra en el átomo de flúor en forma exotérmica. El enlace iónico es un tipo de enlace químico que implica la atracción electrostática entre iones con carga eléctrica opuesta y es la interacción principal que se produce en los compuestos iónicos. Los iones son átomos que han perdido uno o más electrones (denominados cationes) y átomos que han ganado uno o más electrones (denominados aniones). Esta transferencia de electrones se denomina electrovalencia en contraste con la covalencia. En el caso más simple, el catión es un átomo de metal y el anión es un átomo no metálico, pero estos iones pueden ser de naturaleza más complicada, por ejemplo, iones moleculares como NH4+ o SO4 2−. A temperaturas y presiones normales, la unión iónica crea principalmente sólidos (u ocasionalmente líquidos) sin moléculas identificables separadas, pero la vaporización/sublimación de tales materiales produce pequeñas moléculas separadas donde los electrones aún se transfieren lo suficiente como para que los enlaces se consideren iónicos en lugar de covalentes. Descripción La estructura molecular puede ser descrita de diferentes formas. La fórmula molecular es útil para moléculas sencillas, como H2O para el agua o NH3 para el amoniaco. Contiene los símbolos de los elementos presentes en la molécula, así como su proporción indicada por los subíndices. Para moléculas más complejas, como las que se encuentran comúnmente en química orgánica, la fórmula química no es suficiente, y vale la pena usar una fórmula estructural o una fórmula esqueletal, las que indican gráficamente la disposición espacial de los distintos grupos funcionales. Cuando se quieren mostrar variadas propiedades moleculares, o se trata de sistemas muy complejos como proteínas, ADN o polímeros, se utilizan representaciones especiales, como los modelos tridimensionales (físicos o representados por ordenador). En proteínas, por ejemplo, cabe distinguir entre estructura primaria (orden de los aminoácidos), secundaria (primer plegamiento en hélices, hojas, giros…), terciaria (plegamiento de las estructuras tipo hélice/hoja/giro para dar glóbulos) y cuaternaria (organización espacial entre los diferentes glóbulos). Figura 1. Representaciones de la terpenoide, atisano, 3D (centro izquierda) y 2D (derecha). En el modelo 3D de la izquierda, los átomos de carbono están representados por esferas azules; las blancas representan a los átomos de hidrógeno y los cilindros representan los enlaces. El modelo es una representación de la superficies molecular, coloreada por áreas de carga eléctrica positiva (rojo) o negativa (azul). En el modelo 3D del centro, las esferas azul claro representan átomos de carbono, las blancas de hidrógeno y los cilindros entre los átomos son los enlaces simples. Moléculas en la teoría cuántica La mecánica clásica y el electromagnetismo clásico no podían explicar la existencia y estabilidad de las moléculas, ya que de acuerdo con sus ecuaciones una carga eléctrica acelerada emitiría radiación por lo que los electrones necesariamente perderían energía cinética por radiación hasta caer sobre el núcleo atómico. La mecánica cuántica proveyó el primer modelo cualitativamente correcto que además predecía la existencia de átomos estables y proporcionaba explicación cuantitativa muy aproximada para fenómenos empíricos como los espectros de emisión característicos de cada elemento químico. En mecánica cuántica una molécula o un ion poliatómico se describe como un sistema formado por (1) definido sobre el espacio de funciones antisimetrizadas de cuadrado integrable (2) donde el primer término representa la interacción de los electrones entre sí, el segundo la interacción de los electrones con los núcleos atómicos, y el tercero las interacciones de los núcleos entre sí. En una molécula neutra se tendrá obviamente que: Si Aproximación de Born-Oppenheimer Resolver el problema de autovalores y autofunciones para el hamiltoniano cuántico dado por (1) es un problema matemático difícil, por lo que es común simplificarlo de alguna manera. Así dado que los núcleos atómicos son mucho más pesados que los electrones (entre 103 y 105 veces más) puede suponerse que los núcleos atómicos apenas se mueven comparados con los electrones, por lo que se considera que están congelados en posiciones fijas, con lo cual se puede aproximar el hamiltoniano (1) por la aproximación de Born-Oppenheimer dada por: (3) definido sobre el espacio de funciones Teorema de Kato Los operadores Tosio Kato La propiedad de ser autoadjunto implicará que las energías son cantidades reales, y el que sean acotados inferiormente implicará que existe un estado fundamental de mínima energía por debajo del cual los electrones no pueden decaer, y por tanto, las moléculas serán estables, ya que los electrones no pueden perder y perder energía como parecían predecir las ecuaciones del electromagnetismo clásico. Dos resultados matemáticos adicionales nos dicen como son las energías permitidas de los electrones dentro de una molécula: Teorema HVZ para átomos y moléculas BO El espectro esencial inf W. Hunziker, C. Van Winter y G. M. Zhislin Además dentro de la mecánica cuántica puede demostrarse que pueden existir iones positivos (cationes, con carga positiva comparable al núcleo atómico), mientras que no es igual de fácil tener iones negativos (aniones), el siguiente resultado matemático implica tiene que ver con la posibilidad de cationes y aniones."
ksampletext_wikipedia_chem_compuestoquimico: str = "Compuesto químico. Un compuesto químico es una sustancia formada por la combinación química de dos o más elementos de la tabla periódica. Los compuestos son representados por una fórmula química. Por ejemplo, el agua (H2O) está constituida por dos átomos de hidrógeno y uno de oxígeno. Los elementos de un compuesto no se pueden dividir ni separar por procesos físicos (decantación, filtración, destilación), sino solo mediante procesos químicos. Los compuestos están formados por moléculas o iones con enlaces estables que no obedece a una selección humana arbitraria. Por lo tanto, no son mezclas o aleaciones como el bronce o el chocolate. Un elemento químico unido a un elemento químico idéntico no es un compuesto químico, ya que solo está involucrado un elemento, no dos elementos diferentes. Hay cuatro tipos de compuestos, dependiendo de cómo se mantienen unidos los átomos constituyentes: Moléculas unidas por enlaces covalentes. Compuestos iónicos unidos por enlaces iónicos. Compuestos intermetálicos unidos por enlaces metálicos. Ciertos complejos que se mantienen unidos por enlaces covalentes coordinados. Muchos compuestos químicos tienen un identificador numérico único asignado por el Chemical Abstracts Service (CAS): su número CAS. Fórmula Artículo principal: Fórmula molecular En química inorgánica los compuestos se representan mediante fórmulas químicas. Una fórmula química es una forma de expresar información sobre las proporciones de los átomos que constituyen un compuesto químico en particular, utilizando las abreviaturas normalizadas de los elementos químicos y subíndices para indicar el número de átomos involucrados. Por ejemplo, el agua se compone de dos átomos de hidrógeno unidos a uno de oxígeno átomo: la fórmula química es H2O. En el caso de compuestos no estequiométricos, las proporciones pueden ser reproducibles con respecto a su preparación y dar proporciones fijas de sus elementos componentes, pero proporciones que no son integrales [por ejemplo, para el hidruro de paladio, PdH x (0.02 <x <0.58 )]. El orden de los elementos en la fórmula de los compuestos inorgánicos comienza por la izquierda con el elemento menos electronegativo, hasta la derecha con el más electronegativo. Por ejemplo en el NaCl, el cloro que es más electronegativo que el sodio va en la parte derecha. Para los compuestos orgánicos existen otras varias reglas y se utilizan fórmulas esqueletales o semidesarrolladas para su representación. Definiciones Cualquier sustancia que consista en dos o más tipos diferentes de átomos (elementos químicos) en una proporción estequiométrica fija puede denominarse compuesto químico. El concepto se entiende mejor cuando se consideran sustancias químicas puras. De la composición de proporciones fijas de dos o más tipos de átomos se desprende que los compuestos químicos se pueden convertir, mediante una reacción química, en compuestos o sustancias, cada uno con menos átomos. Los compuestos químicos tienen una estructura química única y definida que se mantiene unida en una disposición espacial concebida por enlaces químicos. Los compuestos químicos pueden ser compuestos moleculares, mantenidos juntos por enlaces covalentes, sales mantenidas entre sí por enlaces iónicos, compuestos intermetálicos mantenidos juntos por enlaces metálicos, o el subconjunto de complejos químicos que se mantienen unidos por enlaces covalentes coordinados . Los elementos químicos puros generalmente no se consideran compuestos químicos, ya que no cumplen con el requisito de dos o más átomos, aunque a menudo consisten en moléculas compuestas de múltiples átomos (como en la molécula diatómica H2, o la molécula poliatómica S8, etc.) Muchos compuestos químicos tienen un identificador numérico único asignado por el Chemical Abstracts Service (CAS): su número CAS. Hay nomenclatura variable y a veces inconsistente para diferenciar sustancias, que incluyen ejemplos verdaderamente no estequiométricos de los compuestos químicos, que requieren que las proporciones sean fijas. Muchas sustancias químicas sólidas, por ejemplo muchos minerales de silicato, no tienen fórmulas simples que reflejen el enlace químico de los elementos entre sí en proporciones fijas; aun así, estas sustancias cristalinas a menudo se denominan compuestos no estequiométricos. Se puede argumentar que están relacionados con dichos productos, en lugar de ser compuestos químicos propiamente dichos, en la medida en que la variabilidad en sus composiciones a menudo se debe a la presencia de elementos extraños atrapados dentro de la estructura cristalina de un compuesto químico verdadero, o debido a perturbaciones en su estructura en relación con el compuesto conocido que surge debido a un exceso o déficit de los elementos constituyentes en lugares de su estructura; tales sustancias no estequiométricas forman la mayor parte de la corteza y el manto de la Tierra. Otros compuestos considerados químicamente idénticos pueden tener cantidades variables de isótopos pesados o ligeros de los elementos constituyentes, lo que cambia ligeramente la proporción en masa de los elementos. Clasificación Se pueden clasificar de acuerdo al tipo de enlace químico o a su composición. Atendiendo al tipo de enlace químico, se pueden dividir en: Moléculas Compuestos iónicos Compuestos intermetálicos Complejos Por su composición, se pueden dividir en dos grandes grupos: Compuestos inorgánicos: Óxidos básicos. También llamados óxidos metálicos, que están formados por un metal y oxígeno. Ejemplos: el óxido plúmbico, óxido de litio. Óxidos ácidos. También llamados óxidos no metálicos, formados por un no metal y oxígeno. Ejemplos: óxido hipocloroso, óxido selenioso. Hidruros, que pueden ser tanto metálicos como no metálicos. Están compuestos por un elemento e hidrógeno. Ejemplos: hidruro de aluminio, hidruro de sodio. Hidrácidos, son hidruros no metálicos que, cuando se disuelven en agua, adquieren carácter ácido. Por ejemplo, el ácido yodhídrico. Hidróxidos, compuestos formados por la reacción entre un óxido básico y el agua, que se caracterizan por presentar el grupo hidroxilo (OH). Por ejemplo, el hidróxido de sodio, o sosa cáustica. Oxácidos, compuestos obtenidos por la reacción de un óxido ácido y agua. Sus moléculas están formadas por hidrógeno, un no metal y oxígeno. Por ejemplo, ácido clórico. Sales binarias, compuestos formados por un hidrácido más un hidróxido. Por ejemplo, el cloruro de sodio. Oxisales, formadas por la reacción de un oxácido y un hidróxido, como por ejemplo el hipoclorito de sodio. Compuestos orgánicos: Compuestos alifáticos, son compuestos orgánicos constituidos por carbono e hidrógeno cuyo carácter no es aromático. Compuestos aromáticos, es un compuesto orgánico cíclico conjugado que posee una mayor estabilidad debido a la deslocalización electrónica en enlaces π. Compuestos heterocíclicos, son compuestos orgánicos cíclicos en los que al menos uno de los componentes del ciclo es de un elemento diferente al carbono. Compuestos organometálicos, es un compuesto en el que los átomos de carbono forman enlaces covalentes, es decir, comparten electrones, con un átomo metálico. Polímeros, son macromoléculas formadas por la unión de moléculas más pequeñas llamadas monómeros. Moléculas Artículo principal: Molécula Una molécula es un grupo eléctricamente neutro de dos o más átomos unidos por enlaces químicos. Una molécula puede ser homonuclear, es decir, estar formada por átomos de un mismo elemento químico, como ocurre con dos átomos en la molécula de oxígeno (O2); o puede ser heteronuclear, es decir, un compuesto químico compuesto por más de un elemento, como el agua (dos átomos de hidrógeno y un átomo de oxígeno; H2O). Los átomos y complejos unidos por enlaces no covalentes como los enlaces de hidrógeno no se suelen considerar como moléculas individuales. Compuestos iónicos Artículo principal: Compuesto iónico Un compuesto iónico es un compuesto químico compuesto de anion que se mantienen unidos por fuerzas electrostáticas denominadas enlace iónico. El compuesto es neutro en general, pero consta de iones cargados positivamente llamados cationes y iones cargados negativamente llamados aniones. Estos pueden ser iones simples como el sodio(Na+) y el cloruro (Cl−) en el cloruro de sodio, o especies poliatómicas como el amonio (NH+ 4) y carbonato (CO2− 3) en el carbonato de amonio. Los iones individuales dentro de un compuesto iónico generalmente tienen múltiples vecinos más cercanos, por lo que no se consideran parte de moléculas, sino parte de una red tridimensional continua, generalmente en una estructura cristalina. Los compuestos iónicos que contienen iones básicos hidróxido (OH−) u óxido(O2−) se clasifican como bases. Los compuestos iónicos sin estos iones también se conocen como sales y pueden formarse mediante reacciones ácido-base. Los compuestos iónicos también se pueden producir a partir de sus iones constituyentes por evaporación de su disolvente, precipitación, congelación, una reacción en estado sólido o la reacción de transferencia de electrones de metales reactivos con no metales reactivos, como los gases halógenos. Los compuestos iónicos suelen tener altos puntos de fusión y ebullición, y son duros y quebradizos. Como sólidos, casi siempre son eléctricamente aislantes, pero cuando se funden o disuelven se vuelven altamente conductores, porque se movilizan los iones. Compuestos intermetálicos Un compuesto intermetálico es un tipo de aleación metálica que forma un compuesto de estado sólido ordenado entre dos o más elementos metálicos. Los intermetálicos son generalmente duros y quebradizos, con buenas propiedades mecánicas a altas temperaturas. Se pueden clasificar como compuestos intermetálicos estequiométricos o no estequiométricos. Complejos químicos Artículo principal: Complejo (química) Un complejo de coordinación consiste en un átomo o ion central, que generalmente es metálico y se llama centro de coordinación, y una matriz circundante de moléculas o iones unidos, que a su vez se conocen como ligandos o agentes complejantes. Muchos compuestos que contienen metales, especialmente los de metales de transición, son complejos de coordinación. Un complejo de coordinación cuyo centro es un átomo metálico se denomina complejo metálico o elemento de bloque d. Enlaces y fuerzas Los compuestos se mantienen unidos por medio de diferentes tipos de enlaces y fuerzas. Las diferencias entre los tipos de enlaces de los compuestos dependen del tipo de elemento presente en el compuesto. Las fuerzas de dispersión de London son las fuerzas más débiles entre las fuerzas intermoleculares. Son fuerzas de atracción temporales que se forman cuando los electrones en dos átomos adyacentes se colocan de manera que crean un dipolo temporal. Además, estas fuerzas son responsables de la condensación de sustancias no polares en líquidos y posterior congelación a un estado sólido dependiendo de la temperatura del ambiente. Un enlace covalente, también conocido como enlace molecular, implica el intercambio de electrones entre dos átomos. Principalmente, este tipo de enlace se produce entre elementos que aparecen uno cerca del otro en la tabla periódica de elementos, aunque se observa entre algunos metales y no metales. Esto se debe al mecanismo de este tipo de enlace. Los elementos cercanos en la tabla periódica tienden a tener electronegatividades similares, lo que significa que tienen una afinidad similar por los electrones. Como ninguno de los elementos tiene una afinidad más fuerte para donar o ganar electrones, hace que los elementos compartan electrones de manera que ambos elementos tengan un octeto más estable. El enlace iónico se produce cuando los electrones de valencia se transfieren completamente entre los elementos. Al contrario que el covalente, este enlace químico crea dos iones de carga opuesta. Los metales en enlaces iónicos generalmente pierden sus electrones de valencia, convirtiéndose en cationes, cargados positivamente. El no metal ganará los electrones del metal, haciendo que el no metal sea un anión, es decir, cargado negativamente. Es decir, los enlaces iónicos se producen entre un donador de electrones, generalmente un metal, y un aceptor de electrones, que tiende a ser un no metal. El enlace de hidrógeno se produce cuando un átomo de hidrógeno unido a un átomo electronegativo forma una conexión electrostática con otro átomo electronegativo a través de dipolos o cargas que interactúan. Reacciones Un compuesto se puede convertir en una composición química diferente (productos) mediante la interacción con un segundo compuesto químico (reactivos) a través de una reacción química. En este proceso, los enlaces entre los átomos se rompen en ambos compuestos que interactúan, y luego los enlaces se reforman para obtener nuevas asociaciones entre los mismos átomos. Esquemáticamente, esta reacción podría describirse como AB + CD → AD + CB, donde A, B, C y D son cada uno átomos únicos; y AB, AD, CD y CB son cada uno compuestos ùnicos."

ksampletext_wikipedia_biol_celula: str = "Célula. La célula (del latín cellula, diminutivo de cella, celda) es la unidad morfológica y funcional de todo ser vivo. De hecho, la célula es el elemento de menor tamaño que puede considerarse vivo. De este modo, puede clasificarse a los organismos vivos según el número de células que posean: si solo tienen una, se les denomina unicelulares (como pueden ser los protozoos o las bacterias, organismos microscópicos); si poseen más, se les llama pluricelulares. En estos últimos el número de células es variable: de unos pocos cientos, como en algunos nematodos, a cientos de billones (1014), como en el caso del ser humano. Las células suelen poseer un tamaño de 10 µm y una masa de 1 ng, si bien existen células mucho mayores. Célula animal La teoría celular, propuesta en 1838 para los vegetales y en 1839 para los animales, por Matthias Jakob Schleiden y Theodor Schwann, postula que todos los organismos están compuestos por células, y que todas las células derivan de otras precedentes. De este modo, todas las funciones vitales emanan de la maquinaria celular y de la interacción entre células adyacentes; además, la tenencia de la información genética, base de la herencia, en su ADN permite la transmisión de aquella de generación en generación. La aparición del primer organismo vivo sobre la Tierra suele asociarse al nacimiento de la primera célula. Si bien existen muchas hipótesis que especulan cómo ocurrió, usualmente se describe que el proceso se inició gracias a la transformación de moléculas inorgánicas en orgánicas bajo unas condiciones ambientales adecuadas; tras esto, dichas biomoléculas se asociaron dando lugar a entes complejos capaces de autorreplicarse. Existen posibles evidencias fósiles de estructuras celulares en rocas datadas en torno a 4 o 3,5 miles de millones de años (gigaaños o Ga).[nota 1] Se han encontrado evidencias muy fuertes de formas de vida unicelulares fosilizadas en microestructuras en rocas de la formación Strelley Pool, en Australia Occidental, con una antigüedad de 3,4 Ga.[cita requerida] Se trataría de los fósiles de células más antiguos encontrados hasta la fecha. Evidencias adicionales muestran que su metabolismo sería anaerobio y basado en el sulfuro. Tipos celulares Existen dos grandes tipos celulares: Célula procariota, propia de los procariontes, que comprende las células de arqueas y bacterias. Célula eucariota, propia de los eucariontes, tales como la célula animal, célula vegetal, y las células de hongos y protistas. Historia y teoría celular La historia de la biología celular ha estado ligada al desarrollo tecnológico que pudiera sustentar su estudio. De este modo, el primer acercamiento a su morfología se inicia con la popularización del microscopio rudimentario de lentes compuestas en el siglo XVII, se suplementa con diversas técnicas histológicas para microscopía óptica en los siglos XIX y XX y alcanza un mayor nivel resolutivo mediante los estudios de microscopía electrónica, de fluorescencia y confocal, entre otros, ya en el siglo XX. El desarrollo de herramientas moleculares, basadas en el manejo de ácidos nucleicos y enzimas permitieron un análisis más exhaustivo a lo largo del siglo XX. Descubrimiento Robert Hooke, quien acuñó el término «célula». Las primeras aproximaciones al estudio de la célula surgieron en el siglo XVII; tras el desarrollo a finales del siglo XVI de los primeros microscopios. Estos permitieron realizar numerosas observaciones, que condujeron en apenas doscientos años a un conocimiento morfológico relativamente aceptable. A continuación se enumera una breve cronología de tales descubrimientos: 1665: Robert Hooke publicó los resultados de sus observaciones sobre tejidos vegetales, como el corcho, realizadas con un microscopio de 50 aumentos construido por él mismo. Este investigador fue el primero que, al ver en esos tejidos unidades que se repetían a modo de celdillas de un panal, las bautizó como elementos de repetición, «células» (del latín cellulae, celdillas). Pero Hooke solo pudo observar células muertas por lo que no pudo describir las estructuras de su interior. Década de 1670: Anton van Leeuwenhoek observó diversas células eucariotas (como protozoos y espermatozoides) y procariotas (bacterias). 1745: John Needham describió la presencia de «animálculos» o «infusorios»; se trataba de organismos unicelulares. Dibujo de la estructura del corcho observado por Robert Hooke bajo su microscopio y tal como aparece publicado en Micrographia. Década de 1830: Theodor Schwann estudió la célula animal; junto con Matthias Schleiden postularon que las células son las unidades elementales en la formación de las plantas y animales, y que son la base fundamental del proceso vital. 1831: Robert Brown describió el núcleo celular. 1839: Purkinje observó el citoplasma celular. 1857: Kölliker identificó las mitocondrias. 1858: Rudolf Virchow postuló que todas las células provienen de otras células. 1860: Pasteur realizó multitud de estudios sobre el metabolismo de levaduras y sobre la asepsia. 1880: August Weismann descubrió que las células actuales comparten similitud estructural y molecular con células de tiempos remotos. 1931: Ernst Ruska construyó el primer microscopio electrónico de transmisión en la Universidad de Berlín. Cuatro años más tarde, obtuvo una resolución óptica doble a la del microscopio óptico. 1981: Lynn Margulis publica su hipótesis sobre la endosimbiosis serial, que explica el origen de la célula eucariota. Teoría celular Artículo principal: Teoría celular El concepto de célula como unidad anatómica y funcional de los organismos surgió entre los años 1830 y 1880, aunque fue en el siglo XVII cuando Robert Hooke describió por vez primera la existencia de las mismas, al observar en una preparación vegetal la presencia de una estructura organizada que derivaba de la arquitectura de las paredes celulares vegetales. En 1830 se disponía ya de microscopios con una óptica más avanzada, lo que permitió a investigadores como Theodor Schwann y Matthias Schleiden definir los postulados de la teoría celular, la cual afirma, entre otras cosas: Que la célula es una unidad morfológica de todo ser vivo: es decir, que en los seres vivos todo está formado por células o por sus productos de secreción. Este primer postulado sería completado por Rudolf Virchow con la afirmación Omnis cellula ex cellula, la cual indica que toda célula deriva de una célula precedente (biogénesis). En otras palabras, este postulado constituye la refutación de la teoría de generación espontánea o ex novo, que hipotetizaba la posibilidad de que se generara vida a partir de elementos inanimados. Un tercer postulado de la teoría celular indica que las funciones vitales de los organismos ocurren dentro de las células, o en su entorno inmediato, y son controladas por sustancias que ellas secretan. Cada célula es un sistema abierto, que intercambia materia y energía con su medio. En una célula ocurren todas las funciones vitales, de manera que basta una sola de ellas para que haya un ser vivo (que será un individuo unicelular). Así pues, la célula es la unidad fisiológica de la vida. El cuarto postulado expresa que cada célula contiene toda la información hereditaria necesaria para el control de su propio ciclo y del desarrollo y el funcionamiento de un organismo de su especie, así como para la transmisión de esa información a la siguiente generación celular. Definición Se define a la célula como la unidad morfológica y funcional de todo ser vivo. De hecho, la célula es el elemento de menor tamaño que puede considerarse vivo. Como tal posee una membrana de fosfolípidos con permeabilidad selectiva que mantiene un medio interno altamente ordenado y diferenciado del medio externo en cuanto a su composición, sujeta a control homeostático, la cual consiste en biomoléculas y algunos metales y electrolitos. La estructura se automantiene activamente mediante el metabolismo, asegurándose la coordinación de todos los elementos celulares y su perpetuación por replicación a través de un genoma codificado por ácidos nucleicos. La parte de la biología que se ocupa de ella es la citología. Características Las células, como sistemas termodinámicos complejos, poseen una serie de elementos estructurales y funcionales comunes que posibilitan su supervivencia; no obstante, los distintos tipos celulares presentan modificaciones de estas características comunes que permiten su especialización funcional y, por ello, la ganancia de complejidad. De este modo, las células permanecen altamente organizadas a costa de incrementar la entropía del entorno, uno de los requisitos de la vida. Características estructurales La existencia de polímeros como la celulosa en la pared vegetal permite sustentar la estructura celular empleando un armazón externo. Individualidad: Todas las células están rodeadas de una envoltura (que puede ser una bicapa lipídica desnuda, en células animales; una pared de polisacárido, en hongos y vegetales; una membrana externa y otros elementos que definen una pared compleja, en bacterias Gram negativas; una pared de peptidoglicano, en bacterias Gram positivas; o una pared de variada composición, en arqueas) que las separa y comunica con el exterior, que controla los movimientos celulares y que mantiene el potencial de membrana. Contienen un medio interno acuoso, el citosol, que forma la mayor parte del volumen celular y en el que están inmersos los orgánulos celulares. Poseen material genético en forma de ADN, el material hereditario de los genes, que contiene las instrucciones para el funcionamiento celular, así como ARN, a fin de que el primero se exprese. Tienen enzimas y otras proteínas, que sustentan, junto con otras biomoléculas, un metabolismo activo. Características funcionales Estructura tridimensional de una enzima, un tipo de proteínas implicadas en el metabolismo celular. Las células vivas son un sistema bioquímico complejo. Las características que permiten diferenciar las células de los sistemas químicos no vivos son: Nutrición. Las células toman sustancias del medio, las transforman de una forma a otra, liberan energía y eliminan productos de desecho, mediante el metabolismo. Crecimiento y multiplicación. Las células son capaces de dirigir su propia síntesis. A consecuencia de los procesos nutricionales, una célula crece y se divide, formando dos células, en una célula idéntica a la célula original, mediante la división celular. Diferenciación. Muchas células pueden sufrir cambios de forma o función en un proceso llamado diferenciación celular. Cuando una célula se diferencia, se forman algunas sustancias o estructuras que no estaban previamente formadas y otras que lo estaban dejan de formarse. La diferenciación es a menudo parte del ciclo celular en que las células forman estructuras especializadas relacionadas con la reproducción, la dispersión o la supervivencia. Señalización. Las células responden a estímulos químicos y físicos tanto del medio externo como de su interior y, en el caso de células móviles, hacia determinados estímulos ambientales o en dirección opuesta mediante un proceso que se denomina quimiotaxis. Además, frecuentemente las células pueden interaccionar o comunicar con otras células, generalmente por medio de señales o mensajeros químicos, como hormonas, neurotransmisores, factores de crecimiento... en seres pluricelulares en complicados procesos de comunicación celular y transducción de señales. Evolución. A diferencia de las estructuras inanimadas, los organismos unicelulares y pluricelulares evolucionan. Esto significa que hay cambios hereditarios (que ocurren a baja frecuencia en todas las células de modo regular) que pueden influir en la adaptación global de la célula o del organismo superior de modo positivo o negativo. El resultado de la evolución es la selección de aquellos organismos mejor adaptados a vivir en un medio particular. Las propiedades celulares no tienen por qué ser constantes a lo largo del desarrollo de un organismo: evidentemente, el patrón de expresión de los genes varía en respuesta a estímulos externos, además de factores endógenos. Un aspecto importante a controlar es la pluripotencialidad, característica de algunas células que les permite dirigir su desarrollo hacia un abanico de posibles tipos celulares. En metazoos, la genética subyacente a la determinación del destino de una célula consiste en la expresión de determinados factores de transcripción específicos del linaje celular al cual va a pertenecer, así como a modificaciones epigenéticas. Además, la introducción de otro tipo de factores de transcripción mediante ingeniería genética en células somáticas basta para inducir la mencionada pluripotencialidad, luego este es uno de sus fundamentos moleculares. Tamaño, forma y función Comparativa de tamaño entre neutrófilos, células sanguíneas eucariotas (de mayor tamaño), y bacterias Bacillus anthracis, procariotas (de menor tamaño, con forma de bastón). El tamaño y la forma de las células depende de sus elementos más periféricos (por ejemplo, la pared, si la hubiere) y de su andamiaje interno (es decir, el citoesqueleto). Además, la competencia por el espacio tisular provoca una morfología característica: por ejemplo, las células vegetales, poliédricas in vivo, tienden a ser esféricas in vitro. Incluso pueden existir parámetros químicos sencillos, como los gradientes de concentración de una sal, que determinen la aparición de una forma compleja. En cuanto al tamaño, la mayoría de las células son microscópicas, es decir, no son observables a simple vista. (un milímetro cúbico de sangre puede contener unos cinco millones de células), A pesar de ser muy pequeñas, el tamaño de las células es extremadamente variable. La célula más pequeña observada, en condiciones normales, corresponde a Mycoplasma genitalium, de 0,2 μm, encontrándose cerca del límite teórico de 0,17 μm. Existen bacterias con 1 y 2 μm de longitud. Las células humanas son muy variables: hematíes de 7 micras, hepatocitos con 20 micras, espermatozoides de 53 μm, óvulos de 150 μm e, incluso, algunas neuronas de en torno a un metro de longitud. En las células vegetales los granos de polen pueden llegar a medir de 200 a 300 μm. Respecto a las células de mayor tamaño; por ejemplo, los xenofióforos, son foraminíferos unicelulares que han desarrollado un gran tamaño, los cuales alcanzar tamaños macroscópicos (Syringammina fragilissima alcanza los 20 cm de diámetro). Para la viabilidad de la célula y su correcto funcionamiento siempre se debe tener en cuenta la relación superficie-volumen. Puede aumentar considerablemente el volumen de la célula y no así su superficie de intercambio de membrana, lo que dificultaría el nivel y regulación de los intercambios de sustancias vitales para la célula. Respecto de su forma, las células presentan una gran variabilidad, e, incluso, algunas no la poseen bien definida o permanente. Pueden ser: fusiformes (forma de huso), estrelladas, prismáticas, aplanadas, elípticas, globosas o redondeadas, etc. Algunas tienen una pared rígida y otras no, lo que les permite deformar la membrana y emitir prolongaciones citoplasmáticas (pseudópodos) para desplazarse o conseguir alimento. Hay células libres que no muestran esas estructuras de desplazamiento, pero poseen cilios o flagelos, que son estructuras derivadas de un orgánulo celular (el centrosoma) que dota a estas células de movimiento. De este modo, existen multitud de tipos celulares, relacionados con la función que desempeñan; por ejemplo: Células contráctiles que suelen ser alargadas, como los miocitos esqueléticos. Células con finas prolongaciones, como las neuronas que transmiten el impulso nervioso. Células con microvellosidades o con pliegues, como las del intestino para ampliar la superficie de contacto y de intercambio de sustancias. Células cúbicas, prismáticas o aplanadas como las epiteliales que recubren superficies como las losas de un pavimento. Estudio de las células Los biólogos utilizan diversos instrumentos para lograr el conocimiento de las células. Obtienen información de sus formas, tamaños y componentes, que les sirve para comprender además las funciones que en ellas se realizan. Desde las primeras observaciones de células, hace más de 300 años, hasta la época actual, las técnicas y los aparatos se han ido perfeccionando, originando una rama más de la biología: la microscopía. Dado el pequeño tamaño de la gran mayoría de las células, el uso del microscopio es de enorme valor en la investigación biológica. En la actualidad, los biólogos utilizan dos tipos básicos de microscopio: los ópticos y los electrónicos. Un microscopio óptico utiliza la luz visible para el estudio de muestras. Obteniedo imágenes aumentadas a partir de la desviación de la luz con lentes de cristal. Es utilizado para la observación de tejidos y células desde su invención en el siglo XVII. Los microscopios electrónicos son aquellos que utilizan electrones a alta velocidad para el análisis de muestras. Lo cual ofrece mayores capacidades de aumento que los de tipo óptico. Utilizándose en ramas como la medicina, y el estudio de materiales a nivel atómico. Además de usarse para la observación de células, virus y tejidos a nivel subcelular. Sin embargo, estos presentan limitaciones, debido a una cámara de vacío y a la preparación que requieren las muestras para ser analizadas, no pueden observarse células vivas. Escaneo de microscopio electrónico de barrido muestra el SARS-CoV-2 emergiendo de la superficie de las células cultivadas en el laboratorio. Imagen del virus SARS-CoV-2, tomada con un microscopio electrónico de barrido. La célula procariota Artículo principal: Célula procariota Las células procariotas son pequeñas y menos complejas que las eucariotas. Contienen ribosomas, pero carecen de sistemas de endomembranas (esto es, orgánulos delimitados por membranas biológicas, como puede ser el núcleo celular). Por ello poseen el material genético en el citosol. Sin embargo, existen excepciones: algunas bacterias fotosintéticas poseen sistemas de membranas internos. También en el filo Planctomycetota existen organismos como Pirellula que rodean su material genético mediante una membrana intracitoplasmática y Gemmata obscuriglobus que lo rodea con doble membrana. Esta última posee además otros compartimentos internos de membrana, posiblemente conectados con la membrana externa del nucleoide y con la membrana plasmática, que no está asociada a peptidoglucano. Estudios realizados en 2017, demuestran otra particularidad de Gemmata: presenta estructuras similares al poro nuclear, en la membrana que rodea su cuerpo nuclear. Por lo general podría decirse que los procariotas carecen de citoesqueleto. Sin embargo se ha observado que algunas bacterias, como Bacillus subtilis, poseen proteínas tales como MreB y mbl que actúan de un modo similar a la actina y son importantes en la morfología celular. Fusinita van den Ent, en Nature, va más allá, afirmando que los citoesqueletos de actina y tubulina tienen origen procariótico. De gran diversidad, los procariotas sustentan un metabolismo extraordinariamente complejo, en algunos casos exclusivo de ciertos taxa, como algunos grupos de bacterias, lo que incide en su versatilidad ecológica. Los procariotas se clasifican, según Carl Woese, en arqueas y bacterias. Arqueas Artículo principal: Arquea Estructura bioquímica de la membrana de arqueas (arriba) comparada con la de bacterias y eucariotas (en medio): nótese la presencia de enlaces éter (2) en sustitución de los tipo éster (6) en los fosfolípidos. Las arqueas poseen un diámetro celular comprendido entre 0,1 y 15 μm, aunque las formas filamentosas pueden ser mayores por agregación de células. Presentan multitud de formas distintas: incluso las hay descritas cuadradas y planas. Algunas arqueas tienen flagelos y son móviles. Las arqueas, al igual que las bacterias, no tienen membranas internas que delimiten orgánulos. Como todos los organismos presentan ribosomas, pero a diferencia de los encontrados en las bacterias que son sensibles a ciertos agentes antimicrobianos, los de las arqueas, más cercanos a los eucariotas, no lo son. La membrana celular tiene una estructura similar a la de las demás células, pero su composición química es única, con enlaces tipo éter en sus lípidos. Casi todas las arqueas poseen una pared celular (algunos Thermoplasma son la excepción) de composición característica, por ejemplo, no contienen peptidoglicano (mureína), propio de bacterias. No obstante, pueden clasificarse bajo la tinción de Gram, de vital importancia en la taxonomía de bacterias; sin embargo, en arqueas, poseedoras de una estructura de pared en absoluto común a la bacteriana, dicha tinción es aplicable, pero carece de valor taxonómico. El orden Methanobacteriales tiene una capa de pseudomureína, que provoca que dichas arqueas respondan como positivas a la tinción de Gram. Como en casi todos los procariotas, las células de las arqueas carecen de núcleo, y presentan un solo cromosoma circular. Existen elementos extracromosómicos, tales como plásmidos. Sus genomas son de pequeño tamaño, sobre 2-4 millones de pares de bases. También es característica la presencia de ARN polimerasas de constitución compleja y un gran número de nucleótidos modificados en los ácidos ribonucleicos ribosomales. Por otra parte, su ADN se empaqueta en forma de nucleosomas, como en los eucariotas, gracias a proteínas semejantes a las histonas y algunos genes poseen intrones. Pueden reproducirse por fisión binaria o múltiple, fragmentación o gemación. Bacterias Artículo principal: Bacteria Estructura de la célula procariota. Las bacterias son organismos relativamente sencillos, de dimensiones muy reducidas, de apenas unas micras en la mayoría de los casos. Como otros procariotas, carecen de un núcleo delimitado por una membrana, aunque presentan un nucleoide, una estructura elemental que contiene una gran molécula generalmente circular de ADN. Carecen de núcleo celular y demás orgánulos delimitados por membranas biológicas. En el citoplasma se pueden apreciar plásmidos, pequeñas moléculas circulares de ADN que coexisten con el nucleoide y que contienen genes: son comúnmente usados por las bacterias en la parasexualidad (reproducción sexual bacteriana). El citoplasma también contiene ribosomas y diversos tipos de gránulos. En algunos casos, puede haber estructuras compuestas por membranas, generalmente relacionadas con la fotosíntesis. Poseen una membrana celular compuesta de lípidos, en forma de una bicapa y sobre ella se encuentra una cubierta en la que existe un polisacárido complejo denominado peptidoglicano; dependiendo de su estructura y subsecuente su respuesta a la tinción de Gram, se clasifica a las bacterias en Gram positivas y Gram negativas. El espacio comprendido entre la membrana celular y la pared celular (o la membrana externa, si esta existe) se denomina espacio periplásmico. Algunas bacterias presentan una cápsula. Otras son capaces de generar endosporas (estadios latentes capaces de resistir condiciones extremas) en algún momento de su ciclo vital. Entre las formaciones exteriores propias de la célula bacteriana destacan los flagelos (de estructura completamente distinta a la de los flagelos eucariotas) y los pili (estructuras de adherencia y relacionadas con la parasexualidad). La mayoría de las bacterias disponen de un único cromosoma circular y suelen poseer elementos genéticos adicionales, como distintos tipos de plásmidos. Su reproducción, binaria y muy eficiente en el tiempo, permite la rápida expansión de sus poblaciones, generándose un gran número de células que son virtualmente clones, esto es, idénticas entre sí. La célula eucariota Artículo principal: Célula eucariota Las células eucariotas son el exponente de la complejidad celular actual. Presentan una estructura básica relativamente estable caracterizada por la presencia de distintos tipos de orgánulos intracitoplasmáticos especializados, entre los cuales destaca el núcleo, que alberga el material genético. Especialmente en los organismos pluricelulares, las células pueden alcanzar un alto grado de especialización. Dicha especialización o diferenciación es tal que, en algunos casos, compromete la propia viabilidad del tipo celular en aislamiento. Así, por ejemplo, las neuronas dependen para su supervivencia de las células gliales. Por otro lado, la estructura de la célula varía dependiendo de la situación taxonómica del ser vivo: de este modo, las células vegetales difieren de las animales, así como de las de los hongos. Por ejemplo, las células animales carecen de pared celular, son muy variables, no tiene plastos, puede tener vacuolas, pero no son muy grandes y presentan centríolos (que son agregados de microtúbulos cilíndricos que contribuyen a la formación de los cilios y los flagelos y facilitan la división celular). Las células de los vegetales, por su lado, presentan una pared celular compuesta principalmente de celulosa, disponen de plastos como cloroplastos (orgánulo capaz de realizar la fotosíntesis), cromoplastos (orgánulos que acumulan pigmentos) o leucoplastos (orgánulos que acumulan el almidón fabricado en la fotosíntesis), poseen vacuolas de gran tamaño que acumulan sustancias de reserva o de desecho producidas por la célula y finalmente cuentan también con plasmodesmos, que son conexiones citoplasmáticas que permiten la circulación directa de las sustancias del citoplasma de una célula a otra, con continuidad de sus membranas plasmáticas. Diagrama de una célula animal. (1. Nucléolo, 2. Núcleo, 3. Ribosoma, 4. Vesícula, 5. Retículo endoplasmático rugoso, 6. Aparato de Golgi, 7. Citoesqueleto (microtúbulos), 8. Retículo endoplasmático liso, 9. Mitocondria, 10. Vacuola, 11. Citoplasma, 12. Lisosoma, 13. Centríolos). Diagrama de una célula vegetal Compartimentos Las células son entes dinámicos, con un metabolismo celular interno de gran actividad cuya estructura es un flujo entre rutas anastomosadas. Un fenómeno observado en todos los tipos celulares es la compartimentalización, que consiste en una heterogeneidad que da lugar a entornos más o menos definidos (rodeados o no mediante membranas biológicas) en las cuales existe un microentorno que aglutina a los elementos implicados en una ruta biológica. Esta compartimentalización alcanza su máximo exponente en las células eucariotas, las cuales están formadas por diferentes estructuras y orgánulos que desarrollan funciones específicas, lo que supone un método de especialización espacial y temporal. No obstante, células más sencillas, como los procariotas, ya poseen especializaciones semejantes. Membrana plasmática y superficie celular Artículo principal: Membrana plasmática La composición de la membrana plasmática varía entre células dependiendo de la función o del tejido en la que se encuentre, pero posee elementos comunes. Está compuesta por una doble capa de fosfolípidos, por proteínas unidas no covalentemente a esa bicapa, y por glúcidos unidos covalentemente a lípidos o proteínas. Generalmente, las moléculas más numerosas son las de lípidos; sin embargo, las proteínas, debido a su mayor masa molecular, representan aproximadamente el 50 % de la masa de la membrana. Un modelo que explica el funcionamiento de la membrana plasmática es el modelo del mosaico fluido, de J. S. Singer y Garth Nicolson (1972), que desarrolla un concepto de unidad termodinámica basada en las interacciones hidrófobas entre moléculas y otro tipo de enlaces no covalentes. Esquema de una membrana celular. Se observa la bicapa de fosfolípidos, las proteínas y otras moléculas asociadas que permiten las funciones inherentes a este orgánulo. Dicha estructura de membrana sustenta un complejo mecanismo de transporte, que posibilita un fluido intercambio de masa y energía entre el entorno intracelular y el externo. Además, la posibilidad de transporte e interacción entre moléculas de células aledañas o de una célula con su entorno faculta a estas poder comunicarse químicamente, esto es, permite la señalización celular. Neurotransmisores, hormonas, mediadores químicos locales afectan a células concretas modificando el patrón de expresión génica mediante mecanismos de transducción de señal. Sobre la bicapa lipídica, independientemente de la presencia o no de una pared celular, existe una matriz que puede variar, de poco conspicua, como en los epitelios, a muy extensa, como en el tejido conjuntivo. Dicha matriz, denominada glucocalix (glicocáliz), rica en líquido tisular, glucoproteínas, proteoglicanos y fibras, también interviene en la generación de estructuras y funciones emergentes, derivadas de las interacciones célula-célula. Estructura y expresión génica Artículo principal: Expresión génica El ADN y sus distintos niveles de empaquetamiento. Las células eucariotas poseen su material genético en, generalmente, un solo núcleo celular, delimitado por una envoltura consistente en dos bicapas lipídicas atravesadas por numerosos poros nucleares y en continuidad con el retículo endoplasmático. En su interior, se encuentra el material genético, el ADN, observable, en las células en interfase, como cromatina de distribución heterogénea. A esta cromatina se encuentran asociadas multitud de proteínas, entre las cuales destacan las histonas, así como ARN, otro ácido nucleico. Dicho material genético se encuentra inmerso en una actividad continua de regulación de la expresión génica; las ARN polimerasas transcriben ARN mensajero continuamente, que, exportado al citosol, es traducido a proteína, de acuerdo a las necesidades fisiológicas. Asimismo, dependiendo del momento del ciclo celular, dicho ADN puede entrar en replicación, como paso previo a la mitosis. No obstante, las células eucarióticas poseen material genético extranuclear: concretamente, en mitocondrias y plastos, si los hubiere; estos orgánulos conservan una independencia genética parcial del genoma nuclear. Síntesis y degradación de macromoléculas Dentro del citosol, esto es, la matriz acuosa que alberga a los orgánulos y demás estructuras celulares, se encuentran inmersos multitud de tipos de maquinaria de metabolismo celular: orgánulos, inclusiones, elementos del citoesqueleto, enzimas... De hecho, estas últimas corresponden al 20 % de las enzimas totales de la célula. Estructura de los ribosomas; 1) subunidad mayor, 2) subunidad menor. Imagen de un núcleo, el retículo endoplasmático y el aparato de Golgi; 1, Núcleo. 2, Poro nuclear.3, Retículo endoplasmático rugoso (REr).4, Retículo endoplasmático liso (REl). 5, Ribosoma en el RE rugoso. 6, Proteínas siendo transportadas.7, Vesícula (transporte). 8, Aparato de Golgi. 9, Lado cis del aparato de Golgi.10, Lado trans del aparato de Golgi.11, Cisternas del aparato de Golgi. Ribosoma: Los ribosomas, visibles al microscopio electrónico como partículas esféricas, son complejos supramoleculares encargados de ensamblar proteínas a partir de la información genética que les llega del ADN transcrita en forma de ARN mensajero. Elaborados en el núcleo, desempeñan su función de síntesis de proteínas en el citoplasma. Están formados por ARN ribosómico y por diversos tipos de proteínas. Estructuralmente, tienen dos subunidades. En las células, estos orgánulos aparecen en diferentes estados de disociación. Cuando están completos, pueden estar aislados o formando grupos (polisomas). También pueden aparecer asociados al retículo endoplasmático rugoso o a la envoltura nuclear. Retículo endoplasmático: El retículo endoplasmático es orgánulo vesicular interconectado que forma cisternas, tubos aplanados y sáculos comunicados entre sí. Intervienen en funciones relacionadas con la síntesis proteica, glicosilación de proteínas, metabolismo de lípidos y algunos esteroides, detoxificación, así como el tráfico de vesículas. En células especializadas, como las miofibrillas o células musculares, se diferencia en el retículo sarcoplásmico, orgánulo decisivo para que se produzca la contracción muscular. Aparato de Golgi: El aparato de Golgi es un orgánulo formado por apilamientos de sáculos denominados dictiosomas, si bien, como ente dinámico, estos pueden interpretarse como estructuras puntuales fruto de la coalescencia de vesículas. Recibe las vesículas del retículo endoplasmático rugoso que han de seguir siendo procesadas. Dentro de las funciones que posee el aparato de Golgi se encuentran la glicosilación de proteínas, selección, destinación, glicosilación de lípidos y la síntesis de polisacáridos de la matriz extracelular. Posee tres compartimientos; uno proximal al retículo endoplasmático, denominado «compartimento cis», donde se produce la fosforilación de las manosas de las enzimas que han de dirigirse al lisosoma; el «compartimento intermedio», con abundantes manosidasas y N-acetil-glucosamina transferasas; y el «compartimento o red trans», el más distal, donde se transfieren residuos de galactosa y ácido siálico, y del que emergen las vesículas con los diversos destinos celulares. Lisosoma: Los lisosomas son orgánulos que albergan multitud de enzimas hidrolíticas. De morfología muy variable, no se ha demostrado su existencia en células vegetales. Una característica que agrupa a todos los lisosomas es la posesión de hidrolasas ácidas: proteasas, nucleasas, glucosidasas, lisozima, arilsulfatasas, lipasas, fosfolipasas y fosfatasas. Procede de la fusión de vesículas procedentes del aparato de Golgi, que, a su vez, se fusionan en un tipo de orgánulo denominado endosoma temprano, el cual, al acidificarse y ganar en enzimas hidrolíticos, pasa a convertirse en el lisosoma funcional. Sus funciones abarcan desde la degradación de macromoléculas endógenas o procedentes de la fagocitosis a la intervención en procesos de apoptosis. La vacuola regula el estado de turgencia de la célula vegetal. Vacuola vegetal: Las vacuolas vegetales, numerosas y pequeñas en células meristemáticas y escasas y grandes en células diferenciadas, son orgánulos exclusivos de los representantes del mundo vegetal. Inmersas en el citosol, están delimitadas por el tonoplasto, una membrana lipídica. Sus funciones son: facilitar el intercambio con el medio externo, mantener la turgencia celular, la digestión celular y la acumulación de sustancias de reserva y subproductos del metabolismo. Inclusión citoplasmática: Las inclusiones son acúmulos nunca delimitados por membrana de sustancias de diversa índole, tanto en células vegetales como animales. Típicamente se trata de sustancias de reserva que se conservan como acervo metabólico: almidón, glucógeno, triglicéridos, proteínas..., aunque también existen de pigmentos. Conversión energética El metabolismo celular está basado en la transformación de unas sustancias químicas, denominadas metabolitos, en otras; dichas reacciones químicas transcurren catalizadas mediante enzimas. Si bien buena parte del metabolismo sucede en el citosol, como la glucólisis, existen procesos específicos de orgánulos. Modelo de una mitocondria: 1. Membrana interna; 2. Membrana externa; 3. Cresta mitocondrial; 4. Matriz mitocondrial. Mitocondria: Las mitocondrias son orgánulos de aspecto, número y tamaño variable que intervienen en el ciclo de Krebs, fosforilación oxidativa y en la cadena de transporte de electrones de la respiración. Presentan una doble membrana, externa e interna, que dejan entre ellas un espacio perimitocondrial; la membrana interna, plegada en crestas hacia el interior de la matriz mitocondrial, posee una gran superficie. En su interior posee generalmente una sola molécula de ADN, el genoma mitocondrial, típicamente circular, así como ribosomas más semejantes a los bacterianos que a los eucariotas. Según la teoría endosimbiótica, se asume que la primera protomitocondria era un tipo de proteobacteria. Estructura de un cloroplasto. Cloroplasto: Los cloroplastos son los orgánulos celulares que en los organismos eucariotas fotosintéticos se ocupan de la fotosíntesis. Están limitados por una envoltura formada por dos membranas concéntricas y contienen vesículas, los tilacoides, donde se encuentran organizados los pigmentos y demás moléculas implicadas en la conversión de la energía lumínica en energía química. Además de esta función, los plastidios intervienen en el metabolismo intermedio, produciendo energía y poder reductor, sintetizando bases púricas y pirimidínicas, algunos aminoácidos y todos los ácidos grasos. Además, en su interior es común la acumulación de sustancias de reserva, como el almidón. Se considera que poseen analogía con las cianobacterias. Modelo de la estructura de un peroxisoma. Peroxisoma: Los peroxisomas son orgánulos muy comunes en forma de vesículas que contienen abundantes enzimas de tipo oxidasa y catalasa; de tan abundantes, es común que cristalicen en su interior. Estas enzimas cumplen funciones de detoxificación celular. Otras funciones de los peroxisomas son: las oxidaciones flavínicas generales, el catabolismo de las purinas, la beta-oxidación de los ácidos grasos, el ciclo del glioxilato, el metabolismo del ácido glicólico y la detoxificación en general. Se forman de vesículas procedentes del retículo endoplasmático. Citoesqueleto Artículo principal: Citoesqueleto Las células poseen un andamiaje que permite el mantenimiento de su forma y estructura, pero más aún, este es un sistema dinámico que interactúa con el resto de componentes celulares generando un alto grado de orden interno. Dicho andamiaje está formado por una serie de proteínas que se agrupan dando lugar a estructuras filamentosas que, mediante otras proteínas, interactúan entre ellas dando lugar a una especie de retículo. El mencionado andamiaje recibe el nombre de citoesqueleto, y sus elementos mayoritarios son: los microtúbulos, los microfilamentos y los filamentos intermedios.[nota 2] Microfilamentos: Los microfilamentos o filamentos de actina están formados por una proteína globular, la actina, que puede polimerizar dando lugar a estructuras filiformes. Dicha actina se expresa en todas las células del cuerpo y especialmente en las musculares, ya que está implicada en la contracción muscular, por interacción con la miosina. Además, posee lugares de unión a ATP, lo que dota a sus filamentos de polaridad. Puede encontrarse en forma libre o polimerizarse en microfilamentos, que son esenciales para funciones celulares tan importantes como la movilidad y la contracción de la célula durante la división celular. Citoesqueleto eucariota: microfilamentos en rojo, microtúbulos en verde y núcleo en azul. Microtúbulos: Los microtúbulos son estructuras tubulares de 25 nm de diámetro exterior y unos 12 nm de diámetro interior, con longitudes que varían entre unos pocos nanómetros a micrómetros, que se originan en los centros organizadores de microtúbulos y que se extienden a lo largo de todo el citoplasma. Se hallan en las células eucariotas y están formadas por la polimerización de un dímero de dos proteínas globulares, la alfa y la beta tubulina. Las tubulinas poseen capacidad de unir GTP. Los microtúbulos intervienen en diversos procesos celulares que involucran desplazamiento de vesículas de secreción, movimiento de orgánulos, transporte intracelular de sustancias, así como en la división celular (mitosis y meiosis) y que, junto con los microfilamentos y los filamentos intermedios, forman el citoesqueleto. Además, constituyen la estructura interna de los cilios y los flagelos. Filamentos intermedios: Los filamentos intermedios son componentes del citoesqueleto. Formados por agrupaciones de proteínas fibrosas, su nombre deriva de su diámetro, de 10 nm, menor que el de los microtúbulos, de 24 nm, pero mayor que el de los microfilamentos, de 7 nm. Son ubicuos en las células animales, y no existen en plantas ni hongos. Forman un grupo heterogéneo, clasificado en cinco familias: las queratinas, en células epiteliales; los neurofilamentos, en neuronas; los gliofilamentos, en células gliales; la desmina, en músculo liso y estriado; y la vimentina, en células derivadas del mesénquima. Micrografía al microscopio electrónico de barrido mostrando la superficie de células ciliadas del epitelio de los bronquiolos. Centríolos: Los centríolos son una pareja de estructuras que forman parte del citoesqueleto de células animales. Semejantes a cilindros huecos, están rodeados de un material proteico denso llamado material pericentriolar; todos ellos forman el centrosoma o centro organizador de microtúbulos que permiten la polimerización de microtúbulos de dímeros de tubulina que forman parte del citoesqueleto. Los centríolos se posicionan perpendicularmente entre sí. Sus funciones son participar en la mitosis, durante la cual generan el huso acromático, y en la citocinesis, así como, se postula, intervenir en la nucleación de microtúbulos. Cilios y flagelos: Se trata de especializaciones de la superficie celular con motilidad; con una estructura basada en agrupaciones de microtúbulos, ambos se diferencian en la mayor longitud y menor número de los flagelos, y en la mayor variabilidad de la estructura molecular de estos últimos. Ciclo vital Artículo principal: Ciclo celular Diagrama del ciclo celular: la interfase, en naranja, alberga a las fases G1, S y G2; la fase M, en cambio, únicamente consta de la mitosis y citocinesis, si la hubiere. El ciclo celular es el proceso ordenado y repetitivo en el tiempo mediante el cual una célula madre crece y se divide en dos células hijas. Las células que no se están dividiendo se encuentran en una fase conocida como G0, paralela al ciclo. La regulación del ciclo celular es esencial para el correcto funcionamiento de las células sanas, está claramente estructurado en fases El estado de no división o interfase. La célula realiza sus funciones específicas y, si está destinada a avanzar a la división celular, comienza por realizar la duplicación de su ADN. El estado de división, llamado fase M, situación que comprende la mitosis y citocinesis. En algunas células la citocinesis no se produce, obteniéndose como resultado de la división una masa celular plurinucleada denominada plasmodio.[nota 3] A diferencia de lo que sucede en la mitosis, donde la dotación genética se mantiene, existe una variante de la división celular, propia de las células de la línea germinal, denominada meiosis. En ella, se reduce la dotación genética diploide, común a todas las células somáticas del organismo, a una haploide, esto es, con una sola copia del genoma. De este modo, la fusión, durante la fecundación, de dos gametos haploides procedentes de dos parentales distintos da como resultado un zigoto, un nuevo individuo, diploide, equivalente en dotación genética a sus padres. La interfase consta de tres estadios claramente definidos. Fase G1: es la primera fase del ciclo celular, en la que existe crecimiento celular con síntesis de proteínas y de ARN. Es el período que trascurre entre el fin de una mitosis y el inicio de la síntesis de ADN. En él la célula dobla su tamaño y masa debido a la continua síntesis de todos sus componentes, como resultado de la expresión de los genes que codifican las proteínas responsables de su fenotipo particular. Fase S: es la segunda fase del ciclo, en la que se produce la replicación o síntesis del ADN. Como resultado cada cromosoma se duplica y queda formado por dos cromátidas idénticas. Con la duplicación del ADN, el núcleo contiene el doble de proteínas nucleares y de ADN que al principio. Fase G2: es la segunda fase de crecimiento del ciclo celular en la que continúa la síntesis de proteínas y ARN. Al final de este período se observa al microscopio cambios en la estructura celular, que indican el principio de la división celular. Termina cuando los cromosomas empiezan a condensarse al inicio de la mitosis. La fase M es la fase de la división celular en la cual una célula progenitora se divide en dos células hijas idénticas entre sí y a la madre. Esta fase incluye la mitosis, a su vez dividida en: profase, metafase, anafase, telofase; y la citocinesis, que se inicia ya en la telofase mitótica. La incorrecta regulación del ciclo celular puede conducir a la aparición de células precancerígenas que, si no son inducidas al suicidio mediante apoptosis, puede dar lugar a la aparición de cáncer. Los fallos conducentes a dicha desregulación están relacionados con la genética celular: lo más común son las alteraciones en oncogenes, genes supresores de tumores y genes de reparación del ADN. Origen Artículos principales: Historia de la vida y Anexo:Cronología de la historia evolutiva de la vida. Origen de la primera célula Artículo principal: Abiogénesis La aparición de la vida, y, por ello, de la célula, probablemente se inició gracias a la transformación de moléculas inorgánicas en orgánicas bajo unas condiciones ambientales adecuadas, produciéndose más adelante la interacción de estas biomoléculas generando entes de mayor complejidad. El experimento de Miller y Urey, realizado en 1953, demostró que una mezcla de compuestos orgánicos sencillos puede transformarse en algunos aminoácidos, glúcidos y lípidos (componentes todos ellos de la materia viva) bajo unas condiciones ambientales que simulan las presentes hipotéticamente en la Tierra primigenia (en torno al eón Hádico). Se ha sugerido que el último antepasado común universal vivió hace más de 4200 millones de años. Se postula que dichos componentes orgánicos se agruparon generando estructuras complejas, los coacervados de Oparin, aún acelulares que, en cuanto alcanzaron la capacidad de autoorganizarse y perpetuarse, dieron lugar a un tipo de célula primitiva, el progenote de Carl Woese, antecesor de los tipos celulares actuales. Una vez se diversificó este grupo celular, dando lugar a las variantes procariotas, arqueas y bacterias, pudieron aparecer nuevos tipos de células, más complejos, por endosimbiosis, esto es, captación permanente de unos tipos celulares en otros sin una pérdida total de autonomía de aquellos. De este modo, algunos autores describen un modelo en el cual la primera célula eucariota surgió por introducción de una arquea en el interior de una bacteria, dando lugar esta primera a un primitivo núcleo celular. No obstante, la imposibilidad de que una bacteria pueda efectuar una fagocitosis y, por ello, captar a otro tipo de célula, dio lugar a otra hipótesis, que sugiere que fue una célula denominada cronocito la que fagocitó a una bacteria y a una arquea, dando lugar al primer organismo eucariota. De este modo, y mediante un análisis de secuencias a nivel genómico de organismos modelo eucariotas, se ha conseguido describir a este cronocito original como un organismo con citoesqueleto y membrana plasmática, lo cual sustenta su capacidad fagocítica, y cuyo material genético era el ARN, lo que puede explicar, si la arquea fagocitada lo poseía en el ADN, la separación espacial en los eucariotas actuales entre la transcripción (nuclear), y la traducción (citoplasmática). Una dificultad adicional es el hecho de que no se han encontrado organismos eucariotas primitivamente amitocondriados como exige la hipótesis endosimbionte. Además, el equipo de María Rivera, de la Universidad de California, comparando genomas completos de todos los dominios de la vida ha encontrado evidencias de que los eucariotas contienen dos genomas diferentes, uno más semejante a bacterias y otro a arqueas, apuntando en este último caso semejanzas a los metanógenos, en particular en el caso de las histonas. Esto llevó a Bill Martin y Miklós Müller a plantear la hipótesis de que la célula eucariota surgiera no por endosimbiosis, sino por fusión quimérica y acoplamiento metabólico de un metanógeno y una α-proteobacteria simbiontes a través del hidrógeno (hipótesis del hidrógeno). Esta hipótesis atrae hoy en día posiciones muy encontradas, con detractores como Christian de Duve. Harold Morowitz, un físico de la Universidad Yale, ha calculado que las probabilidades de obtener la bacteria viva más sencilla mediante cambios al azar es de 1 sobre 1 seguido por 100 000 000 000 ceros. «Este número es tan grande ,dijo Robert Shapiro, que para escribirlo en forma convencional necesitaríamos varios centenares de miles de libros en blanco». Presenta la acusación de que los científicos que han abrazado la evolución química de la vida pasan por alto la evidencia aumentante y «han optado por aceptarla como verdad que no puede ser cuestionada, consagrándola así como mitología». Origen de la célula eucariota Artículo principal: Eucariogénesis En la teoría de la simbiogenesis, la fusión entre una arquéa y una bacteria aeróbia creo la célula eucariota, con mitochondrias aeróbicas, hace unos 2500 millones de años. Una segunda fusión, hace 2000 millones de años, añadió los cloroplastos, originando la célula vegetal. Las células eucariotas se formaron hace 2500 millones de años en un proceso llamado eucariogénesis. Se acepta ampliamente que esto implicó una simbiogénesis, en la que una arquea y una bacteria se unieron para crear el primer ancestro común eucariota. Esta célula tenía un nuevo nivel de complejidad y capacidad, con un núcleo y mitocondrias facultativamente aeróbicas. Evolucionó hasta convertirse en una población de organismos unicelulares que incluía al último ancestro común eucariota, acumulando capacidades a lo largo del camino, aunque la secuencia de los pasos involucrados ha sido cuestionada y es posible que no haya comenzado con la simbiogénesis. Presentaba al menos un centriolo y cilio, sexo (meiosis y singamia), peroxisomas y un quiste latente con una pared celular de quitina y/o celulosa. A su vez, el último ancestro común eucariota dio origen al grupo terminal de los eucariotas, que contiene los ancestros de animales, hongos, plantas y una amplia gama de organismos unicelulares. Las células vegetales se formaron hace unos 2000 millones de años con un segundo episodio de simbiogénesis al que se añadieron cloroplastos, derivados de una cianobacteria."
ksampletext_wikipedia_biol_filogenia: str = "Filogenia. La filogenia es la relación de parentesco entre especies o taxones en general. Aunque el término también aparece en lingüística histórica para referirse a la clasificación de las lenguas humanas según su origen común, el término se utiliza principalmente en su sentido biológico. La filogenética es una disciplina de la biología evolutiva que se ocupa de comprender las relaciones históricas entre diferentes grupos de organismos a partir de la distribución en un árbol o cladograma dicotómico de los caracteres derivados (sinapomorfías) de un ancestro común a dos o más taxones que contiene aquellos caracteres plesiomórficos en común. Incluso en el campo del cáncer, la filogenética permite estudiar la evolución clonal (evolución de los clones de la célula cancerosa original, debido a las mutaciones que ocurran) de los tumores y la cronología molecular, viéndose como varían las poblaciones celulares a lo largo de la progresión de la enfermedad, incluso durante el tratamiento de la misma, mediante el empleo de técnicas de secuenciación del genoma completo en muestras de ADN circulante tumoral. Para reconstruir la filogenia de un grupo taxonómico (familia, género, subgénero, etc.) es imprescindible construir matrices basadas en datos morfológicos y/o moleculares (ADN, ARN y proteínas). Las matrices son analizadas con determinados algoritmos que permiten encontrar los árboles filogenéticos más cortos siguiendo el principio de parsimonia, que supone la menor cantidad de cambios bajo el supuesto de que la evolución acontece de la manera más simple, esto es: los árboles que son considerados como la mejor opción filogenética son aquellos más cortos, es decir, más parsimoniosos. Interpretar los árboles obtenidos implica rastrear la historia del grupo bajo un paradigma evolutivo basado en el supuesto de un antecesor común del que van derivando cada uno de los clados, considerando que estos solo se sustentan por homologías. La condición de homología es resultante de la aceptación a priori de la existencia de monofilia. Explicar las relaciones de filogenéticas sobre la base del mapa de caracteres que ofrecen los cladogramas permite construir clasificaciones más naturales, uno de los propósitos centrales de la sistemática, una disciplina cuyos orígenes, en términos académicos, se remontan a los aportes de Linneo. No obstante, muchas clasificaciones han tenido diversos propósitos y responden a metodologías y criterios diferentes. Las primeras han sido artificiales y meramente utilitarias; otras se han basado en criterios que la ciencia ha depuesto en la actualidad, sustituyendo las categorías taxonómicas o los sistemas de clasificación creados bajo esas metodologías por otros que son legitimados por los científicos. Entre las corrientes más relevantes respecto de las clasificaciones biológicas mediadas por la metodología se encuentran en la actualidad dos programas de investigación que en sus inicios se presentaron como antagónicos: el feneticismo y el cladismo y que si bien comparten el propósito de encontrar un sistema que ordene a la diversidad de especies y de categorías taxonómicas se basan en postulados, supuestos y teorías auxiliares diversas y en metodologías diferentes. La sistemática filogenética se ha impuesto con el devenir de los años a causa de que la homología (que no constituye una premisa bajo la lógica feneticista) es consistente con el supuesto de un antecesor común y, por lo tanto, congruente con la evolución y, en consecuencia, con la posibilidad de definir arreglos taxonómicos más naturales. Esta necesidad de conocer la historia evolutiva de los seres vivos inicia con la publicación de El origen de las especies por Charles Darwin en 1859, aunque existen ideas previas que al menos desde Aristóteles han intentado explicar la diversidad de las formas de vida y sus relaciones. No obstante, explicar las relaciones históricas entre especies en función de la evolución es una tarea interminable y provisoria tal como lo es el conocimiento científico, sujeto a marcos teóricos y a coyunturas políticas. Uno de los hitos en relación con la justificación de estas relaciones fueron las contribuciones de Willi Hennig (entomólogo alemán, 1913-1976), Walter Zimmermann (botánico alemán, 1892-1980), Warren H. Wagner, Jr. (botánico estadounidense, 1920-2000) entre otros por la centralidad de sus aportes, tanto desde el punto de vista teórico como metodológico. Técnicas y uso Filogenética molecular Es la técnica de la filogenia que investiga las relaciones de los seres vivos mediante análisis moleculares de la secuencia de ADN, ARN y proteínas. Constituye la herramienta principal de la biología evolutiva moderna para inferir parentescos, especialmente en grupos donde los rasgos morfológicos son escasos o poco informativos, como los microorganismos. En este enfoque, las similitudes en la secuencia de nucleótidos y aminoácidos se interpretan como sinapomorfías en el análisis, es decir, características compartidas derivadas de un ancestro común. No obstante, dichas similitudes pueden variar considerablemente, ya que un organismo puede compartir secuencias idénticas con varios linajes cercanos, lo que puede generar hipótesis filogenéticas alternativas. La filogenética molecular ha permitido identificar numerosos clados evolutivos que no habían sido reconocidos mediante análisis morfológicos y ha corregido múltiples errores derivados del estudio exclusivo de características anatómicas. No obstante, esta aproximación también puede verse afectada por fenómenos como la atracción de ramas largas, en la que linajes con tasas de evolución acelerada se agrupan erróneamente. Por ello, se emplean modelos evolutivos más complejos y métodos estadísticos como la inferencia bayesiana o la máxima verosimilitud para minimizar dichos sesgos y obtener árboles más precisos. En la actualidad, la filogenética molecular se apoya en herramientas bioinformáticas y en bases de datos genómicas de gran escala, lo que permite analizar miles de genes o incluso genomas completos (filogenómica). Estas técnicas han revolucionado la clasificación biológica y la comprensión de la historia evolutiva de los organismos, contribuyendo a redefinir la sistemática moderna y a mejorar la identificación de especies, el estudio de la biodiversidad y la reconstrucción de eventos evolutivos profundos. Filogenética morfológica Es la técnica de la filogenia que investiga las relaciones de los seres vivos mediante análisis morfológicos como anatomía comparada, homología, embriología, alometría y fósiles. Las similitudes morfológicas entre organismos pueden ser un indicativo de parentesco, pero posteriormente se demostró que las similitudes morfológicas pueden evolucionar convergentemente en linajes diferentes. Actualmente, se usan ciertos caracteres morfológicos (sinapomorfías) que pueden emplearse para determinar las relaciones. La filogenética morfológica es empleada por los paleontólogos para determinar las relaciones entre los fósiles y los grupos existentes. También es usada por algunos zoólogos y botánicos evolutivos para determinar ciertos caracteres morfológicos válidos entre sus grupos de estudio (animales y plantas). Antiguamente, se usó para estudiar las relaciones entre los microorganismos, pero su uso quedó obsoleto debido a la ausencia de caracteres morfológicos en estos grupos. Filogenética molecular-estructural Es una técnica filogenética novedosa que investiga las relaciones mediante un tipo de biomolécula específico o similar que porten los organismos, sin tomar en cuenta la secuencia. Es similar a la filogenética morfológica en el hecho de que la sinapomorfía es una biomolécula única o similar que portan dichos organismos sin recurrir a la secuencia. Por ejemplo, las bacterias son un dominio que se caracteriza por tener una pared celular de peptidoglicanos. Las bacterias de Sphingobacteria se caracterizan por tener esfingolípidos. Los dominios de virus Riboviria, Duplodnaviria, Adnaviria y Varidnaviria se determinaron filogenéticamente mediante la presencia de una proteína única o similar estructuralmente. Inferencia de árboles filogenéticos La reconstrucción de árboles filogenéticos, a partir de datos moleculares o morfológicos, requiere de métodos que permitan inferir las relaciones evolutivas entre los taxones de interés. La filogenética computacional es una rama de la bioinformática, que aplica algoritmos y herramientas informáticas para reconstruir y analizar árboles filogenéticos. Entre los enfoques más empleados para estos propósito se encuentran la máxima parsimonia, los métodos basados en distancias, la máxima verosimilitud y la inferencia bayesiana, cada uno basado en distintos supuestos sobre la evolución de los caracteres. Método de máxima parsimonia: es un modelo de reconstrucción filogenética basado en el principio de parsimonia, según el cual se busca el árbol filogenético que implique la menor cantidad posible de cambios evolutivos o transiciones de un estado a otro. Aunque este método no se utiliza con tanta frecuencia en la actualidad, sigue siendo una herramienta importante para la reconstrucción de árboles, especialmente cuando no se cuenta con datos moleculares y se trabaja con caracteres morfológicos. Métodos basados en distancias: estos métodos de reconstrucción estiman las distancias evolutivas entre pares de taxones. Esta distancia se calcula generalmente alineando las secuencias de ADN o de proteínas y evaluando cuánto difieren entre sí. Una vez obtenidas las distancias, se reconstruye el árbol filogenético mediante algoritmos de agrupamiento que unen primero a los taxones más similares. Algunos de los algoritmos más utilizados son Neighbor-Joining y UPGMA. Método de máxima verosimilitud: en este enfoque de reconstrucción filogenética se asume que el árbol representa un modelo de evolución, y se busca encontrar la topología y la longitud de las ramas que maximizan la probabilidad de que los datos observados hayan ocurrido bajo dicho modelo. Para realizar este proceso, el método requiere un modelo de sustitución que describa la probabilidad de que un nucleótido o un aminoácido cambie por otro a lo largo del tiempo. Aunque es computacionalmente más costoso, es uno de los métodos más utilizados actualmente. Método de inferencia bayesiana: este método aplica la inferencia bayesiana para realizar la reconstrucción filogenética. En este método, el modelo de sustitución, la topología del árbol y las longitudes de las ramas se tratan como parámetros del modelo, y se buscan los valores que maximizan la probabilidad posterior, la cual combina la función de verosimilitud de los datos, la probabilidad previa de los parámetros y la probabilidad marginal de los datos. Debido a su alto costo computacional, normalmente se emplea el algoritmo de cadenas de Markov de Monte Carlo (MCMC) para realizar la inferencia de los parametros. Caracteres y estados del carácter El primer paso para reconstruir la filogenia de los organismos es determinar cuanta similitud hay entre sí, ya sea en morfología, anatomía, embriología, biogeografía, moléculas de ADN, ARN o proteínas, ya que en última instancia estos parecidos pueden ser un indicador de su parecido genético, y, por lo tanto, de sus relaciones evolutivas. La evolución es un proceso muy lento, y en la gran mayoría de los casos nadie la ha visto suceder. Lo que se maneja es una serie de hipótesis acerca de cómo ocurrió la diversificación de los organismos, que desembocó en la aparición de las distintas especies variadamente relacionadas entre sí. Esas hipótesis son las que determinan cómo deberían analizarse los organismos para determinar su filogenia. Supongamos una única población ancestral de plantas. Para establecer que los organismos que componen esta población son morfológicamente similares entre sí, determinamos una serie de caracteres: color de pétalo, leñosidad del tallo, presencia o ausencia de tricomas en las hojas, cantidad de estambres, fruto seco o carnoso, y rugosidad de la semilla. Todas las plantas de esta población ancestral comparten los mismos estados del carácter para cada uno de ellos: los pétalos son blancos, el tallo herbáceo, las hojas sin tricomas, los estambres son 5, el fruto es seco, y la semilla lisa. Finalmente, mediante algún mecanismo de aislamiento reproductivo, la población se divide en dos subpoblaciones que no intercambian material genético entre sí. Al cabo de algunas generaciones, se va haciendo evidente que aparecen mutantes en las dos subpoblaciones nuevas. Algunos de ellos son más exitosos reproductivamente que el resto de la población y, por lo tanto, después de unas generaciones más, su genotipo se convierte en el dominante en esa población. Como las mutaciones ocurren al azar en cada subpoblación, y la probabilidad de que ocurra espontáneamente la misma mutación en cada subpoblación es muy baja, las dos subpoblaciones van acumulando diferentes mutaciones exitosas, generando diferentes genotipos, que se pueden ver reflejados en los cambios que ocurren en los estados de los caracteres. Así, por ejemplo, la subpoblación 1 pasó a poseer el tallo leñoso, y la subpoblación 2 pasó a poseer los pétalos rojos (pero conservando el tallo herbáceo ancestral). Como resultado, la última generación de plantas corresponde a dos poblaciones muy similares entre sí, con muchos caracteres compartidos, salvo la leñosidad del tallo y el color de los pétalos. Nosotros, en nuestra corta vida, solo vemos este resultado de la evolución, e hipotetizamos que lo que ocurrió fue el proceso que se indica más arriba. Esta hipótesis se puede reflejar en un árbol filogenético, un diagrama que resume las relaciones de parentesco entre los ancestros y sus descendientes, como el siguiente: Árbol filogenético que muestra cómo, después de un evento de aislamiento reproductivo entre dos poblaciones de la misma especie, apareció una mutación exitosa en cada población, que pasaron a diferenciarse entre ellas mediante la observación de los estados de sus caracteres. En el cladograma, la especie 1 comparte con su ancestro todos los estados de los caracteres salvo el tallo, que es leñoso. La especie 2, a su vez, comparte con su ancestro todos los caracteres salvo el color de los pétalos, que es rojo. Las dos especies comparten entre sí todos los caracteres salvo la leñosidad del tallo y el color de los pétalos. En este ejemplo, se han establecido 2 linajes: secuencias de poblaciones desde el ancestro hasta los descendientes. En los inicios de la sistemática, los caracteres utilizados para comparar a los grupos entre sí eran conspicuos, principalmente morfológicos. A medida que se acumuló más conocimiento se empezó a tomar cada vez más cantidad de caracteres crípticos, como los anatómicos, embriológicos, serológicos, químicos y finalmente caracteres del cariotipo y los derivados del análisis molecular. Los caracteres correspondientes al ancestro de un grupo de organismos que son retenidos por el grupo se dice que son plesiomórficos (ancestrales), mientras que los que fueron adquiridos exclusivamente por ese grupo (en el ejemplo, el tallo leñoso para la especie 1 o los pétalos rojos para la especie 2) se dice que son sinapomórficos o derivados (nuevos). Nótese que solo la presencia de sinapomorfías nos indica que se ha formado un nuevo linaje, nótese también que en árboles filogenéticos más extensos, como el siguiente: Árbol filogenético que muestra un ejemplo de diversificación de una especie ancestral en 5 especies presentes en la actualidad. el mismo carácter puede ser una sinapomorfía o una plesiomorfía, según desde qué porción del árbol se la observe. Por ejemplo, el tallo leñoso es una sinapomorfía de C (y de C+A+B) pero una plesiomorfía para A, ya que comparte ese estado del carácter con B a través de su ancestro común. Otra forma de decirlo es que el tallo leñoso es un carácter derivado desde el punto de vista de la población original, pero es ancestral para A y para B. El aspecto del árbol filogenético (su topología) solo está dado por las conexiones entre sus nodos, y no por el orden en que son diagramados. Así, [[A+B]+C] es el mismo árbol que [C+[A+B]]. La topología tampoco está dada por la posición en que el árbol es dibujado, a veces se los dibuja erectos (con el ancestro abajo y los grupos terminales arriba), a veces se los dibuja recostados (con el ancestro a la izquierda y los grupos terminales a la derecha). Las dos formas de dibujarlos son igualmente válidas. En los árboles filogenéticos como los aquí expuestos, el largo de las ramas tampoco da ninguna información acerca de cuánto diverge ese linaje en términos de sus caracteres ni acerca de en qué momento geológico ocurrió el aislamiento de ese linaje (pero hay árboles que sí dan esa información). Un cladograma es un árbol filogenético que solo muestra las relaciones evolutivas, sin darle un significado a sus ramas. Por el otro lado, hay dos tipos de árboles filogenéticos con significado en la longitud de sus ramas: el cronograma, donde la longitud de las ramas indica el tiempo transcurrido entre un nodo y otro, y la posición en el tiempo de cada nodo con respecto a los otros; y el filograma, donde la longitud de las ramas indica la cantidad de cambio evolutivo desde el ancestro común más cercano. Monofilia, parafilia y polifilia Artículos principales: Monofilético, Parafilético y Polifilético. Grupos filogenéticos: monofilético, parafilético, polifilético. Un grupo formado por un ancestro y todos sus descendientes se denomina monofilético, también llamado clado. Al grupo al que se le ha excluido alguno de sus descendientes se lo llama parafilético. Los grupos formados por los descendientes de más de un ancestro se denominan polifiléticos.[cita requerida] Por ejemplo, se cree que las aves y los reptiles descienden de un único ancestro común, luego este grupo taxonómico (amarillo en el diagrama) es considerado monofilético. Los reptiles actuales como grupo también tienen un ancestro común a todos ellos, pero ese grupo (reptiles modernos) no incluye a todos los descendientes de tal ancestro porque se está dejando a las aves fuera (solo incluye los de color cian en el diagrama); por lo que un grupo así se considera como parafilético. Un grupo que incluyera a los vertebrados de sangre caliente contendría solo a los mamíferos y las aves (rojo/naranja en el diagrama) y sería polifilético, porque entre los miembros de este agrupamiento no está el más reciente ancestro común de ellos. Los animales de sangre caliente son todos descendientes de un ancestro de sangre fría. La condición endotérmica (sangre caliente) ha aparecido dos veces, independientemente, en el ancestro de los mamíferos, por un lado, y en el de las aves (y quizá algunos o todos los dinosaurios), por otro. Algunos autores sostienen que la diferencia entre grupos parafiléticos y polifiléticos es sutil, y prefieren llamar a estos dos tipos de asemblajes como no monofiléticos. Muchos taxones largamente reconocidos de plantas y animales resultaron ser no monofiléticos según los análisis de filogenia hechos en las últimas décadas, por lo que muchos científicos recomendaron abandonar su uso, ejemplos de estos taxones son Prokaryota, Protista, Pisces, Reptilia, Pteridophyta, Dicotyledoneae, y varios otros más. Como su uso está muy extendido por haber sido tradicionalmente reconocidos, y porque muchos científicos consideran a los taxones parafiléticos válidos (discusión que aún no está terminada en el ambiente científico; el ejemplo más claro de un taxón que muchos desean conservar quizás sean Reptilia), a veces se indica el nombre del taxón, con la salvedad de que su nombre se pone entre comillas, para indicar que el taxón no se corresponde con un clado. La diferencia entre un clado y un taxón es que un clado debe ser un grupo natural (monofilético), mientras que un taxón puede ser o no monofilético, pero los taxones no monofiléticos pierden su validez en la clasificación actual de los organismos. El rol de las sinapomorfías en el análisis filogenético Las sinapomorfías que caracterizan a cada grupo monofilético son estados de los caracteres que se originaron en el ancestro común a todos los miembros del grupo, pero que no estaban presentes en los ancestros anteriores a estos, ancestros comunes tanto a los miembros del grupo como a otros grupos más. Hay que tener en cuenta que si bien una sinapomorfía es un estado del carácter que se hipotetiza que está presente en el ancestro del grupo, no necesariamente será encontrada en todos sus descendientes, debido a que la evolución puede modificarla y hasta revertirla a su estado anterior por azar (proceso que se conoce como reversión). Por lo tanto, no está garantizado que una lista de sinapomorfías vaya a encontrarse en todos los miembros de un grupo, y solo mediante un síndrome de caracteres podemos asegurarnos de que cada miembro pertenece a ese clado.[cita requerida] El concepto de sinapomorfía fue formalizado por primera vez por Hennig (1966) y Wagner (1980). Mucho del análisis filogenético actual se basa en la búsqueda de sinapomorfías que permitan establecer grupos monofiléticos. En ese sentido, son revolucionarios los análisis moleculares de ADN que se están realizando desde hace algunos años, que entre otras técnicas determinan la secuencia de bases del mismo trozo de ADN en diferentes taxones, y comparan directamente sus secuencias de bases. En estos análisis, que se realizan con secuencias conservadas de genes concretos (como el ARNr), cada base es un carácter, y los posibles estados del carácter son las 4 posibles bases: adenina, timina, guanina y citosina. Si bien las sinapomorfías encontradas a través de los análisis moleculares de ADN son oscuras y no son útiles para identificar organismos en el campo o para plantear hipótesis acerca de la adaptación de los organismos a su ambiente, poseen ventajas (como la cantidad de caracteres medidos con poca cantidad de recursos, el establecimiento de caracteres menos subjetivos que los basados en fenotipos), que le otorgan a los análisis filogenéticos una precisión sin precedentes, obligando en muchas ocasiones a abandonar hipótesis evolutivas largamente reconocidas. Además, según la hipótesis del reloj molecular, la comparación de secuencias de ADN permite no solo determinar la distancia genética entre dos especies, sino además estimar el tiempo transcurrido desde el último antecesor común.[cita requerida] Sinapomorfías y especies La regla para construir los árboles filogenéticos es el reconocimiento de grupos monofiléticos (clados) a partir de sus sinapomorfías (estados de los caracteres comunes al grupo). Esto es cierto para todos los nodos del árbol salvo el terminal, a nivel de las especies. No se puede establecer monofilia a nivel de las especies debido a que la naturaleza de las relaciones entre los organismos cambia por encima y por debajo del nivel de especie: por encima del nivel de especie, organismos de dos clados diferentes no pueden cruzarse entre sí y dar descendencia fértil, por lo que sus bagajes genéticos se mantienen sin mezclarse. Por debajo del nivel de especie, existe interfertilidad entre los organismos, por lo que el genoma de cada organismo es el resultado del cruce de dos genomas diferentes. Esta diferencia se puede esquematizar como un árbol ramificado para representar a todas las agrupaciones de organismos por encima del nivel de especie, pero en los organismos que pertenecen a la misma especie, las ramas del árbol se entrecruzan entre sí creando una red interconectada de organismos. Como muchas poblaciones del planeta están en diferentes etapas del proceso de especiación, y a veces se reconocen dos poblaciones diferentes como especies diferentes a pesar de ser algo interfértiles, entonces no es fácil determinar si un estado de un carácter es exclusivo de una de las especies o pertenece también en una baja proporción no muestreada a la otra especie, o si pertenecerá en algún momento debido a una hibridación casual, a la otra especie. Avances recientes En los últimos años, el uso de datos genómicos a gran escala ha transformado la filogenia al permitir la reconstrucción de árboles evolutivos con miles de loci ,lo que se conoce como filogenómica, y enfrentarse simultáneamente a nuevos retos como la inferencia de ortología/paralogs, la hibridación, el intercambio genético lateral y la incongruencia entre genes y especies (Zaharias et al., 2022). Además, se está reconociendo que en algunos linajes complejos, como ciertos grupos de angiospermas, el modelo clásico de árbol bifurcado podría no bastar y se requiere considerar redes evolutivas o grafos reticulados para reflejar procesos de introgresión y duplicación genómica (Li et al., 2025)."
ksampletext_wikipedia_biol_botanica: str = "Botánica. La botánica (del griego, hierba) es la rama de la biología que estudia las plantas, en sentido amplio, incluyendo a las algas, hongos y organismos fotosintéticos no necesariamente clasificados como plantas, bajo todos sus aspectos, incluyendo la descripción, clasificación, distribución, identificación, estudio de la reproducción, fisiología, morfología, relaciones recíprocas, relaciones con los otros seres vivos y efectos provocados sobre el medio en el que se encuentran. Los términos para quien se dedica a esta disciplina son dos: botánico /a y botanista. La botánica estudia las plantas en sentido amplio, abarcando las categorías taxonómicas de las plantas sin flores (criptógamas), las plantas sin flores y sin vasos (briofitas), las plantas sin flores y con vasos (pteridofitas), las plantas con flores (espermatofitas), las plantas con flores y sin fruto (gimnospermas) y las plantas con flores y con fruto (angiospermas), dentro de la clasificación clásica de los organismos vegetales. No obstante, en términos históricos, el objeto de estudio de la botánica no se ha restringido estrictamente al Reino Plantae, sino que ha abarcado un grupo de organismos lejanamente emparentados entre sí, esto es, las cianobacterias, los hongos, las algas y las plantas, los que casi no poseen ningún carácter en común salvo la presencia de cloroplastos (a excepción de los hongos y cianobacterias) o el no poseer capacidad de desplazamiento. En el campo de la botánica hay que distinguir entre la botánica pura, cuyo objeto es ampliar el conocimiento de la naturaleza, y la botánica aplicada, cuyas investigaciones están al servicio de la tecnología agraria, forestal y farmacéutica. Su conocimiento afecta a muchos aspectos de nuestra vida y por tanto es una disciplina estudiada por biólogos y ambientólogos, pero también por farmacéuticos, ingenieros agrónomos, ingenieros forestales, entre otros. La botánica cubre una amplia gama de contenidos, que incluyen aspectos específicos propios de los vegetales, así como de las disciplinas biológicas que se ocupan de la composición química (fitoquímica), de la organización celular y tisular (histología vegetal), del metabolismo y el funcionamiento orgánico (fisiología vegetal), del crecimiento y el desarrollo, de la morfología (fitografía), de la reproducción, de la herencia (genética vegetal), de las enfermedades (fitopatología), de las adaptaciones al ambiente (ecología), de la distribución geográfica (fitogeografía o geobotánica), de los fósiles (paleobotánica) y de la evolución. Los organismos que estudia la botánica La idea de que la naturaleza puede ser dividida en tres reinos (mineral, vegetal y animal) fue propuesta por Nicolás Lemery (1675) y popularizada por Carlos Linneo en el siglo XVIII. Carlos Linneo, a finales del siglo XVIII, introdujo el actual sistema de clasificación. Este incluye los conocimientos sobre las diversas especies vegetales dentro de un sistema más amplio, ofreciendo una versión sintética y enriquecedora. No en vano se ha dicho que el sistema de clasificación de Linneo prefigura lo que después serían las teorías evolutivas. A pesar de que con posterioridad fueron determinados como reinos separados para los hongos (en 1783), protozoarios (en 1858) y bacterias (en 1925) la concepción del siglo XVII de que solo existían dos reinos de organismos dominó la biología por tres siglos. El descubrimiento de los protozoarios en 1675, y de las bacterias en 1683, ambos realizados por Leeuwenhoek, finalmente comenzó a minar el sistema de dos reinos. No obstante, un acuerdo general entre los científicos acerca de que el mundo viviente debería ser clasificado en al menos cinco reinos, solo fue logrado luego de los descubrimientos realizados por la microscopía electrónica en la segunda mitad del siglo XX. Tales hallazgos confirmaron que existían diferencias fundamentales entre las bacterias y los eucariotas y, además, revelaron la tremenda diversidad ultraestructural de los protistas. La aceptación generalizada de la necesidad de utilizar varios reinos para incluir a todos los seres vivos también debe mucho a la síntesis sistemática de Herbert Copeland (1956) y a los influyentes trabajos de Roger Y. Stanier (1961-1962) y Robert H. Whittaker (1969).En el sistema de seis reinos, propuesto por Thomas Cavalier-Smith en 1983 y modificado en 1998, las bacterias son tratadas en un único reino (Bacteria) y los eucariotas se dividen en 5 reinos: protozoarios (Protozoa), animales (Animalia), hongos (Fungi), plantas (Plantae) y Chromista (algas cuyos cloroplastos contienen clorofilas a y c, así como otros organismos sin clorofila relacionados con ellas). La nomenclatura de estos tres últimos reinos, clásico objeto de estudio de la botánica, está sujeta a las reglas y recomendaciones del Código Internacional de Nomenclatura Botánica. Divisiones de la botánica Familias botánicas Las plantas pueden estudiarse desde varios puntos de vista, así, pueden diferenciarse distintas líneas de trabajo de acuerdo con los niveles de organización que se estudien: desde las moléculas y las células, pasando por los tejidos y los órganos, hasta los individuos, las poblaciones y las comunidades vegetales. Otras posibilidades se refieren al estudio de las plantas que vivieron en épocas geológicas pasadas o al de las que viven en la actualidad, al examen de los distintos grupos sistemáticos y a la investigación de cómo pueden ser utilizados los vegetales por el ser humano. Una de las metas más importantes para la botánica, es que junto a la biotecnología e ingeniería genética puedan llegar a crear vida. En general, todas esas direcciones de trabajo se basan en el análisis comparativo de los fenómenos particulares y de su variabilidad, para llegar a una generalización y al reconocimiento de las relaciones regulares que unen dichos fenómenos entre sí. Siempre deben asociarse los métodos estático y dinámico: por un lado el reconocimiento y la interpretación de las estructuras y formas y, por el otro, el análisis de los procesos vitales, de funciones y de fenómenos de desarrollo. El objetivo final de ambos métodos debe ser en todo caso la comprensión de las formas y de las funciones en su dependencia recíproca y en su evolución. Los distintos puntos de vista descritos y el empleo de diferentes métodos de trabajo han conducido a que dentro de la botánica se hayan desarrollado numerosas disciplinas. En primer lugar, se puede citar a la Morfología, la cual, en sentido amplio, es la teoría general de la estructura y forma de las plantas, e incluye la Citología y la Histología. La primera se ocupa del estudio de la fina constitución de las células y se asocia, en los aspectos relacionados con las moléculas, con algunas partes de la Biología Molecular. La Histología es el estudio de los tejidos de las plantas. Citología e Histología, conjuntamente, son necesarias para comprender la Anatomía de las plantas, o sea, su constitución interna. Al ocuparse de los procesos de adaptación, la morfología se relaciona con la ecología, disciplina que investiga las relaciones entre la planta y su ambiente. Tales relaciones están basadas en los estudios de la fisiología vegetal, que se ocupa ,de modo general, al estudio del modo en que se realizan las funciones de la planta en los campos del metabolismo, del cambio de forma (que incluye el crecimiento y desarrollo de la planta) y de los movimientos. La reproducción de las plantas y el modo en que se heredan y cambian las características a través de las generaciones es el campo de la Genética. La botánica sistemática trata de averiguar las afinidades que existen entre los diversos tipos de plantas, basándose en los resultados de todas las disciplinas mencionadas previamente, entre las que, al lado de la morfología, son importantes la citología, la anatomía, el estudio de las esporas y del polen (Palinología), el estudio de la generación sexual y del embrión (Embriología), las sustancias producidas y contenidas en las plantas (fitoquímica), la Genética y la Geobotánica. Como parte de la sistemática, se encuentra principalmente la taxonomía, que se ocupa de la descripción, nomenclatura y ordenación de las especies de plantas existentes, las cuales sobrepasan el número de 330 000. A ella se añade el estudio de la historia evolutiva de las plantas (Filogenia), que se apoya especialmente en la Paleobotánica, el estudio de las plantas que vivieron en otras eras geológicas y en la evolución, que ilustra sobre las leyes y las causas que rigen la formación de las estirpes vegetales. Finalmente, dentro de la botánica existen ramas de estudio que se ocupan de modo especial de grupos particulares de organismos, como la Microbiología (que estudia los microorganismos en general, incluyendo muchos de los que se consideran organismos vegetales), la Bacteriología (que se ocupa de las bacterias), la Micología (que estudia los hongos), la Ficología (que estudia las algas), la Liquenología (estudio de los líquenes), la Briología (estudio de los briófitos: los musgos y las hepáticas), la Pteridología (estudio de los helechos).También existen distintas disciplinas aplicadas, que estudian el valor práctico de las plantas para los seres humanos y con ello establecen el enlace con la Agricultura, la Silvicultura y la Farmacia, entre otras. Como ejemplo de estas disciplinas se pueden mencionar el Mejoramiento Genético de Plantas ,o fitomejoramiento, (estudia la variabilidad genética y la selección de plantas), la Fitopatología (se ocupa de las enfermedades de las plantas y de los métodos de control de las mismas), la Farmacognosia (estudia las plantas medicinales y sus principios activos). Historia Esta sección es un extracto de Historia de la botánica.[editar] Busto de Teofrasto, considerado como el padre de la botánica. La historia de la botánica es la exposición y narración de las ideas, investigaciones y obras relacionadas con la descripción, clasificación, funcionamiento, distribución y relaciones de los organismos pertenecientes a los reinos Fungi, Chromista y Plantae a través de los diferentes períodos históricos.[n 1][n 2] Desde la antigüedad, el estudio de los vegetales se ha abordado con dos aproximaciones bastante diferentes: la teórica y la utilitaria. Desde el primer punto de vista, al que se denomina botánica pura, la ciencia de las plantas se erigió por sus propios méritos como una parte integral de la biología. Desde una concepción utilitaria, por otro lado, la denominada botánica aplicada era concebida como una disciplina subsidiaria de la medicina o de la agronomía. En los diferentes períodos de su evolución una u otra aproximación ha predominado, si bien en sus orígenes ,que datan del siglo VIII a. C., la aproximación aplicada fue la preponderante. La botánica, como muchas otras ciencias, alcanzó la primera expresión definida de sus principios y problemas en la Grecia clásica y, posteriormente, continuó su desarrollo durante la época del Imperio romano. Teofrasto, discípulo de Aristóteles y considerado el «padre de la botánica», legó dos obras importantes que se suelen señalar como el origen de esta ciencia: De historia plantarum [Historia de las plantas] y De causis plantarum [Sobre las causas de las plantas]. Los romanos contribuyeron poco a los fundamentos de la botánica, pero hicieron una gran contribución al conocimiento de la botánica aplicada a la agricultura. El enciclopedista romano Plinio el Viejo aborda las plantas en los libros XII a XXVI de sus 37 volúmenes de Naturalis Historia. Se estima que en la época del imperio romano entre 1300 y 1400 plantas se habían registrado en el oeste. Tras la caída del Imperio en el siglo V, todas las conquistas alcanzadas en la antigüedad clásica tuvieron que redescubrirse a partir del siglo XII, por perderse o ignorarse buena parte de ellas durante la alta Edad Media. La tradición conservadora de la Iglesia y la labor de contadas personalidades hicieron avanzar, aunque muy lentamente, el conocimiento de los vegetales durante este período. En los siglos XV y XVI la botánica se desarrolló como una disciplina científica, separada de la herboristería y de la Medicina, si bien continuó contribuyendo a ambas. Diversos factores permitieron el desarrollo y progreso de la botánica durante esos siglos: la invención de la imprenta, la aparición del papel para la elaboración de los herbarios, y el desarrollo de los jardines botánicos, todo ello unido al desarrollo del arte y ciencia de la navegación que permitió la realización de expediciones botánicas. Todos estos factores conjuntamente supusieron un incremento notable en el número de las especies conocidas y permitieron la difusión del conocimiento local o regional a una escala internacional. Impulsada por las obras de Galileo, Kepler, Bacon y Descartes, en el siglo XVII se originó la ciencia moderna. Debido a la creciente necesidad de los naturalistas europeos de intercambiar ideas e información, se comenzaron a fundar las primeras academias científicas. Joachim Jungius fue el primer científico que combinó una mentalidad entrenada en la filosofía con observaciones exactas de las plantas. Tenía la habilidad de definir los términos con exactitud y, por ende, de reducir el uso de términos vagos o arbitrarios en la sistemática. Se lo considera el fundador del lenguaje científico, el que fue desarrollado más tarde por el inglés John Ray y perfeccionado por el sueco Carlos Linneo. A Linneo se le atribuyen varias innovaciones centrales en la taxonomía. En primer lugar, la utilización de la nomenclatura binomial de las especies en conexión con una rigurosa caracterización morfológica de las mismas. En segundo lugar, el uso de una terminología exacta. Basado en el trabajo de Jungius, Linneo definió con precisión varios términos morfológicos que serían utilizados en sus descripciones de cada especie o género, en particular aquellos relacionados con la morfología floral y con la morfología del fruto. No obstante, el mismo Linneo notó las fallas de su sistema y buscó en vano nuevas alternativas. Su concepto de la constancia de cada especie fue un obstáculo obvio para lograr establecer un sistema natural ya que esa concepción de la especie negaba la existencia de las variaciones naturales, las cuales son esenciales para el desarrollo de un sistema natural. Esta contradicción permaneció durante mucho tiempo y no fue resuelta hasta 1859 con la obra de Charles Darwin. Durante los siglos XVII y XVIII también se originaron dos disciplinas científicas que, a partir de ese momento, iban a tener una profunda influencia en el desarrollo de todos los ámbitos de la botánica: la anatomía y la fisiología vegetal. Las ideas esenciales de la teoría de la evolución por selección natural de Darwin influirían notablemente en la concepción de la clasificación de los vegetales. De ese modo, aparecieron las clasificaciones filogenéticas, basadas primordialmente en las relaciones de proximidad evolutiva entre las distintas especies, reconstruyendo la historia de su diversificación desde el origen de la vida en la Tierra hasta la actualidad. El primer sistema admitido como filogenético fue el contenido en el Syllabus der Planzenfamilien (1892) de Adolf Engler y conocido más tarde como sistema de Engler cuyas numerosas adaptaciones posteriores han sido la base de un marco universal de referencia según el cual se han ordenado (y se siguen ordenando) muchos tratados de floras y herbarios de todo el mundo, si bien algunos de sus principios para interpretar el proceso evolutivo en las plantas han sido abandonados por la ciencia moderna. Los siglos XIX y XX han sido particularmente fecundos en las investigaciones botánicas, las que han llevado a la creación de numerosas disciplinas como la ecología, la geobotánica, la citogenética y la biología molecular y, en las últimas décadas, a una concepción de la taxonomía basada en la filogenia y en los análisis moleculares de ADN y a la primera publicación de la secuencia del genoma de una angiosperma: Arabidopsis thaliana. La botánica moderna (desde 1945) Esta sección es un extracto de Botánica moderna.[editar] La botánica moderna es una ciencia que considera una gran cantidad de nuevos conocimientos en la actualidad que han sido generados por el estudio de las plantas modelo y sobre la botánica actual, en concreto, ésta comenzó desde 1945. Arabidopsis thaliana motivó a los biólogos actuales a estudiar a fondo este tipo de plantas, esta mala hierba fue una de las primeras plantas en ver su genoma secuenciado. Otros más importantes comercialmente como alimentos básicos como el arroz, trigo, maíz, cebada, centeno, mijo y la soja están teniendo también sus secuencias del genoma. Algunas de éstas son un reto puesto que tienen en sus secuencias más de dos juegos de cromosomas haploides, una condición conocida como poliploidía, común en el reino vegetal. Un alga verde Chlamydomonas reinhardtii (un célula, sola, verde alga) es otro organismo modelo importante que ha sido extensivamente estudiado y provee importantes conocimientos a la biología celular. Significado de la botánica como ciencia Los distintos grupos de vegetales participan de manera fundamental en los ciclos de la biosfera. Las plantas y algas son los productores primarios, responsables de la captación de energía solar de la que depende la mayoría de la vida terrestre, de la creación de materia orgánica y también, como subproducto, de la generación del oxígeno que inunda la atmósfera y causa que casi todos los organismos saquen ventaja del metabolismo aerobio. Alimentación humana Casi todo lo que comemos proviene de las plantas, ya sea consumiéndolas directamente (frutas, verduras hortalizas), como indirectamente a través del ganado que se alimenta con plantas que componen el forrajeras. Por lo tanto, las plantas son la base de toda la cadena alimentaria, o lo que los ecólogos llaman el primer nivel trófico. El estudio de las plantas y las técnicas de mejoramiento para producir alimentos son claves para ser capaces de alimentar al mundo y proporcionar una seguridad alimentaria para las generaciones futuras. No obstante, como todas las plantas no son beneficiosas para este fin, la botánica también estudia las especies consideradas nocivas para la agricultura. También estudia los patógenos (fitopatología) que afectan al reino vegetal y la interacción de los humanos con este reino (etnobotánica). Procesos biológicos fundamentales Las plantas son susceptibles de ser estudiadas en sus procesos fundamentales (como la división celular y síntesis proteica por ejemplo), pero sin los problemas éticos que supone estudiar animales o seres humanos. Las leyes de la herencia fueron descubiertas de esta manera por Gregor Mendel, que estudió cómo se hereda la morfología del guisante. Las leyes descubiertas por Mendel a partir del estudio de plantas han conocido desarrollos posteriores, y se han aplicado sobre las propias plantas para conseguir nuevas variedades beneficiosas. Otro estudio clásico efectuado en plantas fue el realizado por Bárbara McClintock, quien descubrió los genes saltarines (o transposones) estudiando el maíz. Son ejemplos que muestran cómo la botánica ha tenido una importancia capital para el entendimiento de los procesos biológicos fundamentales. Aplicaciones de las plantas Muchas de nuestras medicinas y drogas, como el cannabis, vienen directamente del reino vegetal. Otros productos medicinales se derivan de sustancias de origen vegetal; así, la aspirina es un derivado del ácido salicílico, que originalmente se obtenía de la corteza de sauce. La investigación sobre productos farmacéuticamente útiles en las plantas es un campo activo de trabajo que rinde buenos resultados. Estimulantes populares como el café (por su contenido en cafeína), el chocolate, el tabaco (por la nicotina), y el té tienen origen vegetal. Muchas bebidas alcohólicas derivan de la fermentación de plantas como la cebada, el maíz y la uva. Las plantas también nos proveen de muchos materiales, como el algodón, la madera, el papel, el lino, el aceite vegetal, algunos tipos de cuerdas y plásticos. La producción de seda no sería posible sin el cultivo de los árboles de morera. La caña de azúcar y otras plantas han sido recientemente usadas como biomasa para producir una energía renovable alternativa al combustible fósil. Entendimiento de cambios ambientales Las plantas también pueden ayudar al entendimiento de los cambios del medio ambiente de muchas formas. Entendimiento de la destrucción de hábitat y de especies en extinción depende de un catálogo completo y exacto de plantas, de la sistemática y taxonomía. Respuesta de las plantas a radiación ultravioleta puede monitorear problemas como los agujeros en la capa de ozono. El análisis de polen depositado por plantas en miles de millones de años atrás puede ayudar a los científicos a reconstruir los climas del pasado y pronosticar el futuro, una parte esencial de investigaciones sobre cambios climáticos. Recopilar y analizar el tiempo del ciclo de vida es importante para la fenología usado para la investigación de cambios climáticos. Líquenes, sensibles a las condiciones atmosféricas, tienen un uso extensivo como indicadores de contaminación. Las plantas pueden servir como sensores, una especie de “señales tempranas de aviso” que den la alerta sobre cambios importantes en el ambiente. Por último, las plantas son sumamente valoradas en el aspecto recreativo para millones de personas que disfrutan de su uso en la jardinería, la horticultura y el arte culinario. Disciplinas Subdisciplinas de la botánica Anatomía vegetal u organografía Botánica aplicada Botánica marina Botánica pura o general Botánica sistemática Dendrología Ecología vegetal Ficología Fisiología vegetal geobotánica Histología vegetal Morfología vegetal Paleobotánica Palinología Sistemática vegetal Disciplinas relacionadas Agricultura Agronomía Bioquímica y fitoquímica Ecología Etnobotánica fitoterapia Fitopatología Fitosociología Genética Horticultura Micología Microbiología Métodos de la botánica Herbario Artículo principal: Herbario Secado de especímenes en un herbario de Burkina Faso. Un herbario (del latín herbarium) es una colección de plantas o partes de plantas, preservadas, casi siempre a través de la desecación, procesadas para su conservación, e identificadas, y acompañadas de información importante, como nombre científico y nombre común, utilidad, características de la planta en vivo y del sitio de muestreo, así como la ubicación del punto donde se colectó. Estas plantas se conservan indefinidamente, y constituyen un banco de información que representa la flora o vegetación de una región determinada en un espacio reducido. Estos especímenes se usan con frecuencia como material de referencia para definir el taxón de una planta; pues contienen los holotipos para estas plantas. El tipo nomenclatural o, simplemente, tipo es un ejemplar de una dada especie sobre el que se ha realizado la descripción de la misma y que, de ese modo, valida la publicación de un nombre científico basado en él. El tipo del nombre de una especie es por lo general el espécimen de herbario (o pliego de herbario) a partir del cual se ha perfilado la descripción que valida el nombre. El tipo del nombre de un género es la especie sobre la cual se basó la descripción original que validaba el nombre. El tipo del nombre de una familia es el género sobre el cual fue basada la descripción original válida. En los nombres de taxones de rango superior al de familia no se aplica el principio de tipificación. Jardín botánico Artículo principal: Jardín botánico Jardín Botánico de Curitiba. Los jardines botánicos (del latín hortus botanicus) son instituciones habilitadas por un organismo público, privado o asociativo (en ocasiones la gestión es mixta) cuyo objetivo es el estudio, la conservación y divulgación de la diversidad vegetal. Se caracterizan por exhibir colecciones científicas de plantas vivas, que se cultivan para conseguir alguno de estos objetivos: su conservación, investigación, divulgación y enseñanza. En los jardines botánicos se exponen plantas originarias de todo el mundo, generalmente con el objetivo de fomentar el interés de los visitantes hacia el mundo vegetal, aunque algunos de estos jardines se dedican, exclusivamente, a determinadas plantas y a especies concretas. Código Internacional de Nomenclatura para algas, hongos y plantas Estos párrafos son un extracto de Código Internacional de Nomenclatura para algas, hongos y plantas.[editar] El Código Internacional de Nomenclatura para algas, hongos y plantas (ICN) es el compendio de reglas que rigen la nomenclatura taxonómica de los organismos tradicionalmente estudiados por la botánica (plantas, algas y hongos) a efectos de determinar, para cada taxón, un único nombre válido internacionalmente. Hasta el año 2011, con la celebración del XVIII Congreso Internacional de Botánica en Melbourne (Australia), se denominaba Código Internacional de Nomenclatura Botánica (en inglés, ICBN, en español CINB)."
ksampletext_wikipedia_biol_bioquimica: str = "Bioquímica. La bioquímica es una rama de la ciencia que estudia la composición química de los seres vivos, especialmente las proteínas, carbohidratos, lípidos y ácidos nucleicos, además de otras pequeñas moléculas presentes en las células y las reacciones químicas que sufren estos compuestos, como en el metabolismo que les permiten obtener energía (catabolismo) y generar biomoléculas propias (anabolismo). La bioquímica se basa en el concepto de que todo ser vivo contiene carbono y en general las moléculas biológicas están compuestas principalmente de carbono, hidrógeno, oxígeno, nitrógeno, fósforo y azufre. Es la rama de la ciencia que estudia la base química de las moléculas que componen algunas células y los tejidos, que catalizan las reacciones químicas del metabolismo celular como la digestión, la fotosíntesis y la inmunidad, entre otras muchas cosas. Podemos entender la bioquímica como una disciplina científica integradora que elabora el estudio de los biomas y biosistemas. Integra de esta forma las leyes químico-físicas y la evolución biológica que afectan a los biosistemas y a sus componentes. Lo hace desde un punto de vista molecular y trata de entender y aplicar su conocimiento a amplios sectores de la medicina (terapia genética y biomedicina), la agroalimentación, la farmacología. Constituye un pilar fundamental de la biotecnología, y se ha consolidado como una disciplina esencial para abordar los grandes problemas y enfermedades actuales y del futuro, tales como el cambio climático, la escasez de recursos agroalimentarios ante el aumento de población mundial, el agotamiento de las reservas de combustibles fósiles, la aparición de nuevas alergias, el aumento del cáncer, las enfermedades genéticas, la obesidad, etc. La bioquímica es una ciencia experimental y por ello recurrirá al uso de numerosas técnicas instrumentales propias y de otros campos, pero la base de su desarrollo parte del hecho de que lo que ocurre en vivo a nivel subcelular se mantiene o se conserva tras el fraccionamiento subcelular, y a partir de ahí, podemos estudiarlo. Historia Siglo XIX y primera mitad del XX La historia de la bioquímica como la conocemos hoy en día es prácticamente moderna; desde el siglo XIX se comenzó a direccionar una buena parte de la biología y la química a la creación de una nueva disciplina integradora: la química fisiológica o la bioquímica. Pero la aplicación de la bioquímica y su conocimiento probablemente comenzó hace 5000 años, con la producción de pan usando levaduras, en un proceso conocido como fermentación. Es difícil abordar la historia de la bioquímica, en cuanto que, es una mezcla compleja de química orgánica y biología, y en ocasiones, se hace complicado discernir entre lo exclusivamente biológico y lo exclusivamente químico orgánico y es evidente que la contribución a esta disciplina ha sido muy extensa. Aunque es cierto que existen datos experimentales que son básicos en la bioquímica. Se suele situar el inicio de la bioquímica en los descubrimientos en 1828 de Friedrich Wöhler que publicó un artículo acerca de la síntesis de urea, probando que los compuestos orgánicos pueden ser creados artificialmente, en contraste con la creencia comúnmente aceptada durante mucho tiempo, de que la generación de estos compuestos era posible solo en el interior de los seres vivos. La diastasa fue la primera enzima descubierta. En 1833 se extrajo de la solución de malta por Anselme Payen y Jean-François Persoz, dos químicos de una fábrica de azúcar francesa. A mediados del siglo XIX, Louis Pasteur demostró los fenómenos de isomería química existente entre las moléculas de ácido tartárico provenientes de los seres vivos y las sintetizadas químicamente en el laboratorio. También estudió el fenómeno de la fermentación y descubrió que intervenían ciertas levaduras, y por tanto no era exclusivamente un fenómeno químico como se había defendido hasta ahora (entre ellos el propio Liebig); así Pasteur escribió: «la fermentación del alcohol es un acto relacionado con la vida y la organización de las células de las levaduras, y no con la muerte y la putrefacción de las células». Además desarrolló un método de esterilización de la leche, el vino y la cerveza (pasteurización) y contribuyó enormemente a refutar la idea de la generación espontánea de los seres vivos. En 1869 se descubre la nucleína y se observa que es una sustancia muy rica en fósforo. Dos años más tarde, Albrecht Kossel concluye que la nucleína es rica en proteínas y contiene las bases púricas adenina y guanina y las pirimidínicas citosina y timina. En 1889 se aíslan los dos componentes mayoritarios de la nucleína: Proteínas (70 %) Sustancias de carácter ácido: ácidos nucleicos (30 %) En 1878 el fisiólogo Wilhelm Kühne acuñó el término enzima para referirse a los componentes biológicos desconocidos que producían la fermentación. La palabra enzima fue usada después para referirse a sustancias inertes tales como la pepsina. En 1897 Eduard Buchner comenzó a estudiar la capacidad de los extractos de levadura para fermentar azúcar a pesar de la ausencia de células vivientes de levadura. En una serie de experimentos en la Universidad Humboldt de Berlín, encontró que el azúcar era fermentado incluso cuando no había elementos vivos en los cultivos de células de levaduras. Llamó a la enzima que causa la fermentación de la sacarosa, “zimasa”. Al demostrar que las enzimas podrían funcionar fuera de una célula viva, el siguiente paso fue demostrar cuál era la naturaleza bioquímica de esos biocatalizadores. El debate fue extenso; muchos, como el bioquímico alemán Richard Willstätter, discrepaban de que la proteína fuera el catalizador enzimático, hasta que en 1926, James B. Sumner demostró que la enzima ureasa era una proteína pura y la cristalizó. La conclusión de que las proteínas puras podían ser enzimas fue definitivamente probada en torno a 1930 por John Howard Northrop y Wendell Meredith Stanley, quienes trabajaron con diversas enzimas digestivas como la pepsina, la tripsina y la quimotripsina. En 1903 Mijaíl Tswett inicia los estudios de cromatografía para separación de pigmentos. En torno a 1915 Gustav Embden y Otto Meyerhof realizan sus estudios sobre la glucólisis. En 1920 se descubre que en las células hay ADN y ARN y que difieren en el azúcar que forma parte de su composición: desoxirribosa o ribosa. El ADN reside en el núcleo. Unos años más tarde, se descubre que en los espermatozoides hay fundamentalmente ADN y proteínas, y posteriormente Feulgen descubre que hay ADN en los cromosomas con su tinción específica para este compuesto. En 1925 Theodor Svedberg demuestra que las proteínas son macromoléculas y desarrolla la técnica de ultracentrifugación analítica. En 1928, Alexander Fleming descubre la penicilina y desarrolla estudios sobre la lisozima. Richard Willstätter (en torno 1910) estudia la clorofila y comprueba la similitud que hay con la hemoglobina. Posteriormente Hans Fischer en torno a 1930, investiga la química de las porfirinas de las que derivan la clorofila o el grupo porfirínico de la hemoglobina. Consiguió sintetizar hemina y bilirrubina. Paralelamente Heinrich Otto Wieland formula teorías sobre las deshidrogenaciones y explica la constitución de muchas otras sustancias de naturaleza compleja, como la pteridina, las hormonas sexuales o los ácidos biliares. En la década de 1940, Melvin Calvin concluye el estudio del ciclo de Calvin en la fotosíntesis y Albert Claude la síntesis del ATP en las mitocondrias. En torno a 1945 Gerty Cori, Carl Cori, y Bernardo Houssay completan sus estudios sobre el ciclo de Cori. En 1953 James Dewey Watson y Francis Crick, gracias a los estudios previos con cristalografía de rayos X de ADN de Rosalind Franklin y Maurice Wilkins, y los estudios de Erwin Chargaff sobre apareamiento de bases nitrogenadas, deducen la estructura de doble hélice del ADN. En 1957, Matthew Meselson y Franklin Stahl demuestran que la replicación del ADN es semiconservativa. Segunda mitad del siglo XX En la segunda mitad del siglo XX, comienza la auténtica revolución de la bioquímica y la biología molecular moderna, especialmente gracias al desarrollo de las técnicas experimentales más básicas como la cromatografía, la centrifugación, la electroforesis, las técnicas radioisotópicas y la microscopía electrónica, y las técnicas más complejas como la cristalografía de rayos X, la resonancia magnética nuclear, la PCR (Kary Mullis), el desarrollo de la inmuno-técnicas. Desde 1950 a 1975 , se conocen en profundidad y detalle aspectos del metabolismo celular inimaginables hasta ahora (fosforilación oxidativa (Peter Dennis Mitchell), ciclo de la urea y ciclo de Krebs (Hans Adolf Krebs), así como otras rutas metabólicas), se produce toda una revolución en el estudio de los genes y su expresión; se descifra el código genético (Francis Crick, Severo Ochoa, Har Gobind Khorana, Robert W. Holley y Marshall Warren Nirenberg), se descubren las enzimas de restricción (finales de 1960, Werner Arber, Daniel Nathans y Hamilton Smith), la ADN ligasa (en 1972, Mertz y Davis) y finalmente en 1973 Stanley Cohen y Herbert Boyer producen el primer ser vivo recombinante, naciendo así la ingeniería genética, convertida en una herramienta poderosísima con la que se supera la frontera entre especies y con la que podemos obtener un beneficio hasta ahora impensable. En 1970, un argentino, Luis Federico Leloir, médico, bioquímico y farmacéutico recibió el Premio Nobel de Química por sus investigaciones sobre los nucleótidos de azúcar, y el rol que cumplen en la fabricación de los hidratos de carbono. En 1984, otro argentino, César Milstein, oriundo de la ciudad de Bahía Blanca, recibe el Premio Nobel de Medicina por sus investigaciones sobre anticuerpos monoclonales, hoy utilizados para tratar muchas enfermedades, incluidos algunos tipos de cáncer. De 1975 hasta principios del siglo XXI, comienza a secuenciarse el ADN (Allan Maxam, Walter Gilbert y Frederick Sanger), comienzan a crearse las primeras industrias biotecnológicas (Genentech), se aumenta la creación de fármacos y vacunas más eficaces, se eleva el interés por las inmunología y las células madres y se descubre la enzima telomerasa (Elizabeth Blackburn y Carol Greider). En 1989 se utiliza la biorremediación a gran escala en el derrame del petrolero Exxon Valdez en Alaska. Se clonan los primeros seres vivos, se secuencia el ADN de decenas de especies y se publica el genoma completo del hombre (Craig Venter, Celera Genomics y Proyecto Genoma Humano), se resuelven decenas de miles de estructuras proteicas y se publican en PDB, así como genes, en GenBank. Comienza el desarrollo de la bioinformática y la computación de sistemas complejos, que se constituyen como herramientas muy poderosas en el estudio de los sistemas biológicos. Se crea el primer cromosoma artificial y se logra la primera bacteria con genoma sintético (2007, 2009, Craig Venter). Se fabrican las nucleasas con dedos de zinc. Se inducen artificialmente células, que inicialmente no eran pluripotenciales, a células madre pluripotenciales (Shinya Yamanaka). Comienzan a darse los primeros pasos. Ramas de la bioquímica Esquema de una célula típica animal con sus orgánulos y estructuras. El pilar fundamental de la investigación bioquímica clásica se centra en las propiedades de las proteínas, muchas de las cuales son enzimas. Sin embargo, existen otras disciplinas que se centran en las propiedades biológicas de carbohidratos (glucobiología) y lípidos (lipobiología). Por razones históricas la bioquímica del metabolismo de la célula ha sido intensamente investigada, en importantes líneas de investigación actuales (como el Proyecto Genoma, cuya función es la de identificar y registrar todo el material genético humano), se dirigen hacia la investigación del ADN, el ARN, la síntesis de proteínas, la dinámica de la membrana celular y los ciclos energéticos. Las ramas de la bioquímica son muy amplias y diversas, y han ido variando con el tiempo y los avances de la biología, la química y la física. Bioquímica estructural: es un área de la bioquímica que pretende comprender la arquitectura química de las macromoléculas biológicas, especialmente de las proteínas y de los ácidos nucleicos (ADN y ARN). Así se intenta conocer las secuencias peptídicas, su estructura y conformación tridimensional, y las interacciones físico-químicas atómicas que posibilitan a dichas estructuras. Uno de sus máximos retos es determinar la estructura de una proteína conociendo solo la secuencia de aminoácidos, que supondría la base esencial para el diseño racional de proteínas (ingeniería de proteínas). Ciencia que estudia la estructura, propiedades físicas, la reactividad y transformación de los compuestos orgánicos. Química Orgánica Química orgánica: es un área de la química que se encarga del estudio de los compuestos orgánicos (es decir, aquellos que tienen enlaces covalentes carbono-carbono o carbono-hidrógeno) que provienen específicamente de seres vivos. Se trata de una ciencia íntimamente relacionada con la bioquímica clásica, ya que en la mayoría de los compuestos biológicos participa el carbono Mientras que la bioquímica clásica ayuda a comprender los procesos biológicos con base en conocimientos de estructura, enlace químico, interacciones moleculares y reactividad de las moléculas orgánicas, la química bioorgánica intenta integrar los conocimientos de síntesis orgánica, mecanismos de reacción, análisis estructural y métodos analíticos con las reacciones metabólicas primarias y secundarias, la biosíntesis, el reconocimiento celular y la diversidad química de los organismos vivos. De allí surge la Química de Productos Naturales (V. Metabolismo secundario). Enzimología: estudia el comportamiento de los catalizadores biológicos o enzimas, como son algunas proteínas y ciertos ARN catalíticos, así como las coenzimas y cofactores como metales y vitaminas. Así se cuestiona los mecanismos de catálisis, los procesos de interacción de las enzimas-sustrato, los estados de transición catalíticos, las actividades enzimáticas, la cinética de la reacción y los mecanismos de regulación y expresión enzimáticas, todo ello desde un punto de vista bioquímico. Estudia y trata de comprender los elementos esenciales del centro activo y de aquellos que no participan, así como los efectos catalíticos que ocurren en la modificación de dichos elementos; en este sentido, utilizan frecuentemente técnicas como la mutagénesis dirigida. Bioquímica metabólica: es un área de la bioquímica que pretende conocer los diferentes tipos de rutas metabólicas a nivel celular, y su contexto orgánico. De esta forma son esenciales conocimientos de enzimología y biología celular. Estudia todas las reacciones bioquímicas celulares que posibilitan la vida, y así como los índices bioquímicos orgánicos saludables, las bases moleculares de las enfermedades metabólicas o los flujos de intermediarios metabólicos a nivel global. De aquí surgen disciplinas académicas como la bioenergética (estudio del flujo de energía en los organismos vivos), la bioquímica nutricional (estudio de los procesos de nutrición asociados a| rutas metabólicas) y la bioquímica clínica (estudio de las alteraciones bioquímicas en estado de enfermedad o traumatismo). La metabolómica es el conjunto de ciencias y técnicas dedicadas al estudio completo del sistema constituido por el conjunto de moléculas que constituyen los intermediarios metabólicos, metabolitos primarios y secundarios, que se pueden encontrar en un sistema biológico. Xenobioquímica: es la disciplina que estudia el comportamiento metabólico de los compuestos cuya estructura química no es propia en el metabolismo regular de un organismo determinado. Pueden ser metabolitos secundarios de otros organismos (por ejemplo las micotoxinas, los venenos de serpientes y los fitoquímicos cuando ingresan al organismo humano) o compuestos poco frecuentes o inexistentes en la naturaleza. La farmacología es una disciplina que estudia a los xenobióticos que benefician al funcionamiento celular en el organismo debido a sus efectos terapéuticos o preventivos (fármacos). La farmacología tiene aplicaciones clínicas cuando las sustancias son utilizadas en el diagnóstico, prevención, tratamiento y alivio de síntomas de una enfermedad así como el desarrollo racional de sustancias menos invasivas y más eficaces contra dianas biomoleculares concretas. Por otro lado, la toxicología es el estudio que identifica, estudia y describe, la dosis, la naturaleza, la incidencia, la severidad, la reversibilidad y, generalmente, los mecanismos de los efectos adversos (efectos tóxicos) que producen los xenobióticos. Actualmente la toxicología también estudia el mecanismo de los componentes endógenos, como los radicales libres de oxígeno y otros intermediarios reactivos, generados por xenobióticos y endobióticos. Inmunología: área de la biología, la cual se interesa por la reacción del organismo frente a otros organismos como las bacterias y virus. Todo esto tomando en cuenta la reacción y funcionamiento del sistema inmune de los seres vivos. Es esencial en esta área el desarrollo de los estudios de producción y comportamiento de los anticuerpos. Endocrinología: es el estudio de las secreciones internas llamadas hormonas, las cuales son sustancias producidas por células especializadas cuyo fin es de afectar la función de otras células. La endocrinología trata la biosíntesis, el almacenamiento y la función de las hormonas, las células y los tejidos que las secretan, así como los mecanismos de señalización hormonal. Existen subdisciplinas como la endocrinología médica, la endocrinología vegetal y la endocrinología animal. Neuroquímica: es el estudio de las moléculas orgánicas que participan en la actividad neuronal. Este término es empleado con frecuencia para referir a los neurotransmisores y otras moléculas como las drogas neuro-activas que influencian la función neuronal. Quimiotaxonomía: es el estudio de la clasificación e identificación de organismos de acuerdo a sus diferencias y similitudes demostrables en su composición química. Los compuestos estudiados pueden ser fosfolípidos, proteínas, péptidos, heterósidos, alcaloides y terpenos. John Griffith Vaughan fue uno de los pioneros de la quimiotaxonomía. Entre los ejemplos de las aplicaciones de la quimiotaxonomía pueden citarse la diferenciación de las familias Asclepiadaceae y Apocynaceae según el criterio de la presencia de látex; la presencia de agarofuranos en la familia Celastraceae; las sesquiterpenlactonas con esqueleto de germacrano que son características de la familia Asteraceae o la presencia de abietanos en las partes aéreas de plantas del género Salvia del viejo Mundo a diferencia de las del Nuevo Mundo que presentan principalmente neo-clerodanos. Ecología química: es el estudio de los compuestos químicos de origen biológico implicados en las interacciones de organismos vivos. Se centra en la producción y respuesta de moléculas señalizadoras (semioquímicos), así como los compuestos que influyen en el crecimiento, supervivencia y reproducción de otros organismos (aleloquímicos). Virología: área de la biología, que se dedica al estudio de los biosistemas más elementales: los virus. Tanto en su clasificación y reconocimiento, como en su funcionamiento y estructura molecular. Pretende reconocer dianas para la actuación de posibles de fármacos y vacunas que eviten su directa o preventivamente su expansión. También se analizan y predicen, en términos evolutivos, la variación y la combinación de los genomas víricos, que podrían hacerlos finalmente, más peligrosos. Finalmente suponen una herramienta con mucha proyección como vectores recombinantes, y han sido ya utilizados en terapia génica. Imagen: Proteína mioglobina Genética molecular e ingeniería genética: es un área de la bioquímica y la biología molecular que estudia los genes, su herencia y su expresión. Molecularmente, se dedica al estudio del ADN y del ARN principalmente, y utiliza herramientas y técnicas potentes en su estudio, tales como la PCR y sus variantes, los secuenciadores masivos, los kits comerciales de extracción de ADN y ARN, procesos de transcripción-traducción in vitro e in vivo, enzimas de restricción, ADN ligasas… Es esencial conocer como el ADN se replica, se transcribe y se traduce a proteínas (Dogma Central de la Biología Molecular), así como los mecanismos de expresión basal e inducible de genes en el genoma. También estudia la inserción de genes, el silenciamiento génico y la expresión diferencial de genes y sus efectos. Superando así las barreras y fronteras entre especies en el sentido que el genoma de una especie podemos insertarlo en otro y generar nuevas especies. Uno de sus máximos objetivos actuales es conocer los mecanismos de regulación y expresión genética, es decir, obtener un código epigenético. Constituye un pilar esencial en todas las disciplinas biocientíficas, especialmente en biotecnología. La biotecnología moderna tiene múltiples aplicaciones y variadas e incluyen, además de la fabricación de medicamentos, alimentos, papel, entre otros, el mejoramiento de animales y plantas de interés agronómico. Biología Molecular: es la disciplina científica que tiene como objetivo el estudio de los procesos que se desarrollan en los seres vivos desde un punto de vista molecular. Así como la bioquímica clásica investiga detalladamente los ciclos metabólicos y la integración y desintegración de las moléculas que componen los seres vivos, la biología molecular pretende fijarse con preferencia en el comportamiento biológico de las macromoléculas (ADN, ARN, enzimas, hormonas, etc.) dentro de la célula y explicar las funciones biológicas del ser vivo por estas propiedades a nivel molecular. Biología celular: (antiguamente citología, de citos=célula y logos=Estudio o Tratado ) es un área de la biología que se dedica al estudio de la morfología y fisiología de las células procariotas y eucariotas. Trata de conocer sus propiedades, estructura, composición bioquímica, funciones, orgánulos que contienen, su interacción con el ambiente y su ciclo vital. Es esencial en esta área conocer los procesos intrínsecos a la vida celular durante el ciclo celular, como la nutrición, la respiración, la síntesis de componentes, los mecanismos de defensa, la división celular y la muerte celular. También se deben conocer los mecanismos de comunicación de células (especialmente en organismos pluricelulares) o las uniones intercelulares. Es un área esencialmente de observación y experimentación en cultivos celulares, que, frecuentemente, tienen como objetivo la identificación y separación de poblaciones celulares y el reconocimiento de orgánulos celulares. Algunas técnicas utilizadas en biología celular tienen que ver con el empleo de técnicas de citoquímica, siembra de cultivos celulares, observación por microscopía óptica y electrónica, inmunocitoquímica, inmunohistoquímica, ELISA o citometría de flujo."


ksampletext_wikipedia_medi_farmacologia: str = "Farmacología. La farmacología (del griego, pharmacon, fármaco y logos, ciencia) es la rama de las ciencias farmacéuticas que estudia la historia, el origen, las propiedades biofisicoquímicas, la presentación, los efectos fisiológicos, los mecanismos de acción, la absorción, la distribución, la biotransformación, la excreción y el uso terapéutico, entre otras actividades biológicas, de las sustancias químicas que interactúan con los organismos vivos. La farmacología estudia como interactúa el fármaco con el organismo, sus acciones, efectos y propiedades.[2]​[3] En un sentido más estricto, se considera la farmacología como el estudio de los fármacos, sea que esas tengan efectos beneficiosos o bien tóxicos. La farmacología tiene aplicaciones clínicas cuando las sustancias son utilizadas en el diagnóstico, prevención y tratamiento de una enfermedad o para el alivio de sus síntomas. Historia Artículo principal: Historia de la farmacia Los orígenes de la farmacología clínica se remontan a la Edad Media, con la farmacognosia y El canon de medicina de Avicena, el Comentario de Pedro de España sobre Isaac Dager y el Comentario de Juan de San Amand sobre el Antedotario de Nicolás. La farmacología temprana se centró en el herbalismo y las sustancias naturales, principalmente extractos de plantas. Las medicinas fueron compiladas en libros llamados farmacopea. Las drogas crudas se han usado desde la prehistoria como una preparación de sustancias de fuentes naturales. Sin embargo, el ingrediente activo de las drogas crudas no se purifica y la sustancia se adulteraba con otras sustancias. La medicina tradicional varía entre culturas y puede ser específica de una cultura particular, como en la medicina tradicional china, mongol, tibetana y coreana. Sin embargo, gran parte de esto se ha considerado como pseudociencia. Las sustancias farmacológicas conocidas como enteógenos pueden tener un uso espiritual y religioso y un contexto histórico. En el siglo XVII, el médico inglés Nicholas Culpeper tradujo y usó textos farmacológicos, en los cuales detalló las plantas y las condiciones que podrían tratar. En el siglo XVIII, gran parte de la farmacología clínica fue establecida por el trabajo de William Withering. La farmacología como disciplina científica no avanzó más hasta mediados del siglo XIX, en medio del gran resurgimiento biomédico de ese período. Antes de la segunda mitad del siglo XIX, la notable potencia y especificidad de drogas como la morfina y la quinina, se explicaron vagamente y con referencia a poderes químicos extraordinarios y afinidades con ciertos órganos o tejidos. El primer departamento de farmacología fue creado por Rudolf Buchheim en 1847, en reconocimiento de la necesidad de comprender cómo las drogas terapéuticas y los venenos producen sus efectos. Posteriormente, el primer departamento de farmacología en Inglaterra se creó en 1905 en el University College de Londres. La farmacología se desarrolló en el siglo XIX como una ciencia biomédica que aplicaba los principios de la experimentación científica a los contextos terapéuticos. El avance de las técnicas de investigación impulsó la investigación farmacológica y su comprensión. El desarrollo de la preparación del baño de órganos, donde las muestras de tejido están conectadas a dispositivos de registro (como un miógrafo) y las respuestas fisiológicas se registran después de la aplicación del medicamento, permitió el análisis de los efectos de los medicamentos en los tejidos. El desarrollo del ensayo de unión al ligando en 1945, permitió la cuantificación de la afinidad de unión de los fármacos en objetivos químicos. Los farmacólogos modernos utilizan técnicas de genética, biología molecular, bioquímica y otras herramientas avanzadas para transformar información sobre mecanismos moleculares y objetivos en terapias dirigidas contra enfermedades, defectos o patógenos, y crear métodos para la atención preventiva, el diagnóstico y, en última instancia, la medicina personalizada. Divisiones La disciplina de la farmacología se puede dividir en muchas subdisciplinas, cada una con un enfoque específico. Sistemas del cuerpo La farmacología también puede centrarse en sistemas específicos que comprenden el cuerpo. Las divisiones relacionadas con los sistemas corporales estudian los efectos de las drogas en diferentes sistemas del cuerpo. Estos incluyen neurofarmacología, en el sistema nervioso central y periférico; inmunofarmacología en el sistema inmune. Otras divisiones incluyen farmacología cardiovascular, renal y endocrina. Psicofarmacología, es el estudio de los efectos de las drogas en la psique, la mente y el comportamiento, como los efectos conductuales de las drogas psicoactivas. Incorpora enfoques y técnicas de neurofarmacología, comportamiento animal y neurociencia conductual, y está interesado en los mecanismos de acción conductuales y neurobiológicos de las drogas psicoactivas. El campo relacionado de la neuropsicofarmacología se centra en los efectos de las drogas en la superposición entre el sistema nervioso y la psique. La farmacometabolómica, también conocida como farmacometabonómica, es un campo que se deriva de la metabolómica. Consiste en la medición directa de metabolitos en los fluidos corporales de un individuo, con el fin de predecir o evaluar el metabolismo de los compuestos farmacéuticos, y para comprender mejor el perfil farmacocinético de un medicamento.[4] La farmacometabolómica se puede aplicar para medir los niveles de metabolitos después de la administración de un medicamento, con el fin de controlar los efectos del medicamento en las vías metabólicas. También estudia el efecto de las variaciones del microbioma en la disposición, acción y toxicidad del fármaco, así como la interacción entre las drogas y el microbioma intestinal. La farmacogenómica es la aplicación de tecnologías genómicas para el descubrimiento de fármacos y la caracterización adicional de fármacos relacionados con el genoma completo de un organismo. Para la farmacología con respecto a genes individuales, la farmacogenética estudia cómo la variación genética da lugar a diferentes respuestas a los fármacos. La farmacoepigenética estudia los patrones de marcado epigenético subyacentes que conducen a variaciones en la respuesta de un individuo al tratamiento médico. Práctica clínica y descubrimiento de fármacos Un toxicólogo trabajando en un laboratorio. La farmacología se puede aplicar dentro de las ciencias clínicas. La farmacología clínica es la ciencia básica de la farmacología que se centra en la aplicación de principios y métodos farmacológicos en la clínica médica y en la atención y los resultados del paciente. Un ejemplo de esto es la posología, que es el estudio de cómo se dosifican los medicamentos. La farmacología está estrechamente relacionada con la toxicología. Tanto la farmacología como la toxicología son disciplinas científicas que se centran en comprender las propiedades y acciones de los productos químicos. Sin embargo, la farmacología enfatiza los efectos terapéuticos de los químicos, usualmente drogas o compuestos que podrían convertirse en drogas, mientras que la toxicología es el estudio de los efectos adversos de los químicos y la evaluación de riesgos. El conocimiento farmacológico se utiliza para aconsejar farmacoterapia en medicina y farmacia. Destino de los fármacos en el organismo Cualquier sustancia que interactúa con un organismo viviente puede ser absorbida por este, distribuida por los distintos órganos, sistemas o espacios corporales, modificada por procesos químicos y finalmente expulsada. La farmacología estudia los procesos en la interacción de fármacos con el hombre y animales llamados procesos LADME que, en orden temporal, son los siguientes: liberación absorción distribución metabolismo excreción El estudio de estos procesos es lo que se conoce como farmacocinética. De la interacción de todos estos procesos, la farmacología puede predecir la biodisponibilidad y vida media de eliminación de un fármaco en el organismo dadas una vía de administración, una dosis y un intervalo de administración. Para que el fármaco ejerza su acción sobre este blanco, debe, generalmente, ser transportado a través de la circulación sanguínea. Absorción Para llegar a la circulación sanguínea el fármaco debe traspasar alguna barrera dada por la vía de administración, que puede ser: cutánea, subcutánea, respiratoria, oral, rectal, muscular, vía ótica, vía oftálmica, vía sublingual. O puede ser inoculada directamente a la circulación por la vía intravenosa. La farmacología estudia la concentración plasmática de un fármaco en relación con el tiempo transcurrido para cada vía de administración y para cada concentración posible, así como las distintas formas de uso de estas vías de administración. Distribución Una vez en la corriente sanguínea, el fármaco, por sus características de tamaño y peso molecular, carga eléctrica, pH, solubilidad, capacidad de unión a proteínas se distribuye entre los distintos compartimientos corporales. La farmacología estudia cómo estas características influyen en el aumento y disminución de concentración del fármaco con el paso del tiempo en distintos sistemas, órganos, tejidos y compartimientos corporales, como por ejemplo, en el líquido cefalorraquídeo, o en la placenta, etc. Metabolismo o biotransformación Muchos fármacos son transformados en el organismo debido a la acción de enzimas. Esta transformación puede consistir en la degradación; (oxidación, reducción o hidrólisis), donde el fármaco pierde parte de su estructura, o en la síntesis de nuevas sustancias con el fármaco como parte de la nueva molécula (conjugación). El resultado de la biotransformación puede ser la inactivación completa o parcial de los efectos del fármaco, el aumento o activación de los efectos, o el cambio por nuevos efectos dependientes de las características de la sustancia sintetizada. La farmacología estudia los mecanismos mediante los cuales se producen estas transformaciones, los tejidos en que ocurre, la velocidad de estos procesos y los efectos de las propias drogas y sus metabolitos sobre los mismos procesos enzimáticos. Excreción Finalmente, el fármaco es eliminado del organismo por medio de algún órgano excretor. Principalmente está el hígado y el riñón, pero también son importantes la piel, las glándulas salivales y lagrimales. Cuando un fármaco es suficientemente hidrosoluble, es derivado hacia la circulación sanguínea, por la cual llega a los riñones y es eliminado por los mismos procesos de la formación de la orina: filtración glomerular, secreción tubular y reabsorción tubular. Si el fármaco, por el contrario, es liposoluble o de tamaño demasiado grande para atravesar los capilares renales, es excretada en la bilis, llegando al intestino grueso donde puede sufrir de la recirculación enterohepática, o bien ser eliminado en las heces. La farmacología estudia la forma y velocidad de depuración de los fármacos y sus metabolitos por los distintos órganos excretores, en relación con las concentraciones plasmáticas del fármaco. El efecto de los fármacos, después de su administración, depende de la variabilidad en la absorción, distribución, metabolismo y excreción. Para que el fármaco alcance su sitio de acción, han de considerarse los siguientes factores: Tasa y grado de absorción a partir del sitio de aplicación. Tasa y grado de distribución en los líquidos y tejidos corporales. Tasa de biotransformación a metabolitos activos o inactivos. Tasa de excreción. Acción de los fármacos sobre el organismo Al estudio del conjunto de efectos sensibles y/o medibles que produce un fármaco en el organismo del ser humano o los animales, su duración y el curso temporal de ellos, se denomina farmacodinámica. Para este estudio, la farmacología entiende al sistema, órgano, tejido o célula destinatario del fármaco u objeto de la sustancia en análisis, como poseedor de receptores con los cuales la sustancia interactúa. La interacción entre sustancia y receptor es un importante campo de estudio, que entre otros aspectos, analiza: Cuantificación de la interacción droga/receptor. Regulación de los receptores, ya sea al aumento, disminución o cambio en el nivel de respuesta. Relación entre dosis y respuesta. La farmacodinámica, define y clasifica a los fármacos de acuerdo con su afinidad, potencia, eficacia y efectos relativos. Algunos de los índices importantes de estas definiciones son la DE50 y la DL50, que son las dosis mínimas necesarias para lograr el efecto deseado y la muerte respectivamente, en el 50% de una población determinada. La relación entre estos valores es el índice terapéutico. De acuerdo con el tipo de efecto preponderante de un fármaco, farmacodinámicamente se les clasifica en: Agonistas farmacológicos, si produce o aumenta el efecto. Antagonistas farmacológicos, si disminuye o elimina el efecto. La farmacodinámica estudia también la variabilidad en los efectos de una sustancia dependientes de factores del individuo tales como: edad, raza, gravidez, estados patológicos, etc. También existe un campo especial de estudio de los efectos farmacológicos de sustancias durante la gestación. En el ser humano, los efectos sobre el embrión y el feto de los fármacos es un campo de intenso estudio. Ramas de la farmacología Farmacocinética: el estudio de los procesos físico-químicos que sufre un fármaco cuando se administra o incorpora a un organismo. Estos procesos serían liberación, absorción, distribución, metabolización y eliminación. Farmacodinámica: ciencia que estudia el mecanismo de acción de los fármacos, es decir estudia como los procesos bioquímicos y fisiológicos dentro del organismo se ven afectados por la presencia del fármaco. Biofarmacia: el estudio de la biodisponibilidad de los fármacos. Farmacognosia: estudio de plantas medicinales y drogas que de ellas se derivan. Química farmacéutica: estudia los fármacos desde el punto de vista químico, lo que comprende el descubrimiento, el diseño, la identificación y preparación de compuestos biológicamente activos, la interpretación de su modo de interacción a nivel molecular, la construcción de su relación estructura-actividad y el estudio de su metabolismo. Farmacia galénica o Farmacotecnia: rama encomendada a la formulación de fármacos como medicamentos. Posología: el estudio de la dosificación de los fármacos. Toxicología: el estudio de los efectos nocivos o tóxicos de los fármacos. Farmacología clínica: evalúa la eficacia y la seguridad de la terapéutica por fármacos. Farmacovigilancia: es una disciplina que permite la vigilancia postcomercialización de los medicamentos a fin de detectar, prevenir y notificar reacciones adversas en grupos de pacientes. Cronofarmacología: El estudio de la correcta administración de medicamentos conforme al ciclo circadiano del ser humano, esto con el fin de maximizar la eficacia y disminuir los efectos colaterales. Margen e índice terapéutico Es un hecho práctico de todos conocido que al incrementar la dosis de un determinado fármaco, se incrementa el riesgo de producción de fenómenos tóxicos o adversos. Para evitar tal situación, los farmacólogos experimentales y clínicos hacen una evaluación de la seguridad del fármaco, con el fin de garantizar que con la dosis empleada se logre el efecto farmacológico deseado con reducción de riesgos de intoxicación. La evaluación más simple y sencilla es la conocida como Margen Terapéutico, que es el margen de dosis que oscila entre la dosis mínima y la dosis máxima terapéutica. De lo anterior se deriva que se puede dosificar un medicamento dentro de este margen, no teniendo sentido alguno el administrar una dosis superior a la máxima terapéutica, ya que con ella no obtendríamos un efecto superior, y nos acercamos a aquella dosis que puede ser tóxicas."
ksampletext_wikipedia_medi_epidemiologia: str = "Epidemiología. La epidemiología es una disciplina científica en el área de la salud pública, no solamente la medicina, que estudia la distribución, frecuencia, magnitud y factores determinantes de las enfermedades existentes en poblaciones humanas definidas. Rich la describió en 1979 como la ciencia que estudia la dinámica de salud en las poblaciones; por lo tanto involucra el análisis e interpretación de las personas que también están sanas.. Quien trabaja como profesional con especialidad en epidemiología se llama epidemiólogo/epidemióloga.[1]​ Principios La epidemiología —que, en sentido estricto, podría denominarse epidemiología humana— constituye una parte muy importante dentro de la salud pública,[1] ocupa un lugar especial en la intersección entre las ciencias biomédicas y las ciencias sociales, e integra los métodos y principios de estas ciencias para estudiar la salud y controlar las enfermedades en grupos humanos bien definidos.[2] Existe también una epidemiología veterinaria, que estudia los mismos aspectos en los padecimientos que afectan la salud de los animales; y también podría hablarse de una epidemiología zoológica y botánica, íntimamente relacionadas con la ecología. En epidemiología se estudian y describen las enfermedades que se presentan en una determinada población, para lo cual se tienen en cuenta una serie de patrones de enfermedad, que se reducen a tres aspectos: tiempo, lugar y persona: el tiempo que tarda en surgir, la temporada del año en la que surge y los tiempos en los que es más frecuente; el lugar (la ciudad, la población, el país, el tipo de zona) en donde se han presentado los casos, y las personas más propensas a padecerla (niños, ancianos, etc., según el caso). La epidemiología surgió del estudio de las epidemias de enfermedades infecciosas; de ahí su nombre. Ya en el siglo XX los estudios epidemiológicos se extendieron también a las enfermedades no infecciosas. Para el análisis adecuado de la información epidemiológica se requiere cada vez con mayor frecuencia un equipo multidisciplinario que prevea la participación de profesionales de otros ámbitos científicos, entre los cuales la demografía y la estadística son especialmente importantes. Ciencia Para causar una enfermedad, un patógeno debe crecer y reproducirse en el hospedador. Los epidemiólogos siguen por esta razón, la historia natural de los patógenos. En muchos casos, un patógeno individual no puede crecer fuera del hospedador; si el hospedador muere, el patógeno también muere. Asimismo, los patógenos que matan al hospedador antes de trasmitirlos a otro hospedador, terminarán por extinguirse. Por tanto, la mayoría de los patógenos dependientes del hospedador deben adaptarse a coexistir con el hospedador. Un patógeno bien adaptado vive en equilibrio con el hospedador, tomando lo que necesita para su existencia, y causando solo un mínimo de daño. Estos patógenos a veces pueden causar infecciones crónicas (infecciones de larga duración) en el hospedador. Cuando existe equilibrio entre el hospedador y el patógeno, ambos sobreviven. Por otra parte, el hospedador puede resultar dañado cuando su resistencia es baja, por factores como una dieta insuficiente, edad avanzada y otros agentes estresantes. Además, algunas veces emergen nuevos patógenos naturales para los cuales el hospedador individual, y algunas veces la especie entera, no ha desarrollado resistencia. Estos patógenos emergentes a menudo causan infecciones agudas, caracterizadas por un comienzo rápido y llamativo. En estos casos, los patógenos pueden actuar como fuerzas selectivas en la evolución del hospedador, igual que el hospedador, al desarrollar resistencia, puede ser una fuerza selectiva en la evolución de los patógenos. En los casos en los que el patógeno no depende del hospedador para sobrevivir, con frecuencia el patógeno puede causar una enfermedad aguda devastadora.[3]​ Objetivos La epidemiología es parte importante de la salud pública y contribuye a: Definir los problemas e inconvenientes de salud importantes de una comunidad; Describir la historia natural de una enfermedad; Descubrir los factores que aumentan el riesgo de contraer una enfermedad (su etiología); Predecir las tendencias de una enfermedad; Determinar si la enfermedad o problema de salud es prevenible o controlable; Determinar la estrategia de intervención (prevención o control) más adecuada; Probar la eficacia de las estrategias de intervención; Cuantificar el beneficio conseguido al aplicar las estrategias de intervención sobre la población; Evaluar los programas de intervención; La medicina moderna, especialmente la mal llamada medicina basada en la evidencia (medicina factual o medicina basada en estudios científicos), está basada en los métodos de la epidemiología.[3]​ Vocabulario Hay una serie de términos que tienen un significado específico para el epidemiólogo. Una enfermedad es una epidemia cuando ocurre en un número inusualmente alto de individuos de una población simultáneamente; una pandemia es una epidemia que se disemina ampliamente, usualmente por todo el mundo. Una enfermedad endémica es la que está constantemente presente en una población, aunque su incidencia suele ser baja. La incidencia de una enfermedad determinada, es el número de nuevos casos de una enfermedad individual en una población de un determinado período de tiempo. La prevalencia de una enfermedad dada, es el número total de casos nuevos y ya existentes informados en una población y durante un determinado período de tiempo. Un brote de una enfermedad ocurre cuando se observa un número de casos, por lo general en un período de tiempo relativamente corto, en un área geográfica que anteriormente solo había presentado casos esporádicos de la enfermedad.[4]​ Mortalidad y morbilidad La mortalidad es la incidencia de muerte en la población. Las enfermedades infecciosas fueron la principal causa de la muerte en 1900 en los países desarrollados, pero ahora son mucho menos significativas. Hoy día, las enfermedades no infecciosas asociadas al estilo de vida, como las enfermedades cardíacas y el cáncer, son mucho más prevalentes y causan mayor mortalidad que las enfermedades infecciosas. Sin embargo, la situación actual podría cambiar rápidamente, si se llegaran a afectar en forma importante las infraestructuras y los servicios de salud públicas. En países en desarrollo, las enfermedades infecciosas son todavía la principal causa de mortalidad. La morbilidad se refiere a la incidencia de enfermedades en la población, incluyendo tanto enfermedades mortales como no mortales. Las estadísticas de la morbilidad definen la salud pública de una población con mayor precisión que las de mortalidad, porque muchas enfermedades tienen una mortalidad relativamente baja.[5]​ Progresión de la enfermedad En términos de sintomatología clínica, el curso de una enfermedad infecciosa aguda puede dividirse en etapas: Infección: el microorganismo invade, coloniza y crece en el hospedador. Período de incubación: el período de tiempo entre la infección y la aparición de los síntomas de la enfermedad. Período agudo: la enfermedad está en su punto culminante, con síntomas claros como fiebre y escalofríos. Período de declive: los síntomas de enfermedad están cediendo, la fiebre disminuye, usualmente después de un período de sudoración intensa, y aparece una sensación de bienestar. Período de convalecencia: el enfermo recupera las fuerzas y vuelve a la normalidad.[5]​ Metodología La epidemiología se basa en el método científico para la obtención de conocimientos, a través de los estudios epidemiológicos. Ante un problema de salud, y los datos disponibles sobre el mismo, se formula una hipótesis, la cual se traduce en una serie de consecuencias contrastables mediante experimentación. Se realiza entonces un proyecto de investigación que comienza con la recolección de datos y su posterior análisis estadístico, que permite obtener medidas de asociación (odds ratio, riesgo relativo, razón de tasas), medidas de efecto (riesgo atribuible) y medidas de impacto (fracción etiológica o riesgo atribuible proporcional), tanto a nivel de los expuestos como a nivel poblacional. De los resultados de esta investigación es posible obtener conocimientos que servirán para realizar recomendaciones de salud pública, pero también para generar nuevas hipótesis de investigación. En la literatura científica reciente se encuentran varios artículos de revisión, revisados por pares, que proveen una valiosa descripción general de las diferentes metodologías de la epidemiología.[6]​[7]​[8]​[9]​[10]​ Etiología de las enfermedades Artículo principal: Etiología Mapa original del Dr. John Snow. Los puntos muestran los casos de muerte por cólera durante la epidemia ocurrida en Londres en 1854. Las cruces representan los pozos de agua de los que bebieron los enfermos. El triángulo epidemiológico causal de las enfermedades está formado por el medio ambiente, los agentes y el huésped. Un cambio en cualquiera de estos tres componentes alterará el equilibrio existente para aumentar o disminuir la frecuencia de la enfermedad, por lo tanto se pueden llamar factores causales o determinantes de la enfermedad. Las bases de la epidemiología moderna fueron sentadas por Girolamo Fracastoro (Verona, 1487-1573) en sus obras De sympathia et antipathia rerum (Sobre la simpatía y la antipatía de las cosas) y De contagione et contagiosis morbis, et eorum curatione (Sobre el contagio y las enfermedades contagiosas y su curación), ambas publicadas en Venecia en 1546, donde Fracastoro expone sucintamente sus ideas sobre el contagio y las enfermedades transmisibles. Se considera al inglés John Graunt (1620-1674) quien publicó en 1662 el libro Natural and Political Observations Made upon the Bills of Mortality —sobre Londres— uno de los precursores de la epidemiología y de la demografía. Sin embargo, es John Snow (1813-1858), a quien se considera el precursor de la epidemiología contemporánea, ya que formuló la hipótesis de la transmisión del cólera por el agua y lo demostró confeccionando un mapa de Londres, en donde un reciente brote epidémico había matado más de 500 personas en un período de 10 días. Snow marcó en el mapa los hogares de los que habían muerto. La distribución mostraba que todas las muertes habían ocurrido en el área de Golden Square. La diferencia clave entre este distrito y el resto de Londres era el origen del agua potable. La compañía de agua privada que suministraba al vecindario de Golden Square extraía el agua de una sección del Támesis especialmente contaminado. Cuando se cambió el agua y comenzó a extraerse río arriba, de una zona menos contaminada, cedió la epidemia de cólera. Un progreso muy importante en el siglo XX, publicado en 1956 con los resultados del estudio de médicos británicos, fue la demostración de la relación causal entre fumar (tabaquismo) y el cáncer de pulmón.[5]​ Transición epidemiológica Constituye un proceso de cambio dinámico a largo plazo en la frecuencia, magnitud y distribución de la morbilidad y mortalidad de la población. La transición epidemiológica, que va acompañada por la transición demográfica, presenta cuatro aspectos a destacar: Desplazamiento en la prevalencia de las enfermedades transmisibles por las no trasmisibles. Desplazamiento en la morbilidad y mortalidad de los grupos jóvenes a los grupos de edad avanzada. Desplazamiento de la mortalidad como fuerza predominante por la morbilidad, sus secuelas e invalideces. Polarización epidemiológica. La polarización epidemiológica sucede cuando en distintas zonas de un país o en distintos barrios de una misma ciudad encontramos diferencias en la morbilidad y mortalidad de la población.[4]​ Ramas relacionadas Epidemiología descriptiva: es la rama de la epidemiología que describe el epidemiológico en tiempo, lugar y persona, cuantificando la frecuencia y distribución del fenómeno mediante medidas de incidencia, prevalencia y mortalidad, con la posterior formulación de hipótesis. Epidemiología analítica: busca, mediante la observación o la experimentación, establecer posibles relaciones causales entre factores a los que se exponen personas y poblaciones y las enfermedades que presentan. Las medidas empleadas en el estudio de esta rama de la epidemiología son los factores de riesgo, cuyo resultado es una probabilidad. Es posible distinguir dos tipos: riesgo absoluto y riesgo relativo. Riesgo absoluto: probabilidad de una enfermedad (baja, moderada, alta); si se considera la probabilidad de la enfermedad durante un periodo de tiempo, de lo que se está hablando es de una incidencia y no de un riesgo absoluto. Riesgo relativo: cuando se comparan dos riesgos absolutos entre sí; se trata de una probabilidad relativa (más alta o más baja que el otro); se ha de tener en cuenta que un riesgo relativo, por muy alto que sea, puede ser irrelevante; por ejemplo, fumar aumenta 100 veces el riesgo de sufrir una enfermedad, el riesgo sin fumar es de 1/100 000 000, por lo que el incremento por fumar es muy pequeño, prácticamente despreciable. Riesgo atribuible: en una población expuesta a un factor de riesgo, es la diferencia entre la incidencia de enfermedad en expuestos y no expuestos al factor de riesgo. La diferencia entre ambos valores proporciona el valor del riesgo de enfermedad en la cohorte expuesta, que se debe exclusivamente a la exposición al factor de riesgo. Epidemiología experimental: busca, mediante el control de las condiciones del grupo a estudiar, sacar conclusiones más complejas que con la mera observación no son deducibles. Se basa en el control de los sujetos a estudiar y en la aleatorización de la distribución de los individuos en dos grupos, un grupo experimental y un grupo control. Se ocupa de realizar estudios en animales de laboratorio y estudios experimentales con poblaciones humanas. Ecoepidemiología: busca, mediante herramientas ecológicas, estudiar integralmente como interaccionan los factores ambientales con las personas y poblaciones en los medios que los rodean y como ello puede influir en la evolución de enfermedades que se producen como consecuencia de dicha interacción."
ksampletext_wikipedia_medi_medicinainterna: str = "Medicina interna. La medicina interna es una especialidad médica que atiende integralmente los problemas de salud en pacientes adultos, ingresados en un centro hospitalario o en consultas ambulatorias.[1]​ Objetivos Guía al enfermo en su compleja trayectoria por el sistema sanitario hospitalario, dirigiendo y coordinando la actuación frente a su enfermedad y coordinando al resto de especialistas necesarios para obtener un diagnóstico y tratamiento adecuados. Los médicos internistas son los expertos a quienes recurren los médicos de atención primaria y el resto de especialistas para atender a enfermos complejos cuyo diagnóstico es difícil, que se encuentran afectados por varias enfermedades o que presentan síntomas en varios órganos, aparatos o sistemas del organismo. Dentro de la extensa formación de los internistas, existe la posibilidad de que algunos de ellos se subespecialicen en ciertos campos de la medicina, focalizándose únicamente en ellos, como el control de los factores de riesgo cardiovascular, enfermedades infecciosas y muy especialmente el VIH, la insuficiencia cardiaca congestiva, la enfermedad tromboembólica venosa y enfermedades autoinmunes, cuidados paliativos o unidades de pacientes crónicos complejos. Generalmente el médico internista requiere la atención de otros especialistas a la hora de la realización de pruebas diagnósticas, como a Radiología en caso de necesitar un TAC o una RM, al Digestivo para endoscopias, al cirujano para toma de biopsias, etc., de tal manera que de forma coordinada selecciona las pruebas que más convengan para lograr un diagnóstico certero del paciente. Historia Artículo principal: Historia de la Medicina A finales del siglo XIX comenzó a desarrollarse la medicina hospitalaria, muy unida a las clínicas universitarias, y surgió una nueva orientación en la medicina general, más ligada a las ciencias básicas biomédicas y a la experimentación, que recibió el nombre de Medicina Interna. El internista ha sido considerado, desde entonces, el clínico por excelencia. Dentro de este campo quedaron excluidas las enfermedades quirúrgicas, las obstétricas y las pediátricas, que, asimismo, constituyeron otras especialidades. Éstas, junto con la Medicina Interna, han sido consideradas, desde esa época, como especialidades básicas. [2]​ La denominación de Medicina Interna parece que tuvo su origen en Alemania, en 1880. En ese año, Strumpell escribió el primer tratado de Enfermedades Internas y, 2 años más tarde, en Wiesbaden, se celebró el I Congreso de Medicina Interna. Se quería indicar un campo de la práctica médica en el que los conceptos se basaban en el nuevo conocimiento que emergía en fisiología, bacteriología y patología, así como la exclusión de los métodos quirúrgicos en la terapéutica empleada. Este nuevo campo también llevaba la connotación de una formación académica y un entrenamiento. Además, estos médicos podían hacer de consultantes de otros especialistas. Es decir, la medicina interna sería como la medicina que trata enfermedades desde dentro, desde el interior del cuerpo, generalmente con medicamentos, en contraposición con la cirugía que trata las enfermedades desde fuera, con intervenciones quirúrgicas.[3]​ A partir de la segunda mitad del siglo XX surgen las especialidades médicas, ramas de la medicina interna. Se puede caer en el error, que perjudica seriamente al paciente, de que los especialistas no se responsabilicen de pacientes que caigan fuera del área de su particular competencia y cada vez ha sido más frecuente que a un mismo enfermo lo estén atendiendo múltiples especialistas, con los más diversos y, a veces, contradictorios enfoques.[cita requerida] Características La medicina interna es la especialidad de la medicina que se encarga de mantener la homeostasis del medio interno. Históricamente es una especialidad exclusivamente hospitalaria, aunque existen tendencias actuales en otras direcciones: consultas en centros periféricos de especialidades, hospitalización domiciliaria con equipos liderados por internistas, e integración en los equipos de Atención Primaria para colaborar como consultores. Un especialista en medicina interna o médico internista no es un médico interno: En España, los médicos internos residentes (MIR) son los médicos que, una vez superada una carrera teórica general en medicina y cirugía de seis años, deben superar el examen MIR y formarse durante 5 años para conseguir una especialidad de médico internista. En México, el médico interno (también conocido como Médico Interno de Pregrado) es aquel que cursa el quinto o sexto año de la carrera de médico cirujano (que dependiendo la universidad tiene una duración de 6 o 7 años) y un médico residente es aquel que, después de haber terminado la carrera de médico cirujano, cursa una especialidad médica (tras haber aprobado el respectivo Examen Nacional de Aspirantes a las Residencias Médicas). En el caso de la Medicina Interna, actualmente tiene una duración de 4 años, realizándose en el último año el servicio social con una duración de 3 a 4 meses, en alguna comunidad rural o ciudad del interior del país. Al término de su especialidad, se le da el diploma correspondiente a la especialidad de Medicina Interna. En el habla popular se le conoce como médico internista."
ksampletext_wikipedia_medi_neurologia: str = "Neurología. La neurología es la rama de la medicina que estudia el sistema nervioso. Específicamente se ocupa de la prevención, diagnóstico, tratamiento y rehabilitación de todas las enfermedades que involucran al sistema nervioso central, sistema nervioso periférico y el sistema nervioso autónomo. Existe gran número de enfermedades neurológicas, las cuales pueden afectar el sistema nervioso central (cerebro y médula espinal), el sistema nervioso periférico, o el sistema nervioso autónomo.[cita requerida] En España, la neurología como especialidad médica nació en el Hospital de la Santa Creu i Sant Pau de Barcelona (entonces Hospital de la Santa Creu) en 1882 de la mano del Dr. Lluís Barraquer i Roviralta. En un primer momento este servicio del Hospital de la Sant Creu se llamó Dispensario de Electroterapia, una consulta dedicada a pacientes con patologías del cerebro y de la médula espinal. Años después, el dispensario se llamó Servicio de Neurología y Electroterapia y finalmente acabó perdiendo la segunda denominación y se quedó como Servicio de Neurología.[2]​ El Dr. Barraquer introdujo, por primera vez en el país, la posibilidad de intervenir determinadas lesiones cerebrales y de ofrecer a estos pacientes la única posibilidad de curación que existía para ellos. A partir de 1910, se intervinieron una serie de pacientes con epilepsia focal, casi todos de origen traumático, que consistían en excisiones de las áreas corticales afectadas por la lesión de la cicatriz, lo que favoreció el nacimiento de la neurocirugía.[2] Uno de sus máximos exponentes fue el Dr. Manuel Corachán i Llort (hijo del Dr. Manuel Corachán Garcia), que en 1936 ya practicaba regularmente estas intervenciones en Sant Pau y que murió durante la guerra civil española.[3]​ Diagnóstico del sujeto con enfermedad neurológica Método clínico en la neurología El objetivo del método clínico en la neurología es servir como base para el tratamiento o la prevención de alguna enfermedad neurológica. En la mayoría de los casos el método consiste en cinco etapas, las cuales son: Pasos para diagnosticar una enfermedad neurológica. Principios de neurología por Adams y Víctor. Identificación de síntomas y signos mediante el interrogatorio y la exploración física. Los síntomas y signos físicos que se consideran importantes respecto al problema en cuestión son interpretados en términos fisiológicos y anatómicos: identificación de trastornos de la función y de la estructura anatómica involucrada. Diagnóstico anatómico/topográfico: Localización del proceso patológico (identificación de las partes del Sistema Nervioso afectadas), donde se reconoce un grupo característico de síntomas y signos, los cuales constituyen un síndrome, lo que nos ayuda a identificar el lugar y la naturaleza de la enfermedad. A esto se le conoce como diagnóstico sindrómico. A partir del diagnóstico anatómico y otros datos médicos (modo, rapidez de inicio, evolución, curso de la enfermedad, afección de sistemas orgánicos extraneurológicos, antecedentes personales y familiares y datos de laboratorio) es posible deducir el diagnóstico patológico. Cuando se identifica el mecanismo y la causalidad de la enfermedad se puede determinar el diagnóstico etiológico. Elaboración del diagnóstico funcional. Esta última etapa se refiere a la valoración del grado de incapacidad, donde se determina si este es temporal o permanente. Es de gran importancia para el tratamiento de la enfermedad y para la estimación del potencial de restablecimiento de la función, es decir, el pronóstico. El método precedente para el diagnóstico de las enfermedades neurológicas puede verse resumido en el diagrama colocado en esta sección. Este enfoque sistemático permite identificar de manera confiable la localización y a menudo el diagnóstico preciso de la enfermedad. Cabe recordar que no siempre es necesario plantear de esta forma la solución a un problema clínico, ya que algunas enfermedades neurológicas tienen cuadros clínicos muy característicos.[4]​ Exploración neurológica Una investigación (1897), obra de Joaquín Sorolla. La pintura muestra el interior del laboratorio del neurólogo Luis Simarro a finales del siglo XIX. Durante un examen neurológico, el neurólogo revisa la historia médica del paciente, con especial atención a sus condiciones recientes. Después le realiza un examen neurológico. Habitualmente, este examen neurológico evalúa el estado mental, las funciones de los nervios craneales, el sistema motor y el sistema sensitivo. Esta información ayuda al neurólogo a determinar si el problema se halla en el sistema nervioso y su localización clínica. La localización de la patología es la clave del proceso por el cual los neurólogos desarrollan sus diferentes diagnósticos. Pueden ser necesarios estudios posteriores para confirmar el diagnóstico, y finalmente una guía y terapia apropiada. La exploración neurológica se inicia con la exploración del paciente en tanto se practica el interrogatorio. La manera en que el paciente cuenta su enfermedad puede manifestar confusión o incoherencia del pensamiento, trastornos de la memoria o del juicio e incluso dificultades para comprender o expresar ideas. El resto de la exploración neurológica debe efectuarse como la última parte de la exploración física general a partir de, como ya se mencionó, la exploración de nervios craneales, cuello y tronco hasta terminar con las pruebas de las funciones motora, refleja y sensitiva de las extremidades superiores e inferiores. Dicha exploración debe modificarse según el estado del paciente. Desde luego muchas partes de la exploración no pueden efectuarse en el paciente comatoso; niños pequeños y lactantes o pacientes con padecimientos psiquiátricos necesitan explorarse de maneras especiales. Procedimientos de exploración y diagnóstico Pruebas de los nervios craneales: la función de los nervios craneales debe investigarse de manera más compleja en los pacientes que presentan síntomas neurológicos que en aquellos que no los experimentan. Si se sospecha una lesión de la fosa anterior debe someterse a prueba el sentido del olfato a través de cada fosa nasal, determinando si el paciente puede distinguir los olores. Los campos visuales se trazan mediante pruebas de confrontación, en algunos casos por investigación de cada ojo por separado buscando cualquier anomalía. La sensibilidad de la cara se somete a prueba con un alfiler y un poco de algodón, debe determinarse la presencia o ausencia de reflejos corneales. Se observan los movimientos faciales cuando el paciente habla y sonríe ya que la debilidad ligera puede ser más evidente en estas circunstancias. Es necesario inspeccionar las cuerdas vocales con instrumentos especiales en caso de sospecha de padecimiento del bulbo raquídeo o del nervio vago sobre todo cuando se presenta ronquera. Pruebas de la función motora: se deben tomar en cuenta las observaciones de la rapidez y fuerza de los movimientos, tamaño, tono y coordinación muscular.  Posiciones prona y supina. Es esencial que el paciente exponga por completo las extremidades para inspeccionarlas por atrofia y fasciculaciones así como para observarlas mientras conserva los brazos estirados en las posiciones prona y supina; que el individuo efectué tareas sencillas como alternar el contacto con su nariz y con el dedo del examinador; hacer que realice movimientos alternos rápidos particularmente los que involucran cambios de dirección, aceleración y desaceleración súbita; que el pulgar toque rápidamente la punta de cada uno de los dedos y efectué movimientos de supinación y pronación del antebrazo; además que complete tareas sencillas como abotonarse la ropa, abrir un broche o manipular herramientas comunes. Pruebas de la función refleja: las pruebas de los reflejos bicipital, tricipital, supinador, rotuliano, aquíleo, cutáneo abdominal y plantar permiten obtener una idea de lo adecuada que es la actividad refleja de la medula espinal. Los reflejos tendinosos requieren que los músculos afectados estén relajados; los reflejos hipoactivos o que apenas pueden descartarse suelen facilitarse mediante contracción voluntaria de otros músculos. La presencia de reflejos cutáneos superficiales de los músculos abdominales, cremasterianos y de otros tipos suele constituir una prueba básica de gran utilidad para identificar lesiones corticospinales. Pruebas de la función sensitiva: esta es la parte más complicada de la exploración neurológica, se reserva para la parte final de la exploración y no debe prolongarse durante más de unos pocos minutos si se requiere que los datos sean confiables. Por lo general se buscan diferencias entre ambos lados del cuerpo, el nivel por debajo del cual se pierde la sensación o la existencia de una zona de anestesia relativa o absoluta. Se explica al paciente con brevedad cada prueba; hablar demasiado sobre estas pruebas con un paciente introspectivo meticuloso puede animarlo para que notifique variaciones menores independientemente de la intensidad del estímulo. No es necesario explorar todas las regiones superficiales de la piel, la investigación rápida de cara, cuello, manos, tronco y pies con un alfiler requiere solo unos cuantos segundos. Las regiones con déficit sensitivo pueden someterse a otras pruebas. El descubrimiento de alguna zona con hiperestesia dirige la atención a un trastorno de sensibilidad superficial. Exploración de la estación y la marcha: ninguna exploración está completa sino se observa al paciente en posición erguida. Quizá la anomalía neurológica más destacada o la única sea la anormalidad de la bipedestación y la marcha, como sucede en algunos trastornos cerebelosos o del lóbulo frontal. Además una alteración de la postura y los movimientos de adaptación automáticos puros al caminar proporciona la pista diagnostica más definitiva en la etapa inicial de la enfermedad de Parkinson y de la parálisis supranuclear progresiva. El paciente médico o quirúrgico sin síntomas neurológicos: para las extremidades superiores suele ser suficiente la observación de los brazos desnudos y estirados en busca de atrofia, debilidad (impulso pronador), temblor o movimientos anormales; la verificación de la fuerza, empuñadura y dorsiflexión a nivel de la muñeca; inquirir acerca de los trastornos sensitivos y desencadenar los reflejos supinador, bicipital y tricipital. El desencadenamiento de los reflejos rotuliano, aquíleo y plantar; las pruebas de vibración y sentido de posición en los dedos de las manos y pies, y la valoración de la coordinación haciendo que el paciente toque de forma alternada su nariz y un dedo del examinador, así como que deslice el talón hacia arriba y abajo por el frente de la pierna opuesta. El paciente comatoso: la exploración cuidadosa del paciente en estupor o comatoso ofrece información considerable en cuanto a la función del sistema nervioso. Se deben reconocer las posturas predominantes de las extremidades y el cuerpo; la presencia o ausencia de movimientos espontáneos en un lado; la posición de la cabeza y los ojos, la velocidad, profundidad y ritmo de la respiración. Se valora la reacción que tiene el paciente al oír su nombre, órdenes sencillas o a estímulos nocivos. Por lo regular es posible determinar si el coma está relacionado con irritación meníngea o enfermedad cerebral focal o del tallo cerebral. En las etapas menos profundas del coma la irritación meníngea produce una resistencia a la flexión pasiva del cuello pero no a la extensión, rotación o inclinación de la cabeza. Diagnóstico de laboratorio: la descripción del método clínico y su aplicación evidencia que la exploración clínica rigurosa debe preceder siempre al empleo de los auxiliares de laboratorio, sin embargo en neurología la finalidad de estos es la prevención. Por tanto en la neurología preventiva la metodología de laboratorio puede adquirir prioridad sobre la metodología clínica. La información genética permite al neurólogo identificar a los pacientes en peligro de desarrollar ciertas enfermedades para iniciar de inmediato la búsqueda de marcadores biológicos antes que los síntomas o signos aparezcan. Las pruebas de investigación bioquímica son aplicables para toda una población y permiten identificar en individuos que aún no muestran síntomas, y en algunas de estas enfermedades es posible aplicar un tratamiento antes de que se sufra una lesión en el sistema nervioso. Trabajo clínico Casos en general Los neurólogos son responsables del diagnóstico, tratamiento y manejo de todas las condiciones mencionadas arriba. Cuando la intervención quirúrgica es requerida, el neurólogo puede referirse al paciente como «neuropaciente». En algunos países, algunas responsabilidades legales de un neurólogo pueden incluir efectuar un diagnóstico de muerte cerebral si el paciente fallece. Suelen tratar personas con enfermedades congénitas si la mayor parte de las manifestaciones son neurológicas. Las punciones lumbares también pueden ser realizadas por estos profesionales. Algunos neurólogos desarrollan un interés a subcampos en particular como las enfermedades cerebrovasculares, los trastornos del movimiento, epilepsia, cefaleas, neurología de la conducta y demencias, trastornos del sueño, control de dolor crónico, esclerosis múltiple o enfermedades neuromusculares. Áreas destacadas Hay superposición de otras especialidades, variando de país en país e incluso en un área geográfica local. El traumatismo craneoencefálico (ETC) agudo es más comúnmente tratado por neurocirujanos, mientras que secuelas de traumas craneoencefálicos pueden ser tratados por neurólogos o especialistas en rehabilitación médica. Aunque los casos de accidente cerebrovascular (ACV) han sido tradicionalmente tratados por médicos internistas u hospitalarios, el surgimiento de neurología vascular y neurólogos intervencionistas han creado una demanda para especialistas en ACV. La organización de JHACO centro certificado en accidentes cerebrovasculares ha incrementado el papel de los neurólogos en el tratamiento de accidentes cerebrovasculares en muchos centros de atención primaria, así como en hospitales de tercer nivel. Algunos casos de enfermedades infecciosas del sistema nervioso son tratados por especialistas en enfermedades infecciosas. La mayoría de los casos de dolor de cabeza son diagnosticados y tratados principalmente por médicos generales, al menos los casos menos severos. Del mismo modo, la mayoría de los casos de ciática y otras radiculopatías mecánicas son atendidos por médicos generales, aunque pueden ser enviados a neurólogos o cirujanos (neurocirujanos o cirujanos ortopédicos). Los trastornos del sueño generalmente son tratados en unidades multidisciplinares en las que participan neurólogos, neumólogos y psiquiatras. Una parálisis cerebral es inicialmente atendida por pediatras, pero el tratamiento puede ser transferido a un neurólogo de adultos después de que el paciente alcanza una cierta edad. Los neuropsicólogos clínicos son usualmente consultados para realizar una evaluación funcional del comportamiento y funciones cognitivas superiores, relacionada con la asistencia en diagnósticos diferenciales, la planificación de estrategias de rehabilitación, el registro de fuerzas y debilidades cognitivas, y la medición de cambios en el tiempo (por ejemplo, para identificar anomalías de envejecimiento o llevando el progreso de una demencia). Relaciones a la neurofisiología clínica En algunos países como Estados Unidos y Alemania, los neurólogos se pueden especializar en neurofisiología clínica, en electroencefalografía, o en el estudio de la conducción nerviosa, en Electromiografías y potenciales evocados. En otros países, es una especialidad independiente (por ejemplo en el Reino Unido y Suecia). Superposición con la psiquiatría A pesar de que las enfermedades mentales son consideradas por algunos de ser desórdenes neurológicos afectando el sistema nervioso central, tradicionalmente se las clasifica por separado, y son tratadas por psiquiatras. En el año 2002, en una reseña del American Journal of Psychiatry, el profesor Joseph B. Martin, decano de Harvard Medical School y neurólogo de profesión, escribió que: «la división en dos categorías es arbitraria, a menudo influenciada por creencias más que por observaciones científicas verificables. Y el hecho de que el cerebro y la mente sean uno solo, hace que esta división sea solamente artificial de todas formas». Esta perspectiva ha propiciado un progresivo acercamiento entre ambas especialidades en las últimas dos décadas, que finalmente se materializó en 2004 con el reconocimiento, en Estados Unidos, de la subespecialidad en «Neurología de la conducta y Neuropsiquiatría». Actualmente, los médicos de esta subespecialidad se encargan del estudio, diagnóstico y tratamiento de las alteraciones de la conducta y los trastornos mentales atribuibles a enfermedades neurológicas. Las enfermedades neurológicas a menudo tienen manifestaciones psiquiátricas, como por ejemplo psicosis, depresión, manía y ansiedad. Estos síndromes neuropsiquiátricos son relativamente habituales en pacientes con ictus, enfermedad de Huntington, parkinsonismos, enfermedad de Alzheimer, enfermedad por cuerpos de Lewy, enfermedad de Pick, encefalitis infecciosas, encefalitis autoinmunes, así como en algunos tipos de epilepsia, por nombrar solo algunas. Efectos del envejecimiento sobre el sistema nervioso Vejez, Emily Samson en la bienvenida al nuevo mundo. De todos los cambios vinculados con la edad tienen una enorme importancia los que tiene el sistema nervioso, algunos signos neurológicos del envejecimiento son: los signos neurooftalmológicos, pérdida de la audición perceptiva progresiva, disminución del sentido del olfato y menor extensión del gusto, reducción de la velocidad y magnitud de actividad motora, tiempo de reacción lento, trastornos de coordinación y agilidad, reducción de la fuerza muscular y adelgazamiento de los músculos, cambios de los reflejos tendinosos y finalmente trastornos del sentido de vibración en los dedos de los pies y en tobillos. Pareja de ancianos. Roger Hsu. Neurología cosmética El emergente campo de la neurología cosmética señala el potencial de terapias para mejorar cuestiones como la eficacia laboral, la atención en la escuela, y una mayor felicidad en la vida personal. A pesar de todo, este campo ha dado también lugar a preguntas acerca de la neuroética o la psicofarmacología. Temas relacionados Temas clásicos Neuroanatomía Neuropediatría Neurootología Neuropsicología y Neurología de la conducta Semiología Métodos Diagnósticos Tomografía axial computarizada Angiografía cerebral Imagen por resonancia magnética (IRM) Electromiografía Tomografía por emisión de positrones Punción lumbar Biopsia cerebral Enfermedades del sistema nervioso central Afasia Anomalías del desarrollo del sistema nervioso central Enfermedades carenciales del sistema nervioso Degeneración combinada subaguda de la médula espinal Encefalopatía de Wernicke Enfermedades cerebrovasculares Enfermedades de la médula espinal Siringomielia Hernia discal Mielitis transversa Enfermedades degenerativas del sistema nervioso central Enfermedad de Alzheimer Atrofia multisistémica Parálisis supranuclear progresiva Enfermedad de Parkinson Esclerosis lateral amiotrófica Enfermedad de Huntington Enfermedades del sistema extrapiramidal Enfermedades desmielinizantes del sistema nervioso central Esclerosis múltiple Enfermedad de Devic Esclerosis concéntrica de Baló Encefalomielitis diseminada aguda Enfermedades infecciosas del sistema nervioso central Meningitis Absceso Cerebral Toxoplasmosis cerebral Encefalitis Enfermedades metabólicas del sistema nervioso central Epilepsias Traumatismos craneoencefálicos Tromboembolismo intracraneal Tumor intracraneal Meningioma Pinealoma Ependimoma Astrocitoma Meduloblastoma Oligodendroglioma Enfermedades del sistema nervioso periférico Síndrome de Guillain-Barré Síndrome de Charcot-Marie-Tooth Enfermedades musculares o miopatías Distrofia muscular de Duchenne Distrofia miotónica de Steinert Enfermedades de la unión neuromuscular Miastenia grave Síndrome miasténico de Lambert-Eaton"

ksampletext_wikipedia_geol_tectonicadeplacas: str = "Tectónica de placas. La tectónica de placas o tectónica global es una teoría que explica la forma en que está estructurada la litosfera (porción externa más fría y rígida de la Tierra). La rama de la Geología que se encarga de su estudio es la tectónica. La teoría da una explicación a las placas tectónicas que forman parte de la superficie de la Tierra y a los deslizamientos que se observan entre ellas en su movimiento sobre el manto terrestre fluido, sus direcciones e interacciones. También explica la formación de las cadenas montañosas (orogénesis). Asimismo, da una explicación satisfactoria al hecho de que los terremotos y los volcanes se concentran en regiones concretas del planeta (como el Cinturón de Fuego del Pacífico) o a la ubicación de las grandes fosas oceánicas junto a los arcos insulares y continentes y no en el centro del océano.[1]​ Las placas tectónicas se desplazan unas respecto de otras con relativa lentitud, a una velocidad nunca perceptible sin instrumentos, pero con tasas diferentes. La mayor velocidad se da en la dorsal del Pacífico Oriental, cerca de la Isla de Pascua, a unos 3400 km de Chile continental, con una velocidad de separación entre placas de más de 15 cm/año y la más lenta se da en la dorsal ártica, con menos de 2,5 cm/año.[2]​[3] Dado que se desplazan sobre la superficie finita de la Tierra, las placas interaccionan unas con otras a lo largo de sus límites provocando intensas deformaciones en la corteza y litosfera de la Tierra, lo que ha dado lugar a la formación de grandes cadenas montañosas (por ejemplo las cordilleras de Himalaya, Alpes, Pirineos, Atlas, Urales, Apeninos, Apalaches, Andes, entre muchos otros) y grandes sistemas de fallas asociadas con estas (por ejemplo, el sistema de fallas de Anatolia del Norte). El contacto por fricción entre los bordes de las placas es responsable de la mayor parte de los terremotos. Otros fenómenos asociados son la creación de volcanes (especialmente notorios en el cinturón de fuego del océano Pacífico) y las fosas oceánicas. Las placas tectónicas se componen de dos tipos distintos de litosfera: la corteza continental, más gruesa, y la corteza oceánica, la cual es relativamente delgada. A la parte superior de la litosfera se la conoce como corteza terrestre, nuevamente de dos tipos (continental y oceánica). Esto significa que una placa litosférica puede ser continental, oceánica, o bien de ambos tipos, en cuyo caso se denomina placa mixta. Uno de los principales puntos de la teoría propone que la cantidad de superficie de las placas (tanto continental como oceánica) que desaparecen en el manto a lo largo de los bordes convergentes de subducción está más o menos en equilibrio con la corteza oceánica nueva que se está formando a lo largo de los bordes divergentes (dorsales oceánicas) a través del proceso conocido como expansión del fondo oceánico. También se suele hablar de este proceso como el principio de la cinta transportadora. En este sentido, el total de la superficie en el globo se mantiene constante, siguiendo la analogía de la cinta transportadora, siendo la corteza la cinta que se desplaza gracias a las fuertes corrientes convectivas de la astenosfera, que hacen las veces de las ruedas que transportan esta cinta, hundiéndose la corteza en las zonas de convergencia, y generándose nuevo piso oceánico en las dorsales. La teoría también explica de forma bastante satisfactoria la forma en que las inmensas masas que componen las placas tectónicas se pueden desplazar, algo que quedaba sin explicar cuando Alfred Wegener propuso la teoría de la deriva continental, aunque existen varios modelos que coexisten: Las placas tectónicas se pueden desplazar porque la litosfera tiene una menor densidad que la astenosfera, que es la capa que se encuentra inmediatamente inferior a la corteza. Esto hace que las placas floten en la astenosfera y el magma líquido más caliente vaya hacia arriba y el más frío y denso hacia abajo, generando una corriente que mueve las placas. Las variaciones de densidad laterales resultan en las corrientes de convección del manto, mencionadas anteriormente. Se cree que las placas son impulsadas por una combinación del movimiento que se genera en el fondo oceánico fuera de la dorsal (debido a variaciones en la topografía y densidad de la corteza, que resultan en diferencias en las fuerzas gravitacionales, arrastre, succión vertical, y zonas de subducción). Una explicación diferente o complementaria se apoya en las diferentes fuerzas que se generan con la rotación del globo terrestre y las fuerzas de marea del Sol y de la Luna; sin embargo, la importancia relativa de cada uno de esos factores no está clara y es objeto de debate.[cita requerida] Placas tectónicas en el mundo Actualmente existen las siguientes placas tectónicas en la superficie de la tierra con límites más o menos definidos, que se dividen en 15 placas mayores (o principales) y 43 placas menores (o secundarias). Las 15 placas mayores Las 15 placas tectónicas mayores. Placa africana Placa antártica Placa arábiga Placa australiana Placa del Caribe Placa de Cocos Placa euroasiática Placa filipina Placa India Placa Juan de Fuca Placa de Nazca Placa norteamericana Placa del Pacífico Placa de Scotia Placa sudamericana Las 42 placas menores Mapa detallado que muestra las placas tectónicas con sus vectores de movimiento. Placa de Altiplano Placa de Amuria Placa de Anatolia Placa de los Andes del Norte Placa Apuliana o Adriática Placa del Arrecife de Balmoral Placa del Arrecife de Conway Placa de Birmania Placa de Bismarck del Norte Placa de Bismarck del Sur Placa Cabeza de Pájaro o Doberai Placa de las Carolinas Placa de Chiloé Placa del Explorador Placa de Futuna Placa Galápagos Placa de Gorda Placa Iraní Placa de Juan Fernández Placa de Kermadec Placa de Manus Placa de Maoke Placa del Mar de Banda Placa del Mar Egeo o Helénica Placa del Mar de las Molucas Placa del Mar de Salomón Placa de las Marianas Placa Niuafo'ou Placa africana Placa de las Nuevas Hébridas Placa de Ojotsk Placa de Okinawa Placa de Panamá Placa de Pascua Placa de Rivera Placa de Sandwich Placa de Shetland Placa somalí Placa de la Sonda Placa de Timor Placa de Tonga Placa Woodlark Placa del Yangtsé Se han identificado tres tipos de bordes: convergentes (dos placas chocan una contra la otra), divergentes (dos placas se separan) y transformantes (dos placas se deslizan una junto a otra). La teoría de la tectónica de placas se divide en dos partes, la de deriva continental, propuesta por Alfred Wegener en la década de 1910, y la de expansión del fondo oceánico, propuesta y aceptada en la década de 1960, que mejoraba y ampliaba a la anterior. Desde su aceptación ha revolucionado las ciencias de la Tierra, con un impacto comparable al que tuvieron las teorías de la gravedad de Isaac Newton y Albert Einstein en la Física o las leyes de Kepler en la Astronomía. Causas del movimiento de las placas Artículo principal: Convección Movimiento por convección. El origen del movimiento de las placas está en unas corrientes de materiales que suceden en el manto, las denominadas corrientes de convección, y sobre todo, en la fuerza de la gravedad. La convección es una de las tres formas de transferencia de calor y se caracteriza porque se produce por intermedio de un fluido (aire, agua) que transporta el calor entre zonas con diferentes temperaturas. La convección se produce únicamente por medio de materiales fluidos. Éstos, al calentarse, aumentan de volumen y, por lo tanto, disminuyen su densidad y ascienden desplazando el fluido que se encuentra en la parte superior y que está a menor temperatura. Lo que se llama convección en sí, es el transporte de calor por medio de las corrientes ascendente y descendente del fluido. Las corrientes de convección se producen por diferencias de temperatura y densidad, de manera que los materiales más calientes pesan menos y ascienden, y los materiales más fríos son más densos, pesados, y descienden. El manto, aunque es sólido, se comporta como un material plástico o dúctil, es decir, se deforma y se estira sin romperse, debido a las altas temperaturas a las que se encuentra, sobre todo el manto inferior. En las zonas profundas el manto hace contacto con el núcleo, el calor es muy intenso, por eso grandes masas de roca se funden parcialmente y al ser más ligeras ascienden lentamente por el manto, produciendo unas corrientes ascendentes de materiales calientes, las plumas o penachos térmicos. Algunos de ellos alcanzan la litosfera, la atraviesan y contribuyen a la fragmentación de los continentes. En las fosas oceánicas, grandes fragmentos de litósfera oceánica fría se hunden en el manto, originando por tanto unas corrientes descendentes, que llegan hasta la base del manto. Las corrientes ascendentes y descendentes del manto podrían explicar el movimiento de las placas, al actuar como una especie de rodillo que las moviera. Antecedentes históricos Deriva continental Artículo principal: Deriva continental A finales del siglo XIX y principios del XX, los geólogos asumían que las principales características de la Tierra eran fijas y que la mayoría de las características geológicas, como el desarrollo de cuencas y cadenas montañosas, podían explicarse por el movimiento vertical de la corteza, descrito en lo que se denomina teoría geosinclinal. Generalmente, esto se colocó en el contexto de un planeta Tierra en contracción debido a la pérdida de calor en el transcurso de un tiempo geológico relativamente corto.[4]​ Ya en 1596 se observó que las costas opuestas del Océano Atlántico (aunque es más preciso hablar de los bordes de las plataformas continentales) tienen formas similares y parecen haber encajado en algún momento pasado. Desde entonces se propusieron muchas teorías para explicar esta aparente complementariedad, pero el supuesto de una Tierra sólida hizo que estas diversas propuestas fueran difíciles de aceptar. El descubrimiento de la radiactividad y sus propiedades de calentamiento asociadas en 1895 impulsó un nuevo examen de la edad aparente de la Tierra. Esto se había estimado previamente por su tasa de enfriamiento bajo el supuesto de que la superficie de la Tierra irradiaba como un cuerpo negro. Esos cálculos habían implicado que, incluso si comenzara con un calor rojo, la Tierra habría caído a su temperatura actual en unas pocas decenas de millones de años. Armados con el conocimiento de una nueva fuente de calor, los científicos se dieron cuenta de que la Tierra sería mucho más antigua y que su núcleo todavía estaba lo suficientemente caliente como para ser líquido. Alfred Wegener en el verano de 1912-13 en Groenlandia. En 1915, después de haber publicado un primer artículo en 1912, Alfred Wegener presentó argumentos serios a favor de la idea de la deriva continental en la primera edición de El origen de los continentes y océanos. En ese libro (reeditado en cuatro ediciones sucesivas hasta la última en 1936), señaló cómo la costa este de América del Sur y la costa oeste de África parecían enacajar (de lo que ya se habían percatado anteriormente Benjamin Franklin entre otros).[5]​. Wegener no fue el primero en notar esto (Abraham Ortelius, Antonio Snider-Pellegrini, Eduard Suess, Roberto Mantovani y Frank Bursley Taylor lo precedieron, solo por mencionar algunos), pero fue el primero en reunir importantes evidencias fósiles, paleo-topográficas y climatológicas para apoyar esta simple observación (y fue apoyado en esto por investigadores como Alex du Toit). También tuvo en cuenta el parecido de la fauna fósil de los continentes septentrionales y ciertas formaciones geológicas. Wegener conjeturó que el conjunto de los continentes actuales estuvieron unidos en el pasado remoto de la Tierra, formando un supercontinente, denominado Pangea.[6] Además, dado que los estratos rocosos de los márgenes de continentes separados son muy similares, sugiere que estas rocas se formaron de la misma manera, lo que implica que estaban unidas en un principio. Por ejemplo, partes de Escocia e Irlanda contienen rocas muy similares a las que se encuentran en Terranova y Nuevo Brunswick. Además, las Montañas Caledonianas de Europa y partes de los montes Apalaches de América del Norte son muy similares en estructura y litología.[7]​ Sin embargo, sus ideas no fueron tomadas en serio por muchos geólogos,[8] quienes señalaron que no existía un mecanismo aparente para la deriva continental. En su tesis original, Wegener propuso que los continentes se desplazaban sobre el manto de la Tierra de la misma forma en que uno desplaza una alfombra sobre el piso de una habitación. Sin embargo, esto no es posible, debido a la enorme fuerza de fricción implicada, lo que motivó el rechazo de la explicación de Wegener, y la puesta en suspenso, como hipótesis interesante pero no probada, de la idea del desplazamiento continental hasta la aparición de la Tectónica de placas. Más concretamente, no vieron cómo la roca continental podría atravesar la roca mucho más densa que forma la corteza oceánica. Wegener no pudo explicar la fuerza que impulsó la deriva continental, y su reivindicación no llegó hasta después de su muerte en 1930.[9]​ Continentes flotantes, paleomagnetismo y zonas sísmicas Como se observó temprano que aunque existía granito en los continentes, el fondo marino parecía estar compuesto de basalto más denso, el concepto predominante durante la primera mitad del siglo XX fue que había dos tipos de corteza, denominada sial (corteza de tipo continental). y sima (corteza de tipo oceánico).[10] Además, se suponía que había una capa estática de estratos debajo de los continentes. Por lo tanto, parecía evidente que una capa de basalto (sial) subyace a las rocas continentales. Sin embargo, basándose en anomalías en la desviación de la plomada de los Andes en Perú, Pierre Bouguer había deducido que las montañas menos densas deben tener una proyección hacia abajo en la capa inferior más densa. El concepto de que las montañas tenían raíces fue confirmado por George B. Airy cien años después, durante un estudio de la gravitación del Himalaya, y los estudios sísmicos detectaron variaciones de densidad correspondientes. Por lo tanto, a mediados de la década de 1950 seguía sin resolverse la cuestión de si las raíces de las montañas estaban apretadas en el basalto circundante o flotaban sobre él como un iceberg. Epicentros de terremotos, 1963–1998. La mayoría de los terremotos tienen lugar en estrechos cinturones que coinciden con los límites entre placas. Durante el siglo XX las mejoras y el mayor uso de instrumentos sísmicos como los sismógrafos permitieron a los científicos comprender que los terremotos tienden a concentrarse en áreas específicas, sobre todo a lo largo de las fosas oceánicas y las dorsales. A finales de la década de 1920 los sismólogos estaban comenzando a identificar varias zonas prominentes de terremotos paralelas a las fosas que normalmente se inclinaban entre 40 y 60° desde la horizontal y se extendían varios cientos de kilómetros hacia el interior de la Tierra. Estas zonas se conocieron más tarde como zonas de Wadati-Benioff, o simplemente zonas de Benioff[11]​, en honor a los sismólogos que las reconocieron por primera vez, Kiyoo Wadati de Japón y Hugo Benioff de Estados Unidos. El estudio de la sismicidad global avanzó enormemente en la década de 1960 con el establecimiento de la Red Mundial de Sismógrafos Estandarizados (WWSSN) para monitorizar el cumplimiento del tratado de 1963 que prohibía las pruebas aéreas de armas nucleares. Los datos muy mejorados de los instrumentos de WWSSN permitieron a los sismólogos mapear con precisión las zonas de concentración de terremotos en todo el mundo. Mientras tanto, se desarrollaron debates en torno al fenómeno de la deriva polar. Desde los primeros debates sobre la deriva continental, los científicos habían discutido y utilizado evidencias de que la deriva polar había ocurrido porque los continentes parecían haberse movido a través de diferentes zonas climáticas durante el pasado. Además, los datos paleomagnéticos habían demostrado que el polo magnético también se había desplazado con el tiempo. Razonando de manera opuesta, los continentes podrían haberse movido y girado, mientras que el polo permanecía relativamente fijo.[12] La primera vez que se utilizó la evidencia de la desviación polar magnética para respaldar los movimientos de los continentes fue en un artículo de Keith Runcorn en 1956, y artículos sucesivos de él y sus estudiantes Ted Irving (quien en realidad fue el primero en estar convencido del hecho de que el paleomagnetismo apoyaba la deriva continental) y Ken Creer. A esto siguió inmediatamente un simposio en Tasmania en marzo de 1956. En este simposio, la evidencia se utilizó en la teoría de una expansión de la corteza global. En esta hipótesis, el desplazamiento de los continentes puede explicarse simplemente por un gran aumento en el tamaño de la Tierra desde su formación. Sin embargo, esto fue insatisfactorio porque sus partidarios no pudieron ofrecer un mecanismo convincente para producir una expansión significativa de la Tierra. Ciertamente, no hay evidencia de que la Luna se haya expandido en los últimos 3 000 millones de años; otros trabajos pronto mostrarían que la evidencia estaba igualmente a favor de la deriva continental en un globo con un radio estable. Durante los años treinta hasta finales de los cincuenta, los trabajos de Vening-Meinesz, Holmes, Umbgrove y muchos otros delinearon conceptos que eran cercanos o casi idénticos a la teoría de la tectónica de placas moderna. En particular, el geólogo inglés Arthur Holmes propuso en 1920 que las uniones de placas podrían encontrarse debajo del mar, y en 1928 que las corrientes de convección dentro del manto podrían ser la fuerza impulsora. A menudo, estas contribuciones se olvidan porque: En ese momento no se aceptaba la deriva continental. Algunas de estas ideas se discutieron en el contexto de ideas fijistas abandonadas de un globo deformante sin deriva continental o una Tierra en expansión. Fueron publicadas durante un episodio de extrema inestabilidad política y económica que obstaculizó la comunicación científica. Muchas fueron publicadas por científicos europeos y al principio no se mencionaron o se les dio poco crédito en los artículos sobre la extensión del fondo marino publicados por los investigadores estadounidenses en la década de 1960. Expansión de la dorsal mediooceánica y convección Artículo principal: Expansión del fondo oceánico Sumergible Alvin, que participó en el proyecto FAMOUS de exploración de la dorsal mesoatlántica. El primer mapa de los fondos oceánicos se consigue elaborar en 1956 gracias a los avances en las tecnologías del sónar. Se investigó el Océano Atlántico y se descubrió que: Había una cordillera submarina, a la que llamaron dorsal. Las rocas cercanas a los continentes eran más antiguas que las del centro. Los epicentros de los terremotos tenían lugar en la dorsal. Existían más de 6000 km de dorsales. Por estas razones en 1960 Harry Hess y en 1961 Robert Dietz sugirieron que el suelo oceánico se expande. En 1963 esta hipótesis se comprobó cuando Vine y Matthews identificaron las líneas de magnetismo de distinta polaridad, es decir, que el campo magnético terrestre se invierte.[13]​ En 1974, dentro del proyecto internacional FAMOUS, un equipo de científicos de la Institución Oceanográfica de Woods Hole (EE. UU.) y del French Centre Oceanologique de Bretagne (Brest, Francia) utilizó buques de investigación en superficie, así como diverso instrumental avanzado que incluía magnetómetros, sonar y sismógrafos, además de dos sumergibles: el Alvin (EE. UU.) y el Archimède (Francia). Las investigaciones confirmaron la existencia de una elevación en el Océano Atlántico central y descubrieron que el fondo del lecho marino, debajo de la capa de sedimentos, consistía en basalto, no en granito, que es el componente principal de los continentes. También encontraron actividad volcánica y sísmica y que la corteza oceánica era mucho más delgada que la corteza continental. Todos estos nuevos hallazgos plantearon preguntas importantes e intrigantes.[14]​ Las fuentes hidrotermales encontradas en las dorsales son consecuencia de una intensa actividad volcánica. Los nuevos datos recopilados sobre las cuencas oceánicas también mostraron características particulares en cuanto a la batimetría. Uno de los principales resultados de estos conjuntos de datos fue que en todo el mundo se detectó un sistema de dorsales oceánicas. Una conclusión importante fue que a lo largo de este sistema se estaba creando un nuevo fondo oceánico, lo que llevó al concepto de la Gran Grieta Global. Esto se describió en el artículo crucial de Bruce Heezen (1960) basado en su trabajo con Marie Tharp, que desencadenaría una verdadera revolución en el pensamiento. Una consecuencia profunda de la expansión del lecho marino es que se crea y se sigue creando una nueva corteza a lo largo de las dorsales oceánicas. Por lo tanto, Heezen defendió la supuesta hipótesis de la Tierra en expansión de S. Warren Carey (ver arriba). Entonces, todavía quedaba la pregunta: ¿cómo se puede agregar continuamente nueva corteza a lo largo de las dorsales oceánicas sin aumentar el tamaño de la Tierra? En realidad, esta cuestión ya había sido resuelta por numerosos científicos durante los años cuarenta y cincuenta, como Arthur Holmes, Vening-Meinesz, Coates y muchos otros: la corteza en exceso desaparece a lo largo de las llamadas fosas oceánicas, donde se produce el proceso conocido como subducción. Por lo tanto, cuando varios científicos a principios de la década de 1960 comenzaron a razonar sobre los datos que tenían a su disposición sobre el fondo del océano, las piezas de la teoría encajaron rápidamente. La pregunta intrigó particularmente a Harry Hammond Hess, un geólogo de la Universidad de Princeton y contraalmirante de la Reserva Naval, y a Robert S. Dietz, un científico de la U.S. National Geodetic Survey, quien acuñó por primera vez el término expansión del fondo oceánico. Dietz y Hess (el primero publicó la misma idea un año antes en Nature, pero la prioridad pertenece a Hess, que ya había distribuido un manuscrito inédito de su artículo de 1962 en 1960) se encontraban entre el pequeño puñado que realmente entendió las amplias implicaciones de la expansión del fondo marino y cómo eventualmente estaría de acuerdo con las ideas, en ese momento poco convencionales y no aceptadas, de la deriva continental y los modelos elegantes y movilistas propuestos por investigadores anteriores como Holmes.[15]​ En el mismo año, Robert R. Coats del U.S. Geological Survey describió las principales características de la subducción del arco insular en las Islas Aleutianas. Su artículo, aunque poco conocido (e incluso ridiculizado) en ese momento, desde entonces ha sido llamado seminal y profético. En realidad, muestra que el trabajo de científicos europeos sobre arcos de islas y cinturones montañosos realizado y publicado durante la década de 1930 hasta la década de 1950 fue aplicado y apreciado también en los Estados Unidos. Lava almohadillada como la producida por la actividad volcánica en las dorsales, apenas cubierta por una fina capa de sedimentos, lo que indica su reciente formación. Si la corteza terrestre se estaba expandiendo a lo largo de las dorsales oceánicas, razonaron Hess y Dietz como Holmes y otros antes que ellos, debe estar encogiéndose en otros lugares. Hess siguió a Heezen, sugiriendo que la nueva corteza oceánica se separa continuamente de las dorsales en un movimiento similar a una cinta transportadora. Y, utilizando los conceptos movilistas desarrollados anteriormente, concluyó correctamente que muchos millones de años después, la corteza oceánica finalmente desciende a lo largo de los márgenes continentales donde se forman fosas oceánicas (cañones estrechos y muy profundos), por ejemplo a lo largo del borde de la cuenca del Océano Pacífico. El paso importante que dio Hess fue que las corrientes de convección serían la fuerza impulsora en este proceso, llegando a las mismas conclusiones que Holmes había obtenido décadas antes con la única diferencia de que el adelgazamiento de la corteza oceánica se realizó utilizando el mecanismo de Heezen de propagación a lo largo de las dorsales. Por lo tanto, Hess concluyó que el Océano Atlántico se estaba expandiendo mientras que el Océano Pacífico se estaba reduciendo. A medida que la vieja corteza oceánica se consume en las fosas (al igual que Holmes y otros, pensó que esto se hacía mediante el engrosamiento de la litosfera continental, no, como se entiende ahora, por el enterramiento a una escala mayor de la propia corteza oceánica en el manto), nuevo magma se eleva y erupciona a lo largo de las dorsales que se extienden para formar una nueva corteza. En efecto, las cuencas oceánicas se están reciclando perpetuamente, con la creación de una nueva corteza y la destrucción de la antigua litosfera oceánica que ocurren simultáneamente. Por lo tanto, los nuevos conceptos movilistas explicaron claramente por qué la Tierra no se agranda con la expansión del fondo del mar, por qué hay tan poca acumulación de sedimentos en el fondo del océano y por qué las rocas oceánicas son mucho más jóvenes que las rocas continentales.[15]​ Inversiones magnéticas y bandeado magnético A partir de la década de 1950, científicos como Victor Vacquier, utilizando instrumentos magnéticos (magnetómetros) adaptados de dispositivos aéreos desarrollados durante la Segunda Guerra Mundial para detectar submarinos, comenzaron a reconocer extrañas variaciones magnéticas en el fondo del océano. Este hallazgo, aunque inesperado, no fue del todo sorprendente porque se sabía que el basalto, la roca volcánica rica en hierro que forma el fondo del océano, contiene un mineral fuertemente magnético (magnetita) y puede distorsionar localmente las lecturas de la brújula. Esta distorsión fue reconocida por los marineros islandeses ya a finales del siglo XVIII. Más importante aún, debido a que la presencia de magnetita le da al basalto propiedades magnéticas mensurables, estas variaciones magnéticas recién descubiertas proporcionaron otro medio para estudiar el fondo del océano profundo. Cuando la roca recién formada se enfriaba, tales materiales magnéticos registraron el campo magnético terrestre en ese momento. Bandeado magnético del fondo marino. La dorsal es el eje de simetría de un patrón de bandas con polaridad alterna normal (color) e invertida (blanco) A medida que se cartografió cada vez más el fondo marino durante la década de 1950, las variaciones magnéticas resultaron no ser ocurrencias aleatorias o aisladas, sino que revelaron patrones reconocibles. Cuando estos patrones magnéticos se mapearon en una amplia región, el fondo del océano mostró un patrón similar a una cebra: una franja con polaridad normal y la franja adyacente con polaridad invertida. El patrón general, definido por estas bandas alternas de roca polarizada normal e inversamente, se conoció como bandas magnéticas y fue publicado por Ron G. Mason y sus colaboradores en 1961, quienes no encontraron, sin embargo, una explicación para estos datos en términos de expansión del fondo marino, como Vine, Matthews y Morley unos años más tarde.[16]​ El descubrimiento de las bandas magnéticas requería una explicación. A principios de la década de 1960, científicos como Heezen, Hess y Dietz habían comenzado a teorizar que las dorsales oceánicas marcan zonas estructuralmente débiles donde el suelo oceánico se estaba partiendo en dos a lo largo de la cresta de la dorsal. El nuevo magma de las profundidades de la Tierra se eleva fácilmente a través de estas zonas débiles y finalmente erupciona a lo largo de la cresta de las dorsales para crear una nueva corteza oceánica. Este proceso, que en un principio se denominó hipótesis de la cinta transportadora y más tarde expansión del fondo oceánico, opera durante muchos millones de años y continúa formando un nuevo fondo oceánico en todo el sistema de cordilleras oceánicas de 64.000 km de longitud.[17]​ Solo cuatro años después de que se publicaran los mapas con el patrón de cebra de bandas magnéticas, el vínculo entre la expansión del fondo oceánico y estos patrones fue establecido, correcta e independientemente, por Lawrence Morley, Fred Vine y Drummond Matthews, en 1963, conocida actualmente como la hipótesis de Vine-Matthews-Morley.[18] Esta hipótesis vinculó estos patrones con reversiones geomagnéticas y fue apoyada por varias líneas de evidencia: Edades de los basaltos del fondo oceánico. En rojo las rocas más jóvenes y en morado las más altiguas las franjas son simétricas alrededor de las crestas de las dorsales oceánicas; en o cerca de la cresta de la dorsal, las rocas son muy jóvenes y envejecen progresivamente lejos de la cresta de la dorsal; las rocas más jóvenes en la cresta de la dorsal siempre tienen la polaridad actual (normal); franjas de roca paralelas a la cresta de la dorsal alternan en polaridad magnética (normal-invertida-normal, etc.), lo que sugiere que se formaron durante diferentes épocas que documentan los episodios normales y de inversión (ya conocidos de estudios independientes) del campo magnético de la Tierra. En las dorsales no existen apenas sedimentos sino rocas volcánicas solidificadas, mientras que la cubierta sedimentaria va aumentando su grosor a ambos lados de la dorsal.[19]​ Al explicar tanto las bandas magnéticas similares a las de una cebra como la construcción del sistema de cordilleras oceánicas, la hipótesis de expansión del fondo oceánico ganó rápidamente adeptos y representó otro avance importante en el desarrollo de la teoría de la tectónica de placas. Además, la corteza oceánica ahora llegó a ser apreciada como una grabación en cinta natural de la historia de las inversiones del campo geomagnético del de la Tierra. En la actualidad, se dedican extensos estudios a la calibración de los patrones de inversión normal en la corteza oceánica, por un lado, y escalas de tiempo conocidas derivadas de la datación de capas de basalto en secuencias sedimentarias (magnetoestratigrafía), por el otro, para llegar a estimaciones de las tasas de propagación pasadas y reconstrucciones de placas.[16]​ La revolución de la tectónica de placas Después de todas estas consideraciones, la tectónica de placas (o, como se llamó inicialmente nueva tectónica global) fue rápidamente aceptada en el mundo científico, y siguieron numerosos artículos que definieron los conceptos implicados: Ciclo de Wilson. En 1965, Tuzo Wilson, quien había sido un promotor de la hipótesis de la extensión del fondo marino y la deriva continental desde el principio, agregó el concepto de fallas transformantes al modelo, completando las clases de tipos de fallas necesarias para hacer que la movilidad de las placas funcionara a nivel global.[20]​ En 1965 se celebró en la Royal Society de Londres un simposio sobre deriva continental que debe considerarse como el inicio oficial de la aceptación de la tectónica de placas por parte de la comunidad científica, y cuyos resúmenes se publican como Blackett, Bullard & Runcorn (1965). En este simposio, Edward Bullard y sus colaboradores mostraron con un cálculo de computadora cómo los continentes a ambos lados del Atlántico encajarían mejor para cerrar el océano, lo que se conoció como el famoso ajuste de Bullard. En 1966 Wilson publicó el artículo que se refería a reconstrucciones de placas tectónicas previas, introduciendo el concepto de lo que ahora se conoce como el ciclo de Wilson.[21]​ En 1967, en la reunión de la Unión Americana de Geofísica, W. Jason Morgan propuso que la superficie de la Tierra consta de 12 placas rígidas que se mueven entre sí. Jason Morgan propuso también la existencia de plumas del manto para explicar los puntos calientes.[22]​ Dos meses después Xavier Le Pichon publicó un modelo completo basado en seis placas principales con sus movimientos relativos, lo que marcó la aceptación final por parte de la comunidad científica de la tectónica de placas. En el mismo año McKenzie y Parker presentaron de forma independiente un modelo similar al de Morgan usando traslaciones y rotaciones en una esfera para definir los movimientos de las placas. La revolución de la tectónica de placas fue el cambio científico y cultural que se desarrolló a partir de la aceptación de la teoría de la tectónica de placas y supuso un cambio de paradigma y una revolución científica que transformó la geología. Límites de placas Son los bordes de una placa y es ahí donde se presenta la mayor actividad tectónica (sismos, formación de montañas, actividad volcánica), ya que es donde se produce la interacción entre placas. Hay tres clases de límite:[23]​ Divergentes: son límites en los que las placas se separan unas de otras y, por lo tanto, emerge magma desde regiones más profundas (por ejemplo, la dorsal mesoatlántica formada por la separación de las placas de Eurasia y Norteamérica y las de África y Sudamérica). Convergentes: son límites en los que una placa choca contra otra, formando una zona de subducción (la placa oceánica se hunde bajo la placa continental) o un cinturón orogénico (si las placas chocan y se comprimen). Son también conocidos como bordes activos. Transformantes: son límites donde los bordes de las placas se deslizan una con respecto a la otra a lo largo de una falla de transformación. En determinadas circunstancias se forman zonas de límite o borde, donde se unen tres o más placas formando una combinación de los tres tipos de límites. Límite divergente o constructivo: las dorsales Dorsal oceánica. Artículo principal: Borde divergente Son las zonas de la litosfera en que se forma nueva corteza oceánica y en las cuales se separan las placas. En los límites divergentes, las placas se alejan y el vacío que resulta de esta separación es rellenado por material de la corteza, que surge del magma de las capas inferiores. Se cree que el surgimiento de bordes divergentes en las uniones de tres placas está relacionado con la formación de puntos calientes. En estos casos se junta material de la astenosfera cerca de la superficie y la energía cinética es suficiente para hacer pedazos la litosfera. El punto caliente que originó la dorsal mesoatlántica se encuentra actualmente debajo de Islandia, y el material nuevo ensancha la isla algunos centímetros cada siglo. Un ejemplo típico de este tipo de límite son las dorsales oceánicas, como la dorsal mesoatlántica entre otras, y en el continente las grietas, como el Gran Valle del Rift. Límite convergente o destructivo La placa oceánica se hunde por debajo de la placa continental. Artículo principal: Borde convergente Las características de los bordes convergentes dependen del tipo de litosfera de las placas que chocan. Con frecuencia las placas no se deslizan en forma continua; sino que se acumula tensión en ambas placas hasta llegar a un nivel de energía acumulada que sobrepasa el necesario para producir el deslizamiento brusco de la placa marina. La energía potencial acumulada es liberada como presión o movimiento; debido a la titánica cantidad de energía almacenada, estos movimientos ocasionan terremotos, de mayor o menor intensidad. Los puntos de mayor actividad sísmica suelen asociarse con este tipo de límites de placas. Cuando una placa oceánica (más densa) choca contra una continental (menos densa) la placa oceánica es empujada debajo, formando una zona de subducción. En la superficie, la modificación topográfica consiste en una fosa oceánica en el agua y un grupo de montañas en tierra. En los Andes centrales, la interacción de la placa de Nazca con la placa sudamericana ha dado lugar a la formación del Oroclinal de Bolivia, una curvatura que refleja la intensa actividad tectónica en esta región.[24]​ Cuando dos placas continentales colisionan (colisión continental), se forman extensas cordilleras formando un borde de obducción. La cadena del Himalaya es el resultado de la colisión entre la placa Indoaustraliana y la placa Euroasiática. Cuando dos placas oceánicas chocan, el resultado es un arco de islas (por ejemplo, Japón). Límite transformante, conservativo o neutro Falla de San Andrés. Artículo principal: Borde transformante El movimiento de las placas a lo largo de las fallas de transformación puede causar considerables cambios en la superficie, lo que es particularmente significativo cuando esto sucede en las proximidades de un asentamiento humano. Debido a la fricción, las placas no se deslizan en forma continua, sino que se acumula tensión en ambas placas hasta llegar a un nivel de energía acumulada que sobrepasa el necesario para producir el movimiento. La energía potencial acumulada es liberada como presión o movimiento en la falla. Debido a la gran cantidad de energía almacenada, estos movimientos ocasionan terremotos de mayor o menor intensidad. Un ejemplo de este tipo de límite es la falla de San Andrés, ubicada en el oeste de Norteamérica, que es parte del sistema de fallas producto del roce entre la placa Norteamericana y la del Pacífico. Medición de la velocidad de las placas tectónicas La medición actual de la velocidad de las placas tectónicas se realiza mediante medidas precisas de GPS. La velocidad antigua de las placas se obtiene mediante la restitución de cortes geológicos (en corteza continental) o mediante la medida de la posición de las inversiones del campo magnético terrestre registradas en el fondo oceánico."
ksampletext_wikipedia_geol_mineral: str = "Mineral. Un mineral es una sustancia natural, de composición química definida. Normalmente es sólido e inorgánico, y tiene una cierta estructura cristalina. Es diferente de una roca, que puede ser un agregado de minerales o no minerales y que no tiene una composición química específica. La definición exacta de mineral es objeto de debate, especialmente con respecto a la exigencia de ser abiogénico, y en menor medida, a si debe tener una estructura atómica ordenada. El estudio de los minerales se llama mineralogía. Hay más de 5300 especies minerales conocidas, de ellas más de 5090 aprobadas por la Asociación Internacional de Mineralogía (IMA por sus siglas en inglés). Continuamente se descubren y describen nuevos minerales, entre 50 y 80 al año.[2] La diversidad y abundancia de especies minerales es controlada por la química de la Tierra. El silicio y el oxígeno constituyen aproximadamente el 75 % de la corteza terrestre, lo que se traduce directamente en el predominio de los minerales de silicato, que componen más del 90% de la corteza terrestre. Los minerales se distinguen por diversas propiedades químicas y físicas. Diferencias en la composición química y en la estructura cristalina distinguen varias especies, y estas propiedades, a su vez, están influidas por el entorno geológico de la formación del mineral. Cambios en la temperatura, la presión o en la composición del núcleo de una masa de roca causan cambios en sus minerales. Los minerales pueden ser descritos por varias propiedades físicas que se relacionan con su estructura química y composición. Las características más comunes que los identifican son la estructura cristalina y el hábito, la dureza, el lustre, la diafanidad, el color, el rayado, la tenacidad, la exfoliación, la fractura, la partición y la densidad relativa. Otras pruebas más específicas para la caracterización de ciertos minerales son el magnetismo, el sabor o el olor, la radioactividad y la reacción a los ácidos fuertes. Los minerales se clasifican por sus componentes químicos clave siendo los dos sistemas dominantes la clasificación de Dana y la clasificación de Strunz. La clase de silicatos se subdivide en seis subclases según el grado de polimerización en su estructura química. Todos los silicatos tienen una unidad básica en forma de tetraedro de sílice [SiO 4]4− , es decir, un catión de silicio unido a cuatro aniones de oxígeno. Estos tetraedros pueden ser polimerizados para dar las subclases: neosilicatos (no polimerizados, y por lo tanto, solo tetraedros), sorosilicatos (dos tetraedros enlazados entre sí), ciclosilicatos (anillos de tetraedros), inosilicatos (cadenas de tetraedros), filosilicatos (láminas de tetraedros), y tectosilicatos (redes en tres dimensiones de tetraedros). Otros grupos minerales importantes son los elementos nativos, sulfuros, óxidos, haluros, carbonatos, sulfatos y fosfatos. Definición Definición básica La definición general de un mineral comprende los siguientes criterios:[3]​ ser de origen natural; ser estable a temperatura ambiente; estar representado por una fórmula química; ser generalmente abiogénico (no resultado de la actividad de los organismos vivos); y tener disposición atómica ordenada. Las tres primeras características generales son menos debatidas que las dos últimas.[3]​: 2–4  El primer criterio significa que un mineral se tiene que formar por un proceso natural, lo que excluye compuestos antropogénicos. La estabilidad a temperatura ambiente, en el sentido más simple, es sinónimo de que el mineral sea sólido. Más específicamente, un compuesto tiene que ser estable o metaestable a 25 °C. Son ejemplos clásicos de excepciones a esta regla el mercurio nativo, que cristaliza a -39 °C, y el hielo de agua, que es sólido solo por debajo de 0 °C; puesto que estos dos minerales se habían descrito con anterioridad a 1959, fueron adoptados por la Asociación Internacional de Mineralogía (IMA).[4]​[5] Los avances modernos suponen un amplio estudio de los cristales líquidos, que también concierne ampliamente a la mineralogía. Los minerales son compuestos químicos, y, como tales, pueden ser descritos por una fórmula fija o una variable. Muchos grupos de minerales y especies están compuestos por una solución sólida; las sustancias puras generalmente no se encuentran debido a la contaminación o sustitución química. Por ejemplo, el grupo del olivino se describe por la fórmula variable (Mg, Fe) 2SiO 4, que es una solución sólida de dos especies de miembro extremo, la forsterita rica en magnesio y la fayalita rica en hierro, que se describen mediante una fórmula química fija. Otras especies minerales podrían tener composiciones variables, tales como el sulfuro de mackinawita, (Fe, Ni) 8, que es principalmente un sulfuro ferroso, pero que tiene una impureza de níquel muy significativa que se refleja en su fórmula.[3]​: 2–4 [6]​ El requisito de que una especie mineral para ser válida ha de ser abiogénica también se ha descrito como similar a que sea inorgánica; sin embargo, este criterio es impreciso y a los compuestos orgánicos se les ha asignado una rama de clasificación separada. Por último, la exigencia de tener una disposición atómica ordenada es generalmente sinónimo de cristalinidad; sin embargo, los cristales también son periódicos, por lo que se utiliza en su lugar el criterio más amplio.[3]​: 2–4  Una disposición atómica ordenada da lugar a una variedad de propiedades físicas macroscópicas, como la forma cristalina, la dureza y la exfoliación.[7]​: 13–14  Ha habido varias propuestas recientes para modificar la definición para considerar las sustancias biogénicas o amorfas como minerales. La definición formal de un mineral aprobada por la IMA en 1995 es: Un mineral es un elemento o compuesto químico que es normalmente cristalino y que se ha formado como resultado de procesos geológicos. IMA (1995)[8]​ Además, las sustancias biogénicas fueron excluidas explícitamente: Las sustancias biogénicas son compuestos químicos producidos totalmente por procesos biológicos sin un componente geológico (por ejemplo, cálculos urinarios, cristales de oxalato en tejidos vegetales, conchas de moluscos marinos, etc.) y no son considerados como minerales. Sin embargo, si hubo procesos geológicos implicados en la génesis del compuesto, entonces el producto puede ser aceptado como un mineral. IMA (1995)[8]​ Avances recientes Los sistemas de clasificación de minerales y sus definiciones están evolucionando para recoger los últimos avances de la ciencia mineral. Los cambios más recientes han sido la adición de una clase orgánica, tanto en el nuevo Dana y en los esquemas de la clasificación de Strunz.[9]​[10] La clase orgánica incluye un grupo muy raro de minerales con hidrocarburos. La Comisión sobre nuevos minerales y nombres de minerales de la IMA aprobó en 2009 un esquema jerárquico para la denominación y clasificación de los grupos minerales y de los nombres de los grupos y estableció siete comisiones y cuatro grupos de trabajo para revisar y clasificar los minerales en una lista oficial de sus nombres publicados.[11]​[12] De acuerdo con estas nuevas reglas, las especies minerales pueden ser agrupadas de diferentes maneras, sobre la base de la química, la estructura cristalina, la aparición, la asociación, la historia genética o los recursos, por ejemplo, dependiendo de la finalidad para que sirva la clasificación. mineral species can be grouped in a number of different ways, on the basis of chemistry, crystal structure, occurrence, association, genetic history, or resource, for example, depending on the purpose to be served by the classification. IMA[11]​ La exclusión de Nickel (1995) de las sustancias biogénicas no fue universalmente respetada. Por ejemplo, Heinz A. Lowenstam (1981) declaró que «los organismos son capaces de formar una gran variedad de minerales, algunos de los cuales no se pueden formar inorgánicamente en la biosfera.»[13] La distinción es una cuestión de clasificación y tiene menos que ver con los constituyentes de los minerales mismos. Skinner (2005) considera todos los sólidos como minerales potenciales e incluye los biominerales en el reino mineral, que son aquellos creados por las actividades metabólicas de los organismos. Skinner amplió la definición previa de un mineral para clasificar como mineral cualquier «elemento o compuesto, amorfo o cristalino, formado a través de los procesos biogeoquímicos».[14]​ Los recientes avances en la genéticas de alta resolución y espectroscopía de absorción de rayos X están proporcionando revelaciones sobre las relaciones biogeoquímicas entre microorganismos y minerales que pueden hacer obsoleta la exclusión biogénica de Nickel (1995) y una necesidad la inclusión biogénica de Skinner (2005).[8]​[14] Por ejemplo, el IMA encargó al Grupo de trabajo de Mineralogía ambiental y Geoquímica[15] tratar de los minerales en la hidrosfera, atmósfera y biosfera. El alcance del grupo incluye microorganismos formadores de minerales, que existen en casi todas las rocas, en el suelo y en la superficie de las partículas que atraviesan el globo hasta una profundidad de al menos 1600 metros por debajo del fondo del mar y 70 kilómetros en la estratosfera (posiblemente se introduzcan en la mesosfera).[16]​[17]​[18] Los ciclos biogeoquímicos han contribuido a la formación de minerales durante miles de millones de años. Los microorganismos pueden precipitar los metales de la disolución, contribuyendo a la formación de yacimientos de mineral. También pueden catalizar la disolución de los minerales.[19]​[20]​[21]​ Antes de la lista de la Asociación Internacional de Mineralogía, más de 60 biominerales ya habían sido descubiertos, nombrados y publicados.[22] Estos minerales (un subconjunto tabulado en Lowenstam (1981)[13]​) se consideran propiamente minerales de acuerdo con la definición de Skinner (2005).[14] Estos biominerales no figuran en la lista oficial de nombres de minerales de la IMA,[23] aunque muchos de estos biominerales representativos se distribuyen entre las 78 clases minerales que figuran en la clasificación de Dana.[14] Otra clase rara de minerales (principalmente de origen biológico) incluye los cristales líquidos minerales que tienen propiedades tanto de líquidos y cristales. Hasta la fecha se han identificado más de 80.000 compuestos cristalinos líquidos.[24]​[25]​ La definición de mineral de Skinner (2005) toma en cuenta esta cuestión afirmando que un mineral puede ser cristalino o amorfo, incluyendo en este último grupo los cristales líquidos.[14] Aunque los biominerales y los cristales líquidos no son la forma más común de minerales,[26] ayudan a definir los límites de lo que constituye propiamente un mineral. La definición formal de Nickel (1995) menciona explícitamente la cristalinidad como una clave para la definición de una sustancia como un mineral. Un artículo de 2011 define la icosahedrita, una aleación de hierro-cobre-aluminio, como mineral; llamada así por su singular simetría icosaédrica natural, es un cuasi cristal. A diferencia de un verdadero cristal, los cuasicristales están ordenados pero no de forma periódica.[27]​[28]​ Rocas, menas y gemas Un esquisto es una roca metamórfica que se caracteriza por la abundancia de placas minerales. En este ejemplo, la roca tiene prominentes porfiroblastos de silimanita (de hasta 3 cm). Los minerales no son equivalentes a las rocas. Una roca puede ser un agregado de uno o más minerales, o no tener ningún mineral.[7]​: 15–16  Rocas como la caliza o la cuarcita se componen principalmente de un mineral —calcita o aragonito en el caso de la caliza, y cuarzo, en la última—.[7]​: 719–721, 747–748  Otras rocas pueden ser definidas por la abundancia relativa de los minerales clave (esenciales); un granito está definido por las proporciones de cuarzo, feldespato alcalino y plagioclasa.[7]​: 694–696  Los otros minerales de la roca se denominan accesorios, y no afectan en gran medida la composición global de la roca. Las rocas también pueden estar compuestas enteramente de material no mineral; el carbón es una roca sedimentaria compuesta principalmente de carbono derivado de manera orgánica.[7]​: 15–16, 728–730  En las rocas, algunas especies y grupos minerales son mucho más abundantes que otros; estos se denominan minerales formativos. Los principales ejemplos son el cuarzo, feldespatos, las micas, los anfíboles, los piroxenos, los olivinos, y la calcita; excepto la última, todos son minerales silicatos.[3]​: 15  En general, alrededor de unos 150 minerales se consideran particularmente importantes, ya sea en términos de su abundancia o valor estético en términos de coleccionismo.[7]​: 14  Los minerales y rocas comercialmente valiosos se conocen como minerales industriales y rocas industriales. Por ejemplo, la moscovita, una mica blanca, puede ser utilizada para ventanas (a veces conocida como isinglass), como material de relleno o como aislante.[7]​: 531–532  Las menas son minerales que tienen una alta concentración de un determinado elemento, normalmente un metal. Ejemplos de ello son el cinabrio (HgS), un mineral de mercurio, esfalerita (ZnS), un mineral de zinc, o la casiterita (SnO 2), un mineral de estaño. Las gemas son minerales con un alto valor ornamental, y se distinguen de las no gemas por su belleza, durabilidad, y por lo general, rareza. Hay alrededor de 20 especies minerales que se califican como minerales gema, que constituyen alrededor de las 35 piedras preciosas más comunes. Los minerales gema están a menudo presentes en diversas variedades, y así un mineral puede dar cuenta de varias piedras preciosas diferentes; por ejemplo, rubí y el zafiro son ambas corindón, Al 3.[7]​: 14–15  Nomenclatura y clasificación Clasificación histórica de los minerales Los minerales se solían clasificar en la antigüedad con criterios de su aspecto físico; Teofrasto, en el s. III a. C., creó la primera lista sistemática cualitativa conocida; Plinio el Viejo (s. I d. C.), en su Historia Natural, realizó una sistemática mineral, trabajo que, en la Edad Media, sirvió de base a Avicena; Linneo (1707-1778) intentó idear una nomenclatura fundándose en los conceptos de género y especie, pero no tuvo éxito y dejó de usarse en el siglo XIX; con el posterior desarrollo de la química, el químico sueco Axel Fredrik Cronstedt (1722-1765) elaboró la primera clasificación de minerales en función de su composición; el geólogo estadounidense James Dwight Dana, en 1837, propuso una clasificación considerando la estructura y composición química. La clasificación más actual se funda en la composición química y la estructura cristalina de los minerales. Las clasificaciones más empleadas son las de Strunz y Kostov. Clasificación moderna Los minerales se clasifican según la variedad, especie, serie y grupo, en orden creciente de generalidad. El nivel básico de definición es el de las especies minerales, que se distinguen de otras especies por sus propiedades químicas y físicas específicas y únicas. Por ejemplo, el cuarzo se define por su fórmula química, SiO 2, y por una estructura cristalina específica que lo distingue de otros minerales con la misma fórmula química (denominados polimorfos). Cuando existe un rango de composición entre dos especies minerales, se define una serie mineral. Por ejemplo, la serie de la biotita está representada por cantidades variables de la endmembers flogopita, siderofilita, annita, y eastonita. Por contraste, un grupo mineral es una agrupación de especies minerales con algunas propiedades químicas comunes que comparten una estructura cristalina. El grupo piroxeno tiene una fórmula común de XY(Si, Al) 6, en donde X e Y son ambos cationes, siendo X generalmente mayor que Y (radio iónico); los piroxenos son silicatos de cadena sencilla que cristalizan en cualquiera de los sistemas cristalinos monoclínico o ortorrómbico. Finalmente, una variedad mineral es un tipo específico de especies minerales que difieren por alguna característica física, como el color o el hábito del cristal. Un ejemplo es la amatista, que es una variedad púrpura del cuarzo.[3]​: 20–22  Para ordenar minerales dos son las clasificaciones más comunes, la de Dana y la de Strunz, ambas basadas en la composición, en especial respecto a los grupos químicos importantes, y en la estructura. James Dwight Dana, un geólogo principal de su tiempo, publicó por primera vez su System of Mineralogy [Sistema de Mineralogía] en 1837; en 1997 se editó su octava edición. La clasificación de Dana asigna un número de cuatro partes a una especie mineral. Su número de clase se basa en los grupos de composición importantes; el número de tipo da la relación de cationes/aniones en el mineral; y los dos últimos números corresponden al grupo de minerales por similitud estructural dentro de un tipo o clase determinada. La clasificación de Strunz —utilizada con menor frecuencia y llamada así por el mineralogista alemán Karl Hugo Strunz— se basa en el sistema de Dana, pero combina tanto criterios químicos como estructurales, estos últimos con respecto a la distribución de los enlaces químicos.[3]​: 558–559  En enero de 2016, la IMA había aprobado 5.090 especies minerales.[29] Se han nombrado en general en honor de una persona (45 %) —ver: Anexo:Minerales nombrados según personas—, seguidos por la ubicación del lugar, mina o yacimiento del descubrimiento (23 %); otras etimologías comunes son los nombres basados en la composición química (14 %) y en las propiedades físicas (8 %).[3]​: 20–22, 556  El sufijo común -ita usado en los nombres de las especies minerales desciende del antiguo sufijo griego - ί τ η ς (-ites), que significa 'relacionado con' o 'que pertenece a'.[30]​ Química mineral Hübnerita, el miembro final rico en manganeso de la serie de la wolframita, con cuarzo menor en el fondo La abundancia y diversidad de minerales es controlada directamente por su composición química, que a su vez, depende de la abundancia de los elementos en la Tierra. La mayoría de los minerales observados derivan de la corteza terrestre. Ocho elementos representan la mayor parte de los componentes clave de los minerales, debido a su abundancia en la corteza terrestre. Estos ocho elementos suponen más del 98 % de la corteza en peso, y son, en orden decreciente: oxígeno, silicio, aluminio, hierro, magnesio, calcio, sodio y potasio. El oxígeno y el silicio son, con mucho, los dos más importantes —el oxígeno compone, en peso, el 46,6 % de la corteza terrestre, y el silicio un 27,7 %.[3]​: 4–7  Los minerales que se forman son controlados directamente por la química mayor del cuerpo matriz. Por ejemplo, un magma rico en hierro y magnesio formará minerales máficos, como el olivino y los piroxenos; por el contrario, un magma más rico en sílice cristalizará para formar minerales que incorporen más SiO 2, como los feldespatos y cuarzos. La caliza, la calcita o la aragonita (todas CaCO 3) se forman porque la roca es rica en calcio y carbonato. Un corolario es que no se encontrará un mineral en una roca cuya química mayor no se parezca a la química mayor del mineral dado, con la excepción de algunas trazas de minerales. Por ejemplo, la cianita, Al 2SiO 5, se forma a partir del metamorfismo de lutitas ricas en aluminio; no sería probable que ocurriera en rocas pobres en aluminio, como la cuarcita. La composición química puede variar entre las especies terminales de una serie de solución sólida. Por ejemplo, los feldespatos plagioclasa comprenden una serie continua que va desde el miembro extremo de la albita, rica en sodio (NaAlSi 8), hasta la anortita, rica en calcio (CaAl 2Si 8), con cuatro variedades intermedias reconocidas entre ellas (recogidas en orden de riqueza del sodio al calcio): oligoclasa, andesina, labradorita y bytownita.[3]​: 586  Otros ejemplos de serie son la serie del olivino, desde la forsterita, rica en magnesio, a la fayalita, rica en hierro, y la serie del wolframita, desde la hübnerita, rica en manganeso, hasta la ferberita, rica en hierro. La sustitución química y la coordinación de poliedros explican esta característica común de los minerales. En la naturaleza, los minerales no son sustancias puras, y se contaminan por otros elementos que están presentes en el sistema químico dado. Como resultado, es posible que un elemento sea sustituido por otro.[3]​: 141  La sustitución química se producirá entre iones de un tamaño y carga similares; por ejemplo, K+  no sustituirá a Si4+  debido a las incompatibilidades químicas y estructurales causadas por la gran diferencia en tamaño y carga. Un ejemplo común de sustitución química es el del Si4+ > por Al3+ , que están próximos en carga, tamaño y abundancia en la corteza terrestre. En el ejemplo de la plagioclasa, hay tres casos de sustitución. Los feldespatos son todos armazones de sílice, que tienen una relación de silicio-oxígeno de 2:1, y el espacio para otros elementos se da por la sustitución del ion Si4+  por el ion Al3+  para dar una unidad de base de [AlSi 8]− ; sin la sustitución, la fórmula puede ser cargada-equilibrada como SiO 2, dando cuarzo.[3]​: 14  La importancia de esta propiedad estructural se explica además por los poliedros de coordinación. La segunda sustitución se produce entre el ion Na+  y el ion Ca2+ ; sin embargo, la diferencia en la carga tiene que contabilizarse haciendo una segunda sustitución del ion Si4+  por el ion Al3+ .[3]​: 585  La coordinación de poliedros es una representación geométrica de cómo un catión está rodeado por un anión. En mineralogía, debido a su abundancia en la corteza terrestre, los poliedros de coordinación se consideran generalmente en términos del oxígeno. La unidad base de los minerales de silicato es el tetraedro de sílice —un ion [SiO 4]4−  rodeado de cuatro O2− —. Una forma alternativa de describir la coordinación del silicato es mediante un número: en el caso del tetraedro de sílice, se dice que tiene un número de coordinación de 4. Diversos cationes tienen un rango específico de posibles números de coordinación; para el silicio, es casi siempre 4, excepto para minerales de muy altas presiones en los que los compuestos se comprimen de tal manera que el silicio está seis veces (octaédrico) coordinado con el oxígeno. Los cationes mayores tienen un número de coordinación más grande debido al aumento en el tamaño relativo en comparación con el oxígeno (la última subcapa orbital de los átomos más pesados es diferente también). Los cambios en los números de coordinación conduce a diferencias físicas y mineralógicas; por ejemplo, a alta presión, tal como en el manto, muchos minerales, especialmente algunos silicatos como el olivino y los granates cambiarán a una estructura de perovskita, en el que el silicio está en coordinación octaédrica. Otro ejemplo son los aluminosilicatos cianita, andalucita y silimanita (polimorfos, ya que comparten la fórmula Al 2SiO 5), que se diferencian por el número de coordinación del Al3+ ; estos minerales transitan de uno al otro como una respuesta a los cambios en la presión y en la temperatura.[3]​: 4–7  En el caso de materiales de silicato, la sustitución del ion Si4+  por Al3+  permite una variedad de minerales, debido a la necesidad de equilibrar las cargas.[3]​: 12–17  Cuando los minerales reaccionan, los productos a veces asumirán la forma del reactivo; el producto mineral se denomina por ser un pseudomorfo de (o después) del reactivo. Aquí se ilustra un pseudomorfo de la caolinita después de la ortoclasa. Aquí, el pseudomorfo conserva la macla Carlsbad común en la ortoclasa. Los cambios de temperatura, de presión y de composición alteran la mineralogía de una roca simple: los cambios en la composición pueden ser causados por procesos como la erosión o metasomatismo (alteración hidrotérmica); los cambios en la temperatura y en la presión se producen cuando la roca madre se somete a movimientos tectónicos o magmáticos en diferentes regímenes físicos; y los cambios en las condiciones termodinámicas favorecen que algunas asociaciones de minerales reaccionen entre sí para producir nuevos minerales. Como tal, es posible que dos rocas tengan una química de roca base idéntica, o muy similar, sin tener una mineralogía similar. Este proceso de alteración mineralógica está relacionado con el ciclo de las rocas. Un ejemplo de una serie de reacciones minerales se ilustra como sigue.[3]​: 549  El feldespato ortoclasa (KAlSi 8) es un mineral que se encuentra comúnmente en el granito, una roca ígnea plutónica. Cuando se expone a la intemperie, reacciona para formar caolinita (Al 2Si 5(OH) 4, un mineral sedimentario, y ácido silícico): 2KAlSi 8 + 5H 2O + 2H+  → Al 2Si 5(OH) 4 + 4H 2SiO 3 + 2K+ Bajo condiciones metamórficas de bajo grado, la caolinita reacciona con el cuarzo para formar pirofilita (Al 2Si 10(OH) 2): 2Si 5(OH) 4 + SiO 2 → Al 2Si 10(OH) A medida que aumenta el grado metamórfico, la pirofilita reacciona para formar cianita y cuarzo: 2Si 10(OH) 2 → Al 2SiO 5 + 3SiO 2 + H Alternativamente, un mineral puede cambiar su estructura cristalina como consecuencia de cambios de temperatura y de presión sin reaccionar. Por ejemplo, el cuarzo se convertirá en una variedad de sus polimorfos de SiO 2, como la tridimita y la cristobalita a altas temperaturas, y en coesita a altas presiones.[3]​: 579  Propiedades físicas de los minerales La caracterización de los minerales puede variar de ser muy simple a muy difícil. Un mineral puede ser identificado por varias propiedades físicas, siendo algunas de ellas suficientes para una plena identificación sin ambigüedades. En otros casos, los minerales solo se pueden clasificar mediante análisis más complejos, ópticos, químicos o de difracción de rayos X; estos métodos, sin embargo, pueden ser costosos y consumen mucho tiempo. Las propiedades físicas que se estudian para la clasificación son la estructura cristalina y el hábito, la dureza y el lustre, la diafanidad, el color, el rayado, la exfoliación y la fractura, y la densidad relativa. Otras pruebas menos generales son la fluorescencia y fosforescencia, el magnetismo, la radioactividad, la tenacidad (respuesta a los cambios mecánicos inducidos de forma), la piezoelectricidad y la reactividad para diluir ácidos.[3]​: 22–23  Estructura cristalina y hábito Acicular natrolita Artículos principales: Sistema cristalino, Hábito cristalino y Macla. El topacio tiene una forma característica de cristal alargado ortorrómbico. La estructura cristalina resulta de la disposición espacial geométrica ordenada de los átomos en la estructura interna de un mineral. Esta estructura cristalina se basa en una disposición atómica o iónica interna regular, que se expresa a menudo en la forma geométrica que el cristal toma. Incluso cuando los granos minerales son demasiado pequeños para ser vistos o son de forma irregular, la estructura cristalina subyacente siempre es periódica y se puede determinar por difracción de rayos X.[3]​: 2–4  Los minerales por lo general son descritos por su contenido de simetría. Los cristales están cristalográficamente restringidos a 32 grupos de puntos, que se diferencian por su simetría. Estos grupos se clasifican a su vez en categorías más amplias, siendo las de mayor alcance seis familias de cristales.[3]​: 69–80  (a veces una de las familias, la hexagonal, también se divide en dos sistemas cristalinos: el trigonal, que tiene un eje tres veces simétrico, y el hexagonal, que tiene un eje seis veces simétrico). Estas familias pueden ser descritas por las longitudes relativas de los tres ejes cristalográficos, y los ángulos que forman entre ellos; estas relaciones corresponden a las operaciones de simetría que definen los grupos de puntos más estrechos. Se resumen a continuación; a, b, y c representan los ejes, y α, β, y γ representan el ángulo opuesto al eje cristalográfico respectivo (por ejemplo, α es el ángulo opuesto al eje a, es decir el ángulo entre los ejes b y c.):[3]​: 69–80  Sistema cristalino Ejes Ángulos entre ejes Ejemplo comunes La química y la estructura cristalina, en conjunto, definen un mineral. Con una restricción a grupos de 32 puntos, los minerales de diferente química pueden tener una estructura cristalina idéntica. Por ejemplo, la halita (NaCl), la galena (PbS) y la periclasa (MgO) pertenecen todas al grupo de puntos hexaoctahedral (familia isométrica), ya que tienen una estequiometría similar entre sus diferentes elementos constitutivos. En contraste, los polimorfos son agrupaciones de minerales que comparten una fórmula química, pero que tienen una estructura diferente. Por ejemplo, la pirita y la marcasita, ambos sulfuros de hierro, tienen la fórmula FeS 2; sin embargo, el primero es isométrico mientras que el último es ortorrómbico. Este polimorfismo se extiende a otros sulfuros de fórmula genérica AX 2; estos dos grupos son conocidos colectivamente como los grupos de la pirita y marcasita.[3]​: 654–655  El polimorfismo se puede extender más allá del contenido de la pura simetría. Los aluminosilicatos son un grupo de tres minerales —cianita, andalucita y silimanita— que comparten la fórmula química Al 2SiO 5. La cianita es triclínica, mientras que la andalucita y la silimanita son ambas ortorrómbicas y pertenecen al grupo de puntos bipiramidal. Estas diferencias surgen en correspondencia a cómo el aluminio se coordina dentro de la estructura cristalina. En todos los minerales, un ion de aluminio está siempre seis veces coordinado con el oxígeno; el silicio, por regla general está en coordinación de cuatro veces en todos los minerales; una excepción es un caso como la stishovita (SiO 2, un polimorfo de cuarzo de ultra-alta presión con estructura de rutilo).[3]​: 581  En la cianita, el segundo aluminio está en coordinación seis veces; su fórmula química se puede expresar como Al [6]Al [6]SiO 5, para reflejar su estructura cristalina. La andalucita tiene el segundo aluminio en coordinación cinco veces (Al [6]Al [5]SiO 5) y la silimanita lo tiene en coordinación de cuatro veces ((Al [6]Al [4]SiO 5).[3]​: 631–632  Las diferencias en la estructura cristalina y la química influyen mucho en otras propiedades físicas del mineral. Los alótropos del carbono, el diamante y el grafito, tienen propiedades muy distintas; el diamante es la sustancia natural más dura, tiene un lustre adamantino, y pertenece a la familia isométrica, mientras que el grafito es muy blando, tiene un lustre grasiento, y cristaliza en la familia hexagonal. Esta diferencia se explica por diferencias en el enlace. En el diamante, los átomos de carbono están en orbitales híbridos sp3, lo que significa que forman un marco o armazón en el que cada carbono está unido covalentemente a cuatro vecinos de una manera tetraédrica. Por otro lado, el grafito forma láminas de átomos de carbono en orbitales híbridos sp2, en los que cada átomo de carbono está unido covalentemente a sólo otros tres. Estas hojas se mantienen unidas por fuerzas mucho más débiles que las fuerzas de van der Waals, y esta discrepancia se traduce en grandes diferencias macroscópicas.[3]​: 166  Maclas de contacto en la espinela La macla es la interpenetración entre dos o más cristales de una única especie mineral. La geometría de la macla está controlada por la simetría del mineral y, como resultado, hay varios tipos: de contacto, reticuladas, geniculadas, de penetración, cíclicas y polisintéticas. Las maclas de contacto, o maclas simples, constan de dos cristales unidos en un plano; este tipo de maclas es común en la espinela; las maclas reticuladas, comunes en forma de rutilo, son cristales entrelazados que se asemejan a un reticulado. Las maclas geniculadas tienen una mezcla en el medio que es causada por el comienzo del maclado. Las maclas de penetración constan de dos cristales individuales que han crecido uno dentro de otro; ejemplos de este hermanamiento son las maclas en forma de cruz de la estaurolita y las maclas de Carlsbad en la ortoclasa. Las maclas cíclicas son causadas por el maclado repetido en torno a un eje de rotación. Se produce alrededor de tres, cuatro, cinco, seis, o ocho ejes de plegado. Las maclas polisintéticas son similares a las maclas cíclicas por la presencia de maclados repetitivos aunque, en lugar de producirse alrededor de un eje de rotación, lo hacen siguiendo planos paralelos, por lo general en una escala microscópica.[3]​: 41–43 [7]​: 39  El hábito cristalino se refiere a la forma general de cristal. Se utilizan varios vocablos para describir esta propiedad: acicular, que describe cristales en forma de aguja como en la natrolita; acuchillado; arborescente o dendrítica (patrón de árbol, común en el cobre nativo); equante, que es típico del granate; prismático (alargado en una dirección); y tabular, que se diferencia de acuchillado en que el primero es plano mientras que este último tiene un alargamiento definido. En relación con la forma cristalina, la calidad de las caras del cristal es diagnóstico de algunos minerales, especialmente con un microscopio petrográfico. Los cristales euhedrales tienen una forma externa definida, mientras que los cristales anhedrales no la tienen; las formas intermedias se denominan subhedrales.[3]​: 32–39 [7]​: 38  Dureza Artículo principal: Escalas de dureza El diamante es el material natural más duro (dureza de Mohs de 10). La dureza de un mineral define cuánto puede resistir el rayado. Esta propiedad física depende de la composición química y de la estructura cristalina, y por ello no es necesariamente constante en todas las caras; la debilidad cristalográfica hace que algunas direcciones sean más blandas que otras.[3]​: 28–29  Un ejemplo de esta propiedad se muestra en la cianita, que tiene una dureza de Mohs de 5½ en la dirección paralela a [001], pero de 7 paralela a [100].[31]​ La escala más común de medición es la escala de dureza de Mohs ordinaria. Definida por diez indicadores, un mineral con un índice más alto raya los minerales que están por debajo de él en la escala. La escala va desde el talco, un silicato estratificado, hasta el diamante, un polimorfo de carbono que es el material natural más duro.[3]​: 28–29  Escala de Mohs de dureza Lustre y diafanidad Artículo principal: Lustre La pirita tiene un brillo metálico La esfalerita tiene un brillo submetálico El lustre o brillo indica cómo se refleja la luz que incide sobre la superficie del mineral, una propiedad que no depende del color y sí de su naturaleza química: es más intenso en sustancias que tienen enlaces metálicos y menor en las de enlaces iónicos o covalentes. El tipo y la intensidad del brillo dependen del índice de refracción y de la relación entre la luz absorbida y la reflejada. Hay numerosos vocablos cualitativos para su descripción, que se agrupan en tres: brillo metálico, cuando reflejan casi toda la luz visible que reciben. Son opacos y con índices de refracción mayores de 3. Suelen ser metales nativos (cuando no están oxidados) y muchos sulfuros (pirita) y óxidos de metales de transición (hematites). brillo submetálico, cuando reflejan una pequeña parte de la luz visible que reciben. Son opacos y su índice de refracción es ligeramente inferior a 3. Suelen ser elementos semimetálicos (grafito), sulfuros y óxidos. brillo no metálico, cuando transmiten la luz en cierto grado. Esta condición es ambigua y se emplean varios vocablos para estimar los matices: vítreo, con índice de refracción 1.33-2.00. Son minerales transparentes, en general compuestos por aniones oxigenados (oxoaniones), como carbonatos, sulfatos, fosfatos, silicatos, nitratos, etc. También varios halogenuros y óxidos (cuarzo hialino o cristal de roca); adamantino, con índice de refracción 2.00-2.50. Es el brillo típico del diamante y de algunas otras variedades aunque a veces para estas se usa el término «subadamantino»; nacarado o perlado, un brillo irisado típico de minerales fácilmente exfoliables, como las micas, el yeso y la apofilita; craso o graso, motivado por la presencia de pequeñas rugosidades en la superficie, a veces microscópicas. Lo tienen algunas blendas, la nefelina y el cuarzo en masa o lechoso; resinoso o céreo, de minerales como el azufre y ciertas blendas y granates; sedoso, característico de minerales fibrosos, como el yeso fibroso, la crisotila y la ulexita; mate, cuando no presentan ningún reflejo, como la creta (calcita) o las arcillas. En este caso también se dice que el mineral no tiene brillo.[3]​: 26–28  Diferentes brillos de minerales Vítreo: cuarzo Vítreo: cuarzo Adamantino: diamantes tallados Adamantino: diamantes tallados Nacarado: moscovita Nacarado: moscovita Craso: ópalo musgoso Craso: ópalo musgoso Resinoso: ámbar Resinoso: ámbar Sedoso: selenita, variedad del yeso Sedoso: selenita, variedad del yeso Ceroso: jade Ceroso: jade Mate: caolinita Mate: caolinita La diafanidad de un mineral describe la capacidad de la luz de pasar a través de él. Los minerales transparentes no disminuyen la intensidad de la luz que pasa a través de ellos. Un ejemplo de estos minerales es la moscovita (mica de potasio); algunas variedades son lo suficientemente claras como para haber sido utilizadas como vidrios en las ventanas. Los minerales translúcidos permiten pasar algo de luz, pero menos que los que son transparentes. La jadeíta y nefrita (formas minerales del jade) son ejemplos de minerales con esta propiedad. Los minerales que no dejan pasar la luz se denominan opacos.[32]​[3]​: 25  La diafanidad de un mineral depende del espesor de la muestra. Cuando un mineral es suficientemente delgado (por ejemplo, en una lámina delgada para petrografía) puede llegar a ser transparente, incluso si esa propiedad no se ve en la muestra de mano. Por el contrario, algunos minerales, como la hematita o la pirita son opacos incluso en láminas delgadas.[3]​: 25  Color y raya Artículo principal: Método de la raya El color, en general, no es una característica que permita caracterizar minerales. Se muestra una uvarovita verde (izquierda) y una grosularia rojo-rosada (derecha), ambos granates. Las propiedades que servirían para el diagnóstico serían los cristales rombododecaédricos, el lustre resinoso, y la dureza, de alrededor de 7. Elbaita dicróica Esmeralda El color es la propiedad más obvia de un mineral, pero a menudo no sirve para caracterizarlo.[3]​: 23  Es causada por la radiación electromagnética que interactúa con los electrones (excepto en el caso de incandescencia, que no se aplica a los minerales).[3]​: 131–144  Por su contribución en el color, se definen tres grandes clases de minerales: minerales idiocromáticos (o 'autocoloreados'), que deben su color a los constituyentes principales y que son diagnosticables.[32]​[3]​: 24  Son minerales siempre del mismo color, como la malaquita (verde), la azurita (azul) y muchos minerales metálicos. Sus colores suelen variar ligeramente debido a la presencia de pequeñas cantidades de otros metales: el oro, por ejemplo, es menos amarillo cuando se mezcla con un poco de plata, y más rosado cuando es mezclado junto con cobre. minerales alocromáticos (o 'coloreados por otros'), que deben su coloración a pequeñas cantidades en la composición consideradas como impurezas, a las que se llama cromóforos, usualmente metales (hierro, cromo, cobre, vanadio o manganeso). Son capaces de adoptar más de una coloración, como el berilo o las dos variedades del corindón, el rubí y el zafiro.[3]​: 24  Algunos minerales alocromáticos que pueden tener prácticamente cada color imaginable, e incluso pueden tener muchos colores en un solo cristal. minerales pseudocromáticos (o 'de color falso'), cuya coloración proviene de la estructura física del cristal y la interferencia con las ondas de luz. Son ejemplos la labradorita, la bornita y el ópalo, que está formado por capas microscópicas de esferas de sílice. Al pasar a su través la luz se separa en los colores que la componen, más o menos como ocurre cuando se refleja en una capa de aceite sobre el agua. Algunos metales, como el hierro, pueden ser tanto alocromático como idiocromático: en el primer caso es considerado como una impureza, mientras que en el segundo forma parte intrínseca del mineral coloreado. El color de algunos minerales puede cambiar, ya sea de manera natural o con un poco de ayuda. Los bajos niveles de radiación, que se dan a menudo en la naturaleza, pueden contribuir a oscurecer algunos minerales incoloros. Los mismos berilos de color amarillo verdoso se tratan artificialmente ahora con calor para darles una coloración más azulada. Además del simple color del cuerpo, los minerales pueden tener otras propiedades ópticas distintivas que pueden implican variabilidad del color: juego de colores, como en el ópalo, significa que la muestra refleja diferentes colores cuando se ilumina, a causa de que la luz se refleja desde las ordenadas esferas de sílice microscópicas de su estructura física.[33]​ pleocroísmo, facultad de absorber las radiaciones luminosas de distinta manera en función de la dirección de vibración: un mismo cristal puede aparecer con coloraciones diferentes dependiendo de la orientación en que haya caído en la preparación microscópica iridiscencia, una variedad del juego de colores por la que la luz se dispersa en un recubrimiento sobre la superficie del cristal, planos de exfoliación o capas desactivas que tienen gradaciones químicas menores.[3]​: 24–26  chatoyancia («ojo de gato») es el efecto de bandas onduladas de color que se observan cuando se rota la muestra; asterismo, una variedad de la chatonyancia, un fenómeno sobre un área que hace aparecer una estrella sobre la superficie reflectante de un corte de cabujón. Se da en algunos rubíes, zafiros y otras gemas (granate-estrella, diópsido-estrella, espinela-estrella, etc.) y particularmente en el corundum de calidad gema.[3]​: 24–26 [33]​ empañamiento Propiedades ópticas Asterismo en un zafiro-estrella azul Asterismo en un zafiro-estrella azul Ojo de tigre Ojo de tigre Iridiscencia en la labradorita Iridiscencia en la labradorita Barras de tungsteno con cristales evaporados, parcialmente oxidados con un colorido empañado Barras de tungsteno con cristales evaporados, parcialmente oxidados con un colorido empañado pleocroísmo en la cordierita, fuertemente dicroica pleocroísmo en la cordierita, fuertemente dicroica Placas de raya con pirita (izqda.) y rodocrosita (dcha.) La raya de un mineral se refiere al color de un mineral en forma de polvo, que puede o no ser idéntico al color de su cuerpo.[3]​: 24  La forma más común de evaluar esta propiedad se hace con una placa de raya, que está hecha de porcelana y es de color blanco o negro. La raya de un mineral es independiente de los elementos traza[32] o de cualquier alteración de la superficie a causa de la intemperie.[3]​: 24  Un ejemplo común de esta propiedad se ilustra con la hematita, que es de color negro, plata o rojo en la muestra, pero que tiene una raya de color rojo cereza.[32] a marrón rojizo.[3]​: 24  La raya es más a menudo distintiva de los minerales metálicos, en contraste con los minerales no metálicos, cuyo color de cuerpo está creada por elementos alocromáticos.[32] La prueba de la raya se ve limitada por la dureza del mineral, ya que los minerales de dureza superior a siete rayan ellos la placa.[3]​: 24  Exfoliación, partición, fractura y tenacidad Artículos principales: Exfoliación, Fractura y Tenacidad. Perfecta exfoliación basal en la biotita (negra), y buena exfoliación en la matrix (ortoclasa rosa) Por definición, los minerales tienen una disposición atómica característica y cualquier debilidad de esa estructura cristalina es la causa de la existencia de los planos de debilidad. La rotura del mineral a lo largo de esos planos se denomina exfoliación. La calidad de la exfoliación puede ser descrita en función de cómo de limpia y fácilmente se rompa el mineral; los vocablos con los que se describen comúnmente esa calidad, en orden decreciente, son «perfecto», «bueno», «distinto» y «pobre». En particular en los minerales transparentes, o en una sección delgada, la exfoliación se puede ver como una serie de líneas paralelas que señalan las superficies planas cuando se ven de lado. La exfoliación no es una propiedad universal de los minerales; por ejemplo, el cuarzo, compuesto por tetraedros de sílice muy interconectados, no tiene ninguna debilidad cristalográfica que le permitiría exfoliarse. Por el contrario, las micas, que tienen una exfoliación basal perfecta, consisten en láminas de tetraedros de sílice que se mantienen juntas muy débilmente.[3]​: 39–40 [7]​: 29–30  Como la exfoliación es función de la cristalografía, hay gran variedad de tipos de exfoliación produciéndose en uno, dos, tres, cuatro o seis direcciones. La exfoliación basal en una única dirección es una característica distintiva de las micas. La exfoliación en dos direcciones, denominada prismática, se produce en anfíboles y piroxenos. Los minerales como la galena o la halita tienen exfoliación cúbica (o isométrica) en tres direcciones, a 90°; cuando hay tres direcciones de exfoliación, pero no a 90°, como en la calcita o en la rodocrosita, se denomina exfoliación romboédrica. La exfoliación octaédrica (cuatro direcciones) está presente en la fluorita y en el diamante, y la esfalerita tiene seis direcciones de exfoliación del dodecaedro.[3]​: 39–40 [7]​: 30–31  Los minerales con muchas exfoliaciones pueden no romper igual de bien en todas las direcciones; por ejemplo, la calcita tiene buena exfoliación en tres direcciones, pero el yeso solo tiene una exfoliación perfecta en una dirección, y pobre en las otras dos. Los ángulos entre los planos de exfoliación varían entre los minerales. Por ejemplo, dado que los anfíboles son silicatos de cadena doble y los piroxenos son silicatos de cadena única, el ángulo entre sus planos de exfoliación es diferente: los piroxenos exfolian en dos direcciones a aproximadamente 90°, mientras que los anfíboles lo hacen claramente en dos direcciones separadas aproximadamente a 120° y 60°. Los ángulos de exfoliación se pueden medir con un goniómetro de contacto, que es similar a un transportador.[3]​: 39–40 [7]​: 30–31  La partición, a veces llamada «exfoliación falsa», es similar en apariencia a la exfoliación pero se produce por defectos estructurales en el mineral en lugar de por una debilidad sistemática. La partición varía de cristal a cristal de un mismo mineral, mientras que todos los cristales de un mineral determinado exfoliaran si la estructura atómica permite tal propiedad. En general, la partición es causada por una cierta tensión aplicada a un cristal. Las fuentes de las tensiones incluyen la deformación (por ejemplo, un aumento de la presión), exsolución o maclado. Los minerales que a menudo muestran partición son los piroxenos, la hematita, la magnetita y el corindón.[3]​: 39–40 [7]​: 30–31  Cuando un mineral se rompe en una dirección que no corresponde a un plano de exfoliación, se habla de fractura. Hay varios tipos: concoidea, cuando se forman superficies redondeadas cóncavas o convexas, de relieve suave. Se produce solo en minerales muy homogéneo, siendo el ejemplo clásico la fractura del cuarzo; lisa, cuando aparecen superficies planas, suaves y sin asperezas; desigual o irregular, cuando surgen superficies rugosas e irregulares. Se da en el cobre nativo[3]​: 31–33 ; fibrosa o astillosa, cuando se rompe como una madera, formando astillas; ganchuda, cuando la superficie de rotura aparece dentada; terrosa, cuando se desmorona como un terrón. La tenacidad está relacionada tanto con la exfoliación y la fractura. Mientras que la fractura y la exfoliación describen las superficies que se crean cuando el mineral se rompe, la tenacidad describe la resistencia que ofrece el mineral a tal ruptura. Los minerales pueden ser:[3]​: 30–31  frágiles, cuando rompen con facilidad con poco esfuerzo; maleables, cuando se laminan mediante golpes; sectiles, cuando se secciona con una cuchilla formando virutas; dúctiles, cuando se puede estirar convirtiéndose en un hilo; flexibles, cuando al ser doblados no recuperan la forma al cesar el esfuerzo; elásticos, cuando al ser doblados recuperan la forma al cesar el esfuerzo. Densidad relativa La galena (PbS) es un mineral de alta densidad relativa. La densidad relativa (a veces llamada gravedad específica) describe numéricamente la densidad de un mineral. Las dimensiones de la densidad son unidades de masa divididas por unidades de volumen: kg/m³ o en g/cm³. La densidad relativa mide la cantidad de agua desplazada por una muestra mineral. Se define como el cociente de la masa de la muestra y la diferencia entre el peso de la muestra en el aire y su correspondiente peso en agua; la densidad relativa es una relación adimensional, sin unidades. Para la mayoría de los minerales, esta propiedad no sirve para caracterizarlos. Los minerales que forman las rocas —normalmente silicatos y ocasionalmente carbonatos— tienen una densidad relativa de 2.5–3.5.<[3]​: 43–44  Una alta densidad relativa si permite diagnosticar algunos minerales. La variación química (y por consiguiente, en la clase mineral) se correlaciona con un cambio en la densidad relativa. Entre los minerales más comunes, los óxidos y sulfuros tienden a tener una alta densidad relativa, ya que incluyen elementos con mayor masa atómica. Una generalización es que los minerales metálicos o con brillo diamantino tienden a tener densidades relativas más altas que las que tienen los minerales no-metálicos o de brillo mate. Por ejemplo, la hematita, Fe 3, tiene una densidad relativa de 5.26[34] mientras que la galena, PbS, tiene una gravedad específica de 7.2–7.6,[35] que es el resultado de su alto contenido en hierro y en plomo, respectivamente. La densidad relativa es muy alta en los metales nativos; la kamacita, una aleación de hierro-níquel común en los meteoritos de hierro, tiene una densidad relativa de 7.9,[36] y el oro tiene una densidad relativa observada entre 15 y 19.3.[3]​: 43–44 [37]​ Otras propiedades Carnotita (amarillo) es un mineral radioactivo Se pueden utilizar otras propiedades para identificar minerales, aunque son menos generales y solo aplicables a ciertos minerales. La inmersión en ácido diluido (a menudo en HCl al 10 %) ayuda a distinguir los carbonatos de otras clases de minerales. El ácido reacciona con el grupo del carbonato ([CO3] 2-), lo que causa que el área afectada sufra efervescencia, con desprendimiento de gas dióxido de carbono. Esta prueba se puede ampliar para poner a prueba el mineral en su forma original de cristal o en polvo. Un ejemplo de esta prueba se realiza para distinguir la calcita de la dolomita, especialmente dentro de las rocas (caliza y dolomía, respectivamente). La efervescencia de la calcita es inmediata en ácido, mientras que para que lo haga la dolomita el ácido debe aplicarse a muestras en polvo o sobre una superficie rayada en una roca.[3]​: 44–45  Los minerales de zeolita no sufren efervescencia en ácido; en vez de eso, se vuelven esmerilados después de 5-10 minutos, y si se dejan en ácido durante un día, se disuelven o se convierten en un gel de sílice.[38]​ El magnetismo es una propiedad muy notable de ciertos minerales. Entre los minerales comunes, la magnetita muestra esta propiedad con fuerza, y también está presente, aunque no con tanta intensidad, en la pirrotita y la ilmenita.[3]​: 44–45  Algunos minerales también pueden identificarse mediante la prueba del sabor u olor. La halita, NaCl, es la sal de mesa; su homólogo de potasio, la silvita, tiene un sabor amargo pronunciado. Los sulfuros tienen un olor característico, sobre todo cuando las muestras están fracturadas, reaccionando o en polvo.[3]​: 44–45  La radiactividad es una propiedad poco frecuente, aunque algunos minerales pueden integrar elementos radiactivos. Pueden ser constituyentes que los definen, como el uranio en la uraninita, la autunita y la carnotita, o como impurezas traza. En este último caso, la desintegración de los elementos radiactivos daña el cristal mineral; el resultado, denominado «halo radiactivo» o «halo pleocroico», es observable mediante diversas técnicas, en especial en las láminas finas de petrografía.[3]​: 44–45  Clases de minerales Dado que la composición de la corteza terrestre está dominada por el silicio y el oxígeno, los elementos con silicatos son, con mucho, la clase de minerales más importante en términos de formación de rocas y diversidad: la mayoría de las rocas se componen en más de un 95% de minerales de silicato, y más del 90% de la corteza terrestre está compuesta por estos minerales.[3]​: 104  Además de los componentes principales, silicio y oxígeno, son comunes en los minerales de silicato otros elementos comunes en la corteza terrestre, como el aluminio, el magnesio, el hierro, el calcio, el sodio y el potasio.[3]​: 5  Los silicatos más importantes que forman rocas son los feldespatos, los cuarzos, los olivinos, los piroxenos, los anfíboles y las micas. A su vez, los minerales no-silicatos se subdividen en varias clases por su química dominante: elementos nativos, sulfuros, haluros, óxidos e hidróxidos, carbonatos y nitratos, boratos, sulfatos, fosfatos y compuestos orgánicos. La mayoría de las especies minerales no silicatos son extremadamente raras (constituyen en total un 8% de la corteza terrestre), aunque algunas son relativamente comunes, como la calcita, pirita, magnetita y hematita. Hay dos estilos estructurales principales observados en los no-silicatos: el empaquetamiento compacto y los tetraedros enlazados como aparecen en los silicatos. Las estructuras compactas son una manera de empaquetar densamente átomos y reducir al mínimo el espacio intersticial. El empaquetado compacto hexagonal consiste en apilar capas en las que cada capa es la misma (ababab), mientras que el empaquetado cúbico consiste en grupos de apilamiento de tres capas (abcabcabc). Análogos a los tetraedros de sílice enlazados son los tetraedros que forman los iones SO 4 (sulfato), PO 4 (fosfato), AsO 4 (arseniato), y VO 4 (vanadato). Los minerales no-silicatos tienen una gran importancia económica, ya que concentran más elementos que los minerales de silicato[3]​: 641–643  y se explotan especialmente como menas.[3]​: 641, 681  Silicatos Artículo principal: Minerales silicatos Esquema del tetraedro [SiO4]4−  base de los silicatos Los silicatos son sales que combinan la sílice SiO 2 con otros óxidos metálicos. La base de la unidad de un mineral de silicato es el tetraedro [SiO4]4− : en la mayoría de casos, el silicio se encuentra coordinado cuatro veces, o en coordinación tetraédrica, con el oxígeno; en situaciones de muy altas presiones, el silicio estará coordinado seis veces, o en coordinación octaédrica, como en la estructura de perovskita o en el cuarzo polimorfo stishovita (SiO2). (En el último caso, el mineral ya no tiene una estructura de silicato, si no de rutilo (TiO 2) y su grupo asociado, que son óxidos simples.) Estos tetraedros de sílice son luego polimerizados en algún grado para crear otras estructuras, como cadenas unidimensionales, láminas bidimensionales o armazones tridimensionales. El mineral de un silicato básico sin polimerización de tetraedros requiere de otros elementos que equilibren la base cargada 4-. En las otras estructuras de silicato son varias las combinaciones de elementos que equilibran esa carga negativa. Es común que el Si4+  sea sustituido por Al3+  debido a la similitud en radio iónico y en carga; en otros casos, los tetraedros de [AlO 4]5−  forman las mismas estructuras que lo hacían los tetraedros no sustituidos, pero los requisitos del equilibrio de cargas son diferentes.[3]​: 104–120  El grado de polimerización puede ser descrito tanto por la estructura formada como por el número de vértices tetraédricos (u oxígenos de coordinación) compartidos (por el aluminio y el silicio en sitios tetraédricos):[3]​: 105  los ortosilicatos (o nesosilicatos) no tienen ninguna vinculación de poliedros, así que los tetraedros no comparten vértices; los disilicatos (o sorosilicatos) tienen dos tetraedros que comparten un átomo de oxígeno; los inosilicatos son silicatos en cadena: los de cadena simple tienen dos vértices compartidos y los de cadena doble dos o tres; los filosilicatos forman una estructura de lámina que requiere tres oxígenos compartidos (en el caso de silicatos de cadena doble, algunos tetraedros deben compartir dos vértices en lugar de tres como harían si resultase una estructura de lámina); los silicatos en armazón o tectosilicatos, tienen tetraedros que comparten los cuatro vértices; los silicatos de anillo, o ciclosilicatos, solo necesitan tetraedros que compartan dos vértices para formar la estructura cíclica.[3]​: 104–117  Se describen a continuación en orden decreciente de polimerización, las subclases de silicato. Enlaces de tetraedros Ortosilicato: tetraedros simples Ortosilicato: tetraedros simples Sorosilicatos: dobles tetraedros Sorosilicatos: dobles tetraedros Inosilicatos: cadenas de tetraedros Inosilicatos: cadenas de tetraedros Inosilicatos: cadenas dobles de tetraedros Inosilicatos: cadenas dobles de tetraedros Ciclosilicatos: Anillos de tetraedros Ciclosilicatos: Anillos de tetraedros Tectosilicatos Artículo principal: Tectosilicato El cuarzo es el principal mineral de la serie de los tectosilicatos (cristal de roca de la mina La Gardette, Francia). Esquema de la estructura interna tridimensional de un cuarzo (cuarzo-β). Las esferas rojas representan iones de oxígeno y las esferas grises iones de silicio. Los tectosilicatos son muy abundantes, constituyendo aproximadamente el 64 % de los minerales de la corteza terrestre.[39]​También conocidos como silicatos de estructura en armazón, tienen el grado de polimerización más alto y tienden a ser químicamente estables como resultado de la fuerza de los enlaces covalentes.[7]​: 502  Son ejemplos el cuarzo, los feldespatos, los feldespatoides, y las zeolitas. Tienen una estructura basada en un entramado tridimensional de tetraedros (ZO 4) con los cuatro vértices ocupados por el ion O2- compartidos, lo que implica relaciones Z:O=1:2.[39] La Z es silicio (Si) (la fórmula resultante es SiO 2, sílice), pero parte del Si4+  puede ser reemplazado por Al3+  (en raras ocasiones por Fe3+ , Ti3+  y B3+ ).[40] Al suceder esto, las cargas negativas resultantes se compensan con la entrada de cationes grandes, como el K+ , el Na+  o el Ca2+  (y con menos frecuencia Ba2+ , Sr2+  y Cs+ ).[40] También pueden tener aniones complementarios F−, Cl−, S2−, CO32−, SO42−.[40]​ El cuarzo (SiO 2) es la especie mineral más abundante, formando el 12 % de la corteza terrestre. Se caracteriza por su alta resistividad química y física. Tiene varios polimorfos, incluyendo la tridimita y la cristobalita a altas temperaturas, la coesita a alta presión y la stishovita a ultra-alta presión. Este último mineral solo puede formarse en la Tierra por impacto de meteoritos, y su estructura está tan compuesta que había cambiado de una estructura de silicato a la de rutilo (TiO 2). El polimorfo de sílice que es más estable en la superficie de la Tierra es el α-cuarzo. Su homólogo, el cuarzo-β, está presente solo a altas temperaturas y presiones (a 1 bar, cambia a cuarzo-α por debajo de 573 °C). Estos dos polimorfos difieren en un retorcimiento de los enlaces; este cambio en la estructura da al cuarzo-β mayor simetría que al cuarzo-α, y por lo tanto también se les llama cuarzo alto (β) y cuarzo bajo (α).[3]​: 104 [3]​: 578–583  Los feldespatos son el grupo más abundante en la corteza terrestre, en torno al 50 %. En los feldespatos, los Al3+  sustitutos de los Si4+  crean un desequilibrio de carga que debe ser explicado por la adición de cationes. La estructura de base se convierte ya en [AlSi 8], ya en [Al 2Si 8]2− . Hay 22 especies minerales de feldespatos, subdivididas en dos grandes subgrupos —alcalino y plagioclasa— y dos grupos menos comunes —celsiana y banalsita—. Los feldespatos alcalinos son los más comunes en una serie que va desde la entre ortoclasa, rica en potasio, a la albita, rica en sodio; en el caso de las plagioclasas, la serie más común varía desde la albita a la anortita, rica en calcio. El maclado de cristales es común en los feldespatos, especialmente con maclas polisintéticas en las plagioclasas y maclas de Carlsbad en los feldespatos alcalinos. Si el último subgrupo se enfría lentamente a partir de una masa fundida, se forma laminillas de exsolución porque los dos componentes —ortoclasa y albita— son inestables en solución sólida. La exsolution puede darse desde una escala microscópica hasta ser fácilmente observable en la muestra de mano; se forma una textura pertitica cuando un feldespato rico en Na exsolve en un huésped rico en K. La textura opuesta (antipertitica), cuando un feldespato rico en K exsolve en un huésped rico en Na, es muy rara.[3]​: 583–588  Los feldespatoides son estructuralmente similares a los feldespatos, pero se diferencian en que se forman en condiciones de carencia de silicio lo que permite una mayor sustitución por Al3+ . Como resultado, los feldespatoides no se pueden asociar con cuarzo. Un ejemplo común de un feldespatoide es la nefelina ((Na, K)AlSiO 4); comparada con los feldespatos alcalinos, la nefelina tiene una relación Al 3: SiO 2 de 1: 2, en lugar de 1:6 en el feldespato.[3]​: 588  Las zeolitas a menudo tienen hábitos de cristal distintivos, produciendo agujas, placas o bloques masivos. Se forman en presencia de agua a bajas temperaturas y presiones, y tienen canales y huecos en su estructura. Las zeolitas tienen varias aplicaciones industriales, especialmente en el tratamiento de aguas residuales.[3]​: 589–593  Ejemplos de tectosilicatos Albita Albita Anortita Anortita Ortoclasa Ortoclasa Nefelina Nefelina Zeolita Zeolita Filosilicatos Artículo principal: Filosilicato Moscovita, una especie mineral del grupo de las micas, dentro de la subclase de los filosilicatos Modelo poliédrico de la lámina de tetraedros de sílice Los filosilicatos son un grupo de minerales muy extendidos en la corteza terrestre, integrantes de muchos tipos de rocas, ígneas, metamórficas y sedimentarias. Las arcillas están formadas fundamentalmente por filosilicatos. La característica principal de los filosilicatos es su disposición en capas, que ocasiona hábitos típicos fácilmente reconocibles (minerales hojosos o escamosos). Además suelen ser minerales blandos y poco densos. Los filosilicatos consisten en apilamientos de láminas de tetraedros polimerizados. Las láminas, desde el punto de vista estructural, son de dos tipos: tetraédricas y octaédricas. Los tetraédricas están enlazados a tres sitios de oxígeno, lo que da una relación característica de silicio:oxígeno de 2:5. Ejemplos importantes son la mica, el grupo de las cloritas y los grupos de caolinita-serpentina. Las láminas están débilmente enlazadas por fuerzas de van der Waals o enlaces de hidrógeno, lo que provoca una debilidad cristalográfica, que a su vez conduce a una prominente exfoliación basal entre los filosilicatos.[7]​: 525  Además de los tetraedros, los filosilicatos tienen una hoja de octaedros (elementos de coordinación seis con oxígeno) que equilibran los tetraedros de base, que tienen una carga negativa (por ejemplo, [Si 10]4− ) Estas hojas de tetraedros (T) y octaedros (O) se apilan en una gran variedad de combinaciones para crear los distintos grupos de los filosilicatos. En una capa octaédrica, hay tres sitios octaédricos en una estructura única; sin embargo, no todos los sitios pueden estar ocupados. En ese caso, el mineral se denomina dioctahédrico, mientras que en otro caso se denomina trioctaédrico.[3]​: 110  El grupo de la caolinita-serpentina consiste en pilas de T-O (minerales de arcilla 1:1); su dureza varía de 2 a 4, cuando las láminas están retenidas por enlaces de hidrógeno. Los minerales de arcilla 2:1 (pirofilita-talco) consisten en pilas T-O-T, pero son más blandos (dureza 1-2), ya que están se mantienen unidos por fuerzas de van der Waals. Estos dos grupos de minerales están divididos en subgrupos según la ocupación octaédrica; específicamente, la caolinita y la pirofilita son dioctaédricos mientras que la serpentina y el talco son trioctaédricos.[3]​: 110–113  Las micas son también filosilicatos T-O-T apilados, pero difieren de los otro miembros de las subclases apiladas T-O-T y T-O en que incorporan aluminio en las láminas tetraédricas (los minerales de arcilla tienen Al3+  en los sitios octaédricos). Ejemplos comunes de micas son la moscovita y las series de la biotita. El grupo de la clorita se relaciona con el grupo de la mica, pero con una capa similar a la brucita (Mg(OH) 2) entre la de las pilas T-O-T.<[3]​: 602–605  A causa de su estructura química, los filosilicatos típicamente tienen capas flexibles, elásticas, transparentes que son aislantes eléctricos y se pueden dividir en escamas muy finas. Las micas se puede utilizar en la electrónica como aislantes, en la construcción, como relleno óptico, o incluso en cosméticos. La crisotila, una especie de serpentina, es la especie mineral más común en el amianto industrial, ya que es menos peligrosa en términos de la salud que los asbestos anfíboles.[3]​: 593–595  Ejemplos de filosilicatos Fuchsita, una mica Fuchsita, una mica Biotita Biotita Crisotilo Crisotilo Brucita Brucita Serpentina Serpentina Inosilicatos Artículo principal: Inosilicato Disposición cristalina de los inosilicatos Tremolita asbestiforme, parte del grupo de los anfiboles en la subclase de los inosilicatos Aegirina, un clinopiroxeno hierro-sodio. Es parte de la subclase inosilicatos. Los inosilicatos son metasilicatos que consisten en tetraedros unidos repetidamente en cadenas. Estas cadenas pueden ser simples —cuando un tetraedro está unido a otros dos para formar una cadena continua— o dobles, cuando dos cadenas sencillas se combinan entre ellas. Los silicatos de cadena individuales tienen una relación de silicio:oxígeno de 1:3 (por ejemplo, [Si 6]4− ), mientras que las variedades de doble cadena tiene una proporción de 4:11, por ejemplo [Si 22]12− . Los inosilicatos tienen dos importantes grupos de minerales que forman rocas; los piroxenos, generalmente silicatos de cadena simple, y los anfiboles, de cadena doble.[7]​: 537  Hay cadenas de orden superior (por ejemplo, cadenas de tres, cuatro o cinco miembros) pero son raras.[41]​ El grupo de los piroxenos consta de 21 especies minerales.[3]​: 112  Los piroxenos tienen una fórmula de estructura general (XYSi 6), siendo X un sitio octaédrico e Y otro que puede variar en número de coordinación de seis a ocho. La mayoría de las variedades de los piroxenos consisten en permutaciones de Ca2+ , Fe2+  y Mg2+  que equilibran la carga negativa de la cadena principal. Los piroxenos son comunes en la corteza terrestre (aproximadamente el 10 %) y son un componente clave de las rocas ígneas máficas.[3]​: 612–613  Los anfiboles tienen una gran variabilidad química, por ello descritos a veces como un «cesto mineralógico» o un «tiburón mineralógico nadando en un mar de elementos». La columna vertebral de los anfíboles es la [Si 22]12− ; está equilibrada por cationes en tres posiciones posibles, aunque la tercera posición no siempre se utiliza y un elemento puede ocupar las restantes. Los anfíboles están generalmente hidratados, es decir, que tienen un grupo hidroxilo ([OH]− ), aunque puede ser reemplazado por un fluoruro, un cloruro, o un ion de óxido.[3]​: 606–612  Debido a su química variable, hay más de 80 especies de anfíboles, aunque las variaciones más comunes, como en los piroxenos, implican mezclas de Ca2+ , Fe2+  y Mg2+ .[3]​: 112  Varias especies minerales de los anfíboles pueden tener un hábito cristalino asbestiforme. Estos minerales de asbesto forman fibras largas, delgadas, flexibles y fuertes, que son aislantes eléctricos, químicamente inertes y resistentes al calor; como tal, tienen varias aplicaciones, especialmente en materiales de construcción. Sin embargo, los asbestos son conocidos carcinógenos, y causan varias enfermedades más, como la asbestosis; los asbestos anfíboles (antofilita, tremolita, actinolita, grunerita y riebeckita) se consideran más peligrosos que el asbesto serpentina crisotilo.[3]​: 611–612  Ejemplos de inosilicatoss Diopsida, un piroxeno Diopsida, un piroxeno Piroxeno Piroxeno Antofilita (anfibol) Antofilita (anfibol) Tremolita (anfibol) Tremolita (anfibol) Crocidolita, variedad de riebeckita (anfíbol) Crocidolita, variedad de riebeckita (anfíbol) Ciclosilicatos Artículo principal: Ciclosilicato Estructura en anillo de la dioptasa La clase de los ciclosilicatos corresponde a la clase 9.C de la clasificación de Strunz y tiene 16 familias. Está integrada por tres o más tetraedros de [SiO4]4− unidos por sus vértices, formando un anillo cerrado, simple o doble, el cual puede tener enlaces iónicos con metales como por ejemplo sodio, calcio, hierro, aluminio, potasio, magnesio, etc.[42] Algunos ejemplos de ciclosilicatos son la turmalina, cordierita, rubelita, benitoita, dioptasa, etc. Los ciclosilicatos, o silicatos de anillo, tienen una relación de silicio a oxígeno de 1:3. Los anillos de seis miembros son los más comunes, con una estructura de base de [Si 28]12− ; ejemplos del grupo son la turmalina y el berilo. Hay otras estructuras de anillo, habiendo sido descritas las de 3, 4, 8, 9 y 12.[3]​: 113–115  Los ciclosilicatos tienden a ser fuertes, con cristales alargados y estriados.[3]​: 558  Los anillos pueden ser simples o ramificados, aislados unos de otros o agrupados en dos. Estos anillos están generalmente apilados en la estructura y determinar canales que puede estar vacíos u ocupados por iones o moléculas. Los ciclosilicates se clasifican según el tipo de anillos, y en particular por el número de tetraedros en el anillo. Las turmalinas tienen una química muy compleja que puede ser descrita por una fórmula general XY 6(BO 3) 3T 18V 3W. El T 18 es la estructura básica del anillo, donde T es generalmente Si4+ , pero pueden ser sustituidos por Al3+  o B3+ . Las turmalinas pueden dividirse en subgrupos por el sitio que ocupe el X, y de ahí se subdividen por la química del sitio W. Los sitios Y y Z pueden acomodar una variedad de cationes, especialmente diversos metales de transición; esta variabilidad en el contenido del metal de transición estructural da al grupo de la turmalina mayor variabilidad en color. Otro ciclosilicato es el berilo, Al 2Be 3Si 18, cuyas variedades incluyen piedras preciosas como la esmeralda (verde) y la aguamarina (azulado). La cordierita es estructuralmente similar al berilo, y es un mineral metamórfico común.[3]​: 617–621  Ejemplos de ciclosilicatoss Elbaita, una turmalina con una distintiva banda coloreada Elbaita, una turmalina con una distintiva banda coloreada Benitoita Benitoita Cordierita Cordierita Dioptasa Dioptasa Berilo Berilo Sorosilicatos Artículo principal: Sorosilicato La epidota a menudo tiene un color verde pistacho distintivo. La clase de los sorosilicatos corresponde a la clase 9.B de la clasificación de Strunz y tiene 10 familias, de dos tipos, el de las epidotas y el de las idocrasas. Los sorosilicatos, también denominados disilicatos, tienen un enlace tetraedro-tetraedro en un oxígeno, lo que resulta en una relación de 2:7 de silicio al oxígeno. El elemento estructural común resultante es el grupo [Si 7]6− . Los disilicatos más comunes son, con mucho, los miembros del grupo de la epidota. Las epidotas se encuentran en diversos entornos geológicos, que van desde las cordilleras oceánicas a los granitos y hasta las metapelitas. Las epidotas se construyen alrededor de la estructura [(Si 4)(Si 7)]10− ; por ejemplo, las especies minerales de epidota tiene calcio, aluminio y hierro férrico para equilibrar las cargas: Ca 2Al 2(Fe3+ ,Al)(SiO 4)(Si 7)O(OH). La presencia de hierro como Fe3+  y Fe2+  ayuda a entender la fugacidad de oxígeno, que a su vez es un factor significativo en petrogénesis.[3]​: 612–627  Otros ejemplos de sorosilicatos son la lawsonita, un mineral metamórfico que forma las facies blueschist (ajuste de zona de subducción con baja temperatura y alta presión), la vesuvianita, que ocupa una cantidad significativa de calcio en su estructura química.[3]​: 612–627 [7]​: 565–573  Ortosilicatos Artículo principal: Ortosilicato Andradita negra, un miembro terminal del grupo de granates Modelo estructural del zirconio La clase de los ortosilicatos corresponde a la clase 9.A de la clasificación de Strunz y tiene 10 familias con cerca de 120 especies. Los ortosilicatos consisten en tetraedros aislados que tienen las cargas equilibrada por otros cationes.[3]​: 116–117  También denominados nesosilicatos, este tipo de silicatos tiene una relación silicio:oxígeno de 1:4 (por ejemplo, SiO 4). Los ortosilicatos típicos tienden a formar bloques de cristales equantes, y son bastante pesados.[7]​: 573  Varios minerales que forman rocas son parte de esta subclase, como los aluminosilicatos, el grupo del olivino o el grupo del granate. Los aluminosilicatos —cianita, andalucita, y silimanita, todos Al 2SiO 5— están estructuralmente compuestos por un tetraedro [SiO 4]−  y un Al3+  en coordinación octaédrica. El restante Al3+  puede estar en coordinación de seis (cianita), cinco (andalucita) o cuatro (silimanita); qué mineral se forma en un entorno dado depende de las condiciones de presión y temperatura. En la estructura del olivino, la serie principal de olivino (Mg, Fe) 2SiO 4 consisten en forsterita, rica en magnesio, y fayalita, rica en hierro. Tanto el hierro como el magnesio están en coordinación octaédrica con el oxígeno. Existen otras especies minerales que tienen esta estructura, como la tefroita, Mn 2SiO 4.[7]​: 574–575  El grupo del granate tiene una fórmula general de X 2(SiO 3, donde X es un gran catión ocho veces coordinado, e Y es un catión menor seis veces coordinado. Hay seis miembros terminales ideales de granate, divididos en dos grupos. Los granates piralspita tienen Al3+  en la posición Y: piropo (Mg 3Al 2(SiO 3), almandino (Fe 3Al 2(SiO 3), y espesartina (Mn 3Al 2(SiO 3). Los granates ugrandita tienen Ca2+  en la posición X: uvarovita (Ca 3Cr 2(SiO 3), grossular (Ca 3Al 2(SiO 3) y andradita (Ca 3Fe 2(SiO 3). Si bien hay dos subgrupos de granate, existen soluciones sólidas entre los seis miembros finales.[3]​: 116–117  Otros ortosilicatos son el zircón, la estaurolita y el topacio. El zirconio (ZrSiO 4) es útil en geocronología ya que el Zr4+  puede ser sustituido por U6+ ; además, debido a su estructura muy resistente, es difícil resetearlo como un cronómetro. La estaurolita es un común mineral índice de grado intermedio metamórfico. Tiene una estructura cristalina particularmente complicada que solo fue descrita plenamente en 1986. El topacio (Al 2SiO 4(F, OH) 2, que se encuentra a menudo en pegmatitas graníticas asociadas con turmalina, piedra preciosa es un mineral común.[3]​: 627–634  Ejemplos de ortosilicatos Andalucita, un aluminosilicato Andalucita, un aluminosilicato Almandina, del grupo del granate Almandina, del grupo del granate Humita Humita Ludwigita, del grupo del olivino Ludwigita, del grupo del olivino Zirconio Zirconio Minerales no silicatos Elementos nativos Artículo principal: Elementos nativos Oro nativo. Raro espécimen de cristales gruesos que crecen fuera de un tallo central (3.7 x 1.1 x 0.4 cm, de Venezuela). Los elementos nativos son aquellos minerales integrados por elementos que no están unidos químicamente a otros elementos. Este grupo incluye minerales metales nativos, semi-metales y no metales, y varias aleaciones sólidas y soluciones. Los metales se mantienen unidos por enlaces metálicos, lo que les confiere propiedades físicas distintivas, como su lustre metálico brillante, ductilidad y maleabilidad, y conductividad eléctrica. Los elementos nativos se subdividen en grupos por su estructura o atributos químicos. El grupo del oro, con una estructura cercana al empaquetamiento cúbico, incluye metales como el oro, la plata y el cobre. El grupo del platino es similar en estructura al grupo de oro. El grupo del hierro-níquel se caracteriza por tener varias especies de aleaciones de hierro-níquel. Dos ejemplos son la kamacita y la taenita, que se encuentran en meteoritos de hierro; estas especies difieren en la cantidad de Ni en la aleación; la kamacita tiene menos de 5–7 % de níquel y es una variedad de hierro nativo, mientras que el contenido de níquel de la taenita es del 7–37 %. Los minerales del grupo del arsénico se componen de semi-metales, que tienen solamente algunos metálicos; por ejemplo, carecen de la maleabilidad de los metales. El carbono nativo aparece en dos alótropos, el grafito y el diamante; el último se forma a muy alta presiones en el manto, lo que le confiere una estructura mucho más fuerte que el grafito.[3]​: 644–648  Sulfuros Artículo principal: Minerales sulfuros Cinabrio rojo (HgS), una mena del mercurio, sobre dolomita La clase de los minerales sulfuros y sulfosales —denominación engañosa pues los sulfuros solo son una parte del grupo— corresponde a la clase 2 de la clasificación de Strunz y en ella se incluyen: minerales sulfuros —con el ion S2− —-, los seleniuros, teluriuros, arseniuros, antimoniuros, bismutiuros, sulfoarseniuros y sulfosales. Los sulfuros se clasifican por la relación del metal o del semimetal con el azufre, M:S igual a 2:1, o 1:1.[3]​: 649  A pesar de que los sulfuros son mucho menos abundantes que los silicatos, su química y sus estructuras son muy variadas, lo que explica porque el número de minerales de sulfuro es muy alto en relación con su abundancia. Se agrupan entre los sulfuros los minerales compuestos de uno o más metales o semimetales con un azufre, que tienen una fórmula de tipo general de M p, donde M es un metal (Ag, Cu, Pb, Zn, Fe, Ni, Hg, As, Sb, Mo, Hg, Tl, V). Los arseniuros, los antimoniuros, los telurios... se clasifican entre los «sulfuros» sensu lato debido a su similitud estructural con los sulfuros. Los sulfuros minerales se caracterizan por la unión covalente, la opacidad y el brillo metálico; se estudian con el microscopio de reflexión. Los sulfuros tienden a ser blandos y frágiles, con un alto peso específico y la mayoría son semiconductores. Muchos sulfuros en polvo, como la pirita, tienen un olor sulfuroso cuando son pulverizados. Los sulfuros son susceptibles a la intemperie, y muchos se disuelven fácilmente en agua; estos minerales disueltos se pueden después volver a redepositar, lo que crea yacimientos de menas secundarias.[3]​: 357  Muchos minerales de sulfuro son importantes económicamente como minerales metálicos; son ejemplos la esfalerita (ZnS), una mena de zinc; la galena (PbS), una mena de plomo; el cinabrio (HgS), una mena de mercurio; y la molibdenita (MoS 2, una mena de molibdeno.[3]​: 651–654  La pirita (FeS 2) es el sulfuro que aparece más y se puede encontrar en la mayoría de entornos geológicos. No es, sin embargo, una mena de hierro, pero puede ser oxidada para producir ácido sulfúrico.[3]​: 654  Relacionados con los sulfuros están las raras sulfosales, en las que un elemento metálico está unido al azufre y a un semimetal, como antimonio, arsénico o bismuto. Al igual que los sulfuros, las sulfosales son típicamente minerales blandos, pesados y frágiles.[7]​: 383  Ejemplos de sulfuros Pirita, disulfuro de hierro Pirita, disulfuro de hierro Esfalerita, mena de zinc Esfalerita, mena de zinc Molibdenita, mena de molibdeno Molibdenita, mena de molibdeno Estannita, mena de estaño Estannita, mena de estaño Reálgar, sulfuro de arsénico Reálgar, sulfuro de arsénico Óxidos Artículo principal: Minerales óxidos La clase de los minerales óxidos e hidróxidos corresponde a la clase 4 de la clasificación de Strunz y en ella se incluyen: óxidos, hidróxidos, vanadatos, arsenitos, antimonitos, bismutitos, sulfitos, selenitos, teluritos y yodatos. Los minerales óxidos se dividen en tres categorías: óxidos simples, hidróxidos y óxidos múltiples. Los óxidos simples se caracterizan por O2−  como anión principal y enlace principalmente iónico. Se pueden subdividir además por la relación del oxígeno a los cationes. El grupo de la periclasa consta de minerales con una relación 1:1. Óxidos con una relación 2:1 incluyen la cuprita (Cu 2O) y el hielo de agua. minerales del grupo del corindón tienen una proporción de 2:3, e incluye minerales como el corindón (Al 3) y la hematita (Fe 3). Los minerales del grupo del rutilo tienen una proporción de 1:2; la especie del mismo nombre, rutilo (TiO 2) es el principal mena del titanio; Otros ejemplos incluyen la casiterita (SnO 2, mena de estaño), y pirolusita (MnO 2, mena de manganeso).[7]​: 400–403 [3]​: 657–660  En hidróxidos, el anión dominante es el ion hidroxilo, OH− . Las bauxitas son la mena principal del aluminio, y son una mezcla heterogénea de minerales de hidróxido de diáspora, gibbsita, y bohmita; se forman en áreas con una alta tasa de meteorización química (principalmente condiciones tropicales).[3]​: 663–664  Por último, varios óxidos son compuestos de dos metales con oxígeno. Un grupo importante dentro de esta clase son las espinelas, con una fórmula general de X2+ Y3+ 4. Ejemplos de especies incluyen la propia espinela (MgAl 4), la cromita (FeCr 4) y la magnetita (Fe 4). Esta última es fácilmente distinguible por su fuerte magnetismo, que se produce ya que tiene hierro en dos estados de oxidación (Fe2+ Fe3+ 4), lo que hace que sea un óxido múltiple en lugar de un óxido simple.[3]​: 660–663  Ejemplos de minerales óxidos Anatasa, dióxido de titanio (TiO 2) Anatasa, dióxido de titanio (TiO Cuprita, óxido de cobre Cuprita, óxido de cobre Casiterita, óxido de estaño Casiterita, óxido de estaño Gibbsita, hidróxido de aluminio Gibbsita, hidróxido de aluminio Magnetita, óxido de hierro Magnetita, óxido de hierro Haluros Cristales de halita cúbica rosa (NaCl; clase haluro) en una matriz de nahcolita (NaHCO 3; un carbonato, y la forma mineral del bicarbonato sódico, que se utilizan como bicarbonato de sodio). baking soda). Artículo principal: Minerales haluros La clase de los minerales haluros corresponde a la clase 3 de la clasificación de Strunz y en ella se incluyen: haluros o halogenuros simples o complejos, con H2O o sin ella, así como derivados oxihaluros, hidroxihaluros y haluros con doble enlace. Los minerales haluros son compuestos en los que un halógeno (flúor, cloro, yodo y bromo) es el anión principal. Estos minerales tienden a ser blandos, débiles, quebradizos y solubles en agua. Los ejemplos más comunes de haluros son la halita (NaCl, sal de mesa), la silvita (KCl) y la fluorita (CaF 2). La halita y la silvita se forman comúnmente como evaporitas, y pueden ser minerales dominantes en las rocas sedimentarias químicas. La criolita, Na 3AlF 6, es un mineral clave en la extracción de aluminio a partir de la bauxita; Sin embargo, dado que la única ocurrencia significativa está en Ivittuut, Groenlandia, en una pegmatita granítica, ya agotada, la criolita sintética se puede hacer a partir de la fluorita.[7]​: 425–430  Carbonatos Artículo principal: Minerales carbonatos Cristales de calcita de la mina Sweetwater, condado de Reynolds, Misuri (6,2 × 6 × 3,3 cm) La clase de los minerales carbonatos y nitratos corresponde a la clase 5 de la clasificación de Strunz y en ella se incluyen carbonatos, carbonatos de uranilo y nitratos. Los minerales carbonatos son aquellos en los que el grupo aniónico principal es un carbonato, [CO 3]2− . Los carbonatos tienden a ser frágiles, muchos tienen exfoliación romboédrica, y todos reaccionan con ácido.[7]​: 431  Debido a la última característica, los geólogos de campo a menudo llevan ácido clorhídrico diluido para distinguir los carbonatos de los no-carbonatos. La reacción del ácido con los carbonatos, que se encuentra más comúnmente como los polimorfos calcita y aragonita (CaCO 3), se refiere a la disolución y precipitación del mineral, que es un elemento clave en la formación de las cuevas de caliza —con elementos como estalactitas y estalagmitas— y los accidentes geográficos kársticos. Los carbonatos se forman con mayor frecuencia en forma de sedimentos biogénicos o químicos en ambientes marinos. El grupo carbonato es estructuralmente un triángulo, donde un catión central de C4+  está rodeado por tres aniones O2− ; diferentes grupos de minerales se forman a partir de diferentes disposiciones de estos triángulos.[3]​: 667  El mineral de carbonato más común es la calcita, que es el componente principal de la sedimentaria caliza y del mármol metamórfico. La calcita, CaCO 3, puede tener una impureza de alto contenido en magnesio; en condiciones de alto magnesio, se formará en su lugar su polimorfo, la aragonita; la geoquímica marina se puede describir, en este sentido, como un mar de aragonito o mar de calcita, dependiendo de qué mineral se forme preferentemente. La dolomita es un carbonato doble, de fórmula CaMg(CO 2. La dolomitization secundaria de la caliza es común, en la que la calcita o la aragonita se convierten en dolomita; esta reacción aumenta el espacio de los poros (el volumen de la celda unidad de la dolomita es el 88 % del de la calcita), lo que puede crear un yacimiento de petróleo y gas. Estas dos especies minerales son miembros de los grupos de minerales del mismo nombre: el grupo de la calcita incluye carbonatos con fórmula general XCO 3 y el de la dolomita la de XY(CO 2.[3]​: 668–669  Ejemplos de minerales carbonatos Rodocrosita Rodocrosita Smithsonita Smithsonita Dolomita con calcita y calcopirita Dolomita con calcita y calcopirita Azurita y malaquita Azurita y malaquita Hanksita, uno de los pocos minerales considerado un carbonato y un sulfato Hanksita, uno de los pocos minerales considerado un carbonato y un sulfato Sulfatos Artículo principal: Minerales sulfatos Rosa del desierto de yeso La clase de los minerales sulfatos corresponde a la clase 7 de la clasificación de Strunz y en ella se incluyen: sulfatos, selenatos, teluratos, cromatos, molibdatos y wolframatos. Los minerales sulfatos tienen todos el anión sulfato, [SO 4]2− . Tienden a ser de transparentes a translúcidos, blandos, y muchos son frágiles.[3]​: 453  Los minerales de sulfato se forman comúnmente como evaporitas, donde se precipitan de la evaporación de las aguas salinas; alternativamente, los sulfatos también se pueden encontrar en los sistemas de vetas hidrotermales asociados con sulfuros,[3]​: 456–457  o como productos de oxidación de sulfuros.[3]​: 674  Los sulfatos se pueden subdividir en minerales anhidros e hidratados. El sulfato hidratado más común, con mucho, es el yeso, CaSO 4⋅2H 2O. Se forma como un evaporita, y se asocia con otros evaporitas como la calcita y la halita; si incorpora granos de arena cuando cristaliza, el yeso puede formar rosas del desierto. El yeso tiene muy baja conductividad térmica y mantiene una temperatura baja cuando se calienta a medida que pierde el calor por deshidratación; como tal, el yeso se utiliza como aislante en materiales de construcción. El equivalente anhidro del yeso es la anhidrita; se puede formar directamente de agua de mar en condiciones muy áridas. El grupo de la barita tiene la fórmula general XSO 4, donde X es un catión grande 12-enlazado. Son ejemplos la barita (BaSO 4), la celestina (SrSO 4), y la anglesita (PbSO 4); la anhidrita no es parte del grupo de la barita, ya que el más pequeño Ca2+  sólo tiene enlace ocho veces.[3]​: 672–673  Ejemplos de minerales sulfatos Barita con cerusita Barita con cerusita Fenicocroíta, un cromato Fenicocroíta, un cromato Lindgrenita, molibdato de cobre Lindgrenita, molibdato de cobre Anhidrita Anhidrita Xocomecatlita, un tellurato Xocomecatlita, un tellurato Fosfatos Artículo principal: Minerales fosfatos La clase de los minerales fosfatos corresponde a la clase 8 de la clasificación de Strunz y en ella se incluyen fosfatos, arseniatos y vanadatos. Son 51 familias agrupadas en 7 divisiones, un grupo grande y diverso, que sin embargo, tiene solo unas pocas especies relativamente comunes. Los minerales fosfatos se caracterizan por el anión fosfato coordinado tetraédricamente [PO 4]3− , aunque la estructura se puede generalizar siendo el fósforo sustituido por antimonio ([SbO 4]3− ), arsénico ([AsO 4]3− ), o vanadio ([VO 4]3− ). Los aniones de cloro (Cl− ), flúor (F− ) e hidróxido (OH− ) también encajan en la estructura cristalina. El fosfato más común es el grupo de la apatita, un nombre genérico que designa fosfatos hexagonales de composición bastante variable, Ca 5(PO 3(OH, Cl,F). Las especies más comunes del grupo son la fluorapatita (Ca 5(PO 3F), la clorapatita (Ca 5(PO 3Cl) y la hidroxiapatita (Ca 5(PO 3(OH)). Los minerales de este grupo son los principales constituyentes cristalinos de los dientes y de los huesos de los vertebrados. Otro grupo relativamente abundante es el grupo de la monacita, que tiene una estructura general de ATO 4, donde T es el fósforo o arsénico, y A es, a menudo, un elemento de las tierras raras. La monacita es importante en dos sentidos: en primer lugar, como sumidero de tierras raras, puede concentrar la cantidad suficiente de estos elementos para convertirse en una mena; en segundo lugar, los elementos del grupo de la monacita pueden incorporar cantidades relativamente grandes de uranio y torio, que pueden ser utilizadas para datar una roca basándose en la desintegración del U y Th en plomo.[3]​: 675–680  Ejemplos de minerales fosfatos Apatita Apatita Vivianita, un fosfato hidratado de hierro Vivianita, un fosfato hidratado de hierro Piromorfita, un cloro-fosfato anhidro de plomo Piromorfita, un cloro-fosfato anhidro de plomo Turquesa, fosfato hidratado de cobre y aluminio Turquesa, fosfato hidratado de cobre y aluminio Lazulita, un fosfato de hierro, aluminio y magnesio Lazulita, un fosfato de hierro, aluminio y magnesio Minerales orgánicos Artículo principal: Minerales compuestos orgánicos La clase de los minerales compuestos orgánicos corresponde a la clase 10 de la clasificación de Strunz y en ella se incluyen sales y ácidos orgánicos que aparezcan en minas y los hidrocarburos. Son 7 familias agrupadas en 3 divisiones, un grupo escaso. Estos raros compuestos contienen carbono orgánico, pero se pueden formar también mediante un proceso geológico. Por ejemplo, la whewellita, CaC 4⋅H 2O es un oxalato que se puede depositar en las venas de menas hidrotermales. Mientras el oxalato de calcio hidratado se puede encontrar en las vetas de carbón y en otros depósitos sedimentarios que comprenden materia orgánica, la ocurrencia hidrotérmica no se considera que está relacionada con la actividad biológica.[3]​: 681  Importancia y utilidad Artículo principal: Mineral industrial Minerales diversos Los minerales tienen gran importancia por sus múltiples aplicaciones en los diversos campos de la actividad humana. La industria moderna depende directa o indirectamente de los minerales. Algunos minerales se utilizan prácticamente tal como se extraen; por ejemplo el azufre, el talco, la sal de mesa, etc. Otros, en cambio, deben ser sometidos a diversos procesos para obtener el producto deseado, como el hierro, cobre, aluminio, estaño, etc. Los minerales constituyen la fuente de obtención de los diferentes metales, base tecnológica de la sociedad actual. Así, de distintos tipos de cuarzo y silicatos, se produce el vidrio. Los nitratos y fosfatos son utilizados como abono para la agricultura. Ciertos materiales, como el yeso, son utilizados profusamente en la construcción. Los minerales que entran en la categoría de piedras preciosas o semipreciosas, como los diamantes, topacios, rubíes, se destinan a la confección de joyas. Astrobiología Se ha sugerido que los biominerales podrían ser indicadores importantes de vida extraterrestre y que por lo tanto podrían desempeñar un papel importante en la búsqueda de vida pasada o presente en el planeta Marte. Por otra parte, se cree que los componentes orgánicos (biofirmas), que a menudo se asocian con los biominerales, juegan un papel crucial tanto en reacciones pre-bióticas como bióticas.[43]​ El 24 de enero de 2014 la NASA informó que los estudios actuales de los astromóviles Curiosity y Opportunity en Marte estarán ahora destinados a la búsqueda de evidencia de vida antigua, incluyendo una biosfera basada en microorganismos autótrofos, quimiótrofos y/o quimiolitoautotróficos, así como en agua antigua, incluyendo ambientes fluvo-lacustres (llanuras relacionadas con antiguos ríos o lagos) que pueden haber sido habitables.[44]​[45]​[46]​[47] La búsqueda de evidencia de habitabilidad, tafonomía (relacionada con los fósiles), y el carbono orgánico en el planeta Marte son ahora un objetivo primordial de la NASA.["
ksampletext_wikipedia_geol_tierra: str = "Tierra. La Tierra (del latín terra) es un planeta del sistema solar que gira alrededor de su estrella —el Sol— en la tercera órbita más interna. Es el más denso y el quinto mayor de los ocho planetas del sistema solar. También es el más grande de los cuatro planetas terrestres o rocosos (planetas interiores). La Tierra se formó hace aproximadamente 4550 millones de años y la vida surgió unos mil millones de años después.[19] Es el hogar de millones de especies, incluidos los seres humanos y actualmente el único cuerpo astronómico donde se conoce la existencia de vida.[20] La atmósfera y otras condiciones abióticas han sido alteradas significativamente por la biosfera del planeta, favoreciendo la proliferación de organismos aerobios, así como la formación de una capa de ozono que junto con el campo magnético terrestre bloquean la radiación solar dañina, permitiendo así la vida en la Tierra.[21] Las propiedades físicas de la Tierra, la historia geológica y su órbita han permitido que la vida siga existiendo. Se estima que el planeta seguirá siendo capaz de sustentar vida durante otros 500 millones de años,[22] ya que según las previsiones actuales, pasado ese tiempo la creciente luminosidad del Sol terminará causando la extinción de la biosfera.[23]​[24]​[25]​ La superficie terrestre o corteza está dividida en varias placas tectónicas que se deslizan sobre el magma durante periodos de varios millones de años. La superficie está cubierta por continentes e islas; estos poseen varios lagos, ríos y otras fuentes de agua, que junto con los océanos de agua salada que representan cerca del 71 % de la superficie constituyen la hidrósfera. No se conoce ningún otro planeta con este equilibrio de agua líquida,[nota 6] que es indispensable para cualquier tipo de vida conocida. Los polos de la Tierra están cubiertos en su mayoría de hielo sólido (indlandsis de la Antártida) o de banquisas (casquete polar ártico). El interior del planeta es geológicamente activo, con una gruesa capa de manto relativamente sólido, un núcleo externo líquido que genera un campo magnético, y un sólido núcleo interior compuesto por aproximadamente un 88 % de hierro.[27]​ La Tierra interactúa gravitatoriamente con otros objetos en el espacio, especialmente el Sol y la Luna. En la actualidad, la Tierra completa una órbita alrededor del Sol cada vez que realiza 366.26 giros sobre su eje, lo cual es equivalente a 365.26 días solares o un año sideral.[nota 7] El eje de rotación de la Tierra se encuentra inclinado 23.4° con respecto a la perpendicular a su plano orbital, lo que produce las variaciones estacionales en la superficie del planeta con un período de un año tropical (365.24 días solares).[28] La Tierra posee un único satélite natural, la Luna, que comenzó a orbitar la Tierra hace 4530 millones de años; esta produce las mareas, estabiliza la inclinación del eje terrestre y reduce gradualmente la velocidad de rotación del planeta. Hace aproximadamente 3800 a 4100 millones de años, durante el llamado bombardeo intenso tardío, numerosos asteroides impactaron en la Tierra, causando significativos cambios en la mayor parte de su superficie. Tanto los minerales del planeta como los productos de la biosfera aportan recursos que se utilizan para sostener a la población humana mundial. Sus habitantes están agrupados en unos 200 estados soberanos independientes, que interactúan a través de la diplomacia, los viajes, el comercio y la acción militar. Las culturas humanas han desarrollado muchas ideas sobre el planeta, incluida la personificación de una deidad, la creencia en una Tierra plana o en la Tierra como centro del universo, y una perspectiva moderna del mundo como un entorno integrado que requiere administración. Eponimia y etimología El nombre del planeta Tierra se diferencia del de otros planetas del sistema solar porque no proviene de la mitología grecorromana de manos de autores griegos o romanos. El término latino «terra» significa literalmente ‘suelo’ o ‘tierra firme’ y de ahí deriva la palabra en español. «La Tierra» también se usa como sinónimo intercambiable por «mundo», «globo» y «planeta».[18] En la Antigüedad la palabra ‘tierra’ se usaba indistintamente para referirse al suelo, a la tierra como uno de los cuatro elementos, así como al mundo habitado, sin distinción clara entre ambos.[29]​ Durante la Edad Media y hasta el Renacimiento, hay textos en latín que usan terra para referirse al mundo habitable y al orbe terrestre. En el tratado De sphaera mundi (~1230) de Johannes de Sacrobosco se refiere a “orbis” al ámbito de la tierra (o mundo terrestre) como esfera. Textos como Cosmographia de Bernardo Silvestre o los geógrafos medievales usaban “orbis terrarum” (círculo de las tierras) para referirse al mundo. En De revolutionibus orbium coelestium (Copérnico, 1543), en latín, aparecen frases como “terra quoque sphaerica sit” («que la Tierra también sea esférica»); Copérnico presentó el Sol como centro y situó la Tierra como uno de los planetas.[30] En los trabajos de Kepler, en obras como Epitome Astronomiae Copernicanae, también aparece “Terra” en contextos genéricos (“in Terra” o “Terra et Luna”). Pero fue Valentín Naboth (o Valentinus Nabodus), un astrónomo y matemático del siglo XVI, en su obra Primae de coelo et terra institutiones (1573), quien asoció la Tierra con la diosa romana Terra o Tellus. Se trata de una costumbre renacentista de armonizar conocimiento científico con la mitología clásica: «La Tierra, llamada en latín Terra o Tellus, es la madre fértil que sostiene todas las criaturas; por eso la designamos con el nombre de la antigua diosa que los romanos veneraban como la dadora de vida y la cuidadora del suelo».[31]​ Cronología Artículos principales: Historia de la Tierra y Edad de la Tierra. Los científicos han podido reconstruir información detallada sobre el pasado de la Tierra. Según estos estudios el material más antiguo del sistema solar se formó hace 4567.2 ± 0.6 millones de años,[32] y en torno a unos 4550 millones de años atrás (con una incertidumbre del 1 %)[19] se habían formado ya la Tierra y los otros planetas del sistema solar a partir de la nebulosa solar, una masa en forma de disco compuesta del polvo y gas remanente de la formación del Sol. Este proceso de formación de la Tierra a través de la acreción tuvo lugar mayoritariamente en un plazo de 10-20 millones de años.[33] La capa exterior del planeta, inicialmente fundida, se enfrió hasta formar una corteza sólida cuando el agua comenzó a acumularse en la atmósfera. La Luna se formó poco antes, hace unos 4530 millones de años.[34]​ Representación gráfica de la teoría del gran impacto. El actual modelo consensuado[35] sobre la formación de la Luna es la teoría del gran impacto, que postula que la Luna se creó cuando un objeto del tamaño de Marte, con cerca del 10 % de la masa de la Tierra,[36] impactó tangencialmente contra esta.[37] En este modelo, parte de la masa de este cuerpo podría haberse fusionado con la Tierra, mientras otra parte habría sido expulsada al espacio, proporcionando suficiente material en órbita como para desencadenar nuevamente un proceso de aglutinamiento por fuerzas gravitatorias, y formando así la Luna. La desgasificación de la corteza y la actividad volcánica produjeron la atmósfera primordial de la Tierra. La condensación de vapor de agua, junto con el hielo y el agua líquida aportada por los asteroides y por protoplanetas, cometas y objetos transneptunianos, produjeron los océanos.[38] El recién formado Sol solo tenía el 70 % de su luminosidad actual: sin embargo, existen evidencias que muestran que los primitivos océanos se mantuvieron en estado líquido; una contradicción denominada la «paradoja del joven Sol débil», ya que aparentemente el agua no debería ser capaz de permanecer en ese estado líquido, sino en el sólido, debido a la poca energía solar recibida.[39] Sin embargo, una combinación de gases de efecto invernadero y mayores niveles de actividad solar contribuyeron a elevar la temperatura de la superficie terrestre, impidiendo así que los océanos se congelaran.[40] Hace 3500 millones de años se formó el campo magnético de la Tierra, lo que ayudó a evitar que la atmósfera fuese arrastrada por el viento solar.[41]​ Se han propuesto dos modelos para el crecimiento de los continentes:[42] el modelo de crecimiento constante,[43] y el modelo de crecimiento rápido en una fase temprana de la historia de la Tierra.[44] Las investigaciones actuales sugieren que la segunda opción es más probable, con un rápido crecimiento inicial de la corteza continental,[45] seguido de un largo período de estabilidad.[23]​[nota 8]​[25] En escalas de tiempo de cientos de millones de años de duración, la superficie terrestre ha estado en constante remodelación, formando y fragmentando continentes. Estos continentes se han desplazado por la superficie, combinándose en ocasiones para formar un supercontinente. Hace aproximadamente 750 millones de años (Ma), uno de los primeros supercontinentes conocidos, Rodinia, comenzó a resquebrajarse. Los continentes más tarde se recombinaron nuevamente para formar Pannotia, entre 600 a 540 Ma, y finalmente Pangea, que se fragmentó hace 180 Ma hasta llegar a la configuración continental actual.[47]​ Evolución de la vida Historia de la vida ver • discusión • editar -4500 —–-4000 —–-3500 —–-3000 —–-2500 —–-2000 —–-1500 —–-1000 —–-500 —–0 — Agua Vida unicelular Fotosíntesis Eucariotas Vida multicelular Vida terrestre Dinosaurios     Mamíferos Flores Tierra primitiva (−4540) Primeras aguas Vida temprana Meteoritos LHB Primeras evidencias de oxígeno Oxígeno atmosférico Gran Oxidación Primeras evidencias de reproducción sexual Biota ediacárica Explosión cámbrica Primeros humanos PongolanoHuronianoCriogénicoAndinoKarooCuaternario Escala vertical: millones de años. Etiquetas color naranja: eras de hielo conocidas. Artículo principal: Historia de la vida La Tierra proporciona el único ejemplo conocido de un entorno que ha dado lugar a la evolución de la vida.[48] Se presume que procesos químicos altamente energéticos produjeron una molécula autorreplicante hace alrededor de 4000 millones de años, y hace entre 3500 y 3800 millones de años existió el último antepasado común universal.[49] El desarrollo de la fotosíntesis permitió que los seres vivos recogiesen de forma directa la energía del Sol; el oxígeno resultante acumulado en la atmósfera formó una capa de ozono (una forma de oxígeno molecular [O3]) en la atmósfera superior. La incorporación de células más pequeñas dentro de las más grandes dio como resultado el desarrollo de las células complejas llamadas eucariotas.[50] Los verdaderos organismos multicelulares se formaron cuando las células dentro de colonias se hicieron cada vez más especializadas. La vida colonizó la superficie de la Tierra en parte gracias a la absorción de la radiación ultravioleta por parte de la capa de ozono.[51]​ En la década de 1960 surgió una hipótesis que afirmaba que durante el período Neoproterozoico, desde 750 hasta los 580 Ma, se produjo una intensa glaciación en la que gran parte del planeta fue cubierto por una capa de hielo. Esta hipótesis ha sido denominada la «Glaciación global», y es de particular interés, ya que este suceso precedió a la llamada explosión del Cámbrico, en la que las formas de vida multicelulares comenzaron a proliferar.[52]​ Tras la explosión del Cámbrico, hace unos 535 Ma se han producido cinco extinciones en masa.[53] De ellas, el evento más reciente ocurrió hace 65 Ma, cuando el impacto de un asteroide provocó la extinción de los dinosaurios no aviarios, así como de otros grandes reptiles, sobreviviendo algunos pequeños animales como los mamíferos, que por aquel entonces eran similares a las actuales musarañas. Durante los últimos 65 millones de años los mamíferos se diversificaron, hasta que hace varios millones de años, un animal africano con aspecto de simio conocido como el Orrorin tugenensis adquirió la capacidad de mantenerse en pie.[54] Esto le permitió utilizar herramientas y favoreció su capacidad de comunicación, proporcionando la nutrición y la estimulación necesarias para desarrollar un cerebro más grande, y permitiendo así la evolución de la especie humana. El desarrollo de la agricultura y de la civilización permitió a los humanos alterar la Tierra en un corto espacio de tiempo como no lo había hecho ninguna otra especie,[55] afectando tanto a la naturaleza como a la diversidad y cantidad de formas de vida. El presente patrón de edades de hielo comenzó hace alrededor de 40 Ma y luego se intensificó durante el Pleistoceno, hace alrededor de 3 Ma. Desde entonces las regiones en latitudes altas han sido objeto de repetidos ciclos de glaciación y deshielo, en ciclos de 40 000-100 000 años. La última glaciación continental terminó hace 10 000 años.[56]​ Véase también: Anexo:Cronología de la historia evolutiva de la vida Futuro Artículo principal: Futuro de la Tierra Ciclo de la vida solar. El futuro del planeta está estrechamente ligado al del Sol. Como resultado de la acumulación constante de helio en el núcleo del Sol, la luminosidad total de la estrella irá poco a poco en aumento. La luminosidad del Sol crecerá en un 10 % en los próximos 1.1 Ga (1100 millones de años) y en un 40 % en los próximos 3.5 Ga.[57] Los modelos climáticos indican que el aumento de la radiación podría tener consecuencias nefastas en la Tierra, incluyendo la pérdida de los océanos del planeta.[58]​ Se espera que la Tierra sea habitable por alrededor de otros 500 millones de años a partir de este momento,[22] aunque este período podría extenderse hasta 2300 millones de años si se elimina el nitrógeno de la atmósfera.[59] El aumento de temperatura en la superficie terrestre acelerará el ciclo del CO2 inorgánico, lo que reducirá su concentración hasta niveles letalmente bajos para las plantas (10 ppm para la fotosíntesis C4) dentro de aproximadamente 500[22] a 900 millones de años. La falta de vegetación resultará en la pérdida de oxígeno en la atmósfera, lo que provocará la extinción de la vida animal a lo largo de varios millones de años más.[60] Después de otros mil millones de años, todas las aguas superficiales habrán desaparecido[61] y la temperatura media global alcanzará los 70 °C.[60] Incluso si el Sol fuese eterno y estable, el continuo enfriamiento interior de la Tierra se traduciría en una gran pérdida de CO2 debido a la reducción de la actividad volcánica,[62] y el 35 % del agua de los océanos podría descender hasta el manto debido a la disminución del vapor de ventilación en las dorsales oceánicas.[63]​ El Sol, siguiendo su evolución natural, se convertirá en una gigante roja en unos 5 Ga. Los modelos predicen que el Sol se expandirá hasta unas 250 veces su tamaño actual, alcanzando un radio cercano a 1 UA (unos 150 millones de kilómetros).[57]​[64] El destino que sufrirá la Tierra entonces no está claro. Siendo una gigante roja, el Sol perderá aproximadamente el 30 % de su masa, por lo que sin los efectos de las mareas, la Tierra se moverá a una órbita de 1.7 UA (unos 250 millones de kilómetros) del Sol cuando la estrella alcance su radio máximo. Por lo tanto se espera que el planeta escape inicialmente de ser envuelto por la tenue atmósfera exterior expandida del Sol. Aun así, cualquier forma de vida restante sería destruida por el aumento de la luminosidad del Sol (alcanzando un máximo de cerca de 5000 veces su nivel actual).[57] Sin embargo, una simulación realizada en 2008 indica que la órbita de la Tierra decaerá debido a los efectos de marea y arrastre, ocasionando que el planeta penetre en la atmósfera estelar y se vaporice.[64]​ Véase también: Extinción humana Composición y estructura Artículo principal: Ciencias de la Tierra La Tierra es un planeta terrestre, lo que significa que es un cuerpo rocoso y no un gigante gaseoso como Júpiter. Es el más grande de los cuatro planetas terrestres del sistema solar en tamaño y masa, y también es el que tiene la mayor densidad, la mayor gravedad superficial, el campo magnético más fuerte y la rotación más rápida de los cuatro.[65] También es el único planeta terrestre con placas tectónicas activas.[66] El movimiento de estas placas produce que la superficie terrestre esté en constante cambio, siendo responsables de la formación de montañas, de la sismicidad y del vulcanismo. El ciclo de estas placas también juega un papel preponderante en la regulación de la temperatura terrestre, contribuyendo al reciclaje de gases con efecto invernadero como el dióxido de carbono, por medio de la renovación permanente de los fondos oceánicos.[67]​ Forma Comparación de tamaño de los planetas interiores (de izquierda a derecha): Mercurio, Venus, Tierra y Marte. Artículo principal: Historia de la geodesia La forma de la Tierra es muy parecida a la de un geoide o esferoide oblato, una esfera achatada por los polos, resultando en un abultamiento alrededor del ecuador.[68] Este abultamiento está causado por la rotación de la Tierra, y ocasiona que el diámetro en el ecuador sea 43 km más largo que el diámetro de un polo a otro.[69] Hace aproximadamente 22 000 años la Tierra tenía una forma más esférica, la mayor parte del hemisferio norte se encontraba cubierto por hielo, y a medida que el hielo se derretía causaba una menor presión en la superficie terrestre en la que se sostenía, causando esto un tipo de «rebote».[70] Este fenómeno siguió ocurriendo hasta mediados de los años noventa, cuando los científicos se percataron de que este proceso se había invertido, es decir, el abultamiento aumentaba.[71] Las observaciones del satélite GRACE muestran que, al menos desde 2002, la pérdida de hielo de Groenlandia y de la Antártida ha sido la principal responsable de esta tendencia. Volcán Chimborazo, el punto terrestre más alejado del centro de la Tierra. La topografía local se desvía de este esferoide idealizado, aunque las diferencias a escala global son muy pequeñas: la Tierra tiene una desviación de aproximadamente una parte entre 584, o el 0.17 %, desde el esferoide de referencia, que es menor que la tolerancia del 0.22 % permitida en las bolas de billar.[72] Las mayores desviaciones locales en la superficie rocosa de la Tierra son el monte Everest (8 848 m sobre el nivel local del mar) y el abismo Challenger, al sur de la fosa de las Marianas (10 911 m bajo el nivel local del mar). Debido a la protuberancia ecuatorial, el punto terrestre más alejado del centro de la Tierra es el volcán Chimborazo en Ecuador.[73]​[74]​[75]​ La idea de que la forma de la Tierra se aproxima a la de un elipsoide data del siglo XVIII por Pierre Louis Maupertuis. Las primeras ideas antiguas sobre la forma de la Tierra sostenían que la Tierra era plana. Así, por ejemplo, en la antigua Mesopotamia, donde el mundo era visto como un disco rodeado por el océano, más allá del cual se levantaban los pilares de un cielo esférico.[76] También lo es de la cosmología bíblica, tal como aparece en libro de Isaías.[77]​[78] Más adelante surgió el concepto de la Tierra esférica como materia de especulación filosófica hasta el siglo III a. C., cuando la astronomía helenística estableció como un hecho, gracias sobre todo a la medición empírica de Eratóstenes. El paradigma helenístico fue gradualmente adoptado en el Viejo Mundo durante la Antigüedad y la Edad Media.[79]​[80]​[81]​[82] Una demostración práctica de la esfericidad de la Tierra fue llevada a cabo por Fernando de Magallanes y Juan Sebastián Elcano en su expedición de circunnavegación del mundo.[83]​ Para un artículo más detallado sobre las pruebas que demuestran la esfericidad de la Tierra, véase Evidencias empíricas de la forma esférica de la Tierra. Tamaño La circunferencia en el ecuador es de 40 091 km. El diámetro en el ecuador es de 12 756 km y en los polos de 12 730 km.[84] El diámetro medio de referencia para el esferoide es de unos 12 742 km, que es aproximadamente 40 000 km/π, ya que el metro se definió originalmente como la diezmillonésima parte de la distancia desde el ecuador hasta el Polo Norte por París, Francia.[85]​ Estimaciones del tamaño de la Tierra aparecieron desde los tiempos de Aristóteles.[86] La primera medición fue hecha por Eratóstenes, el 240 a. C. En esa época se aceptaba que la Tierra era esférica. Eratóstenes calculó el tamaño de la Tierra midiendo el ángulo con que alumbraba el Sol en el solsticio, tanto en Alejandría como en Siena, distante 750 km. El tamaño que obtuvo fue de un diámetro de 12 000 km y una circunferencia de 40 000 km,[87] es decir, con un error de solo el 6 % respecto a los datos actuales. Posteriormente Posidonio de Apamea repitió las mediciones en el año 100 a. C., obteniendo el dato de 29 000 km para la circunferencia, considerablemente más impreciso respecto a los datos actuales. Este último valor fue el que aceptó Ptolomeo, por lo que prevaleció ese valor en los siglos siguientes.[87]​ Por la Edad Media el astrónomo islámico Al-Biruni utilizó un nuevo método para computar la circunferencia terráquea, obteniendo un valor cercano a los valores modernos.[88] En contraste con sus predecesores, Al-Biruni desarrolló un nuevo método utilizando cálculos trigonométricos basado en el ángulo formado entre un plano y la cima de una montaña, con lo que obtuvo mejores mediciones de la circunferencia terrestre e hizo posible el realizar esta medición desde un solo lugar, por una sola persona.[89]​[90] Desde la cima, divisó el ángulo con el horizonte, lo cual, junto con la altura de la montaña (que había calculado previamente), le permitió calcular la curvatura de la Tierra.[91]​[92] También hizo uso del álgebra para formular ecuaciones trigonométricas y utilizó el astrolabio para medir ángulos.[93]​ Composición química de la corteza[94]​ Compuesto Fórmula Composición Continental Oceánica sílice SiO2 60.2 % 48.6 % alúmina Al2O3 15.2 % 16.5 % cal CaO 5.5 % 12.3 % magnesio MgO 3.1 % 6.8 % óxido de hierro (II) FeO 3.8 % 6.2 % óxido de sodio Na2O 3.0 % 2.6 % óxido de potasio K2O 2.8 % 0.4 % óxido de hierro (III) Fe2O3 2.5 % 2.3 % agua H2O 1.4 % 1.1 % dióxido de carbono CO2 1.2 % 1.4 % óxido de titanio TiO2 0.7 % 1.4 % óxido de fósforo P2O5 0.2 % 0.3 % Total 99.6 % 99.9 % Composición química La masa de la Tierra es aproximadamente de 5.98 × 1024 kg. Se compone principalmente de hierro (32.1 %), oxígeno (30.1 %), silicio (15.1 %), magnesio (13.9 %), azufre (2.9 %), níquel (1.8 %), calcio (1.5 %) y aluminio (1.4 %), con el 1.2 % restante formado por pequeñas cantidades de otros elementos. Debido a la segregación de masa, se cree que la zona del núcleo está compuesta principalmente de hierro (88.8 %), con pequeñas cantidades de níquel (5.8 %), azufre (4.5 %), y menos del 1 % formado por trazas de otros elementos.[95]​ El geoquímico F. W. Clarke (1847-1931), llamado «el padre de la geoquímica por haber determinado la composición de la corteza de la Tierra», calculó que un poco más del 47 % de la corteza terrestre se compone de oxígeno. Los componentes de las rocas más comunes de la corteza de la Tierra son casi todos los óxidos. Cloro, azufre y flúor son las únicas excepciones significativas, y su presencia total en cualquier roca es generalmente mucho menor del 1 %. Los principales óxidos son sílice, alúmina, óxido de hierro, de calcio, de magnesio, potasio a y sodio. La sílice actúa principalmente como un ácido, formando silicatos, y los minerales más comunes de las rocas ígneas son de esta naturaleza. A partir de un cálculo sobre la base de 1672 análisis de todo tipo de rocas, Clarke dedujo que un 99.22 % de las rocas están compuestas por 11 óxidos (véase el cuadro a la derecha). Todos los demás compuestos aparecen solamente en cantidades muy pequeñas.[96]​ Véase también: Abundancia de los elementos en la Tierra Estructura interna Artículo principal: Estructura de la Tierra El interior de la Tierra, al igual que el de los otros planetas terrestres, está dividido en capas según su composición química o sus propiedades físicas (reológicas), pero, a diferencia de los otros planetas terrestres, tiene un núcleo interno y externo distintos. Su capa externa es una corteza de silicato sólido, químicamente diferenciado, bajo la cual se encuentra un manto sólido de alta viscosidad. La corteza está separada del manto por la discontinuidad de Mohorovičić, variando el espesor de la misma desde un promedio de 6 km en los océanos a entre 30 y 50 km en los continentes. La corteza y la parte superior fría y rígida del manto superior se conocen comúnmente como la litosfera, y es de la litosfera de lo que están compuestas las placas tectónicas. Debajo de la litosfera se encuentra la astenosfera, una capa de relativamente baja viscosidad sobre la que flota la litosfera. Dentro del manto, entre los 410 y 660 km bajo la superficie, se producen importantes cambios en la estructura cristalina. Estos cambios generan una zona de transición que separa la parte superior e inferior del manto. Bajo el manto se encuentra un núcleo externo líquido de viscosidad extremadamente baja, descansando sobre un núcleo interno sólido.[97] El núcleo interno puede girar con una velocidad angular ligeramente superior que el resto del planeta, avanzando de 0.1 a 0.5° por año.[98]​ Capas geológicas de la Tierra[99]​ Corte de la Tierra desde el núcleo hasta la exosfera (no está a escala). Profundidad[100]​ km Componentes de las capas Densidad g/cm³ 0-60 Litosfera[nota 9] — 0-35 Corteza[nota 10] 2.2-2.9 35-60 Manto superior 3.4-4.4   35-2890 Manto 3.4-5.6 100-700 Astenosfera — 2890-5100 Núcleo externo 9.9-12.2 5100-6378 Núcleo interno 12.8-13.1 Calor El calor interno de la Tierra proviene de una combinación del calor residual de la acreción planetaria (20 %) y el calor producido por la desintegración radiactiva (80 %).[101] Los isótopos con mayor producción de calor en la Tierra son el potasio-40, el uranio-238, el uranio-235 y el torio-232.[102] En el centro del planeta, la temperatura puede llegar hasta los 7000 K y la presión puede alcanzar los 360 GPa.[103] Debido a que gran parte del calor es proporcionado por la desintegración radiactiva, los científicos creen que en la historia temprana de la Tierra, antes de que los isótopos de reducida vida media se agotaran, la producción de calor de la Tierra fue mucho mayor. Esta producción de calor extra, que hace aproximadamente 3000 millones de años era el doble que la producción actual,[101] pudo haber incrementado los gradientes de temperatura dentro de la Tierra, incrementando la convección del manto y la tectónica de placas, permitiendo la producción de rocas ígneas como las komatitas que no se forman en la actualidad.[104]​ Isótopos actuales de mayor producción de calor[105]​ Isótopo Calor emitido Vatios/kg isótopo Vida media años Concentración media del manto kg isótopo/kg manto Calor emitido W/kg manto 238U 9.46 × 10−5 4.47 × 109 30.8 × 10−9 2.91 × 10−12 235U 5.69 × 10−4 7.04 × 108 0.22 × 10−9 1.25 × 10−13 232Th 2.64 × 10−5 1.40 × 1010 124 × 10−9 3.27 × 10−12 40K 2.92 × 10−5 1.25 × 109 36.9 × 10−9 1.08 × 10−12 El promedio de pérdida de calor de la Tierra es de 87 mW m−2, que supone una pérdida global de 4.42 × 1013 W.[106] Una parte de la energía térmica del núcleo es transportada hacia la corteza por plumas del manto, una forma de convección que consiste en afloramientos de roca a altas temperaturas. Estas plumas pueden producir puntos calientes y coladas de basalto.[107] La mayor parte del calor que pierde la Tierra se filtra entre las placas tectónicas, en las surgencias del manto asociadas a las dorsales oceánicas. Casi todas las pérdidas restantes se producen por conducción a través de la litosfera, principalmente en los océanos, ya que allí la corteza es mucho más delgada que en los continentes.[108]​ Placas tectónicas Artículo principal: Tectónica de placas Placas tectónicas[109]​ Muestra de la extensión y los límites de las placas tectónicas, con superposición de contornos en los continentes que se apoyan Nombre de la placa Área 106 km² color #FB9B7A Placa Africana[nota 8]​ 78.0 color #8A9BBE Placa Antártica 60.9           Placa Indoaustraliana 47.2 color #7FA172 Placa Euroasiática 67.8 color #AC8D7F Placa Norteamericana 75.9 color #AD82B0 Placa Sudamericana 43.6 color #FEE6AA Placa Pacífica 103.3 La mecánicamente rígida capa externa de la Tierra, la litosfera, está fragmentada en piezas llamadas placas tectónicas. Estas placas son elementos rígidos que se mueven en relación uno con otro siguiendo uno de estos tres patrones: bordes convergentes, en los que dos placas se aproximan; bordes divergentes, en los que dos placas se separan, y bordes transformantes, en los que dos placas se deslizan lateralmente entre sí. A lo largo de estos bordes de placa se producen los terremotos, la actividad volcánica, la formación de montañas y la formación de fosas oceánicas.[110] Las placas tectónicas se deslizan sobre la parte superior de la astenosfera, la sólida pero menos viscosa sección superior del manto, que puede fluir y moverse junto con las placas,[111] y cuyo movimiento está fuertemente asociado a los patrones de convección dentro del manto terrestre. A medida que las placas tectónicas migran a través del planeta, el fondo oceánico se subduce bajo los bordes de las placas en los límites convergentes. Al mismo tiempo, el afloramiento de material del manto en los límites divergentes crea las dorsales oceánicas. La combinación de estos procesos recicla continuamente la corteza oceánica nuevamente en el manto. Debido a este proceso de reciclaje, la mayor parte del suelo marino tiene menos de 100 millones de años de edad. La corteza oceánica más antigua se encuentra en el Pacífico Occidental, y tiene una edad estimada de unos 200 millones de años.[112]​[113] En comparación, la corteza continental más antigua registrada tiene 4030 millones de años de edad.[114]​ Las siete placas más grandes son la Pacífica, Norteamericana, Euroasiática, Africana Antártica, Indoaustraliana y Sudamericana. Otras placas notables son la placa Índica, la placa arábiga, la placa del Caribe, la placa de Nazca en la costa occidental de América del Sur y la placa Escocesa en el sur del océano Atlántico. La placa de Australia se fusionó con la placa de la India hace entre 50 y 55 millones de años. Las placas con movimiento más rápido son las placas oceánicas, con la placa de Cocos avanzando a una velocidad de 75 mm/año[115] y la placa del Pacífico moviéndose 52-69 mm/año. En el otro extremo, la placa con movimiento más lento es la placa eurasiática, que avanza a una velocidad típica de aproximadamente 21 mm/año.[116]​ Superficie Histograma de elevación de la corteza terrestre. Artículos principales: Superficie terrestre, Accidente geográfico y Anexo:Puntos extremos del mundo. El relieve de la Tierra varía enormemente de un lugar a otro. Cerca del 70.8 %[117] de la superficie está cubierta por agua, con gran parte de la plataforma continental por debajo del nivel del mar. La superficie sumergida tiene características montañosas, incluyendo un sistema de dorsales oceánicas, así como volcanes submarinos,[69] fosas oceánicas, cañones submarinos, mesetas y llanuras abisales. El restante 29.2 % no cubierto por el agua se compone de montañas, desiertos, llanuras, mesetas y otras geomorfologías. La superficie del planeta se moldea a lo largo de períodos de tiempo geológicos, debido a la erosión tectónica. Las características de esta superficie formada o deformada mediante la tectónica de placas están sujetas a una constante erosión a causa de las precipitaciones, los ciclos térmicos y los efectos químicos. La glaciación, la erosión costera, la acumulación de los arrecifes de coral y los grandes impactos de meteoritos[118] también actúan para remodelar el paisaje. Altimetría y batimetría actual. Datos del Modelo Digital de Terreno del National Geophysical Data Center de Estados Unidos. La corteza continental se compone de material de menor densidad, como las rocas ígneas, el granito y la andesita. Menos común es el basalto, una densa roca volcánica que es el componente principal de los fondos oceánicos.[119] Las rocas sedimentarias se forman por la acumulación de sedimentos compactados. Casi el 75 % de la superficie continental está cubierta por rocas sedimentarias, a pesar de que estas solo forman un 5 % de la corteza.[120] El tercer material rocoso más abundante en la Tierra son las rocas metamórficas, creadas a partir de la transformación de tipos de roca ya existentes mediante altas presiones, altas temperaturas, o ambas. Los minerales de silicato más abundantes en la superficie de la Tierra incluyen el cuarzo, los feldespatos, el anfíbol, la mica, el piroxeno y el olivino.[121] Los minerales de carbonato más comunes son la calcita (que se encuentra en piedra caliza) y la dolomita.[122]​ La pedosfera es la capa más externa de la Tierra. Está compuesta de tierra y está sujeta a los procesos de formación del suelo. Existe en el encuentro entre la litosfera, la atmósfera, la hidrosfera y la biosfera. Actualmente el 13.31 % del total de la superficie terrestre es tierra cultivable, y solo el 4.71 % soporta cultivos permanentes.[8] Cerca del 40 % de la superficie emergida se utiliza actualmente como tierras de cultivo y pastizales, estimándose un total de 1.3 × 107 km² para tierras de cultivo y 3.4 × 107 km² para tierras de pastoreo.[123]​ La elevación de la superficie terrestre varía entre el punto más bajo de −418 m en el mar Muerto a una altitud máxima, estimada en 2005, de 8848 m en la cima del monte Everest. La altura media de la tierra sobre el nivel del mar es de 840 m.[124]​ Imágenes satelitales de la Tierra Planisferio terrestre (composición de fotos satelitales). El satélite ambiental Envisat de la ESA desarrolló un retrato detallado de la superficie de la Tierra. A través del proyecto GLOBCOVER se desarrolló la creación de un mapa global de la cobertura terrestre con una resolución tres veces superior a la de cualquier otro mapa por satélite hasta aquel momento. Utilizó reflectores radar con antenas de ancho sintéticas, capturando con sus sensores la radiación reflejada.[125]​ La NASA completó un nuevo mapa tridimensional, que es la topografía más precisa del planeta, elaborada durante cuatro años con los datos transmitidos por el transbordador espacial Endeavour. Los datos analizados corresponden al 80 % de la masa terrestre. Cubre los territorios de Australia y Nueva Zelanda con detalles sin precedentes. También incluye más de mil islas de la Polinesia y la Melanesia en el Pacífico sur, así como islas del Índico y el Atlántico. Muchas de esas islas apenas se levantan unos metros sobre el nivel del mar y son muy vulnerables a los efectos de las marejadas y tormentas, por lo que su conocimiento ayudará a evitar catástrofes; los datos proporcionados por la misión del Endeavour tendrán una amplia variedad de usos, como la exploración virtual del planeta.[126]​ Véase también: Cartografía Hidrosfera Los océanos poseen el mayor volumen de agua en la Tierra. Artículo principal: Hidrosfera La abundancia de agua en la superficie de la Tierra es una característica única que distingue al «Planeta Azul» de otros en el sistema solar. La hidrosfera de la Tierra está compuesta fundamentalmente por océanos, pero técnicamente incluye todas las superficies de agua en el mundo, incluidos los mares interiores, lagos, ríos y aguas subterráneas hasta una profundidad de 2000 m. El lugar más profundo bajo el agua es el abismo Challenger de la fosa de las Marianas, en el océano Pacífico, con una profundidad de −10 911.4 m.[nota 11]​[127]​ La masa de los océanos es de aproximadamente 1.35 × 1018 toneladas métricas, o aproximadamente 1/4400 de la masa total de la Tierra. Los océanos cubren un área de 361.84 × 106 km² con una profundidad media de 3682.2 m, lo que resulta en un volumen estimado de 1.3324 × 109 km³.[128] Si se nivelase toda la superficie terrestre, el agua cubriría la superficie del planeta hasta una altura de más de 2.7 km. El área total de la Tierra es de 5.1 × 108 km². Para la primera aproximación, la profundidad media sería la relación entre los dos, o de 2.7 km. Aproximadamente el 97.5 % del agua es salada, mientras que el restante 2.5 % es agua dulce. La mayor parte del agua dulce, aproximadamente el 68.7 %, se encuentra actualmente en estado de hielo.[129]​ La salinidad media de los océanos es de unos 35 gramos de sal por kilogramo de agua (35 ‰).[130] La mayor parte de esta sal fue liberada por la actividad volcánica, o extraída de las rocas ígneas ya enfriadas.[131] Los océanos son también un reservorio de gases atmosféricos disueltos, siendo estos esenciales para la supervivencia de muchas formas de vida acuática.[132] El agua de los océanos tiene una influencia importante sobre el clima del planeta, actuando como un foco calórico de gran tamaño.[133] Los cambios en la distribución de la temperatura oceánica pueden causar alteraciones climáticas, tales como la Oscilación del Sur, El Niño.[134]​ Atmósfera Artículo principal: Atmósfera terrestre La presión atmosférica media al nivel del mar se sitúa en torno a los 101.325 kPa, con una escala de altura de aproximadamente 8.5 km.[1] Está compuesta principalmente de un 78 % de nitrógeno y un 21 % de oxígeno, con trazas de vapor de agua, dióxido de carbono y otras moléculas gaseosas. La altura de la troposfera varía con la latitud, entre 8 km en los polos y 17 km en el ecuador, con algunas variaciones debido a la climatología y los factores estacionales.[135]​ La biosfera de la Tierra ha alterado significativamente la atmósfera. La fotosíntesis oxigénica evolucionó hace 2700 millones de años, formando principalmente la atmósfera actual de nitrógeno-oxígeno. Este cambio permitió la proliferación de los organismos aeróbicos, así como la formación de la capa de ozono que bloquea la radiación ultravioleta proveniente del Sol, permitiendo la vida fuera del agua. Otras funciones importantes de la atmósfera para la vida en la Tierra incluyen el transporte de vapor de agua, proporcionar gases útiles, quemar los meteoritos pequeños antes de que alcancen la superficie, y moderar la temperatura.[136] Este último fenómeno se conoce como el efecto invernadero: trazas de moléculas presentes en la atmósfera capturan la energía térmica emitida desde el suelo, aumentando así la temperatura media. El dióxido de carbono, el vapor de agua, el metano y el ozono son los principales gases de efecto invernadero de la atmósfera de la Tierra. Sin este efecto de retención del calor, la temperatura superficial media sería de −18 °C y la vida probablemente no existiría.[117]​ Clima y tiempo atmosférico Artículos principales: Clima y Tiempo atmosférico. Imagen satelital de la nubosidad de la Tierra usando el espectroradiómetro de imágenes de media resolución de la NASA. La atmósfera terrestre no tiene unos límites definidos, haciéndose poco a poco más delgada hasta desvanecerse en el espacio exterior. Tres cuartas partes de la masa atmosférica están contenidas dentro de los primeros 11 km de la superficie del planeta. Esta capa inferior se llama troposfera. La energía del Sol calienta esta capa y la superficie bajo esta, causando la expansión del aire. El aire caliente se eleva debido a su menor densidad, siendo sustituido por aire de mayor densidad, es decir, aire más frío. Esto da como resultado la circulación atmosférica que genera el tiempo y el clima a través de la redistribución de la energía térmica.[137]​ Las líneas principales de circulación atmosférica las constituyen los vientos alisios en la región ecuatorial por debajo de los 30° de latitud, y los vientos del oeste en latitudes medias entre los 30° y 60°.[138] Las corrientes oceánicas también son factores importantes para determinar el clima, especialmente la circulación termohalina que distribuye la energía térmica de los océanos ecuatoriales a las regiones polares.[139]​ El vapor de agua generado a través de la evaporación superficial es transportado según los patrones de circulación de la atmósfera. Cuando las condiciones atmosféricas permiten la elevación del aire caliente y húmedo, el agua se condensa y se deposita en la superficie en forma de precipitaciones.[137] La mayor parte del agua es transportada a altitudes más bajas mediante los sistemas fluviales y por lo general regresa a los océanos o es depositada en los lagos. Este ciclo del agua es un mecanismo vital para sustentar la vida en la tierra y es un factor primario de la erosión que modela la superficie terrestre a lo largo de períodos geológicos. Los patrones de precipitación varían enormemente, desde varios metros de agua por año a menos de un milímetro. La circulación atmosférica, las características topológicas y las diferencias de temperatura determinan las precipitaciones medias de cada región.[140]​ La cantidad de energía solar que llega a la Tierra disminuye al aumentar la latitud. En las latitudes más altas la luz solar incide en la superficie en un ángulo menor, teniendo que atravesar gruesas columnas de atmósfera. Como resultado, la temperatura media anual del aire a nivel del mar se reduce en aproximadamente 0.4 °C por cada grado de latitud alejándose del ecuador.[141] La Tierra puede ser subdividida en franjas latitudinales más o menos homogéneas con un clima específico. Desde el ecuador hasta las regiones polares, se encuentran la zona intertropical (o ecuatorial), el clima subtropical, el clima templado y los climas polares.[142] El clima también puede ser clasificado en función de la temperatura y las precipitaciones, en regiones climáticas caracterizadas por masas de aire bastante uniformes. La metodología de clasificación más usada es la clasificación climática de Köppen (modificada por el estudiante de Wladimir Peter Köppen, Rudolph Geiger), que cuenta con cinco grandes grupos (zonas tropicales húmedas, zonas áridas, zonas húmedas con latitud media, clima continental y frío polar), que se dividen en subtipos más específicos.[138]​ Atmósfera superior Imagen de la NASA en la que se observa la Luna parcialmente oscurecida y deformada por la refracción atmosférica. Artículo principal: Espacio exterior Por encima de la troposfera, la atmósfera suele dividir en estratosfera, mesosfera y termosfera.[136] Cada capa tiene un gradiente adiabático diferente, que define la tasa de cambio de la temperatura con respecto a la altura. Más allá de éstas se encuentra la exosfera, que se atenúa hasta penetrar en la magnetosfera, donde los campos magnéticos de la Tierra interactúan con el viento solar.[143] Dentro de la estratosfera se encuentra la capa de ozono; un componente que protege parcialmente la superficie terrestre de la luz ultravioleta, siendo un elemento importante para la vida en la Tierra. La línea de Kármán, definida en los 100 km sobre la superficie de la Tierra, es una definición práctica usada para establecer el límite entre la atmósfera y el espacio.[144]​ La energía térmica hace que algunas de las moléculas en el borde exterior de la atmósfera de la Tierra incrementen su velocidad hasta el punto de poder escapar de la gravedad del planeta. Esto da lugar a una pérdida lenta pero constante de la atmósfera hacia el espacio. Debido a que el hidrógeno no fijado tiene un bajo peso molecular puede alcanzar la velocidad de escape más fácilmente, escapando así al espacio exterior a un ritmo mayor que otros gases.[145] La pérdida de hidrógeno hacia el espacio contribuye a la transformación de la Tierra desde su inicial estado reductor a su actual estado oxidante. La fotosíntesis proporcionó una fuente de oxígeno libre, pero se cree que la pérdida de agentes reductores como el hidrógeno fue una condición previa necesaria para la acumulación generalizada de oxígeno en la atmósfera.[146] Por tanto, la capacidad del hidrógeno para escapar de la atmósfera de la Tierra puede haber influido en la naturaleza de la vida desarrollada en el planeta.[147] En la atmósfera actual, rica en oxígeno, la mayor parte del hidrógeno se convierte en agua antes de tener la oportunidad de escapar. En cambio, la mayor parte de la pérdida de hidrógeno actual proviene de la destrucción del metano en la atmósfera superior.[148]​ Campo magnético Diagrama que muestra las líneas del campo magnético de la magnetosfera de la Tierra. Las líneas son arrastradas de vuelta en el sentido contrario a las solares bajo la influencia del viento solar. Esquema de la magnetosfera de la Tierra. Los flujos de viento solar, de izquierda a derecha Artículo principal: Campo magnético terrestre El campo magnético de la Tierra tiene una forma similar a un dipolo magnético, con los polos actualmente localizados cerca de los polos geográficos del planeta. En el ecuador del campo magnético (ecuador magnético), la fuerza del campo magnético en la superficie es 3.05 × 10−5T, con un momento magnético dipolar global de 7.91 × 1015 T m³.[149] Según la teoría del dínamo, el campo se genera en el núcleo externo fundido, región donde el calor crea movimientos de convección en materiales conductores, generando corrientes eléctricas. Estas corrientes inducen a su vez el campo magnético de la Tierra. Los movimientos de convección en el núcleo son caóticos; los polos magnéticos se mueven y periódicamente cambian de orientación. Esto da lugar a reversiones geomagnéticas a intervalos de tiempo irregulares, unas pocas veces cada millón de años. La inversión más reciente tuvo lugar hace aproximadamente 700 000 años.[150]​[151]​ El campo magnético forma la magnetosfera, que desvía las partículas de viento solar. En dirección al Sol, el arco de choque entre el viento solar y la magnetosfera se encuentra a unas 13 veces el radio de la Tierra. La colisión entre el campo magnético y el viento solar forma los cinturones de radiación de Van Allen; un par de regiones concéntricas, con forma tórica, formadas por partículas cargadas muy energéticas. Cuando el plasma entra en la atmósfera de la Tierra por los polos magnéticos se crean las auroras polares.[152]​ Rotación y órbita Rotación Inclinación del eje de la Tierra (u oblicuidad) y su relación con el eje de rotación y el plano orbital. Artículo principal: Rotación de la Tierra El período de rotación de la Tierra con respecto al Sol, es decir, un día solar, es de alrededor de 86 400 segundos de tiempo solar (86 400.0025 segundos SIU).[153] El día solar de la Tierra es ahora un poco más largo de lo que era durante el siglo XIX debido a la aceleración de marea, los días duran entre 0 y 2 ms SIU más.[154]​[155]​ La rotación de la Tierra fotografiada por DSCOVR EPIC el 29 de mayo de 2016, unas semanas antes del solsticio. El período de rotación de la Tierra en relación con las estrellas fijas, llamado día estelar por el Servicio Internacional de Rotación de la Tierra y Sistemas de Referencia (IERS por sus siglas en inglés), es de 86 164.098903691 segundos del tiempo solar medio (UT1), o de 23h 56m 4.098903691s.[4]​[nota 12] El período de rotación de la Tierra en relación con el equinoccio vernal, mal llamado el día sidéreo, es de 86 164.09053083288 segundos del tiempo solar medio (UT1) (23h 56m 4.09053083288s).[4] Por tanto, el día sidéreo es más corto que el día estelar en torno a 8.4 ms.[156] La longitud del día solar medio en segundos SIU está disponible en el IERS para los períodos 1623-2005[157] y 1962-2005.[158]​ Aparte de los meteoros en la atmósfera y de los satélites en órbita baja, el movimiento aparente de los cuerpos celestes vistos desde la Tierra se realiza hacia al oeste, a una velocidad de 15°/h = 15′/min. Para las masas cercanas al ecuador celeste, esto es equivalente a un diámetro aparente del Sol o de la Luna cada dos minutos (desde la superficie del planeta, los tamaños aparentes del Sol y de la Luna son aproximadamente iguales).[159]​[160]​ Órbita Artículo principal: Traslación de la Tierra Galaxia espiral barrada Ilustración de la galaxia Vía Láctea, mostrando la posición del Sol La Tierra orbita alrededor del Sol a una distancia media de unos 150 millones de kilómetros, completando una órbita cada 365.2564 días solares, o un año sideral. Desde la Tierra, esto genera un movimiento aparente del Sol hacia el este, desplazándose con respecto a las estrellas a un ritmo de alrededor de 1°/día, o un diámetro del Sol o de la Luna cada 12 horas. Debido a este movimiento, en promedio la Tierra tarda 24 horas (un día solar) en completar una rotación sobre su eje hasta que el sol regresa al meridiano. La velocidad orbital de la Tierra es de aproximadamente 29.8 km/s (107 000 km/h), que es lo suficientemente rápida como para recorrer el diámetro del planeta (12 742 km) en siete minutos, o la distancia entre la Tierra y la Luna (384 000 km) en cuatro horas.[1]​ La Luna gira con la Tierra en torno a un baricentro común, debido a que este se encuentra dentro de la Tierra, a 4541 km de su centro, el sistema Tierra-Luna no es un planeta doble, la Luna completa un giro cada 27.32 días con respecto a las estrellas de fondo. Cuando se combina con la revolución común del sistema Tierra-Luna alrededor del Sol, el período del mes sinódico, desde una luna nueva a la siguiente, es de 29.53 días. Visto desde el polo norte celeste, el movimiento de la Tierra, la Luna y sus rotaciones axiales son todas contrarias a la dirección de las manecillas del reloj (sentido antihorario). Visto desde un punto de vista situado sobre los polos norte del Sol y la Tierra, la Tierra parecería girar en sentido antihorario alrededor del Sol. Los planos orbitales y axiales no están alineados: El eje de la Tierra está inclinado unos 23.4 grados con respecto a la perpendicular al plano Tierra-Sol, y el plano entre la Tierra y la Luna está inclinado unos 5 grados con respecto al plano Tierra-Sol. Sin esta inclinación, habría un eclipse cada dos semanas, alternando entre los eclipses lunares y eclipses solares.[1]​[161]​ La esfera de Hill, o la esfera de influencia gravitatoria, de la Tierra tiene aproximadamente 1.5 Gm (o 1 500 000 kilómetros) de radio.[162]​[nota 13] Esta es la distancia máxima en la que la influencia gravitatoria de la Tierra es más fuerte que la de los más distantes Sol y resto de planetas. Los objetos deben orbitar la Tierra dentro de este radio, o terminarán atrapados por la perturbación gravitatoria del Sol. Desde el año de 1772, se estableció que cuerpos pequeños pueden orbitar de manera estable la misma órbita que un planeta, si esta permanece cerca de un punto triangular de Lagrange (también conocido como «punto troyano») los cuales están situados 60° delante y 60° detrás del planeta en su órbita. La Tierra es el cuarto planeta con un asteroide troyano (2010 TK7) después de Júpiter, Marte y Neptuno de acuerdo a la fecha de su descubrimiento[nota 14] Este fue difícil de localizar debido al posicionamiento geométrico de la observación, este fue descubierto en 2010 gracias al telescopio WISE (Wide-Field Infrared Survey Explorer) de la NASA, pero fue en abril de 2011 con el telescopio «Canadá-Francia-Hawái» cuando se confirmó su naturaleza troyana,[165] y se estima que su órbita permanezca estable dentro de los próximos 10 000 años.[166]​ La Tierra, junto con el sistema solar, está situada en la galaxia Vía Láctea, orbitando a alrededor de 28 000 años luz del centro de la galaxia. En la actualidad se encuentra unos 20 años luz por encima del plano ecuatorial de la galaxia, en el brazo espiral de Orión.[167]​ Estaciones e inclinación axial Artículo principal: Oblicuidad de la eclíptica Las estaciones se producen en la Tierra debido a la inclinación de su eje de rotación respecto al plano definido por su órbita (de la eclíptica). En la ilustración es invierno en el hemisferio norte y verano en el hemisferio sur. (La distancia y el tamaño entre los cuerpos no está a escala). Debido a la inclinación del eje de la Tierra, la cantidad de luz solar que llega a un punto cualquiera en la superficie varía a lo largo del año. Esto ocasiona los cambios estacionales en el clima, siendo verano en el hemisferio norte ocurre cuando el Polo Norte está apuntando hacia el Sol, e invierno cuando apunta en dirección opuesta. Durante el verano, el día tiene una duración más larga y la luz solar incide más perpendicularmente en la superficie. Durante el invierno, el clima se vuelve más frío y los días más cortos. En la zona del círculo polar ártico se da el caso extremo de no recibir luz solar durante una parte del año; fenómeno conocido como la noche polar. En el hemisferio sur se da la misma situación pero de manera inversa, con la orientación del Polo Sur opuesta a la dirección del Polo Norte. Espacio oscuro con la Tierra creciente a menor Luna izquierda, media luna en la parte superior derecha, el 30 % del diámetro aparente de la Tierra, cinco veces el diámetro aparente distancia entre la Tierra en la parte izquierda baja, la Luna creciente en la esquina superior derecha, el diámetro aparente de la Tierra es del 30 %; cinco veces el diámetro aparente entre la Tierra desde el espacio; la luz solar proveniente del lado derecho. La Tierra y la Luna vistas desde Marte, imagen del Mars Reconnaissance Orbiter. Desde el espacio, la Tierra puede verse en fases similares a las fases lunares. Por convenio astronómico, las cuatro estaciones están determinadas por solsticios (puntos de la órbita en los que el eje de rotación terrestre alcanza la máxima inclinación hacia el Sol —solsticio de verano— o hacia el lado opuesto —solsticio de invierno—) y por equinoccios, cuando la inclinación del eje terrestre es perpendicular a la dirección del Sol. En el hemisferio norte, el solsticio de invierno se produce alrededor del 21 de diciembre, el solsticio de verano el 21 de junio, el equinoccio de primavera el 20 de marzo y el equinoccio de otoño el 23 de septiembre. En el hemisferio sur la situación se invierte, con el verano y los solsticios de invierno en fechas contrarias a la del hemisferio norte. De igual manera sucede con el equinoccio de primavera y de otoño.[168]​ El ángulo de inclinación de la Tierra es relativamente estable durante largos períodos de tiempo. Sin embargo, la inclinación se somete a nutaciones; un ligero movimiento irregular, con un período de 18.6 años.[169] La orientación (en lugar del ángulo) del eje de la Tierra también cambia con el tiempo, precesando un círculo completo en cada ciclo de 25 800 años. Esta precesión es la razón de la diferencia entre el año sidéreo y el año tropical. Ambos movimientos son causados por la atracción variante del Sol y la Luna sobre el abultamiento ecuatorial de la Tierra. Desde la perspectiva de la Tierra, los polos también migran unos pocos metros sobre la superficie. Este movimiento polar tiene varios componentes cíclicos, que en conjunto reciben el nombre de movimientos cuasiperiódicos. Además del componente anual de este movimiento, existe otro movimiento con ciclos de 14 meses llamado el bamboleo de Chandler. La velocidad de rotación de la Tierra también varía en un fenómeno conocido como variación de duración del día.[170]​ En tiempos modernos, el perihelio de la Tierra se produce alrededor del 3 de enero y el afelio alrededor del 4 de julio. Sin embargo, estas fechas cambian con el tiempo debido a la precesión orbital y otros factores, que siguen patrones cíclicos conocidos como ciclos de Milankovitch. La variación de la distancia entre la Tierra y el Sol resulta en un aumento de alrededor del 6.9 %[nota 15] de la energía solar que llega a la Tierra en el perihelio en relación con el afelio. Puesto que el hemisferio sur está inclinado hacia el Sol en el momento en que la Tierra alcanza la máxima aproximación al Sol, a lo largo del año el hemisferio sur recibe algo más de energía del Sol que el hemisferio norte. Sin embargo, este efecto es mucho menos importante que el cambio total de energía debido a la inclinación del eje, y la mayor parte de este exceso de energía es absorbido por la superficie oceánica, que se extiende en mayor proporción en el hemisferio sur.[171]​ Satélite natural y otros elementos orbitales Características Diámetro 3474.8 km Masa 7.349 × 1022 kg Semieje mayor 384 400 km Periodo orbital 27 d 7 h 43.7 m Luna Artículos principales: Luna y Sistema Tierra-Luna. La Luna es el satélite natural de la Tierra. Es un cuerpo del tipo terrestre relativamente grande: con un diámetro de alrededor de la cuarta parte del de la Tierra, es el segundo satélite más grande del sistema solar en relación con el tamaño de su planeta, después del satélite Caronte de su planeta enano Plutón. Los satélites naturales que orbitan los demás planetas se denominan «lunas» en referencia a la Luna de la Tierra. Detalles del sistema Tierra-Luna. Además del radio de cada objeto, de la distancia entre ellos, y de la inclinación del eje de cada uno, se muestra la distancia del baricentro del sistema Tierra-Luna al centro de la Tierra (4641 km). Imágenes Archivado el 1 de noviembre de 2011 en Wayback Machine. e información de la NASA. El eje de la Luna se localiza por la tercera ley de Cassini. La atracción gravitatoria entre la Tierra y la Luna causa las mareas en la Tierra. El mismo efecto en la Luna ha dado lugar a su acoplamiento de marea, lo que significa que su período de rotación es idéntico a su periodo de traslación alrededor de la Tierra. Como resultado, la luna siempre presenta la misma cara hacia nuestro planeta. A medida que la Luna orbita la Tierra, diferentes partes de su cara son iluminadas por el Sol, dando lugar a las fases lunares. La parte oscura de la cara está separada de la parte iluminada del terminador solar. Debido a la interacción de las mareas, la Luna se aleja de la Tierra a una velocidad de aproximadamente 38 mm al año. Acumuladas durante millones de años, estas pequeñas modificaciones, así como el alargamiento del día terrestre en alrededor de 23 µs, han producido cambios significativos.[172] Durante el período devónico, por ejemplo, (hace aproximadamente 410 millones de años) un año tenía 400 días, cada uno con una duración de 21.8 horas.[173]​ Secuencia de imágenes que muestran la rotación de la Tierra y la traslación de la Luna vistas desde la sonda espacial Galileo. La Luna pudo haber afectado dramáticamente el desarrollo de la vida, moderando el clima del planeta. Evidencias paleontológicas y simulaciones computarizadas muestran que la inclinación del eje terrestre está estabilizada por las interacciones de marea con la Luna.[174] Algunos teóricos creen que sin esta estabilización frente al momento ejercido por el Sol y los planetas sobre la protuberancia ecuatorial de la Tierra, el eje de rotación podría ser caóticamente inestable, mostrando cambios caóticos durante millones de años, como parece ser el caso de Marte.[175]​ Vista desde la Tierra, la Luna está justo a una distancia que la hace que el tamaño aparente de su disco sea casi idéntico al del Sol. El diámetro angular (o ángulo sólido) de estos dos cuerpos coincide porque aunque el diámetro del Sol es unas 400 veces más grande que el de la Luna, también está 400 veces más distante.[160] Esto permite que en la Tierra se produzcan los eclipses solares totales y anulares. La teoría más ampliamente aceptada sobre el origen de la Luna, la teoría del gran impacto, afirma que esta se formó por la colisión de un protoplaneta del tamaño de Marte, llamado Tea, con la Tierra primitiva. Esta hipótesis explica (entre otras cosas) la relativa escasez de hierro y elementos volátiles en la Luna, y el hecho de que su composición sea casi idéntica a la de la corteza terrestre.[176]​ Representación a escala del tamaño y distancia relativa entre la Tierra y la Luna. Representación a escala del tamaño y distancia relativa entre la Tierra y la Luna. Otros elementos orbitales A fecha de 2016, el planeta Tierra tiene nueve cuasisatélites naturales o asteroides coorbitales conocidos: el (3753) Cruithne, el 2002 AA29,[177]​[178] 2003 YN107, 2004 GU9,[179] 2006 FV35, 2010 SO16[180] 2013 LX28, 2014 OL339 y 2016 HO3.[181] El 15 de febrero de 2020 se descubrió que 2020 CD3 es un satélite natural temporal terrestre. A fecha de septiembre de 2021, existen 4550 satélites operativos creados por el hombre orbitando la Tierra.[5]​ Localización de la Tierra Artículo principal: Anexo:Localización de la Tierra en el universo Diagrama de nuestra ubicación dentro del universo observable. (Click aquí para ver en pantalla completa.) Habitabilidad Artículo principal: Habitabilidad planetaria Un planeta que pueda sostener vida se denomina habitable, incluso aunque en él no se originara vida. La Tierra proporciona las (actualmente entendidas como) condiciones necesarias, tales como el agua líquida, un ambiente que permite el ensamblaje de moléculas orgánicas complejas, y la energía suficiente para mantener un metabolismo.[182] Hay otras características que se cree que también contribuyen a la capacidad del planeta para originar y mantener la vida: la distancia entre la Tierra y el Sol, así como su excentricidad orbital, la velocidad de rotación, la inclinación axial, la historia geológica, la permanencia de la atmósfera, y la protección ofrecida por el campo magnético.[183]​ Biosfera Artículo principal: Biosfera Se denomina «biosfera» al conjunto de los diferentes tipos de vida del planeta junto con su entorno físico, modificado por la presencia de los primeros. Generalmente se entiende que la biosfera empezó a evolucionar hace 3500 millones de años. La Tierra es el único lugar donde se sabe que existe vida. La biosfera se divide en una serie de biomas, habitados por plantas y animales esencialmente similares. En tierra, los biomas se separan principalmente por las diferencias en latitud, la altitud sobre el nivel del mar y la humedad. Los biomas terrestres situados en los círculos ártico o antártico, en gran altura o en zonas extremadamente áridas son relativamente estériles de vida vegetal y animal; la diversidad de especies alcanza su máximo en tierras bajas y húmedas, en latitudes ecuatoriales.[184]​ Recursos naturales y uso de la tierra Artículo principal: Recurso natural La Tierra proporciona recursos que son explotados por los seres humanos con diversos fines. Algunos de estos son recursos no renovables, tales como los combustibles fósiles, que son difícilmente renovables a corto plazo. De la corteza terrestre se obtienen grandes depósitos de combustibles fósiles, consistentes en carbón, petróleo, gas natural y clatratos de metano. Estos depósitos son utilizados por los seres humanos para la producción de energía, y también como materia prima para la producción de sustancias químicas. Los cuerpos minerales también se han formado en la corteza terrestre a través de distintos procesos de mineralogénesis, como consecuencia de la erosión y de los procesos implicados en la tectónica de placas.[185] Estos cuerpos albergan fuentes concentradas de varios metales y otros elementos útiles. La biosfera de la Tierra produce muchos productos biológicos útiles para los seres humanos, incluyendo (entre muchos otros) alimentos, madera, fármacos, oxígeno, y el reciclaje de muchos residuos orgánicos. El ecosistema terrestre depende de la capa superior del suelo y del agua dulce, y el ecosistema oceánico depende del aporte de nutrientes disueltos desde tierra firme.[186] Los seres humanos también habitan la tierra usando materiales de construcción para construir refugios. Para 1993, el aprovechamiento de la tierra por los humanos era de aproximadamente: Uso de la tierra Tierra cultivable Cultivos permanentes Pastos permanentes Bosques y tierras arboladas Áreas urbanas Otros Porcentaje 13.13 %[8] 4.71 %[8] 26 % 32 % 1.5 % 30 % La cantidad de tierras de regadío en 1993 se estimaban en 2 481 250 km².[8]​ Medio ambiente y riesgos Grandes áreas de la superficie de la Tierra están sujetas a condiciones climáticas extremas, tales como ciclones tropicales, huracanes, o tifones que dominan la vida en esas zonas. Muchos lugares están sujetos a terremotos, deslizamientos, tsunamis, erupciones volcánicas, tornados, dolinas, ventiscas, inundaciones, sequías y otros desastres naturales. Muchas áreas concretas están sujetas a la contaminación causada por el hombre del aire y del agua, a la lluvia ácida, a sustancias tóxicas, a la pérdida de vegetación (sobrepastoreo, deforestación, desertificación), a la pérdida de vida salvaje, la extinción de especies, la degradación del suelo y su agotamiento, a la erosión y a la introducción de especies invasoras. Según las Naciones Unidas, existe un consenso científico que vincula las actividades humanas con el calentamiento global, debido a las emisiones industriales de dióxido de carbono y el calor residual antropogénico. Se prevé que esto produzca cambios tales como el derretimiento de los glaciares y superficies heladas, temperaturas más extremas, cambios significativos en el clima y un aumento global del nivel del mar.[187]​[188]​[189]​ Geografía humana Artículo principal: Geografía humana La cartografía —el estudio y práctica de la elaboración de mapas—, y subsidiariamente la geografía, han sido históricamente las disciplinas dedicadas a describir la Tierra. La topografía o determinación de lugares y distancias, y en menor medida la navegación, o determinación de la posición y de la dirección, se han desarrollado junto con la cartografía y la geografía, suministrando y cuantificando la información necesaria. La Tierra tiene aproximadamente 8200 millones de habitantes (según datos a julio de 2024).[190] Las proyecciones indicaban que la población humana mundial llegaría a 7000 millones a principios de 2012, pero esta cifra fue superada a mediados de octubre de 2011[191] y se espera llegar a 10 300 millones en 2080.[190] Se piensa que la mayor parte de este crecimiento tendrá lugar en los países en vías de desarrollo. La región del África subsahariana tiene la tasa de natalidad más alta del mundo. La densidad de población varía mucho en las distintas partes del mundo, pero la mayoría de la población vive en Asia. Está previsto que para el año 2020 el 60 % de la población mundial se concentre en áreas urbanas, frente al 40 % en áreas rurales.[192]​ Se estima que solamente una octava parte de la superficie de la Tierra es apta para su ocupación por los seres humanos; tres cuartas partes está cubierta por océanos, y la mitad de la superficie terrestre es: desierto (14 %),[193] alta montaña (27 %),[194] u otros terrenos menos adecuados. El asentamiento permanente más septentrional del mundo es Alert, en la Isla de Ellesmere en Nunavut, Canadá.[195] (82°28′N). El más meridional es la Base Amundsen-Scott, en la Antártida, casi exactamente en el Polo Sur. (90°S) La Tierra de noche. Imagen compuesta a partir de los datos de iluminación del DMSP/OLS, representando una imagen simulada del mundo de noche. Esta imagen no es fotográfica y muchas características son más brillantes de lo que le parecería a un observador directo. Las naciones soberanas independientes reclaman la totalidad de la superficie de tierra del planeta, a excepción de algunas partes de la Antártida y la zona no reclamada de Bir Tawil entre Egipto y Sudán. En el año 2011 existen 204 Estados soberanos, incluidos los 192 Estados miembros de las Naciones Unidas. Hay también 59 territorios dependientes, y una serie de áreas autónomas, territorios en disputa y otras entidades.[8] Históricamente, la Tierra nunca ha tenido un gobierno soberano con autoridad sobre el mundo entero, a pesar de que una serie de estados-nación han intentado dominar el mundo, sin éxito.[196]​ Las Naciones Unidas es una organización mundial intergubernamental que se creó con el objetivo de intervenir en las disputas entre las naciones, a fin de evitar los conflictos armados.[197] Sin embargo, no es un gobierno mundial. La ONU sirve principalmente como un foro para la diplomacia y el derecho internacional. Cuando el consenso de sus miembros lo permite, proporciona un mecanismo para la intervención armada.[198]​ Duración: 48 segundos.0:48 La Tierra de noche. El vídeo de la EEI comienza justo al sureste de Alaska. La primera ciudad que pasa por encima de la Estación Espacial Internacional (vista unos 10 segundos en el vídeo) es la de San Francisco y sus alrededores. Si se mira con mucho cuidado, se puede ver que en el puente Golden Gate se encuentra: una franja más pequeña de luces justo antes de la cercana ciudad de San Francisco, nubes a la derecha de la imagen. También se pueden ver tormentas eléctricas muy evidentes en la costa del océano Pacífico, con nubes. A medida que el video avanza, la EEI pasa por encima de América Central (las luces verdes se pueden ver aquí), con la península de Yucatán a la izquierda. El paseo termina en la Estación Espacial Internacional es la ciudad capital de Bolivia, La Paz. El primer humano en orbitar la Tierra fue Yuri Gagarin el 12 de abril de 1961.[199] Hasta 2004, alrededor de 400 personas visitaron el espacio exterior y alcanzado la órbita de la Tierra. De estos, doce han caminado sobre la Luna.[200]​[201]​[202] En circunstancias normales, los únicos seres humanos en el espacio son los de la Estación Espacial Internacional (EEI). La tripulación de la estación, compuesta en la actualidad por seis personas, suele ser reemplazada cada seis meses.[203] Los seres humanos que más se han alejado de la Tierra se distanciaron 400 171 kilómetros, alcanzados en la década de 1970 durante la misión Apolo 13.[204]​ Véase también: Mundo Perspectiva cultural La primera fotografía hecha por astronautas del «amanecer de la Tierra», tomada desde el Apolo 8. La palabra «Tierra» proviene del latín terra (en minúsculas)[205] y que, en mayúsculas, se asoció a dos diosas arquetipos de la «madre tierra», Gea para los griegos y Tellus para los romanos. Especialmente, en la Edad Contemporánea, se le ha dado el nombre poético de Gaia.[206] El símbolo astronómico estándar de la Tierra consiste en una cruz circunscrita por un círculo.[207]​ A diferencia de lo sucedido con el resto de los planetas del sistema solar, la humanidad no comenzó a ver la Tierra como un objeto en movimiento, en órbita alrededor del Sol, hasta alcanzado el siglo XVI.[208] La Tierra a menudo se ha personificado como una deidad, en particular, una diosa. En muchas culturas la diosa madre también es retratada como una diosa de la fertilidad. En muchas religiones los mitos sobre la creación recuerdan una historia en la que la Tierra es creada por una deidad o deidades sobrenaturales. Varios grupos religiosos, a menudo asociados a las ramas fundamentalistas del protestantismo[209] o el islam,[210] afirman que sus interpretaciones sobre estos mitos de creación, relatados en sus respectivos textos sagrados son la verdad literal, y que deberían ser consideradas junto con los argumentos científicos convencionales de la formación de la Tierra y el desarrollo y origen de la vida, o incluso reemplazarlos.[211] Tales afirmaciones son rechazadas por la comunidad científica[212]​[213] y otros grupos religiosos.[214]​[215]​[216] Un ejemplo destacado es la controversia entre el creacionismo y la teoría de la evolución. En el pasado hubo varias creencias en una Tierra plana,[217] pero esta creencia fue desplazada por el concepto de una Tierra esférica, debido a la gran evidencia de esta como su circunnavegación.[218] La perspectiva humana acerca de la Tierra ha cambiado tras el comienzo de los vuelos espaciales, y actualmente la biosfera se interpreta desde una perspectiva global integrada.[219]​[220] Esto se refleja en el creciente movimiento ecologista, que se preocupa por los efectos que causa la humanidad sobre el planeta."
ksampletext_wikipedia_geol_volcan: str = "Volcán. Un volcán (del portugués, y este del latín Vulcano, dios romano del fuego) es una estructura geológica en la tierra o en el mar, generalmente una montaña, por la que emerge el magma que se divide en lava y gases provenientes del interior de la Tierra.[4] El ascenso del magma ocurre en episodios de actividad violenta denominados erupciones, que pueden variar en intensidad, duración y frecuencia, desde suaves corrientes de lava hasta explosiones extremadamente destructivas. En ocasiones, los volcanes adquieren una forma cónica por la acumulación de material de erupciones anteriores. En la cumbre se encuentra su cráter o caldera. Por lo general los volcanes se forman en los límites de las placas tectónicas, aunque existen los llamados puntos calientes, donde no hay contacto entre placas, como es el caso de las islas Hawái. Aproximadamente el 75% de los volcanes activos del mundo están ubicados en el llamado cinturón de fuego del Pacífico.[5]​ Los volcanes pueden tener muchas formas y despedir distintos materiales. Algunas de las formas más comunes son el estratovolcán, el cono de escoria, la caldera volcánica y el volcán en escudo. También existen numerosos volcanes submarinos ubicados a lo largo de las dorsales mediooceánicas. Algunos volcanes alcanzan una altitud superior a los 6000 metros sobre el nivel del mar. El volcán más alto del mundo es el Nevado Ojos del Salado, en Argentina y Chile, siendo además la segunda cumbre más alta de los hemisferios sur y occidental (solo superado por el cerro argentino Aconcagua).[6]​ Los volcanes no solo existen en la Tierra, sino también en otros planetas y satélites. Algunos están formados por materiales considerados fríos y se denominan criovolcanes. En ellos, el hielo actúa como roca, mientras que el agua fría líquida interna actúa como magma; esto ocurre en la luna de Júpiter llamada Europa. Relación entre vulcanismo y las placas tectónicas Límites de placa divergentes En las crestas oceánicas medias, dos placas tectónicas divergen entre sí a medida que se forma una nueva corteza oceánica por el enfriamiento y la solidificación de la roca fundida caliente. Debido a que la corteza es muy delgada en estas crestas debido al tirón de las placas tectónicas, la liberación de presión conduce a la expansión adiabática (sin transferencia de calor o materia) y al derretimiento parcial del manto, causando vulcanismo y creando una nueva corteza oceánica. La mayoría de los límites de placas divergentes se encuentran en el fondo de los océanos; por lo tanto, la mayor parte de la actividad volcánica en la Tierra es submarina, formando un nuevo fondo marino. Los fumadores negros (también conocidos como respiraderos de aguas profundas) son evidencia de este tipo de actividad volcánica. Donde la cresta oceánica media está sobre el nivel del mar, se forman islas volcánicas; por ejemplo, Islandia. Placas convergentes Las zonas de subducción son lugares donde chocan dos placas, generalmente una placa oceánica y una placa continental. En este caso, la placa oceánica se subduce, o se sumerge, debajo de la placa continental, formando una trinchera oceánica profunda en alta mar. En un proceso llamado fusión de flujo, el agua liberada de la placa subductora reduce la temperatura de fusión de la cuña del manto suprayacente, creando así magma. Este magma tiende a ser extremadamente viscoso debido a su alto contenido de sílice, por lo que a menudo no alcanza la superficie sino que se enfría y solidifica en profundidad. Cuando llega a la superficie, sin embargo, se forma un volcán. Ejemplos típicos son el Etna y los volcanes en el Anillo de Fuego del Pacífico. Puntos calientes Los puntos calientes son áreas volcánicas formadas por plumas de manto, que son columnas de material caliente que se elevan desde el límite núcleo-manto en un espacio fijo que causa la fusión de grandes volúmenes. En algunos casos, debido a que las placas tectónicas se mueven a través de ellas, cada volcán se vuelve inactivo y se forma uno nuevo a medida que la placa avanza sobre el penacho térmico, como en el caso del archipiélago de Hawái; también lo ha hecho la llanura del río Snake, con la caldera de Yellowstone como parte de la placa norteamericana sobre el punto caliente. Otros ejemplos de vulcanismo asociado a punto caliente son las islas Canarias, esta vez con un desplazamiento mínimo de la placa africana, o Islandia, que además coincide con un límite divergente de placas. Tipos de volcanes según su actividad Los volcanes, teniendo en cuenta la frecuencia de sus erupciones, se pueden clasificar en tres tipos: activos, inactivos (durmientes) o extintos. Volcanes activos Los volcanes activos son aquellos que pueden entrar en actividad eruptiva en cualquier momento, es decir, que permanecen en estado de latencia. Esto ocurre con la mayoría de los volcanes, pues ocasionalmente entran en actividad, permaneciendo en reposo la mayor parte del tiempo. El período de actividad eruptiva puede durar desde una hora hasta varios años, como fue el caso del volcán de Pacaya y del Irazú. Hasta el momento, no se ha descubierto ningún método seguro para predecir las erupciones. Volcanes durmientes o inactivos Los volcanes durmientes o inactivos son aquellos que mantienen ciertos signos de actividad, como la presencia de aguas termales, y han entrado en actividad esporádicamente. Dentro de esta categoría suelen incluirse las fumarolas y los volcanes con largos períodos de inactividad entre una erupción y otra. Un volcán se considera durmiente si desde hace siglos no ha tenido una erupción. Volcanes extintos Artículo principal: Volcán extinto Los volcanes extintos son aquellos cuya última erupción fue registrada hace más de 25 000 años. Sin embargo, no se descarta la posibilidad de que puedan despertar y liberar una erupción más fuerte que la de un volcán que está activo, causando grandes desastres. También se les llama extintos cuando han sido alejados de su fuente de magma, perdiendo poco a poco su actividad, esto sucede únicamente en volcanes de punto caliente, a diferencia de los volcanes de zonas de subducción. Tipos de erupciones volcánicas Artículo principal: Erupción volcánica Erupción en el 2011 del volcán Tungurahua, Ecuador. La temperatura, composición, viscosidad y elementos disueltos en el magma son los factores que determinan el tipo de erupción y la cantidad de productos volátiles que la acompañan. Hawaiana Artículo principal: Erupción hawaiana Volcán hawaiano en Kilauea. En este tipo de erupción, la lava generalmente es bastante fluida y no ocurren desprendimientos gaseosos explosivos. Estas lavas se desbordan cuando rebasan el cráter y se deslizan con facilidad por la ladera del volcán, formando verdaderas corrientes que recorren grandes distancias. Por esta razón, los volcanes de tipo hawaiano son de pendiente suave. Algunos residuos de lava, al ser arrastrados por el viento, forman hilos cristalinos que los nativos hawaianos llaman cabellos de la diosa Pele, la diosa del fuego. El volcán hawaiano más famoso es el Kilauea. Estromboliana o mixta Artículo principal: Erupción estromboliana Erupción del Estrómboli en verano de 2015 (animado). Erupción del Estrómboli (Italia) en 1980. Este tipo de erupción recibe el nombre del Estrómboli, volcán de las islas Eolias (mar Tirreno), al norte de Sicilia. Se origina cuando hay alternancia de los materiales en erupción, formándose un cono estratificado en capas de lavas fluidas y materiales sólidos. La lava es fluida, va desprendiendo gases abundantes y violentos con proyecciones de escorias, bombas y lapilli. Debido a que los gases pueden desprenderse con facilidad, no se producen pulverizaciones o cenizas. Cuando la lava rebosa por los bordes del cráter, desciende por las laderas y barrancos, pero no alcanza grandes extensiones como en las erupciones de tipo hawaiano. Vulcaniana Vulcano. Del nombre del volcán Vulcano en las islas Eolias. Esta erupción se caracteriza porque en ella se desprenden grandes cantidades de gases, la lava liberada es poco fluida y se consolida con rapidez. En este tipo de erupción, las explosiones son muy fuertes y pulverizan la lava, produciendo mucha ceniza, la cual es lanzada al aire acompañada de otros materiales fragmentarios. Cuando el magma sale al exterior en forma de lava, se solidifica rápidamente, pero los gases que se desprenden rompen y resquebrajan su superficie, volviéndola áspera y muy irregular y formando lava de tipo Aa. Los conos de estos volcanes son de pendiente muy inclinada. Pliniana o vesubiana Artículo principal: Erupción pliniana Nombrada así en honor a Plinio el Joven, difiere de la erupción volcánica en que en ésta la presión de los gases es muy fuerte y produce explosiones muy violentas, que en los casos extremos (plinianos) puede dar lugar a unas coladas piroclásticas o nubes ardientes que bruscamente se precipitan por las laderas del volcán alcanzando gran rapidez y sepultando en sólo unos minutos una gran extensión de terreno. Estos fenómenos críticos pueden sepultar y abrasar de golpe ciudades enteras, como ocurrió con Pompeya y Herculano por la actividad del volcán Vesubio. Al final de la deposición de esta colada piroclástica ardiente se transforma en la denominada roca ignimbrita; además se genera precipitaciones de cenizas, las cuales también pueden llegar a sepultar grandes extensiones como última capa fría. Se caracteriza por alternar erupciones de piroclasto con erupciones de coladas de lava, dando lugar a una superposición en estratos, lo que hace que este tipo de volcanes alcance grandes dimensiones; que también se denominan «Estratovolcanes». Ejemplo de ellos son el Teide, el Popocatépetl y el Fujiyama. Freatomagmática o surtseyana Artículos principales: Erupción surtseyana y Erupción freatomagmática. Los volcanes de tipo freatomagmático se encuentran en aguas someras, presentan un lago en el interior de su cráter y en ocasiones forman atolones. Sus erupciones son extraordinariamente violentas, ya que a la energía propia del volcán se le suma la expansión del vapor de agua súbitamente calentado. Normalmente no presentan emisiones de lava ni extrusiones de rocas. Algunas de las mayores erupciones freáticas son las del Krakatoa, el Kīlauea y la Isla de Surtsey. Peleana De los volcanes de las Antillas es célebre la Montaña Pelada, ubicada en la isla Martinica, que en la erupción de 1902 destruyó la capital, Saint-Pierre. La lava en esta erupción es extremadamente viscosa y se consolida con gran rapidez, llegando a tapar por completo el cráter formando un pitón o aguja. La enorme presión de los gases sin salida provoca una enorme explosión que levanta el pitón, o bien destroza la parte superior de la ladera. Así ocurrió el 8 de mayo de 1902, cuando las paredes del volcán cedieron a tan enorme empuje que se abrió un conducto por el que salieron con extraordinaria fuerza los gases acumulados a elevada temperatura y que, mezclados con cenizas, formaron una nube ardiente que ocasionó 28 000 víctimas. [cita requerida] Erupciones submarinas Artículo principal: Erupción submarina En el fondo oceánico se producen erupciones volcánicas cuyas lavas pueden formar islas volcánicas si llegan a la superficie. Las erupciones suelen ser de corta duración en la mayoría de los casos, debido al equilibrio isostático de las lavas al enfriarse cuando entran en contacto con el agua y también por la erosión marina. Algunas islas como las Cícladas en Grecia o las islas Canarias en España tienen este origen. Avalanchas de origen volcánico Artículo principal: Lahar Armero después de la tragedia (Colombia). Hay volcanes que generan un número de víctimas elevado, debido a que sus grandes cráteres están durante el periodo de reposo convertidos en lagos o cubiertos de nieve. Al recobrar su actividad, el agua mezclada con cenizas y otros restos, es lanzada formando torrentes y avalanchas o coladas de barro (que se denominan «lahares») que tienen una enorme capacidad destructiva. Un ejemplo de esto fue la erupción del Nevado de Ruiz en Colombia, el 13 de noviembre de 1985. El Nevado del Ruiz es un volcán explosivo en el que la cumbre del cráter (5321 m s.n.m.) estaba recubierta por un casquete de hielo; al ascender la lava se recalentaron las capas de hielo y se formaron unas coladas de barro que invadieron el valle del río Lagunilla, sepultando la ciudad de Armero, dejando 24 000 muertos y decenas de miles de heridos.[cita requerida] Erupciones fisurales Se originan en una larga dislocación de la corteza terrestre, que puede ser desde apenas unos metros hasta varios kilómetros. La lava que fluye a lo largo de la rotura es fluida y recorre grandes extensiones formando amplias mesetas (traps), con uno o más kilómetros de espesor y miles de km². Un ejemplo de vulcanismo fisural es la meseta del Decán en la India. Véase también: Índice de explosividad volcánica Volcán en escudo Artículo principal: Volcán en escudo Columnas de basalto de la «Calzada del Gigante» en Irlanda del Norte. Cuando la lava expulsada por el volcán es fluida, de tipo hawaiano, el volcán adquiere una forma de una estructura amplia y abovedada, que por su apariencia se los denomina en escudo. Los volcanes de escudo se asemejan a la superficie superior de un escudo que reposara en el suelo con el lado convexo hacia arriba. Un volcán en escudo está formado principalmente por lavas basálticas (ricas en hierro) y poco material piroclástico. El mayor volcán de la Tierra es el Mauna Loa, un volcán en escudo en las islas Hawái. El Mauna Loa nace en las profundidades del mar, a unos 5000 metros y se eleva sobre el nivel del mar por unos 4170 metros. Los volcanes en escudo como el Mauna Loa se forman a lo largo de millones de años gracias a ciclos de erupciones de lava que se van superponiendo unas con otras. El volcán de escudo más activo es el Kīlauea, localizado en la Isla de Hawái, al lado de Mauna Loa. En el período histórico el Kilauea ha entrado unas cincuenta veces en erupción y es, por lo tanto, el volcán de este tipo más estudiado. El resultado de erupciones constantes durante millones de años ha dado lugar a la creación de las montañas más grandes de la Tierra (si se tiene en cuenta la altura contando desde la base en el lecho marino). Por ejemplo, el Mauna Loa, desde su base submarina hasta su cúspide, cuenta con una altura de 9.5 km, más alto que el monte Everest. Los geólogos creen que las primeras etapas de formación de los volcanes en escudo consisten en erupciones frecuentes de delgadas coladas de basalto muy líquidas. Además de estas erupciones también se producen erupciones laterales. Normalmente con el cese de cada fase eruptiva se produce el hundimiento del área de la cima. En las últimas fases, las erupciones son más esporádicas y la erupción piroclástica se hace más frecuente. A medida que esto sucede, las coladas de lava tienden a ser más viscosas, lo que provoca que sean más cortas y potentes. Así, va aumentando la pendiente de la ladera del área de la cima. Los volcanes en escudo son muy comunes y también se han identificado en el sistema solar. El más grande conocido hasta la fecha es el monte Olimpo, sobre la superficie de Marte, encontrándose también varios de estos volcanes sobre la superficie de Venus, aunque de apariencia más achatada. Flujo piroclástico Artículo principal: Flujo piroclástico Flujo piroclástico expulsado por el volcán Mayón en Filipinas. Cuando las erupciones de un volcán llegan acompañadas de gases calientes y cenizas se produce lo que se conoce como flujo piroclástico o «nube ardiente». También conocida como avalancha incandescente, el flujo piroclástico se desplaza pendiente abajo a velocidades cercanas a los 200 km/h. La sección basal de estas nubes contienen gases calientes y partículas que flotan en ellos. De esta forma, las nubes transportan fragmentos de rocas que –gracias al rebote de los gases calientes en expansión– se depositan a lo largo de más de 100 km desde su punto de origen. En 1902 una nube ardiente de un pequeño volcán llamado monte Pelée en la isla caribeña de Martinica destruyó la ciudad portuaria de San Pedro. La destrucción fue tan devastadora que murió casi toda la población (unos 28 000 habitantes). A diferencia de Pompeya, que quedó enterrada en un manto de cenizas en un plazo de tres días y las casas quedaron intactas (salvo los techos por el peso de las cenizas), la ciudad de San Pedro fue destruida solo en minutos y la energía liberada fue tal que los árboles fueron arrancados de raíz, las paredes de las casas desaparecieron y las monturas de los cañones se desintegraron. La erupción del monte Pelée muestra cuan distintos pueden ser dos volcanes del mismo tipo. Lahar Artículo principal: Lahar Los conos compuestos también producen coladas de barro llamadas lahar, una palabra de origen indonesio. Estos flujos se producen cuando las cenizas y derrubios volcánicos se saturan de agua y descienden pendiente abajo, normalmente siguiendo los cauces de los ríos. Algunos de los lahares se producen cuando la saturación es provocada por la lluvia, mientras que en otros casos cuando grandes volúmenes de hielo y nieve se funden por una erupción volcánica. En Islandia, el último caso se denomina jökulhlaup y es un fenómeno devastador. Destrucciones importantes de lahares se dieron en 1980 con la erupción del monte Santa Helena, en Estados Unidos, que a pesar de los destrozos producidos, no produjo muchas víctimas debido a que la región está poco poblada. Otro fue en 1985 con la erupción del Nevado del Ruiz, en Colombia, la cual generó un lahar que acabó con la vida de 25 000 personas. Formas volcánicas relacionadas Calderas Artículo principal: Caldera volcánica Caldera Aniakchak, en Alaska. La mayoría de los volcanes presentan en su cima un cráter de paredes empinadas, por el interior. Cuando el cráter supera 1 km de diámetro se denomina caldera volcánica. Las calderas son estructuras de forma circular y la mayoría se forma cuando la estructura volcánica se hunde sobre la cámara magmática parcialmente vacía que se sitúa por debajo. Si bien la mayoría de las calderas se crea por el hundimiento producido después de una erupción explosiva, esto no es así en todos los casos. En el caso de los enormes volcanes en escudo de Hawái, las calderas se crearon por la continua subsidencia a medida que el magma se drenaba desde la cámara magmática durante las erupciones laterales. También las calderas de las islas Galápagos se han ido hundiendo por derrames laterales. Las calderas de gran tamaño se forman cuando un cuerpo lavático granítico (félsico) se ubica cerca de la superficie curvando de esta manera las rocas superiores. Posteriormente, una fractura en el techo permite al magma rico en gases y muy viscoso ascender hasta la superficie, donde expulsa de manera explosiva, enormes volúmenes de material piroclástico, fundamentalmente cenizas y fragmentos de pumita. Estos materiales se denominan coladas piroclásticas y pueden alcanzar velocidades de 100 km/h. Cuando estos materiales se detienen, los fragmentos calientes se fusionan para formar una toba soldada que se asemeja a una colada de lava solidificada. Finalmente, el techo se derrumba dando lugar a una caldera. Este procedimiento puede repetirse varias veces en el mismo lugar. Se conocen al menos 138 calderas que superan los 5 km de diámetro. Muchas de estas calderas son difíciles de ubicar, por lo que han sido identificadas con imágenes de satélites. Entre las más importantes se encuentra La Garita con unos 32 km de diámetro y una longitud de 80 que está ubicada en las montañas de San Juan al sur del estado de Colorado. Erupciones fisurales y llanuras de lava Artículo principal: Fisura volcánica Cono piroclástico en el volcán fisural Laki en Islandia. A pesar de que las erupciones volcánicas están relacionadas con estructuras en forma de cono, la mayor parte del material volcánico es extruido por fracturas en la corteza denominadas fisuras. Estas fisuras permiten la salida de lavas de baja viscosidad que recubren grandes áreas. La meseta del Columbia en el noroeste de Estados Unidos se formó de esta manera. Las erupciones fisurales expulsaron lava basáltica muy líquida. Las coladas siguientes cubrieron el relieve y formaron una llanura de lava (plateau) que en algunos lugares tiene casi 1.5 km de grosor. La fluidez se evidencia en la superficie recorrida por la lava: unos 150 km desde su origen. A estas coladas se las denomina basalto de inundación. Este tipo de coladas sucede principalmente en el suelo oceánico y no puede verse. A lo largo de las dorsales oceánicas, donde la expansión del suelo oceánico es activa, las erupciones fisurales generan nuevo suelo oceánico. Islandia está ubicada encima de la dorsal centroatlántica y ha experimentado numerosas erupciones fisurales. Las erupciones fisurales más grandes de Islandia ocurrieron en 1783 y se denominaron erupciones de Laki. Laki es una fisura o volcán fisural de 25 km de largo que generó más de veinte chimeneas separadas que expulsaron corrientes de lava basáltica muy fluida. El volumen total de lava expulsada por las erupciones de Laki fue superior a los 12 km³. Los gases arruinaron las praderas y mataron al ganado islandés. La hambruna subsiguiente mató cerca de diez mil personas. La caldera está situada muy por debajo de la boca del volcán. Domo de lava Artículo principal: Domo de lava Domos de lava en el cráter del monte Santa Helena (Estados Unidos). La lava rica en sílice es viscosa y por lo tanto, apenas fluye; cuando es extruida fuera de la chimenea puede producir una masa bulbosa de lava solidificada que se denomina domo de lava. Debido a su viscosidad, la mayoría está compuesto por riolitas y otros por obsidianas. La mayoría de los domos volcánicos se desarrollan a partir de una erupción explosiva de un magma rico en gases. Aunque la mayoría de los domos volcánicos están asociados a conos compuestos, algunos se forman de manera independiente. Tal es el caso de la línea de domos riolíticos y de obsidiana en los en California. Chimeneas y pitones volcánicos Artículos principales: Chimenea volcánica y Cuello volcánico. Volcán Teide (Tenerife, España). Los volcanes se alimentan del magma a través de conductos denominados chimeneas. Estas tuberías pueden extenderse hasta unos 200 km de profundidad. En este caso, las estructuras proveen de muestras del manto que han experimentado muy pocas alteraciones durante su ascenso. Las chimeneas volcánicas mejor conocidas son las sudafricanas que están cargadas de diamantes. Las rocas que rellenan estas chimeneas se originaron a profundidades de 150 km, donde la presión es lo bastante elevada como para generar diamantes y otros minerales de alta presión. Debido a que los volcanes están siendo rebajados constantemente por la erosión y la meteorización, los conos de cenizas son desgastados con el tiempo, pero no sucede lo mismo con otros volcanes. Conforme la erosión progresa, la roca que ocupa la chimenea y que es más resistente, puede permanecer de pie sobre el terreno circundante mucho después de que haya desaparecido el cono que la contiene. A estas estructuras de las denomina pitón volcánico. Shiprock, en Nuevo México, es un claro ejemplo de este tipo de estructuras. Cuevas volcánicas Artículo principal: Cueva volcánica Una cueva volcánica es cualquier cavidad formada en rocas volcánicas, aunque el uso común de este término se reserva a cuevas primarias o singenéticas creadas por procesos volcánicos de modo que tanto la oquedad como la roca encajante se forman a la vez. Material volcánico El Pu‘u ‘Ō‘ō, cono volcánico de Hawái. Artículo principal: Roca volcánica El material que se forma por la actividad de un volcán son las rocas volcánicas, efusivas o extrusivas, principalmente basaltos y andesitas. Según su textura pueden ser coladas, piroclastos (lapilli, pumita), obsidiana, etc. Volcanes extraterrestres Monte Olimpo, el volcán más grande del sistema solar situado en el planeta Marte. La Tierra no es el único planeta del sistema solar que tiene actividad volcánica. Venus tiene un intenso vulcanismo con unos cientos de miles de volcanes. Marte tiene la cumbre más alta del sistema solar: el monte Olimpo, un volcán dado por apagado con una base de unos 600 km y más de 27 km de altura. No obstante, este planeta parece tener cierta actividad volcánica apreciable.[7]​ Nuestra Luna está cubierta de inmensos campos de basalto y tiene presencia de domos lunares de origen volcánico similares a un volcán en escudo como por ejemplo el Mons Rümker, lo que sugiere que tuvo una corta pero considerable actividad volcánica que hoy muy probablemente está extinta. Debido a las bajas temperaturas del espacio, algunos volcanes de nuestro sistema solar están formados de hielo que actúa como roca, mientras su agua líquida interna actúa como la magma; esto ocurre -por ejemplo- en la fría luna de Júpiter llamada Europa. Estos reciben el nombre de criovolcán, de los cuales hay también en Encélado. La Voyager 2 descubrió en agosto de 1989, sobre Tritón, rastros de criovulcanismo y géiseres. La búsqueda de vida extraterrestre se ha interesado en buscar rastros de vida en sistemas criovolcánicos donde hay agua líquida y por ende, una fuente de radiación en calor considerable; estos son elementos esenciales para la vida. Existen volcanes un poco más similares a los terrestres, sobre otros satélites de Júpiter como en el caso de Ío. La sonda Voyager 1 permitió fotografiar en marzo de 1979 una erupción en Ío. Los astrofísicos estudian los datos de esta información, que extiende el campo de estudio de la vulcanología. El conocimiento del fenómeno tal como se produce sobre la Tierra pasa en adelante por su estudio en el espacio. La temperatura y composición química de los volcanes del sistema solar varían considerablemente entre los planetas y los satélites. Además, el tipo de materiales que arrojan en sus erupciones es muy diferente de los arrojados en la Tierra.[cita requerida] Peligros Esta sección es un extracto de Peligros volcánicos.[editar] Un diagrama esquemático muestra algunas de las muchas formas en que los volcanes pueden causar problemas a los que están cerca. Un peligro volcánico es la probabilidad de que ocurra una erupción volcánica o un suceso geofísico relacionado, en una determinada área geográfica y dentro de un período de tiempo específico. El riesgo asociado depende de la proximidad y vulnerabilidad de un bien, recurso natural o una población, cerca de donde podría ocurrir un suceso volcánico. Protección civil Véase también: Protección Civil España En España, la Norma Básica de Protección Civil, aprobada por Real Decreto 407/1992, de 24 de abril,[8] dispone en su apartado 6 que el riesgo volcánico será objeto de planes especiales en los ámbitos territoriales que lo requieran. Estos planes especiales habrán de ser elaborados de acuerdo con una Directriz Básica previamente aprobada por el Gobierno. La Directriz Básica de Planificación de Protección Civil ante el Riesgo Volcánico[9] fue aprobada por Acuerdo del Consejo de Ministros del 19 de enero de 1996 y publicada por Resolución de la Secretaría de Estado de Interior el 21 de febrero de 1996. En ella, se consideran tres niveles de planificación: estatal, autonómico y de ámbito local. Por Acuerdo del Consejo de Ministros del 25 de enero de 2013, se aprueba el Plan Estatal de Protección Civil ante el Riesgo Volcánico.[10]​ Creencias tradicionales sobre los volcanes Vulcano forjando los rayos de Júpiter (1636), de Pedro Pablo Rubens. Muchos cuentos antiguos atribuyen las erupciones volcánicas a causas sobrenaturales, tales como la acción de dioses o semidioses. Los antiguos griegos aun pensaban que el poder caprichoso de los volcanes sólo podía ser explicado como un acto divino, mientras que el astrónomo de los siglos XVI-XVII Johannes Kepler creía que eran los conductos lagrimales de la Tierra. Previamente, el jesuita Atanasio Kircher, luego de haber sido testigo de erupciones del Etna y el Estrómboli y haber visitado el cráter del monte Vesubio, publicó su propuesta de que el planeta Tierra tenía un fuego central conectado a numerosos otros causados por la combustión de azufre, betún y carbón. Varias explicaciones fueron propuestas para explicar el comportamiento de los volcanes antes de que el entendimiento moderno de la estructura de la tierra se desarrollara. La acción volcánica solía atribuirse a reacciones químicas y a la delgada capa de piedra fundida cerca de la superficie. Volcanes activos en América del Sur Argentina Artículo principal: Anexo:Volcanes de Argentina Numerosos volcanes se distribuyen a lo largo del territorio de la República Argentina. Algunos volcanes se encuentran definitivamente extintos y otros activos, aunque la proporción va a depender de la definición de activo y extinguido; aquí se consideran activos los que han tenido erupciones probables o verificadas en los últimos 10 000 años. Los volcanes de Argentina son variados tanto en forma como en emplazamiento tectónico. La mayoría de los volcanes argentinos pertenecen al Cinturón volcánico de los Andes, aunque hay grandes y voluminosos volcanes de retroarco. Dada la naturaleza del vulcanismo, es imposible establecer un número exacto de volcanes. Cabe destacar que Argentina junto con Chile acogen al volcán más alto del mundo: Nevado Ojos del Salado. Bolivia Artículo principal: Anexo:Volcanes de Bolivia Bolivia acoge numerosos volcanes activos y extinguidos a través de su territorio. Los volcanes activos se encuentran en el oeste de Bolivia. Nevado Sajama (del aimara: chak xaña ‘oeste’) es un estratovolcán en Bolivia, ubicado en el parque nacional Sajama al oeste del país en el departamento de Oruro. No se sabe con certeza la fecha de su última erupción. Sin embargo, se le considera un volcán extinto. El volcán Ollagüe es un volcán activo situado en la frontera de Bolivia y Chile, en la región de Antofagasta en Chile y el Departamento de Potosí en Bolivia, en la cordillera de los Andes, con una altura de 5870 metros. Acotango es un estratovolcán ubicado en la frontera de Bolivia y Chile, entre el departamento de Oruro y la región de Arica y Parinacota. Su zona de influencia directa está protegida por el parque nacional Lauca, por el lado chileno, y el parque nacional Sajama, por el lado boliviano. Chile El volcán Villarrica es el más activo de Sudamérica, ha presentado alta actividad desde el s. VII d. C.. Artículo principal: Anexo:Volcanes de Chile Los volcanes en Chile son supervisados por el Servicio Nacional de Geología y Minería de Chile (SERNAGEOMIN).[11]​[12] Entre las tareas de este organismo están, desde 1974, la publicación de la revista científica Andean Geology —que se llamaba Revista Geológica de Chile hasta 2009—,[13] y visualizar el Sistema de Información de Geología de Exploración (SIGEX) —que reúne información sistematizada de los proyectos de exploración en Chile y los antecedentes técnicos y administrativos, entre otros—. La información fue obtenida de sitios web y otras fuentes públicas. De este modo, SERNAGEOMIN contribuye a consolidar el conocimiento geológico-minero del país (Art. 21 del Código de Minería de 1988). Según la Red Nacional de Vigilancia Volcánica del Servicio Nacional de Geología y Minería de Chile, el país posee 90 volcanes considerados como «activos»,[14] de entre los cuales destacan: Nevados de Chaitén, Villarrica, Planchón Peteroa y el complejo volcánico Laguna del Maule, este último compartido con Argentina. Colombia Artículo principal: Anexo:Volcanes de Colombia El Nevado del Ruiz: De acuerdo con el Servicio Geológico Colombiano este volcán presenta actividad sísmica regular, así como emisiones de ceniza. Su altura es de 5364 m y se encuentra en la zona cafetera del país. En noviembre de 1985 tuvo una erupción donde fallecieron más de 25 000 habitantes de la población de Armero. El volcán Galeras: Se ubica en el departamento de Nariño y está considerado como el volcán más activo de Colombia. En 1993 unos turistas y un grupo de científicos que se encontraban dentro de su cráter murieron, luego de una erupción. Durante los últimos años ha mantenido una actividad constante, con explosiones pequeñas y expulsión de ceniza y humo ocasional. Ecuador Artículo principal: Anexo:Volcanes de Ecuador Los volcanes activos del Ecuador continental pertenecen a la Zona Volcánica Norte (ZVN) de los Andes, la cual es parte del Cinturón Volcánico de los Andes. La Escuela Politécnica Nacional, también conocida como EPN, es una universidad pública, ubicada en Quito, Ecuador. El Instituto Geofísico dirige en los países volcanes en las montañas de los Andes de Ecuador y en las Islas Galápagos. El Instituto Geofísico EPN dirige desde 1999.[15]​[16]​[17] El Instituto Geofísico de la Escuela Politécnica Nacional (IGEPN) reportó un rápido aumento en la actividad sísmica, el número de explosiones y una nube de cenizas que alcanzó los 2 km (1.2 millas) de altura, llegando la nube de ceniza a la ciudad de Guayaquil. El 26 de abril de 2011 hubo otra erupción de proporciones considerables, lanzando una columna de ceniza que ascendió hasta los 12 km de altura.[18]​[19]​ El Instituto Geofísico E.P.N dispone de equipos internacionales de Sismología y Vulcanología y dirige volcanes en las islas Galápagos. En agosto de 2015, el Volcán Cotopaxi experimentó un incremento significativo de su actividad, motivando incluso la declaración de un estado de excepción[20] en el territorio nacional. Actualmente se encuentra bajo vigilancia constante por parte del Instituto Geofísico de la EPN.[21]​[22]​[23] El 25 de mayo de 2015, Isla Wolf (Galápagos) tuvo una erupción volcánica y ahora está siendo dirigida por el Instituto Geofísico de la Escuela Politécnica Nacional[24]​[25]​[26]​[27] En un informe que detalla la erupción, los investigadores del Instituto Geofísico de Ecuador EPN declararon que la columna de humo alcanzó una altitud de 15 kilómetros aproximadamente. Perú Artículo principal: Anexo:Volcanes del Perú El volcán Ubinas, es el volcán más activo del Perú, ha registrado más de 25 erupciones en los últimos 500 años. El Perú está situado en el cinturón de Fuego del Pacífico, región del planeta que se caracteriza por su gran actividad sísmica y volcánica. Como resultado de ello, el sur del Perú está atravesado por más de 400 volcanes que componen el llamado Arco volcánico del Perú y que forman parte de la Zona Volcánica Central de los Andes (ZVC).[28]​ El Perú cuenta con dos centros de monitoreo volcánico ubicados en la ciudad de Arequipa, los denominados Observatorio Vulcanológico del Sur del Instituto Geofísico del Perú (OVS-IGP) y el Observatorio Vulcanológico del INGEMMET (OVI), que mancomunadamente se han centrado en el objetivo de vigilar permanentemente los 16 volcanes activos y potencialmente activos (Sabancaya, Misti, Ubinas, Coropuna, Tutupaca, Huaynaputina, Ticsani, Chachani, Yucamani, Sara Sara, Ampato, Casiri, Purupuruni, Auquihuato y el Valle de los Volcanes en Andahua y Huambo),[29] para ello cuentan con redes de vigilancia multiparamétricas que proporcionan información valiosa sobre el estado y niveles de actividad de los volcanes a su cargo. El Ubinas es considerado como el volcán más activo del Perú por sus 25 eventos de alta actividad fumarólica y actividad explosiva moderada registrada desde el año de 1550.[29] Está situado en el distrito de Ubinas, departamento de Moquegua. Culmina a 5672 ms y cubre una superficie de 45 km². La más reciente erupción tuvo lugar entre marzo de 2006 a junio de 2009, afectado fuertemente la actividad agrícola en el valle de Ubinas. El inicio de esta crisis eruptiva se presentó dominado por una actividad freática y luego, a partir del 19 de abril de 2006, la actividad deviene en magmática de tipo vulcaniano con emisión de material andesítico básico. Posteriormente, y luego de cuatro años de inactividad, en septiembre de 2013 el volcán Ubinas entró en un nuevo proceso eruptivo, el cual se fue acelerando en febrero de 2014 al tiempo que se registraba Tremores sísmicos de gran energía, eventos de tipo Híbrido, así como emisiones persistentes de gases y ceniza, etc. Finalmente, esta alta actividad sísmica y fumarólica culminó con la ocurrencia de la primera explosión magmática el día 14 de febrero de 2014. A partir de entonces y hasta el presente, la actividad eruptiva del volcán Ubinas ha continuado de manera intermitente."

ksampletext_wikipedia_tech_telefonomovil: str = "Teléfono móvil. Un teléfono móvil o teléfono celular (Esp. móvil; Am. celular) (acortado como móvil o celular) es un dispositivo electrónico portátil, que puede permitir llamadas a través de una onda de radiofrecuencia, mientras el usuario se está moviendo dentro de un área de servicio telefónico.[5]​[6] El enlace de radiofrecuencia establece una conexión con los sistemas de conmutación de un operador de telefonía móvil, que proporciona acceso a la red telefónica pública conmutada (PSTN). La mayoría de los servicios de telefonía móvil modernos utilizan una arquitectura de red de celdas (red celular) y por lo tanto los teléfonos móviles son, con frecuencia, llamados celulares.[5]​ Durante los inicios de la telefonía móvil, estos dispositivos tenían la función de realizar y recibir llamadas de voz, además de ser considerablemente grandes de tamaño, por lo que estos teléfonos eran conocidos como «ladrillos» en su época. Con la evolución de la tecnología celular analógica a la digital, fueron incluyéndose otras funciones como: mensajes de texto (SMS), MMS, comunicaciones inalámbricas de corto alcance (infrarrojos, bluetooth), videojuegos, cámara digital, acceso a Internet, entre otros. Además, con el tiempo, fue cambiado el factor de forma de los teléfonos móviles, pasando de los antiguos ladrillos, a los barras y plegables de los años 1990 y 2000 (con teclado físico), hasta los actuales slates (de pantalla táctil y teclado virtual).[cita requerida] Estos aparatos fueron popularizándose en el mundo desarrollado durante el transcurso de los años 1990 y 2000. El teléfono de la Marca Blackberry también marco una etapa en la evolución de esta telefonía. Recientemente, en los años 2010 se popularizan los hoy día conocidos como «teléfonos inteligentes», que son teléfonos portátiles que pueden cumplir funciones de una computadora u ordenador, aparte de las funciones mencionadas anteriormente. Historia Artículo principal: Historia del teléfono móvil Martin Cooper de Motorola hizo la primera publicidad de una llamada con teléfono móvil portátil en un prototipo de modelo DynaTAC el 3 de abril de 1973. Esta es una dramatización hecha en 2007. En las primeras etapas de la ingeniería de radio, se concibió un servicio de radio móvil de mano. En 1917, el inventor finlandés Eric Tigerstedt presentó una patente para un Teléfono plegable de bolsillo con un micrófono de carbono muy delgado. Los primeros predecesores de los teléfonos móviles incluyen las comunicaciones de radio analógicas de barcos y trenes. La carrera para crear dispositivos telefónicos portátiles realmente comenzó después de la Segunda Guerra Mundial, con la evolución que tiene lugar en muchos países avanzados. En la URSS a finales de los años 1950 , el inventor Leonid Kupriánovich desarrolló un sistema de telefonía inalámbrica, el radioteléfono portátil dúplex portátil LK-1, que se extendió por la URSS y otros países.[7] En 1973 John F. Mitchell[8]​[9] y Martin Cooper de empresa estadounidense Motorola presentaron un sistema con terminales de 2 kgs de peso.[10] En 1983 Motorola presentó el DynaTAC 8000x fue el primer teléfono móvil de mano disponible comercialmente. Los avances de la telefonía móvil se han trazado en generaciones sucesivas, empezando por los servicios 0G (generación cero), tales como Servicio de Telefonía Móvil de Sistemas de Bell y su sucesor, el Servicio de Telefonía Móvil Mejorada. Estos sistemas 0G no eran celular, soportaban algunas llamadas simultáneas, y eran muy caros. El Motorola DynaTAC 8000X (1984). El primer teléfono móvil celular disponible comercialmente. El primer teléfono celular de mano fue presentado por Motorola en 1983, aunque el primer teléfono móvil de mano estuvo disponible comercialmente en los años 80. La primera red celular automatizada comercial fue lanzada en Japón por Nippon Telegraph and Telephone en 1979. Esto fue seguido en 1981 por el lanzamiento simultáneo del sistema de Telefonía Móvil Nórdica (NMT) en Dinamarca, Finlandia, Noruega y Suecia.[11] Muchos otros países siguieron lanzando la red celular analógica (1G) en la década de 1980 y principios de la década de 1990. Estos sistemas de primera generación (1G) podían hacer llamadas simultáneas más lejos, pero todavía se utilizaba la tecnología analógica. La primera llamada digital entre teléfonos celulares fue realizada en Estados Unidos en 1990.[12]​ En 1991, la segunda generación (2G) de tecnología celular digital fue lanzada en Finlandia por Radiolinja, en el estándar GSM. Esto provocó la competencia en el sector, ya que los nuevos operadores desafiaron a los operadores de red 1G existentes. Esta tecnología fue popularizándose en la segunda mitad de la década. Nokia 3310. Diez años más tarde, en el 2001, la tercera generación (3G) fue lanzada en Japón por NTT DoCoMo en el estándar WCDMA. Esto fue seguido de 3.5G (H o 3G+) y luego el HSPAP (H+), que son mejoras basadas en el acceso de paquetes de alta velocidad (HSPA) de la familia, lo que permite a las redes UMTS tienen mayores velocidades de transferencia de datos y la capacidad. Para el año 2009, se hizo evidente que, en algún momento, las redes 3G se verían abrumadas por el crecimiento de las aplicaciones de banda ancha, tales como transmisión multimedia.[13] En consecuencia, la industria comenzó a buscar a las tecnologías de datos de cuarta generación optimizadas, con la promesa de mejorar la velocidad hasta diez veces sobre tecnologías 3G existentes. Las dos primeras tecnologías disponibles en el mercado facturadas como 4G eran el estándar WiMAX, ofrecido en Norteamérica por Sprint, y el estándar LTE, quien se ofreció por primera vez en Escandinavia por TeliaSonera. Posteriormente apareció el 4.5 G (LTE-A). Para el año 2019 se lanzaron las primeras redes comerciales 5G en algunas partes del mundo, aunque más bien sigue siendo una tecnología experimental actualmente, con proyecciones a expandirse en el transcurso de la década de 2020. Desde 1973 a 2005, las suscripciones de teléfonos móviles en todo el mundo crecieron a más de siete mil millones, habiendo más teléfonos móviles que personas en el planeta y llegando hasta el fondo de la pirámide económica.[14] Durante los años 1990 y 2000, los principales fabricantes de celulares eran Nokia, Motorola, entre otros. Actualmente, los principales fabricantes de teléfonos móviles son: Samsung, Apple y Huawei.[15]​ Características Todos los teléfonos celulares tienen una variedad de características en común, pero los fabricantes buscan diferenciación de producto por añadir funciones para atraer consumidores. Esta competición ha dirigido a una gran innovación en el desarrollo del teléfono celular en los últimos 20 años. Los componentes comunes encontrados en todos los teléfonos son: Una batería, proporcionando la fuente de energía para las funciones del teléfono. Un teléfono moderno generalmente usa una batería de iones de litio (LIB), mientras que los teléfonos más antiguos usaban baterías de hidruro de níquel-metal (Ni-MH). Desde fines de los años 2010 la mayoría de los teléfonos inteligentes las baterías no son extraíbles por el consumidor. Un mecanismo de entrada para dejar interactuar al usuario con el teléfono. La entrada más común son las pantallas táctiles en los teléfonos inteligentes, pero en los más antiguos se usan botones, es decir por medio del teclado físico (del tipo 3x4). Una pantalla que repite al usuario lo que está escribiendo, muestra mensajes de texto, contactos y más. Las pantallas cada vez son de mayor tamaño y resolución, además de ser capacitivas o táctiles (a diferencia de los teléfonos antiguos que solamente actuaban como visualizador). Los servicios básicos de telefonía móvil que permiten a los usuarios hacer llamadas y enviar mensajes de texto. Todos los teléfonos GSM utilizan una tarjeta SIM que permiten tener una cuenta que puede intercambiarse entre los dispositivos. Algunos dispositivos CDMA también tienen una tarjeta similar llamado un R-UIM. Algunos dispositivos GSM, WCDMA, iDEN y algunos teléfonos satelitales se identifican por un número de Identidad del Equipo Móvil Internacional (IMEI). Los teléfonos móviles de gama baja se refieren generalmente como teléfonos con funciones, y ofrecen servicios de telefonía básica. Los terminales con capacidad de computación más avanzada mediante el uso de aplicaciones de software nativas son conocidos como teléfonos inteligentes. Se han introducido varias series de teléfonos para hacer frente a segmentos de mercado específicos, tales como el RIMBlackBerry centrándose en las necesidades de correo electrónico de clientes corporativos/empresariales, la serie Sony-Ericsson 'Walkman' de música/móvil y serie 'Cyber-shot' de cámara/teléfono, el Nokia Nseries de teléfonos multimedia, el Palm Pre, el HTC Dream y el iPhone de Apple. En Argentina cuando se compra un equipo con abono, la empresa de telefonía exige un período mínimo de permanencia, que figura en el contrato. Cuando termina ese plazo, la empresa tiene la obligación de dar el código de desbloqueo del equipo. Si un cliente quiere cambiar de compañía, sin respetar ese período de permanencia, puede hacerlo pagando una suma por esa rescisión anticipada.[16]​ Calidad de sonido En calidad de sonido, los teléfonos inteligentes y los teléfonos con funciones varían muy poco. Han aparecido nuevos teléfonos inteligentes con algunas características que mejora la calidad de audio, tales como Voz sobre LTE y Voz HD. La calidad del sonido aún continúa siendo un problema ya que esto depende no tanto del propio teléfono, sino de la calidad de la red y, en llamadas de larga distancia, los cuellos de botella encontrados en el camino.[17]​[18] Como tal, para llamadas de larga distancia, incluso las características Voz sobre LTE y voz HD puede no mejorar las cosas. En algunos casos, los teléfonos inteligentes pueden mejorar la calidad de audio incluso en llamadas de larga distancia, usando el servicio de telefonía VoIP, con la conexión Wifi/Internet de otra persona. Algunos teléfonos celulares tienen pequeños altavoces de modo que el usuario puede utilizar una función de altavoz y hablar con una persona en el teléfono sin sujetarlo a su oído. También se pueden utilizar pequeños altavoces para escuchar archivos de audio digitales de música o de voz, o ver vídeos con un componente de audio, sin sostener el teléfono cerca de la oreja. Mensajes de texto Artículo principal: SMS La aplicación de datos más utilizada en los teléfonos móviles es el Servicio de Mensajes de texto cortos (SMS). El primer mensaje SMS fue enviado desde un ordenador a un teléfono móvil en 1992 en el Reino Unido, mientras que el primer SMS de persona a persona de un teléfono a otro fue enviado en Finlandia en 1993. El primer servicio móvil de noticias, emitido a través de SMS, fue lanzado en Finlandia en 2000,[cita requerida] y posteriormente, muchas organizaciones proporcionan los servicios de noticias a través de SMS bajo demanda e instantánea. El Servicio de Mensajería Multimedia (MMS) fue introducido en el 2001.[cita requerida] Tarjeta SIM Artículo principal: Tarjeta SIM Típica tarjeta SIM de un teléfono móvil. Los teléfonos con funciones GSM requieren de pequeños microchips llamados Módulo de Identidad de Abonado o tarjeta SIM, para poder funcionar en la red. La tarjeta SIM es aproximadamente del tamaño de un sello de correos pequeño y por lo general se coloca debajo de la batería en la parte trasera del dispositivo. La SIM almacena de forma segura la clave de servicio del abonado (IMSI) y la K¡ usada para identificar y autenticar al usuario del teléfono móvil. La tarjeta SIM permite a los usuarios cambiar de teléfono simplemente retirando la tarjeta SIM de un teléfono móvil e insertándola en otro teléfono móvil o dispositivo de telefonía de banda ancha, siempre que no esté impedido por un bloqueo de SIM. La primera tarjeta SIM fue fabricada en 1991 por el fabricante de tarjetas inteligentes Múnich Giesecke & Devrient para el operador de red inalámbrico finlandés Radiolinja.[20]​[21]​ Existen teléfonos móviles que pueden contener hasta cuatro tarjetas SIM, llamados teléfonos híbridos. Las tarjetas SIM y R-UIM se pueden sincronizar para permitir acceso a las redes GSM y CDMA disponibles. A partir de 2010 este tipo de teléfonos se hizo popular en la India e Indonesia y otros mercados emergentes,[22] esto se atribuyó al deseo de obtener la tasa más baja de llamadas on-net. En el 3T de 2011, Nokia envió 18 millones de teléfonos de doble SIM de su gama de bajo costo en un intento de recuperar el terreno perdido en el mercado de teléfonos inteligentes de alta gama.[23]​ Tipos Teléfonos inteligentes Artículo principal: Teléfono inteligente Diseño de teléfono inteligente (Galaxy Z Fold2 y Galaxy Z Flip2 de Samsung). El término teléfono inteligente (del inglés smartphone) se utiliza más bien con fines comerciales para distinguir de los teléfonos móviles básicos. En gran parte del mundo, los teléfonos inteligentes superaron el uso de los teléfonos convencionales básicos en la década de 2010. Estos dispositivos funcionan sobre una plataforma informática móvil, con mayor capacidad de almacenar datos y capaz de realizar tareas simultáneamente, tareas que realiza una computadora, y con una mayor conectividad que un teléfono convencional. La mayoría de estos dispositivos cuentan con una pantalla capacitiva (táctil) de alta resolución para poder interactuar por medio de la entrada (teclado) virtual, y visualizar el contenido multimedia en mejor calidad.[cita requerida] Teléfonos básicos Artículo principal: Teléfono básico Los términos teléfono básico[24] (del inglés feature phone) y teléfono convencional son retrónimos aplicados a ciertos teléfonos móviles de baja gama o de características límitadas frente a la introducción de los teléfonos inteligentes. Los teléfonos básicos dominaron el mercado de la telefonía celular desde sus inicios, hasta finales de la década de 2000 con el avance de los teléfonos inteligentes. Hoy día siguen existiendo, aunque en menor medida y con algunas diferencias con respecto a los teléfonos básicos que estaban de moda en los años 1990 y 2000. Poseen funciones esenciales como la posibilidad de llamar o enviar mensajes de texto, y en algunos dispositivos el emplear archivos multimedia o navegar por internet usando conexiones de alta velocidad, GSM o WiFi. La mayoría de estos dispositivos cuentan con teclado físico y pantalla pequeña no capacitiva. Teléfonos Kosher Hay restricciones religiosas del judaísmo ortodoxo, que, según algunas interpretaciones, los teléfonos móviles estándares puedan sobrepasar. Para hacer frente a este problema, algunas organizaciones rabínicas han recomendado que los móviles con capacidad de mensajería de texto no puedan ser utilizados por niños.[25] Es así que, a los teléfonos con funciones limitadas son conocidos como teléfonos kosher y tienen la aprobación para su uso rabínico en Israel y en otros lugares por los judíos ortodoxos practicantes. A pesar de que estos teléfonos estén destinados a evitar contenidos obscenos, algunos vendedores reportan buenas ventas en adultos que prefieren la simplicidad de los dispositivos. Algunos teléfonos están aprobados para su uso por los trabajadores esenciales (como trabajadores de salud, de seguridad y de servicios públicos) el Sabbat, a pesar de que en general el uso de cualquier dispositivo eléctrico esté prohibido durante este tiempo.[26]​ Operadores de telefonía móvil Artículo principal: Operador de telefonía móvil Crecimiento en suscriptores de teléfono celular por país desde 1980 a 2009 Son las compañías de teléfono que proporcionan la red telefónica pública conmutada (PSTN), para que los usuarios de teléfonos móviles puedan acceder al servicio celular, ya sea por medio de un contrato (pospago) o por medio de recargas (prepago). El mayor operador móvil del mundo por número de suscriptores es China Mobile, el cual tiene más de 500 millones de suscriptores de telefonía móvil.[27] Más de 50 operadores móviles tienen más de diez millones de suscriptores cada uno, y más de 150 operadores móviles han tenido por lo menos un millón de abonados a finales de 2009.[28] En 2014, había más de siete mil millones de abonados de teléfonos móviles en todo el mundo, un número que se espera que siga aumentando. Fabricantes Artículo principal: Fabricantes de teléfonos móviles por país Antes de 2010, Nokia era el líder del mercado. Sin embargo, desde entonces ha emergido la competencia en la región de Asia y el Pacífico, de marcas como Micromax, Nexian e i-Mobile, que han disminuido la cuota de mercado de Nokia. Los teléfonos inteligentes Android también ganaron mucho terreno en toda la región gracias a Nokia. En la India, la cuota de mercado de Nokia se redujo significativamente desde 56 % hasta aproximadamente el 31 % en el mismo período. Su participación fue desplazada por proveedores de China e India de teléfonos móviles de gama baja.[29]​ En el primer trimestre de 2012, según Strategy Analytics, Samsung superó a Nokia en ventas, de 93,5 millones unidades frente a 82,7 millones de unidades de Nokia. En 2012 Standard & Poor's degradó a Nokia a la condición de estatus basura, en BB+/B, con perspectiva negativa debido a la alta pérdida y una mayor disminución esperada debido al insuficiente crecimiento en las ventas de teléfonos inteligentes Lumia para compensar una rápida disminución de los ingresos procedentes de los teléfonos inteligentes basados en Symbian que se pronostica para los próximos trimestres.[30] Los dispositivos más vendidos, como Samsung y Xiaomi, poseen Android como sistema operativo, siendo este último el sistema operativo celular más popular del mundo. En segundo lugar le sigue el sistema operativo iOS de Apple, con cerca del 20% de la cuota. Cuota de mercado de entre los 5 fabricantes de teléfonos móviles en el mundo, 1T-2022 Rango Fabricante Informe de Shipments Market Share[31]​ 1 Samsung 23 % 2 Apple 18 % 5 Xiaomi 12 % 3 Oppo 9 % 4 VIVO 9 % Otros 30 % Nota: Envíos de proveedores son envíos de marca y se excluyen las ventas OEM para todos los proveedores Otros fabricantes fuera de los primeros cinco lugares incluyen TCL Comunicatión, Lenovo, Sony Mobile Comunications, Motorola y LG Electronics. Pequeños jugadores actuales y pasados incluyen Audiovox (ahora UTStarcom), BenQ-Siemens, BlackBerry, Casio, CECT, Coolpad, Fujitsu, HTC, Just5, Intex, Karbonn Mobiles, Kyocera, Lumigon, Micromax Mobile, Mitsubishi Electric, Modu, NEC, Neonode, Openmoko, Panasonic, Palm, Pantech Wireless Inc., Philips, Sagem, Sanyo, Sharp, Sierra Wireless, SK Teletech, Trium y Toshiba. Uso General Suscriptores de teléfono celular por 100 habitantes. Figura de 2014 estimada. Los teléfonos móviles son usados para una variedad de propósitos, tales como mantener el contacto con miembros de la familia, conducir negocios, y con el fin de tener acceso a un teléfono en el caso de una emergencia. Algunas personas llevan más de un teléfono móvil para diferentes propósitos, tales como para uso comercial y personal. Se pueden usar múltiples tarjetas SIM para tomar ventaja de los beneficios de los diferentes planes de llamadas. Por ejemplo, un plan en particular podría prever llamadas más baratas locales, llamadas de larga distancia, llamadas internacionales, o itinerancia. Suscripciones de banda ancha móviles activas por 100 habitantes[32]​ El teléfono móvil se ha usado en una variedad de diversos contextos de la sociedad. Por ejemplo: Un estudio realizado por Motorola encontró que uno de cada diez usuarios de telefonía móvil tienen un segundo teléfono que a menudo se mantiene en secreto de otros miembros de la familia. Estos teléfonos se pueden utilizar para participar en actividades tales como relaciones extramaritales o tratos comerciales clandestinos.[33]​ Algunas organizaciones ayudan a las víctimas de la violencia doméstica, proporcionando teléfonos móviles para su uso en situaciones de emergencia. Estos son a menudo los teléfonos reacondicionados. El advenimiento de la generalizada mensajería de texto ha producido la novela de teléfono celular, el primer género literario que surge en la era celular, a través de mensajes de texto a un sitio web que recopila novelas en su conjunto.[34]​ La telefonía móvil también facilita el activismo y el periodismo público siendo explorado por Reuters y Yahoo! y pequeñas empresas de noticias independientes como Jasmine News en Sri Lanka. Las Naciones Unidas informaron que los teléfonos móviles se han extendido más rápido que cualquier otra forma de tecnología y pueden mejorar la vida de las personas más pobres en los países en desarrollo, mediante el acceso a la información en los lugares donde la red fija o Internet no están disponibles, especialmente en los países menos desarrollados. El uso de los teléfonos móviles también genera una gran cantidad de microempresas, proporcionando este tipo de trabajo como la venta de tiempo aire en las calles y la reparación o reacondicionamiento de teléfonos.[35]​ En Malí y otros países africanos, la gente solía viajar de pueblo en pueblo para que sus amigos y familiares sepan sobre las bodas, nacimientos y otros eventos. Esto ahora se puede evitar en áreas con cobertura de telefonía móvil, que suelen ser más extensas que las zonas con la penetración de línea fija. La industria de la televisión recientemente ha empezado a utilizar los teléfonos móviles para impulsar la televisión en vivo a través de la visualización de las aplicaciones móviles, publicidad, televisión social, y televisión móvil.[36] Se estima que el 86% de los estadounidenses utilizan sus teléfonos móviles mientras ve la televisión. En algunas partes del mundo, el intercambio de teléfono móvil es común. Es frecuente en la India urbana, ya que las familias y grupos de amigos a menudo comparten uno o más teléfonos móviles entre sus miembros. Hay evidentes beneficios económicos, pero a menudo las costumbres familiares y los roles tradicionales de género juegan un papel.[37] Es común que un pueblo tenga acceso a un solo teléfono móvil, tal vez propiedad de un maestro o misionero, que está disponible para todos los miembros del pueblo para llamadas necesarias.[38]​ Para la distribución de contenidos En 1998, uno de los primeros ejemplos de la distribución y venta de contenidos multimedia a través del teléfono móvil fue la venta de tonos de llamada por Radiolinja en Finlandia. Poco después, apareció otro contenido multimedia, tales como noticias, videojuegos, chistes, horóscopos, contenidos de televisión y publicidad. La mayoría del contenido inicial para los teléfonos móviles tienden a ser copias de medios heredados, tales como anuncios publicitarios o informativos de videoclips televisivos más destacados. Recientemente, un contenido único para los teléfonos móviles ha ido surgiendo, desde tonos de llamada y tonos de espera para movisodios, contenido de vídeo producido exclusivamente para teléfonos móviles. En 2006, el valor total de los contenidos de los medios pagados de telefonía móvil superó al contenido multimedia de Internet pagado por 31 000 millones de dólares. El valor de la música en los teléfonos móviles superó los 9300 millones de dólares en 2007, y de juegos fue más de 5000 millones de dólares en el 2007.[39]​ Al conducir un vehículo Un conductor de Nueva York utilizando dos teléfonos móviles El uso del teléfono móvil mientras se conduce, incluso hablar por teléfono, mensajes de texto u operar otras funciones del teléfono, es muy común pero controvertido. Es ampliamente considerado como peligroso debido a la distracción al manejar. Distraerse mientras se conduce un vehículo motorizado se ha demostrado que aumenta el riesgo de accidentes. En septiembre de 2010, la Administración Nacional de Seguridad del Tráfico de Carreteras (NHTSA) de Estados Unidos informó que 995 personas fueron asesinadas por conductores distraídos por los teléfonos celulares. En marzo de 2011 una compañía de seguros de Estados Unidos, State Farm Insurance, anunció los resultados de un estudio que mostró 19 % de los conductores encuestados accede a Internet en un teléfono inteligente, mientras conduce.[40] Muchas jurisdicciones prohíben el uso de teléfonos móviles mientras conducen. En Egipto, Israel, Japón, Portugal y Singapur está prohibido el uso de ambos, tanto para el uso del dispositivo como del manos libres (que utiliza un altavoz) están prohibidos. En otros países, incluyendo el Reino Unido y Francia y en muchos estados de Estados Unidos, solo el uso de los teléfonos de mano está prohibido, pero se permite el uso de manos libres. Un estudio de 2011 informó que más del 90 % de los estudiantes universitarios encuestados mensajean (iniciar, resporder o leer) mientras conducen. La literatura científica sobre los peligros de conducir mientras se envía un mensaje de texto desde un teléfono móvil, o enviar mensajes de texto mientras se conduce, es limitada. Un estudio de simulación en la Universidad de Utah encontró un aumento de seis veces en accidentes relacionados con la distracción en cuanto a los mensajes de texto.[41]​ Debido a la complejidad creciente de los teléfonos móviles, que a menudo son más como ordenadores móviles por sus usos disponibles. Esto ha introducido dificultades adicionales para las fuerzas del orden cuando se trata de distinguir una de otra en el uso de los conductores que utilizan sus dispositivos. Esto es más evidente en los países que prohíben el uso tanto del aparato de mano y manos libres, en lugar de aquellas que prohíben el uso del aparato telefónico, ya que las autoridades no pueden saber fácilmente qué función del teléfono móvil se utiliza simplemente mirando al conductor. Esto puede llevar a los conductores de ser detenidos por usar su dispositivo de forma ilegal para una llamada telefónica cuando, de hecho, estaban usando el dispositivo de forma legal, por ejemplo, cuando se utilizan controles incorporadas del teléfono para el estéreo del coche, el GPS o el satnav. Una señal a lo largo de Bellaire Boulevard en Southside Place, Texas (Greater Houston) prohíbe el uso de teléfonos celulares mientras se conduce desde las 7:30 a. m. hasta las 9:00 a. m. y las 14:00 hasta las 16:15 Un estudio de 2010 examinó la incidencia de uso del teléfono móvil durante el ciclismo y sus efectos sobre el comportamiento y la seguridad.[42] En 2013 una encuesta nacional en los EE. UU. informó el número de conductores que reportó el uso de sus teléfonos móviles para acceder a Internet mientras conducían se había elevado a casi uno de cada cuatro.[43] Un estudio realizado por la Universidad de Illinois examinó enfoques para reducir el uso inapropiado y problemático de los teléfonos móviles, tales como el uso de teléfonos móviles durante la conducción.[44]​ Los accidentes que involucran a un conductor distraído por hablar con un teléfono móvil han comenzado a ser perseguido como una negligencia similar al exceso de velocidad. En el Reino Unido, desde el 27 de febrero de 2007, los automovilistas que sean encontrados usando un teléfono móvil de mano mientras conduce ganarán tres puntos de penalización en su licencia, además de la multa de 60 libras.[45] Esta medida se introdujo para intentar frenar el incremento de conductores que infringían la ley. En Japón se prohíbe el uso de teléfonos móviles mientras conduce, incluyendo el uso de dispositivos de manos libres. Nueva Zelanda ha prohibido el uso de celulares de mano desde el 1 de noviembre de 2009. Muchos estados en los Estados Unidos han prohibido los mensajes de texto en teléfonos celulares mientras se conduce. Illinois se convirtió en el estado americano número 17 en hacer cumplir esta ley.[46] A partir de julio de 2010, 30 estados habían prohibido enviar mensajes de texto mientras se conduce, con Kentucky convirtiéndose en la más reciente adición el 15 de julio.[47] En México desde 2015 también es considerado un delito que puede ser castigado hasta con MXN$2642.15[48]​[49] En España, conducir y usar el teléfono a la vez está penalizado hasta con 200 Euros y la pérdida de tres puntos.[50]​ Public Health Law Research mantiene una lista de las leyes de distracción al conducir en los Estados Unidos. Esta base de datos de leyes ofrece una visión completa de las provisiones de leyes que restringen el uso de dispositivos de comunicación móvil mientras se conduce para los 50 estados y el Distrito de Columbia entre 1992 (cuando se aprobó la primera ley), hasta el 1 de diciembre de 2010. El conjunto de datos contiene información sobre 22 variables dicotómicas, continuas o categóricas que incluyen, por ejemplo, actividades reguladas (por ejemplo, mensajes de texto versus hablar, manos libres versus sostenerlo), poblaciones específicas y exenciones.[51]​ Banca móvil y pagos Artículos principales: Banca móvil y Pago móvil. Véase también: Pago sin contacto Sistema de pago móvil En muchos países, los teléfonos móviles se utilizan para proporcionar servicios de banca móvil, que pueden incluir la capacidad de transferir los pagos en efectivo por mensaje de texto SMS seguro. El servicio de banca móvil M-PESA de Kenia, por ejemplo, permite a los clientes del operador de telefonía móvil Safaricom retener saldos de efectivo que son registrados en sus tarjetas SIM. El efectivo puede ser depositado o retirado de las cuentas de M-PESA en los puntos de venta Safaricom de todo el país, y se puede transferir electrónicamente de persona a persona y utilizar para pagar las facturas de las empresas. La banca sin sucursales también ha tenido éxito en Sudáfrica y Filipinas. Un proyecto piloto en Bali se puso en marcha en 2011 por la Corporación Financiera Internacional y un banco de Indonesia, Bank Mandiri.[52]​ Otra aplicación de la tecnología de la banca móvil es Zidisha, una plataforma de micropréstamos sin fines de lucro con sede en Estados Unidos que permite a los residentes de los países en desarrollo aumentar los préstamos de pequeñas empresas de usuarios Web en todo el mundo. Zidisha utiliza la banca móvil para los desembolsos de préstamos y reembolsos, la transferencia de fondos de los prestamistas en los Estados Unidos a los prestatarios en zonas rurales de África que tienen teléfonos móviles y que pueden utilizar el Internet.[53]​ Los pagos móviles se pusieron a prueba por primera vez en Finlandia en 1998, cuando se habilitaron dos máquinas expendedoras de Coca-Cola en Espoo para trabajar con pagos SMS. Con el tiempo, la idea se extendió y en 1999, las Filipinas lanzó los primeros sistemas de pagos móviles comerciales del país con los operadores móviles Globe y Smart. Algunos teléfonos móviles pueden realizar pagos móviles a través de programas de facturación móvil directa, o mediante los pagos sin contacto, si el teléfono y el punto de venta soportan Near Field Communication (NFC).[54] La activación de los pagos sin contacto a través de los teléfonos móviles equipados con NFC requiere la cooperación de los fabricantes, operadores de redes y comerciantes al por menor.[55]​[56]​[57]​ Seguimiento y privacidad Artículos principales: Vigilancia de teléfonos móviles y Localización GSM. Los teléfonos móviles se utilizan comúnmente para recopilar datos de localización. Mientras que el teléfono está encendido, la ubicación geográfica de un teléfono móvil se puede determinar con facilidad (si se utiliza o no) usando una técnica conocida como multilateración para calcular las diferencias en tiempo que una señal viaja desde el teléfono móvil a cada una de varias torres de telefonía móvil cercanas al propietario del teléfono.[58]​[59]​ Los movimientos de un usuario de teléfono móvil pueden ser rastreados por su proveedor de servicios y, si se desea, por las fuerzas del orden y sus gobiernos. Tanto la tarjeta SIM como el teléfono pueden ser rastreados.[58]​ China ha propuesto el uso de esta tecnología para realizar un seguimiento de las pautas de movilidad de los residentes de la ciudad de Beijing. En el Reino Unido y los Estados Unidos, la policía y los servicios de inteligencia utilizan teléfonos móviles para realizar operaciones de vigilancia. Poseen tecnología que les permite activar los micrófonos en los teléfonos móviles de forma remota con el fin de escuchar las conversaciones que tienen lugar cerca del teléfono.[60]​[61]​ Los hackers son capaces de rastrear la ubicación de un teléfono, leer los mensajes y grabar las llamadas, con solo saber el número de teléfono.[62]​ Robos De acuerdo con la Comisión Federal de Comunicaciones, uno de cada tres robos involucran el robo de un teléfono móvil. Datos de la Policía en San Francisco muestran que la mitad de todos los robos en 2012 fueron robos de teléfonos móviles. Una petición en línea en Change.org, llamada Asegurar nuestras smartphones (Secure our Smartphones), instó a fabricantes de teléfonos inteligentes a que instalaran interruptores de apagado (kill switches) en sus dispositivos para que queden inutilizables en caso de robo. La petición es parte de un esfuerzo conjunto por el fiscal general de Nueva York, Eric Schneiderman y el fiscal del distrito de San Francisco, George Gascón, y fue dirigido a los directores generales de los principales fabricantes de teléfonos inteligentes y operadores de telecomunicaciones.[63] En México el robo de celulares aumentó un 500% en el año 2017 convirtiéndolos en los objetos más robados, puesto que permite a los delincuentes ganar dinero con su reventa y las penas por ese delito son mínimas.[64]​ El lunes 10 de junio de 2013, Apple anunció que instalaría un interruptor de apagado en su próximo sistema operativo iPhone, debido a su debut en octubre de 2013.[65] Por otro lado Android ofrece su aplicación Encontrar mi dispositivo para encontrar el teléfono perdido, bloquearlo o restablecerlo borrando toda su información a distancia.[66] Asimismo, los proveedores ofrecen el bloqueo por IMEI para dejar inservibles los teléfonos robados.[67] También existen formas de evitar este delito como evitar exponer el teléfono en lugares concurridos, contestar llamadas y mensajes solo en lugares seguros, evitar resistirse al asalto, bloquear el teléfono y evitar usarlo en lugares peligrosos.[68]​ En Argentina, como medida para combatir los robos a los teléfonos celulares y dar más seguridad a los usuarios del servicio, los titulares de líneas de telefonía celular deben registrar sus datos en los registros de las compañías de telefonía celular.[69]​ Efectos en la salud Artículo principal: Radiación de teléfonos móviles y salud El efecto de la radiación del teléfono móvil en la salud humana es el tema de reciente interés y estudio, como resultado del enorme aumento en el uso de teléfonos móviles en todo el mundo. Los teléfonos móviles utilizan radiación electromagnética en el rango de las microondas, que algunos creen que puede ser perjudicial para la salud humana. Existe una gran cantidad de investigaciones, tanto epidemiológica y experimental, en animales no humanos y en los seres humanos. La mayoría de estas investigaciones no muestran relación causal clara entre la exposición a los teléfonos móviles y los efectos biológicos nocivos en los seres humanos. Esto a menudo se parafraseó simplemente como el balance de evidencia que muestra ningún daño de los teléfonos móviles a los seres humanos, aunque un número significativo de estudios individuales sí sugieren una relación, o no son concluyentes. Otros sistemas inalámbricos digitales, tales como las redes de comunicación de datos, producen radiación similar. El 31 de mayo de 2011, la Organización Mundial de la Salud declaró que el uso de teléfonos móviles, posiblemente, puede representar riesgo para la salud a largo plazo,[70]​[71] clasificando la radiación de los teléfonos móviles como Posiblemente cancerígeno para los seres humanos después de que un equipo de científicos revisaran estudios en teléfono móvil seguros.[72] El teléfono móvil está en la categoría 2B, lo que la ubica junto al café y otras posibles sustancias cancerígenas.[73]​[74]​ Algunos estudios recientes han encontrado una asociación entre el uso de teléfonos móviles y algunos tipos de tumores cerebrales y de la glándula salival. Lennart Hardell y otros autores de un metaanálisis del 2009 sobre 11 estudios de revistas revisadas por pares concluyeron que el uso del teléfono celular durante al menos diez años duplica aproximadamente el riesgo de ser diagnosticado con un tumor cerebral en el mismo ('ipsilateral') lado de la cabeza que prefiera usar teléfonos celulares.[75]​ Un estudio anterior sobre el uso del teléfono móvil, muestra un informe del 40% de aumento en el riesgo de gliomas (cáncer cerebral) en la máxima categoría de grandes usuarios (media: 30 minutos por día durante un período de 10 años).[76] Esto es un revés al estudio previo con la posición de que el cáncer es poco probable que sea causado por los teléfonos móviles o sus estaciones base y los comentarios que no habían encontrado evidencia convincente para otros efectos en la salud.[71]​[77] Sin embargo, un estudio publicado el 24 de marzo de 2012 en la British Medical Journal cuestionando estas estimaciones, debido a que el aumento en los cánceres de cerebro no ha sido paralelo al aumento del uso de teléfonos móviles.[78] Ciertos países, como Francia, han advertido sobre el uso de teléfonos móviles por los menores de edad en particular, debido a la incertidumbre sobre el riesgo para la salud.[79] La contaminación de móviles mediante la transmisión de ondas electromagnéticas puede disminuir hasta un 90 % adoptando el circuito tal como fue diseñado en el teléfono móvil (MS) y central móvil (BTS, MSC, etc.).[80]​ En mayo de 2016, los resultados preliminares de un estudio a largo plazo por el gobierno de los Estados Unidos sugirió que la radiación de la radiofrecuencia (RF), del tipo emitida por los teléfonos celulares pueden causar cáncer.[81]​[82]​ Evolución futura Artículo principal: Telefonía móvil 5G 5G es una tecnología y un término utilizado en trabajos y proyectos de investigación para referirse a la siguiente fase importante en las normas de telecomunicaciones móviles más allá de los estándares 4G/IMT-Avanced. El término 5G no se utiliza oficialmente en cualquier especificación o documento aún hecha pública oficial por las empresas de telecomunicaciones o los organismos de normalización, tales como 3GPP, WiMAX Forum o UIT-R. Nuevos estándares más allá de 4G actualmente están siendo desarrolladas por los organismos de normalización, pero son en este momento visto como bajo el paraguas 4G, no para una nueva generación móvil. Deloitte predice un colapso en el rendimiento inalámbrico a venir tan pronto como 2016, a medida que más dispositivos que utilizan cada vez más servicios compiten por el ancho de banda limitado para su operación.[83] En 2019 ya hay a la venta teléfonos con capacidad 5G, como el LG V50 ThinQ 5G,[84] aunque apenas hay redes con cobertura 5G en el mundo. Es una tecnología que se impondrá progresivamente, a partir del año 2022. Impacto ambiental Un quiosco de reparación de teléfonos celulares en Hong Kong Los estudios han demostrado que alrededor del 40-50 % del impacto ambiental de los teléfonos móviles se produce durante la fabricación de sus placas de circuitos impresos y circuitos integrados.[85]​ El usuario promedio reemplaza su teléfono móvil cada 11 a 18 meses,[86] y los celulares descartados luego contribuyen a la basura electrónica. Fabricantes de teléfonos móviles en Europa están sujetos a la directiva WEEE, y Australia ha introducido un sistema de reciclaje de teléfonos móviles.[87]​ Apple se ha dado cuenta de cómo sus productos cuando no se reciclan impactan al medio ambiente y los residuos son valiosos recursos. Liam de Apple se introdujo al mundo como un desensamblador robótico avanzado y clasificador diseñada por Ingenieros de Apple en California específicamente para el reciclaje de iPhones obsoletos o rotos. Reutiliza y recicla piezas de productos negociados. Minerales conflictivos La demanda de los metales usados en los teléfonos móviles y otros electrónicos alimentaron la segunda guerra del Congo, que cobró casi 5,5 millones de vidas.[88] En una noticia de 2012, The Guardian informó: Inseguridad en minas subterráneas profundas en el este del Congo, niños trabajan para extraer minerales esenciales para la industria de la electrónica. Los beneficios de los minerales financian el conflicto más sangriento desde la segunda guerra mundial; la guerra ha durado más de 20 años y recientemente ha estallado de nuevo... Durante los últimos 15 años, la República Democrática del Congo ha sido una importante fuente de recursos naturales para la industria de la telefonía móvil.[89]​ La compañía Fairphone ha intentado desarrollar un teléfono móvil que no contiene minerales conflictivos. Reciclaje Esta sección es un extracto de Reciclaje de teléfonos móviles.[editar] Teléfonos celulares desechados. El reciclaje de teléfonos móviles es el proceso de reciclaje que pueden seguir los teléfonos móviles (también llamados celulares) al final de su vida útil. A causa del frecuente cambio de dispositivos, el bajo costo, e incluso la obsolescencia programada, muchísimos teléfonos son desechados anualmente, lo cual contribuye a la creciente cantidad de residuos electrónicos alrededor del mundo. Los recicladores consideran a este tipo de deshechos un problema que se expande rápidamente.[90] En los Estados Unidos, aproximadamente un 70% de metales pesados en los basureros proviene de tecnología desechada. Los residuos electrónicos apenas representan entre un 2% y un 5% de la basura en los vertederos de aquel país, sin embargo esta cifra crece rápidamente.[91]​[92]​ Los teléfonos celulares son considerados residuos peligrosos en California. Muchas sustancias químicas presentes en tales dispositivos drenan de los basureros al sistema de agua subterránea.[93] La organización Greenpeace advierte de que al estar soldada la batería del iPhone a su carcasa, se dificulta su reciclado. También señala que sus científicos han encontrado tóxicos Ftalatos en los cables de este modelo de teléfono celular, y añade que esto va en contra de la proposición 65, la cual exige advertir de esto en las etiquetas de los productos que exponen a los consumidores a esta sustancia tóxica.[94]​ Estados Unidos no ha ratificado la Convención de Basilea ni su enmienda de Prohibición, y no posee leyes nacionales que prohíban la exportación de residuos tóxicos. La Basel Action Network estima que, aproximadamente un 80% de los residuos electrónicos que se enviaron a reciclar en los EE. UU. no se reciclaron del todo, sino que se dirigieron en buques portacontenedores a países como China. Algunos lugares como Guiyu en la región de Shantou, China; y Delhi y Bangalore en India, tienen áreas de procesamiento de residuos electrónicos."
ksampletext_wikipedia_tech_inteligenciaartificial: str = "Inteligencia artificial. La inteligencia artificial, abreviado como IA, en el contexto de las ciencias de la computación, es una disciplina y un conjunto de capacidades cognoscitivas e intelectuales expresadas por sistemas informáticos o combinaciones de algoritmos cuyo propósito es la creación de máquinas que imiten la inteligencia humana. Estas tecnologías permiten que las máquinas aprendan de la experiencia, se adapten a nuevas entradas y realicen tareas humanas como el reconocimiento de voz, la toma de decisiones, la traducción de idiomas o la visión por computadora.[1]​[2]​ En la actualidad, la inteligencia artificial abarca una gran variedad de subcampos. Estos van desde áreas de propósito general, aprendizaje y percepción, a otras más específicas como el reconocimiento de voz, el juego de ajedrez, la demostración de teoremas matemáticos, la escritura de poesía y el diagnóstico de enfermedades. La inteligencia artificial sintetiza y automatiza tareas que en principio son intelectuales y, por lo tanto, es potencialmente relevante para cualquier ámbito de actividades intelectuales humanas. En este sentido, es un campo genuinamente universal, además, la IA se encuentra en constante evolución gracias al desarrollo de tecnologías como el aprendizaje profundo, redes neuronales y procesamiento del lenguaje natural, lo cual permite un avance acelerado en su capacidad para resolver problemas complejos.[3]​ La arquitectura de las inteligencias artificiales y los procesos por los cuales aprenden, se mejoran y se implementan en algún área de interés que varía según el enfoque de utilidad que se les quiera dar, pero de manera general, estos van desde la ejecución de sencillos algoritmos hasta la interconexión de complejas redes neuronales artificiales que intentan replicar los circuitos neuronales del cerebro humano y que aprenden mediante diferentes modelos de aprendizaje tales como el aprendizaje automático, el aprendizaje por refuerzo, el aprendizaje profundo y el aprendizaje supervisado.[4]​ Por otro lado, el desarrollo y aplicación de la inteligencia artificial en muchos aspectos de la vida cotidiana también ha propiciado la creación de nuevos campos de estudio como la roboética y la ética de las máquinas, que abordan aspectos relacionados con la ética en la inteligencia artificial y que se encargan de analizar cómo los avances en este tipo de tecnologías impactarían en diversos ámbitos de la vida, así como el manejo responsable y ético que se les debería dar a los mismos, además de establecer cuál debería ser la manera correcta de proceder de las máquinas y las reglas que deberían cumplir.[5]​[6]​ En cuanto a su clasificación, tradicionalmente se divide a la inteligencia artificial en inteligencia artificial débil, la cual es la única que existe en la actualidad y que se ocupa de realizar tareas específicas, e inteligencia artificial general, que sería una IA que excediese las capacidades humanas. Algunos expertos creen que si alguna vez se alcanzara este nivel, se podría dar lugar a la aparición de una singularidad tecnológica, es decir, una entidad tecnológica superior que se mejoraría a sí misma constantemente, volviéndose incontrolable para los humanos, dando pie a teorías como el basilisco de Roko.[7]​ Algunas de las inteligencias artificiales más conocidas y utilizadas en la actualidad alrededor del mundo incluyen inteligencia artificial en el campo de la salud, asistentes virtuales como Alexa, el asistente de Google o Siri, traductores automáticos como el traductor de Google y DeepL, sistemas de recomendación como el de la plataforma digital de YouTube, motores de ajedrez y otros juegos como Stockfish y AlphaZero, chatbots como ChatGPT, creadores de arte de inteligencia artificial como Midjourney, Dall-e, Leonardo y Stable Diffusion, e incluso la conducción de vehículos autónomos como Tesla Autopilot.[8]​ Denominación Ilustración generada mediante inteligencia artificial, con estilo de Acuarela, de Alan Turing, considerado uno de los padres fundadores de la IA.[9]​ En 2019 la Comisión Mundial de Ética del Conocimiento Científico y la Tecnología (COMEST) de la UNESCO definió la inteligencia artificial como un campo que implica máquinas capaces de imitar determinadas funcionalidades de la inteligencia humana, incluidas características como la percepción, el aprendizaje, el razonamiento, la resolución de problemas, la interacción lingüística e incluso la producción de trabajos creativos. Coloquialmente, la locución «inteligencia artificial» se aplica cuando una máquina imita las funciones «cognitivas» que los humanos asocian como competencias humanas; por ejemplo: «percibir», «razonar», «aprender» y «resolver problemas».[10] Andreas Kaplan y Michael Haenlein definen la inteligencia artificial como «la capacidad de un sistema para interpretar correctamente datos externos, y así aprender y emplear esos conocimientos para lograr tareas y metas concretas a través de la adaptación flexible».[11] A medida que las máquinas se vuelven cada vez más capaces, se elimina de la definición la tecnología que alguna vez se pensó que requería de inteligencia. Marvin Minsky, uno de los ideadores de la IA, hablaba del término inteligencia artificial como una palabra maleta («suitcase word») porque en él se pueden meter una diversidad de elementos.[12]​[13]​ Por ejemplo, el reconocimiento óptico de caracteres ya no se percibe como un ejemplo de la «inteligencia artificial», habiéndose convertido en una tecnología común.[14] Avances tecnológicos todavía clasificados como inteligencia artificial son los sistemas de conducción autónomos o los capaces de jugar al ajedrez o Go.[15]​ La inteligencia artificial es una nueva forma de resolver problemas dentro de los cuales se incluyen los sistemas expertos, el manejo y control de robots y los procesadores, que intenta integrar el conocimiento en tales sistemas; en otras palabras, un sistema inteligente capaz de escribir su propio programa. Un sistema experto definido como una estructura de programación capaz de almacenar y utilizar un conocimiento sobre un área determinada que se traduce en su capacidad de aprendizaje.[16] De igual manera, se puede considerar a la IA como la capacidad de las máquinas para usar algoritmos, aprender de los datos y utilizar lo aprendido en la toma de decisiones tal y como lo haría un ser humano.[17]​ Según Takeyas (2007), la IA es una rama de las ciencias computacionales encargada de estudiar modelos de cómputo capaces de realizar actividades propias de los seres humanos con base en dos de sus características primordiales: el razonamiento y la conducta.[18]​ En 1956, John McCarthy acuñó la expresión «inteligencia artificial», y la definió como «la ciencia e ingenio de hacer máquinas inteligentes, especialmente programas de cómputo inteligentes».[19]​ Grau-Luque contrasta diferentes definiciones desde diversas fuentes y autores, destacando que difieren dependiendo de «en qué campo específico se usen».[20]​Esto lleva al autor a definir «inteligencia artificial» como «sistemas que llevan a cabo tareas consideradas inteligentes», para luego asociar conceptos como «aprendizaje» y «razonamiento» con el aprendizaje automático como una subdisciplina de la inteligencia artificial. También existen distintos tipos de percepciones y acciones, que pueden ser obtenidas y producidas, respectivamente, por sensores físicos y sensores mecánicos en máquinas, pulsos eléctricos u ópticos en computadoras, tanto como por entradas y salidas de bits de un software y su entorno hardware. Varios ejemplos se encuentran en el área de control de sistemas, planificación automática, la capacidad de responder a diagnósticos y a consultas de los consumidores, reconocimiento de escritura, reconocimiento del habla y reconocimiento de patrones. Los sistemas de IA actualmente son parte de la rutina en campos como economía, medicina, ingeniería, el transporte, las comunicaciones y la milicia, y se ha usado en gran variedad de programas informáticos, juegos de estrategia, como ajedrez de computador, y otros videojuegos. Asimismo la inteligencia artificial se está desarrollando en la plataforma digital cada vez más, evolucionando y creando nuevas herramientas, como la plataforma laboral que existe desde el año 2023 llamada SIVIUM, una herramienta por la cual una persona postula en forma automatizada a todas las ofertas laborales de todos los portales de trabajo, sin necesidad de estar revisando cada oferta laboral que se presente y enviar su CV uno por uno.[21]​ Tipos Stuart J. Russell y Peter Norvig diferencian varios tipos de inteligencia artificial:[22]​ Los sistemas que piensan como humanos: Estos sistemas tratan de emular el pensamiento humano; por ejemplo, las redes neuronales artificiales. La automatización de actividades que vinculamos con procesos de pensamiento humano, actividades como la toma de decisiones, resolución de problemas y aprendizaje.[23]​ Los sistemas que actúan como humanos: Estos sistemas tratan de actuar como humanos; es decir, imitan el comportamiento humano; por ejemplo, la robótica (el estudio de cómo lograr que los computadores realicen tareas que, por el momento, los humanos hacen mejor).[24]​ Los sistemas que piensan racionalmente: Es decir, con lógica (idealmente), tratan de imitar el pensamiento racional del ser humano; por ejemplo, los sistemas expertos, (el estudio de los cálculos que hacen posible percibir, razonar y actuar).[25]​ Los sistemas que actúan racionalmente: Tratan de emular de forma racional el comportamiento humano; por ejemplo, los agentes inteligentes, que están relacionados con conductas inteligentes en artefactos.[26]​ Inteligencia artificial generativa Artículo principal: Inteligencia artificial generativa La inteligencia artificial generativa es un tipo de sistema de inteligencia artificial capaz de generar texto, imágenes u otros medios en respuesta a comandos de texto conocidos como prompts.[27] Un prompt es la instrucción, pregunta o conjunto de indicaciones para que la inteligencia artificial realice la tarea específica proporcione la respuesta requerida. La calidad del prompt influye directamente en la calidad de la respuesta.[28]​ Los modelos de IA generativa aprenden los patrones y la estructura de sus datos de entrenamiento de entrada, y luego generan nuevos datos que tienen características similares. Los sistemas de IA generativa notables incluyen ChatGPT (y su variante Microsoft Copilot), un bot conversacional creado por OpenAI usando sus modelos de lenguaje grande fundacionales GPT-3 y GPT-4; Gemini (anteriormente llamado Bard), un bot conversacional creado por Google usando el modelo de lenguaje Gemini; y Claude, un bot conversacional creado por Anthropic usando los modelos del mismo nombre.[29]​Otros modelos generativos de IA incluyen sistemas de arte de inteligencia artificial como Stable Diffusion, Midjourney y DALL-E, que permiten crear imágenes. Actualmente la IA generativa puede crear texto, código, imágenes, vídeo, música, voces y efectos de sonido. Inteligencia artificial fuerte Artículo principal: Inteligencia artificial fuerte La Inteligencia artificial fuerte (IGA) es un tipo hipotético de inteligencia artificial que iguala o excede la inteligencia humana promedio. Si se hiciera realidad, una IGA podría aprender a realizar cualquier tarea intelectual que los seres humanos o los animales puedan llevar a cabo. Alternativamente, la IGA se ha definido como un sistema autónomo que supera las capacidades humanas en la mayoría de las tareas económicamente valiosas. Algunos sostienen que podría ser posible en años o décadas; otros, que podría tardar un siglo o más; y una minoría cree que quizá nunca se consiga. Existe un debate sobre la definición exacta de IGA y sobre si los grandes modelos de lenguaje (LLM) modernos, como el GPT-4, son formas tempranas pero incompletas de IGA. Inteligencia artificial explicable Artículo principal: Inteligencia artificial explicable La inteligencia artificial explicable se refiere a métodos y técnicas en la aplicación de tecnología de inteligencia artificial por los que el ser humano es capaz de comprender las decisiones y predicciones realizadas por la inteligencia artificial. Inteligencia artificial amigable Artículo principal: Inteligencia artificial amigable La inteligencia artificial amigable es una IA fuerte e hipotética que puede tener un efecto positivo más que uno negativo sobre la humanidad. 'Amigable' es usado en este contexto como terminología técnica y escoge agentes que son seguros y útiles, no necesariamente aquellos que son «amigables» en el sentido coloquial. El concepto es invocado principalmente en el contexto de discusiones de agentes artificiales de automejora recursiva que rápidamente explota en inteligencia, con el argumento de que esta tecnología hipotética pudiera tener una larga, rápida y difícil tarea de controlar el impacto en la sociedad humana. Inteligencia artificial multimodal Artículo principal: Inteligencia artificial multimodal La inteligencia artificial multimodal es un tipo de inteligencia artificial que puede procesar e integrar datos de diferentes modalidades, como texto, imágenes, audio y video, para obtener una comprensión más completa y contextualizada de una situación. La inteligencia artificial multimodal se inspira en la forma en que los humanos usan varios sentidos para percibir e interactuar con el mundo, y ofrece una forma más natural e intuitiva de comunicarse con la tecnología. Inteligencia artificial cuántica Artículo principal: Inteligencia Artificial Cuántica La inteligencia artificial Cuántica es un campo interdisciplinar que se enfoca en construir algoritmos cuánticos para mejorar las tareas computacionales dentro de la IA, incluyendo subcampos como el aprendizaje automático. Existen evidencias que muestran una posible ventaja cuadrática cuántica en operaciones fundamentales de la IA. Escuelas de pensamiento La IA se divide en dos escuelas de pensamiento: La inteligencia artificial convencional. La inteligencia computacional. Inteligencia artificial convencional Se conoce también como IA simbólica-deductiva. Está basada en el análisis formal y estadístico del comportamiento humano ante diferentes problemas: Razonamiento basado en casos: Ayuda a tomar decisiones mientras se resuelven ciertos problemas concretos y, aparte de que son muy importantes, requieren de un buen funcionamiento. Sistemas expertos: Infieren una solución a través del conocimiento previo del contexto en que se aplica y utiliza ciertas reglas o relaciones.[30]​ Redes bayesianas: Propone soluciones mediante inferencia probabilística.[31]​ Inteligencia artificial basada en comportamientos: Esta inteligencia contiene autonomía, es decir, puede autorregularse y controlarse para mejorar. Smart process management: Facilita la toma de decisiones complejas, proponiendo una solución a un determinado problema al igual que lo haría un especialista en dicha actividad. Inteligencia artificial computacional Artículo principal: Inteligencia computacional La inteligencia computacional (también conocida como IA subsimbólica-inductiva) implica desarrollo o aprendizaje interactivo (por ejemplo, modificaciones interactivas de los parámetros en sistemas de conexiones). El aprendizaje se realiza basándose en datos empíricos. La inteligencia computacional tiene una doble finalidad. Por un lado, su objetivo científico es comprender los principios que posibilitan el comportamiento inteligente (ya sea en sistemas naturales o artificiales) y, por otro, su objetivo tecnológico consiste en especificar los métodos para diseñar sistemas inteligentes.[32]​ Historia Artículo principal: Historia de la inteligencia artificial La expresión «inteligencia artificial» fue acuñada formalmente en 1955 en el documento[33] con la propuesta para la Conferencia de Dartmouth como una manera de separar la disciplina del campo de los autómatas y señalar su objetivo prioritario de la creación de máquinas que realizasen tareas similares a la inteligencia humana. El término además parecía más apropiado para obtener subvenciones que el original de «estudios de autómatas» , como explicó uno de sus autores, John McCarthy, en el debate Lighthill de 1973[34]​ Las ideas más básicas se remontan a los antiguos griegos. Aristóteles (384-322 a. C.) fue el primero en describir un conjunto de reglas que describen una parte del funcionamiento de la mente para obtener conclusiones racionales, y Ctesibio de Alejandría (250 a. C.) construyó la primera máquina autocontrolada, un regulador del flujo de agua (racional pero sin razonamiento). En 1315 Ramon Llull en su libro Ars magna tuvo la idea de que el razonamiento podía ser efectuado de manera artificial. En 1840 Ada Lovelace previó la capacidad de las máquinas para ir más allá de los simples cálculos y aportó una primera idea de lo que sería el software. En 1912 Leonardo Torres Quevedo, desarrolló un autómata capaz de jugar al ajedrez (el ajedrecista). En 1936 Alan Turing diseña formalmente una Máquina universal que demuestra la viabilidad de un dispositivo físico para implementar cualquier cómputo formalmente definido. En 1943 Warren McCulloch y Walter Pitts presentaron su modelo de neuronas artificiales, el cual se considera el primer trabajo del campo, aun cuando todavía no existía el término. Los primeros avances importantes comenzaron a principios del año 1950 con el trabajo de Alan Turing, a partir de lo cual la ciencia ha pasado por diversas situaciones. En 1955 Herbert Simon, Allen Newell y Joseph Carl Shaw, desarrollan el primer lenguaje de programación orientado a la resolución de problemas, el IPL-11. Un año más tarde desarrollan el LogicTheorist, el cual era capaz de demostrar teoremas matemáticos. En 1955 fue ideada la expresión «inteligencia artificial» por John McCarthy, Marvin Minsky y Claude Shannon en el documento[33] con la propuesta para la Conferencia de Dartmouth de 1956. El término fue introducido como una manera de separar la disciplina del campo de los autómatas y señalar su objetivo prioritario de la creación de máquinas que realizasen tareas similares a la inteligencia humana. El término además parecía más apropiado para obtener subvenciones que el original de «estudios de autómatas» , como explicó uno de sus autores, John McCarthy, en el debate Lighthill de 1973[34] En la conferencia se hicieron previsiones triunfalistas a diez años que jamás se cumplieron, lo que provocó el abandono casi total de las investigaciones durante quince años. En 1957 Newell y Simon continúan su trabajo con el desarrollo del General Problem Solver (GPS). GPS era un sistema orientado a la resolución de problemas. En 1958 John McCarthy desarrolla en el Instituto Tecnológico de Massachusetts (MIT) el LISP. Su nombre se deriva de LISt Processor. LISP fue el primer lenguaje para procesamiento simbólico. En 1959 Rosenblatt introduce el «perceptrón». A finales de la década de 1950 y comienzos de la de 1960 Robert K. Lindsay desarrolla «Sad Sam», un programa para la lectura de oraciones en inglés y la inferencia de conclusiones a partir de su interpretación. En 1963 Quillian desarrolla las redes semánticas como modelo de representación del conocimiento. En 1964 Bertrand Raphael construye el sistema SIR (Semantic Information Retrieval) el cual era capaz de inferir conocimiento basado en información que se le suministra. Bobrow desarrolla STUDENT. A mediados de los años 60, aparecen los sistemas expertos, que predicen la probabilidad de una solución bajo un set de condiciones. Por ejemplo, DENDRAL, iniciado en 1965 por Buchanan, Feigenbaum y Lederberg, el primer Sistema Experto, que asistía a químicos en estructuras químicas complejas, MACSYMA, que asistía a ingenieros y científicos en la solución de ecuaciones matemáticas complejas. Posteriormente entre los años 1968-1970 Terry Winograd desarrolló el sistema SHRDLU, que permitía interrogar y dar órdenes a un robot que se movía dentro de un mundo de bloques. En 1966 y 1972 Artificial Intelligence Center desarrollo el Robot Shakey, considerado el primer robot inteligente de la historia. El robot incluía visión artificial, y tenía la capacidad de percibir y razonar sobre su entorno. En 1968 Marvin Minsky publica Semantic Information Processing. En 1968 Seymour Papert, Danny Bobrow y Wally Feurzeig desarrollan el lenguaje de programación LOGO. En 1969 Alan Kay desarrolla el lenguaje Smalltalk en Xerox PARC y se publica en 1980. En 1973 Alain Colmenauer y su equipo de investigación en la Universidad de Aix-Marseille crean PROLOG (del francés PROgrammation en LOGique) un lenguaje de programación ampliamente utilizado en IA. En 1973 Shank y Abelson desarrollan los guiones, o scripts, pilares de muchas técnicas actuales en inteligencia artificial y la informática en general. En 1974 Edward Shortliffe escribe su tesis con MYCIN, uno de los Sistemas Expertos más conocidos, que asistió a médicos en el diagnóstico y tratamiento de infecciones en la sangre. En las décadas de 1970 y 1980, creció el uso de sistemas expertos, como MYCIN: R1/XCON, ABRL, PIP, PUFF, CASNET, INTERNIST/CADUCEUS, etc. Algunos permanecen hasta hoy (Shells) como EMYCIN, EXPERT, OPSS. En 1981 Kazuhiro Fuchi anuncia el proyecto japonés de la quinta generación de computadoras. En 1986 McClelland y Rumelhart publican Parallel Distributed Processing (Redes Neuronales). En 1987 Hitachi desarrollo el Sendai Subway 1000, el primer tren autónomo de la historia. Fue el primer de tren del mundo en utilizar lógica difusa para controlar su velocidad, arranques y paradas. En 1988 se establecen los lenguajes Orientados a Objetos. En 1997 Gari Kaspárov, campeón mundial de ajedrez, pierde ante la computadora autónoma Deep Blue. En 2006 se celebró el aniversario con el Congreso en español 50 años de inteligencia artificial - Campus Multidisciplinar en Percepción e Inteligencia 2006. En 2009 ya había en desarrollo sistemas inteligentes terapéuticos que permiten detectar emociones para poder interactuar con niños autistas. En 2011 IBM desarrolló un superordenador llamado Watson, el cual ganó una ronda de tres juegos seguidos de Jeopardy!, venciendo a sus dos máximos campeones, y ganando un premio de 1 millón de dólares que IBM luego donó a obras de caridad.[35]​ En 2016, un programa informático ganó cinco a cero al triple campeón de Europa de Go.[36]​ En 2016, el entonces presidente Obama habla sobre el futuro de la inteligencia artificial y la tecnología.[37]​ Existen personas que al dialogar sin saberlo con un chatbot no se percatan de hablar con un programa, de modo tal que se cumple la prueba de Turing como cuando se formuló: «Existirá inteligencia artificial cuando no seamos capaces de distinguir entre un ser humano y un programa informático en una conversación a ciegas». En 2017 AlphaGo desarrollado por DeepMind derrota 4-1 en una competencia de Go al campeón mundial Lee Sedol. Este suceso fue muy mediático y marcó un hito en la historia de este juego.[38] A finales de ese mismo año, Stockfish, el motor de ajedrez considerado el mejor del mundo con 3 400 puntos ELO, fue abrumadoramente derrotado por AlphaZero con solo conocer las reglas del juego y tras solo 4 horas de entrenamiento jugando contra sí mismo.[39]​ Como anécdota, muchos de los investigadores sobre IA sostienen que «la inteligencia es un programa capaz de ser ejecutado independientemente de la máquina que lo ejecute, computador o cerebro». En 2017 un grupo de ingenieros en Google inventan la arquitectura de transformador, un modelo de deep learning que alumbró una nueva generación de modelos grandes de lenguaje, empezando por BERT, y luego el revolucionario GPT de OpenAI.[40]​ En 2018, se lanza el primer televisor con inteligencia artificial por parte de LG Electronics con una plataforma denominada ThinQ.[41]​ En 2019, Google presentó su Doodle en que, con ayuda de la inteligencia artificial, hace un homenaje a Johann Sebastian Bach, en el que, añadiendo una simple melodía de dos compases la IA crea el resto. En 2020, la OECD (Organización para la Cooperación y el Desarrollo Económico) publica el documento de trabajo intitulado Hola, mundo: La inteligencia artificial y su uso en el sector público, dirigido a funcionarios de gobierno con el afán de resaltar la importancia de la IA y de sus aplicaciones prácticas en el ámbito gubernamental.[42]​ Al final del año 2022, se lanzó ChatGPT, una inteligencia artificial generativa capaz de escribir textos y responder preguntas en muchos idiomas. Dado que la calidad de las respuestas recordaba inicialmente al nivel humano, se generó un entusiasmo mundial por la IA[43] y ChatGPT alcanzó más de 100 millones de usuarios dos meses después de su lanzamiento.[44] Más tarde, los expertos notaron que ChatGPT proporciona información errónea en áreas donde no tiene conocimiento («alucinaciones de datos»), la cual a primera vista parece creíble debido a su perfecta redacción.[45]​ En 2023, las fotos generadas por IA alcanzaron un nivel de realismo que las hacía confundirse con fotos reales. Como resultado, hubo una ola de «fotos» generadas por IA que muchos espectadores creyeron que eran reales. Una imagen generada por Midjourney se destacó, mostrando al papa Francisco con un elegante abrigo blanco de invierno.[46]​ Tendencias En 2024, se realizaron avances significativos en varias áreas de la IA: Aprendizaje automático y profundo: Se espera que estas técnicas permitan a las máquinas aprender de manera más eficiente y precisa de grandes volúmenes de datos, mejorando capacidades en procesamiento del lenguaje natural, visión por computadora y toma de decisiones automatizada.[2]​[3]​ Procesamiento del Lenguaje Natural (PLN): Los avances en PLN permitirán a las máquinas comprender y responder al lenguaje humano de manera más natural y precisa, abriendo nuevas posibilidades en atención al cliente automatizada y generación de contenido.[1]​[5]​ Analítica predictiva y prescriptiva: Estas técnicas utilizarán datos y modelos matemáticos para prever el futuro y recomendar acciones, permitiendo a las organizaciones anticipar y abordar problemas de manera proactiva.[1]​ Integración del Internet de las cosas (IoT) y la IA: Permitirá a las máquinas recopilar y analizar datos en tiempo real para tomar decisiones autónomas y mejorar la eficiencia.[1]​ IA Generativa: Estará más al alcance de las personas sin conocimientos técnicos, permitiendo la creación de chatbots personalizados y otros modelos generativos.[47]​ Implicaciones sociales, éticas y filosóficas Artículo principal: Ética en la inteligencia artificial Ante la posibilidad de crear máquinas dotadas de inteligencia, se volvió importante preocuparse por la cuestión ética de las máquinas para tratar de garantizar que no se produzca ningún daño a los seres humanos, a otros seres vivos e incluso a las mismas máquinas según algunas corrientes de pensamiento.[48] Es así como surgió un amplio campo de estudios conocido como ética de la inteligencia artificial de relativamente reciente aparición y que generalmente se divide en dos ramas, la roboética, encargada de estudiar las acciones de los seres humanos hacia los robots, y la ética de las máquinas encargada del estudio del comportamiento de los robots para con los seres humanos. El acelerado desarrollo tecnológico y científico de la inteligencia artificial que se ha producido en el siglo XXI supone también un importante impacto en otros campos. En la economía mundial durante la segunda revolución industrial se vivió un fenómeno conocido como desempleo tecnológico, que se refiere a cuando la automatización industrial de los procesos de producción a gran escala reemplaza la mano de obra humana. Con la inteligencia artificial podría darse un fenómeno parecido, especialmente en los procesos en los que interviene la inteligencia humana, tal como se ilustraba en el cuento ¡Cómo se divertían! de Isaac Asimov, en el que su autor vislumbra algunos de los efectos que tendría la interacción de máquinas inteligentes especializadas en pedagogía infantil, en lugar de profesores humanos, con los niños en etapa escolar. Este mismo escritor diseñó lo que hoy se conocen como las tres leyes de la robótica, aparecidas por primera vez en el relato Círculo vicioso (Runaround) de 1942, donde establecía lo siguiente: Primera Ley Un robot no hará daño a un ser humano ni, permitirá que un ser humano sufra daño. Segunda Ley Un robot debe cumplir las órdenes dadas por los seres humanos, a excepción de aquellas que entren en conflicto con la primera ley. Tercera Ley Un robot debe proteger su propia existencia en la medida en que esta protección no entre en conflicto con la primera o con la segunda ley.[49]​ Otras obras de ciencia ficción más recientes también exploran algunas cuestiones éticas y filosóficas con respecto a la Inteligencia artificial fuerte, como las películas Yo, robot o A.I. Inteligencia Artificial, en los que se tratan temas tales como la autoconsciencia o el origen de una conciencia emergente de los robots inteligentes o sistemas computacionales, o si éstos podrían considerarse sujetos de derecho debido a sus características casi humanas relacionadas con la sintiencia, como el poder ser capaces de sentir dolor y emociones o hasta qué punto obedecerían al objetivo de su programación, y en caso de no ser así, si podrían ejercer libre albedrío. Esto último es el tema central de la famosa saga de Terminator, en la que las máquinas superan a la humanidad y deciden aniquilarla, historia que, según varios especialistas, podría no limitarse a la ciencia ficción y ser una posibilidad real en una sociedad posthumana que dependiese de la tecnología y las máquinas completamente.[50]​[51]​ Regulación Artículo principal: Regulación de la inteligencia artificial Cronología de estrategias, planes de acción y documentos de políticas que definen enfoques nacionales, regionales e internacionales para la IA.[52]​ El Derecho[53] desempeña un papel fundamental en el uso y desarrollo de la IA. Las leyes establecen reglas y normas de comportamiento para asegurar el bienestar social y proteger los derechos individuales, y pueden ayudarnos a obtener los beneficios de esta tecnología mientras minimizamos sus riesgos, que son significativos. De momento no hay normas jurídicas que regulen directamente a la IA. Pero con fecha 21 de abril de 2021, la Comisión Europea ha presentado una propuesta de Reglamento europeo para la regulación armonizada de la inteligencia artificial (IA) en la UE. Su título exacto es Propuesta de Reglamento del Parlamento Europeo y del Consejo por el que se establecen normas armonizadas en materia de inteligencia artificial –Ley de Inteligencia Artificial– y se modifican otros actos legislativos de la Unión. En marzo de 2023, cientos de empresarios como Elon Musk, Steve Wozniak (cofundador de Apple) o los presidentes de numerosas compañías tecnológicas; intelectuales como Yuval Noah Harari y cientos de académicos e investigadores especializados en inteligencia artificial firmaron una carta abierta avisando del peligro de la falta de regulación de la IA, poniendo el foco sobre OpenAI, la empresa que ha desarrollado ChatGPT. Pidieron una pausa de al menos 6 meses para sus experimentos más potentes, hasta que el mundo logre un consenso internacional para que estos sistemas «sean más precisos, seguros, interpretables, transparentes, robustos, neutrales, confiables y leales».[54]​ Dos meses más tarde, en mayo, 350 ejecutivos de las principales empresas desarrolladoras de IA, académicos e investigadores expertos firmaron un nuevo manifiesto alertando de que la IA avanzada sin regular representa un peligro de extinción para la humanidad: «Mitigar el riesgo de extinción de la IA debería ser una prioridad mundial junto a otros riesgos a escala social como las pandemias y la guerra nuclear»[55] Entre los impulsores de esta petición está toda la plana mayor de OpenAI,[56] el jefe de Tecnología de Microsoft, el líder de Google DeepMind con 38 ejecutivos, investigadores o profesores de universidad relacionados con la empresa, y representantes de desarrolladoras más pequeñas como Anthropic, Stability AI o Inflection AI.[57]​ Algunos autores han comenzado a explorar la posibilidad de reconocer a las inteligencias artificiales avanzadas no sólo como herramientas, sino como potenciales sujetos jurídicos limitados, capaces de adquirir derechos y asumir cargas fiscales en relación con los bienes digitales o intangibles que producen.[58] Esta propuesta encuentra inspiración en figuras ya reconocidas por el derecho, como las personas jurídicas ficticias, que sin ser humanas, han sido dotadas de capacidad para actuar en el tráfico jurídico. En este contexto, se ha planteado que ciertos autómatas complejos podrían constituir patrimonios propios, asumir responsabilidades civiles por los efectos de sus decisiones autónomas, e incluso tributar, como una forma de responder por su impacto económico y redistribuir los beneficios que generan en sociedades cada vez más digitalizadas. Esta perspectiva no implica una equiparación entre seres humanos e inteligencias artificiales, sino una arquitectura jurídica funcional que permitiría canalizar las consecuencias derivadas de la producción autónoma de bienes y servicios por parte de sistemas de IA. Se propone, por ejemplo, que los ingresos generados por bots creativos o vehículos autónomos puedan ser destinados a fondos de garantía o tributación, aliviando así presiones sobre las economías humanas y evitando zonas grises en la responsabilidad jurídica. Esta visión contribuye al debate sobre la singularidad tecnológica, proponiendo respuestas normativas desde el derecho privado y fiscal.[58]​ Objetivos Razonamiento y resolución de problemas Una imagen de IA generada por Dall-e tras escribir el texto: «Un edificio arquitectónico moderno con grandes ventanales de vidrio, situado en un acantilado con vista a un océano sereno al atardecer». Los primeros investigadores desarrollaron algoritmos que imitaban el razonamiento paso a paso que los humanos usan cuando resuelven acertijos o hacen deducciones lógicas.[59] A finales de la década de 1981-1990, la investigación de la inteligencia artificial había desarrollado métodos para tratar con información incierta o incompleta, empleando conceptos de probabilidad y economía.[60]​ Estos algoritmos demostraron ser insuficientes para resolver grandes problemas de razonamiento porque experimentaron una «explosión combinatoria»: se volvieron exponencialmente más lentos a medida que los problemas crecían.[61] De esta manera, se concluyó que los seres humanos rara vez usan la deducción paso a paso que la investigación temprana de la inteligencia artificial seguía; en cambio, resuelven la mayoría de sus problemas utilizando juicios rápidos e intuitivos.[62]​ Representación abstracta de una canción generada por inteligencia artificial. Representación del conocimiento Artículo principal: Representación del conocimiento La representación del conocimiento[63] y la ingeniería del conocimiento[64] son fundamentales para la investigación clásica de la inteligencia artificial. Algunos «sistemas expertos» intentan recopilar el conocimiento que poseen los expertos en algún ámbito concreto. Además, otros proyectos tratan de reunir el «conocimiento de sentido común» conocido por una persona promedio en una base de datos que contiene un amplio conocimiento sobre el mundo. Entre los temas que contendría una base de conocimiento de sentido común están: objetos, propiedades, categorías y relaciones entre objetos,[65] situaciones, eventos, estados y tiempo[66] causas y efectos;[67] y el conocimiento sobre el conocimiento (lo que sabemos sobre lo que saben otras personas)[68] entre otros. Planificación Otro objetivo de la inteligencia artificial consiste en poder establecer metas y finalmente alcanzarlas.[69] Para ello necesitan una forma de visualizar el futuro, una representación del estado del mundo y poder hacer predicciones sobre cómo sus acciones lo cambiarán, con tal de poder tomar decisiones que maximicen la utilidad (o el «valor») de las opciones disponibles.[70]​ En los problemas clásicos de planificación, el agente puede asumir que es el único sistema que actúa en el mundo, lo que le permite estar seguro de las consecuencias de sus acciones.[71] Sin embargo, si el agente no es el único actor, entonces se requiere que este pueda razonar bajo incertidumbre. Esto requiere un agente que no solo pueda evaluar su entorno y hacer predicciones, sino también evaluar sus predicciones y adaptarse en función de su evaluación.[72] La planificación de múltiples agentes utiliza la cooperación y la competencia de muchos sistemas para lograr un objetivo determinado. El comportamiento emergente como este es utilizado por algoritmos evolutivos e inteligencia de enjambre.[73]​ Aprendizaje El aprendizaje automático es un concepto fundamental de la investigación de la inteligencia artificial desde el inicio de los estudios de este campo; consiste en la investigación de algoritmos informáticos que mejoran automáticamente a través de la experiencia.[74]​ El aprendizaje no supervisado es la capacidad de encontrar patrones en un flujo de entrada, sin que sea necesario que un humano etiquete las entradas primero. El aprendizaje supervisado incluye clasificación y regresión numérica, lo que requiere que un humano etiquete primero los datos de entrada. La clasificación se usa para determinar a qué categoría pertenece algo y ocurre después de que un programa observe varios ejemplos de entradas de varias categorías. La regresión es el intento de producir una función que describa la relación entre entradas y salidas y predice cómo deben cambiar las salidas a medida que cambian las entradas.[74] Tanto los clasificadores como los aprendices de regresión intentan aprender una función desconocida; por ejemplo, un clasificador de spam puede verse como el aprendizaje de una función que asigna el texto de un correo electrónico a una de dos categorías, «spam» o «no spam». La teoría del aprendizaje computacional puede evaluar a los estudiantes por complejidad computacional, complejidad de la muestra (cuántos datos se requieren) o por otras nociones de optimización.[75]​ El mundo está en constante evolución, y herramientas como ChatGPT están en el centro de esta transformación. Mientras que muchas personas ven a ChatGPT como una oportunidad para mejorar la experiencia de sus negocios o personales, hay quienes se muestran escépticos sobre su implementación.[76]​ Procesamiento de lenguajes naturales Artículo principal: Procesamiento de lenguajes naturales El procesamiento del lenguaje natural[77] permite a las máquinas leer y comprender el lenguaje humano. Un sistema de procesamiento de lenguaje natural suficientemente eficaz permitiría interfaces de usuario de lenguaje natural y la adquisición de conocimiento directamente de fuentes escritas por humanos, como los textos de noticias. Algunas aplicaciones sencillas del procesamiento del lenguaje natural incluyen la recuperación de información, la minería de textos, la respuesta a preguntas y la traducción automática.[78] Muchos enfoques utilizan las frecuencias de palabras para construir representaciones sintácticas de texto. Las estrategias de búsqueda de «detección de palabras clave» son populares y escalables, pero poco óptimas; una consulta de búsqueda para «perro» solo puede coincidir con documentos que contengan la palabra literal «perro» y perder un documento con el vocablo «caniche». Los enfoques estadísticos de procesamiento de lenguaje pueden combinar todas estas estrategias, así como otras, y a menudo logran una precisión aceptable a nivel de página o párrafo. Más allá del procesamiento de la semántica, el objetivo final de este es incorporar una comprensión completa del razonamiento de sentido común.[79] En 2019, las arquitecturas de aprendizaje profundo basadas en transformadores podían generar texto coherente.[80]​ Percepción La detección de características (en la imagen se observa la detección de bordes) ayuda a la inteligencia artificial a componer estructuras abstractas informativas a partir de datos sin procesar. La percepción de la máquina[81] es la capacidad de utilizar la entrada de sensores (como cámaras de espectro visible o infrarrojo, micrófonos, señales inalámbricas y lidar, sonar, radar y sensores táctiles) para entender aspectos del mundo. Las aplicaciones incluyen reconocimiento de voz,[82] reconocimiento facial y reconocimiento de objetos.[83] La visión artificial es la capacidad de analizar la información visual, que suele ser ambigua; un peatón gigante de cincuenta metros de altura muy lejos puede producir los mismos píxeles que un peatón de tamaño normal cercano, lo que requiere que la inteligencia artificial juzgue la probabilidad relativa y la razonabilidad de las diferentes interpretaciones, por ejemplo, utilizando su «modelo de objeto» para evaluar que los peatones de cincuenta metros no existen.[84]​ Importancia de la inteligencia artificial La gran importancia de la IA radica en el hecho de que tiene una amplia gama de aplicaciones, desde la automatización de tareas tediosas hasta la creación de sistemas avanzados de asistencia médica y diagnóstico de enfermedades, la detección de fraudes y la optimización de procesos empresariales.[85] En muchos casos, la IA puede hacer cosas que los humanos no pueden hacer, como el procesamiento de datos en grandes cantidades y la localización de patrones e interrelaciones entre estos que serían difíciles o imposibles de detectar de otra manera. Esta herramienta ayuda a automatizar el aprendizaje y descubrimiento repetitivo a través de datos, realiza tareas computarizadas frecuentes de manera confiable, sin embargo, necesita intervención humana para la configuración del sistema. Analiza datos más profundos y agrega inteligencia ya que no se puede vender como una aplicación individual, por lo que es un valor agregado a los productos. Tiene una gran precisión a través de redes neuronales profundas; por ejemplo, en medicina se puede utilizar la IA para detectar cáncer con MRIs (imágenes ppr resonancia magnética). Se adapta a través de algoritmos de aprendizaje progresivo, encuentra estructura y regularidades en los datos de modo que el algoritmo se convierte en un clasificador o predictor. Y, por último, la inteligencia artificial, saca el mayor provecho de datos. Además, una de las principales razones por las que la IA es importante es porque puede automatizar tareas repetitivas y monótonas, liberando tiempo y recursos para que las personas se centren en tareas más creativas y valiosas. Por ejemplo, la IA puede ayudar a las empresas a automatizar tareas de back office, como la contabilidad y el procesamiento de facturas, lo que puede reducir los costos y mejorar la eficiencia. De manera similar, la IA puede ayudar a los trabajadores a realizar tareas más complejas y creativas, como el diseño y la planificación estratégica. Otra razón por la que la IA es importante es porque puede ayudar a las empresas a tomar decisiones informadas y precisas. Así mismo, la IA puede procesar grandes cantidades de datos y proporcionar información valiosa para la toma de decisiones empresariales, lo que puede ayudar a las empresas a identificar oportunidades comerciales, predecir tendencias de mercado y mejorar la eficiencia del mercado financiero.[86] Además, la IA puede ayudar a los trabajadores a tomar decisiones informadas en tiempo real, como en el caso de la atención médica, donde la IA puede ayudar a los médicos a identificar enfermedades y personalizar el tratamiento. La IA también es importante en el campo de la ciberseguridad. La IA puede ayudar a detectar y prevenir amenazas, desde ciberataques hasta la detección de comportamientos sospechosos. La IA puede analizar grandes cantidades de datos en tiempo real y detectar patrones y anomalías que podrían indicar una amenaza de seguridad. Además, la IA puede aprender de los patrones de comportamiento y mejorar su capacidad para detectar amenazas en el futuro.[87] En el campo de la seguridad cibernética, la IA puede ayudar a proteger los sistemas y las redes de los ataques de virus informáticos y la infiltración de malware. Otra área donde la IA es importante es en el descubrimiento de conocimientos. La IA puede descubrir patrones y relaciones en los datos que los humanos no podrían detectar, lo que puede llevar a nuevas ideas y avances en diversos campos. Por ejemplo, la IA puede ayudar a los investigadores a identificar nuevos tratamientos para enfermedades, o ayudar a los científicos a analizar datos de sensores y satélites para entender mejor el calentamiento global. Ventajas de la inteligencia Artificial La inteligencia artificial (IA) ofrece múltiples ventajas a la sociedad, muchas de las cuales ya comienzan a hacerse evidentes. Una de las principales áreas beneficiadas es la educación, ya que permite que las clases sean más dinámicas y comprensibles, facilitando el aprendizaje y promoviendo un pensamiento más autónomo en los estudiantes.[88]​ Desventajas de la inteligencia artificial A pesar de sus beneficios, la IA también presenta algunas desventajas. Una de las principales preocupaciones es la pérdida de empleos, ya que algunas profesiones se han visto afectadas, como los diseñadores gráficos, analistas financieros y matemáticos, cuyos trabajos pueden ser reemplazados por sistemas automatizados.[88]​ Controversias Sophia, un robot humanoide controlado por IA. Sophia En marzo de 2016, se hizo popular el comentario que la robot humanoide llamada Sophia de la empresa Hanson Robotics hizo durante su presentación cuando su creador, David Hanson, le preguntara si estaba dispuesta a destruir a la humanidad, a lo que la robot contestó: «Está bien, voy a destruir a la humanidad». Posteriormente, Sophía se ganó el reconocimiento y la atención mediática mundial debido a sus conductas casi humanas, siendo entrevistada en muchas ocasiones por distintos medios y sosteniendo conversaciones con personalidades famosas y reconocidas. En 2017, Sophia obtuvo la ciudadanía saudí, convirtiéndose así en la primera robot en ser reconocida como ciudadana por un país, lo cual levantó la controversia sobre si se les debería otorgar los mismos derechos y obligaciones a los robots como si se trataran de sujetos de derecho.[89]​ Alice y Bob A finales de julio de 2017, varios medios internacionales dieron a conocer que el laboratorio de investigación de inteligencia artificial del Instituto Tecnológico de Georgia, en conjunto con el Grupo de Investigación de inteligencia artificial (FAIR) de Facebook, ahora Meta, tuvieron que apagar dos inteligencias artificiales de tipo chatbot denominadas Bob y Alice, ya que habían desarrollado un lenguaje propio más eficiente que el inglés, idioma en el que habían sido entrenados para aprender a negociar, desarrollando finalmente un tipo de comunicación incomprensible que se alejaba de las reglas gramaticales del lenguaje natural y que favorecía el uso de abreviaturas. El lenguaje creado por estas IA mostraba características de un inglés corrupto y patrones repetitivos, en especial de pronombres y determinantes.[90]​ Este inesperado suceso fue visto con pánico en los medios de comunicación, ya que se aseguraba que los chatbots supuestamente habían salido del control humano y habían desarrollado la capacidad de comunicarse entre sí. Sin embargo, posteriormente esto también fue desmentido, pues se argumentó que en realidad Facebook no apagó las inteligencias artificiales, sino que simplemente las puso en pausa y cambió los parámetros de los chatbots, desechando el experimento al final por no tener ningún interés práctico o útil dentro de la investigación sobre IA.[91]​ Ameca A principios del 2022, en la Feria de Electrónica de Consumo (CES) que tomó lugar en Las Vegas, el robot desarrollado por Engineered Arts nombrado Ameca causó duda y miedo a los espectadores durante su exposición principalmente por la semejanza de su rostro a uno de un ser humano, la compañía expresó que el desarrollo de este robot humanoide aún se encontraba en proceso y hasta septiembre del mismo año el robot aún no era capaz de caminar ni tener interacción alguna con las personas.[92] Por otro lado, en septiembre de 2023 la compañía volvió a exponer a Ameca al público mostrando al robot en videos en donde se le puede ver frente a un espejo haciendo 25 expresiones humanas,[93] así como dibujando un gato al ya contar con brazos y piernas que le otorgaron movilidad y, de igual manera, empleando ironía en conversaciones con personas e incluso declarando que realizó una broma al ser cuestionada sobre su capacidad de soñar como un humano siendo un robot al decir «soñé con dinosaurios luchando una guerra contra alienígenas en Marte»[94] esto lo desmintió momentos después explicando cómo es que la IA implementada en su sistema le permitía crear escenarios sobre hechos de la humanidad e iba aprendiendo sobre ellos mientras se encontraba apagada; estos hechos impactaron a la sociedad sobre la semejanza que este robot humanoide estaba teniendo con el ser humano y sobre el avance tecnológico que está permitiendo que este robot esté cada vez más cercano a vivir entre las personas como un miembro más de la comunidad. Falsos desnudos La utilización de aplicaciones gratuitas de IA para transformar fotografías de personas en falsos desnudos está generando problemas que afectan a menores. El caso saltó a los medios de comunicación en septiembre de 2023 cuando en Almendralejo (Badajoz, España) aparecieron varias fotografías de niñas y jóvenes (entre 11 y 17 años) que habían sido modificadas mediante inteligencia artificial para aparecer desnudas. Las imágenes fueron obtenidas de los perfiles de Instagram y de la aplicación WhatsApp de al menos 20 niñas de la localidad. Las fotografías de niñas desnudas habían circulado después mediante Whatsapp y a partir de ellas se había creado un vídeo que también había circulado entre menores. Los autores de dicha transformación también eran menores y compañeros de colegio o instituto. La Agencia Española de Protección de Datos abrió una investigación y se comunicó con el Ayuntamiento de Almendralejo y con la Junta de Extremadura informándoles de que se podía solicitar la retirada de cualquier imagen circulando en internet en el canal prioritario de la agencia.[95]​ Críticas La «revolución digital» y, más concretamente, el desarrollo de la inteligencia artificial, está suscitando temores y preguntas, incluso en el ámbito de personalidades relevantes en estas cuestiones. En esta imagen, se observa a Bill Gates, exdirector general de Microsoft; el citado y Elon Musk (director general de Tesla) opinan que se debe ser «muy cauteloso con la inteligencia artificial»; si tuviéramos que «apostar por lo que constituye nuestra mayor amenaza a la existencia», serían precisamente ciertas aplicaciones sofisticadas del citado asunto, que podrían llegar a tener derivaciones por completo impensadas. Uno de los mayores críticos de la denominación de estos procesos informáticos con el término de inteligencia artificial es Jaron Lanier. Para ello, objeta la idea de que esta sea realmente inteligente y de que podríamos estar en competencia con un ente artificial. «Esta idea de superar la capacidad humana es ridícula porque está hecha de habilidades humanas».[96]​ Las principales críticas a la inteligencia artificial tienen que ver con su capacidad de imitar por completo a un ser humano.[97] Sin embargo, hay expertos[98]​en el tema que indican que ningún humano individual tiene capacidad para resolver todo tipo de problemas, y autores como Howard Gardner han teorizado sobre la solución. En los humanos, la capacidad de resolver problemas tiene dos aspectos: los aspectos innatos y los aspectos aprendidos. Los aspectos innatos permiten, por ejemplo, almacenar y recuperar información en la memoria, mientras que en los aspectos aprendidos reside el saber resolver un problema matemático mediante el algoritmo adecuado. Del mismo modo que un humano debe disponer de herramientas que le permitan solucionar ciertos problemas, los sistemas artificiales deben ser programados para que puedan llegar a resolverlos. Muchas personas consideran que la prueba de Turing ha sido superada, citando conversaciones en que al dialogar con un programa de inteligencia artificial para chat no saben que hablan con un programa. Sin embargo, esta situación no es equivalente a una prueba de Turing, que requiere que el participante se encuentre sobre aviso de la posibilidad de hablar con una máquina. Otros experimentos mentales como la habitación china, de John Searle, han mostrado cómo una máquina podría simular pensamiento sin realmente poseerlo, pasando la prueba de Turing sin siquiera entender lo que hace, tan solo reaccionando de una forma concreta a determinados estímulos (en el sentido más amplio de la palabra). Esto demostraría que la máquina en realidad no está pensando, ya que actuar de acuerdo con un programa preestablecido sería suficiente. Si para Turing el hecho de engañar a un ser humano que intenta evitar que le engañen es muestra de una mente inteligente, Searle considera posible lograr dicho efecto mediante reglas definidas a priori. Uno de los mayores problemas en sistemas de inteligencia artificial es la comunicación con el usuario. Este obstáculo es debido a la ambigüedad del lenguaje, y se remonta a los inicios de los primeros sistemas operativos informáticos. La capacidad de los humanos para comunicarse entre sí implica el conocimiento del lenguaje que utiliza el interlocutor. Para que un humano pueda comunicarse con un sistema inteligente hay dos opciones: o bien que el humano aprenda el lenguaje del sistema como si aprendiese a hablar cualquier otro idioma distinto al nativo, o bien que el sistema tenga la capacidad de interpretar el mensaje del usuario en la lengua que el usuario utiliza. También puede haber desperfectos en las instalaciones de los mismos. Un humano, durante toda su vida, aprende el vocabulario de su lengua nativa o materna, siendo capaz de interpretar los mensajes (a pesar de la polisemia de las palabras) y utilizando el contexto para resolver ambigüedades. Sin embargo, debe conocer los distintos significados para poder interpretar, y es por esto que lenguajes especializados y técnicos son conocidos solamente por expertos en las respectivas disciplinas. Un sistema de inteligencia artificial se enfrenta con el mismo problema, la polisemia del lenguaje humano, su sintaxis poco estructurada y los dialectos entre grupos. Los desarrollos en inteligencia artificial son mayores en los campos disciplinares en los que existe mayor consenso entre especialistas. Un sistema experto es más probable que sea programado en física o en medicina que en sociología o en psicología. Esto se debe al problema del consenso entre especialistas en la definición de los conceptos involucrados y en los procedimientos y técnicas a utilizar. Por ejemplo, en física hay acuerdo sobre el concepto de velocidad y cómo calcularla. Sin embargo, en psicología se discuten los conceptos, la etiología, la psicopatología, y cómo proceder ante cierto diagnóstico. Esto dificulta la creación de sistemas inteligentes porque siempre habrá desacuerdo sobre la forma en que debería actuar el sistema para diferentes situaciones. A pesar de esto, hay grandes avances en el diseño de sistemas expertos para el diagnóstico y toma de decisiones en el ámbito médico y psiquiátrico (Adaraga Morales, Zaccagnini Sancho, 1994). Al desarrollar un robot con inteligencia artificial se debe tener cuidado con la autonomía,[99] hay que tener en cuenta el no vincular el hecho de que el robot tenga interacciones con seres humanos a su grado de autonomía. Si la relación de los humanos con el robot es de tipo maestro esclavo, y el papel de los humanos es dar órdenes y el del robot obedecerlas, entonces sí cabe hablar de una limitación de la autonomía del robot. Pero si la interacción de los humanos con el robot es de igual a igual, entonces su presencia no tiene por qué estar asociada a restricciones para que el robot pueda tomar sus propias decisiones.[100]​ Con el desarrollo de la tecnología de inteligencia artificial, muchas compañías de software como el aprendizaje profundo y el procesamiento del lenguaje natural han comenzado a producirse y la cantidad de películas sobre inteligencia artificial ha aumentado. Stephen Hawking advirtió sobre los peligros de la inteligencia artificial y lo consideró una amenaza para la supervivencia de la humanidad.[101]​ Problemas de privacidad y derechos de autor Los algoritmos de aprendizaje automático requieren grandes cantidades de datos. Las técnicas utilizadas para adquirir estos datos generan preocupaciones sobre temas de privacidad y vigilancia. Las empresas tecnológicas recopilan un gran número de datos de sus usuarios, incluida la actividad en internet, los datos de geolocalización, video y audio.[102] Por ejemplo, para construir algoritmos de reconocimiento de voz, Amazon, entre otros, ha grabado millones de conversaciones privadas y han permitido que trabajadores temporales las escuchen para transcribirlas algunas de ellas.[103] Las opiniones sobre esta vigilancia generalizada van desde aquellos que la ven como un mal necesario hasta aquellos para quienes no es ética y constituye una violación del derecho a la intimidad.[104] Los desarrolladores de IA argumentan que esta es la única forma de ofrecer aplicaciones valiosas y han desarrollado varias técnicas que intentan preservar la privacidad mientras se obtienen los datos, como la agregación de datos, la desidentificación y la privacidad diferencial.[105]​ Desde 2016, algunos expertos en privacidad, como Cynthia Dwork, comenzaron a ver la privacidad desde la perspectiva de la equidad: Brian Christian escribió que los expertos han cambiado «de la pregunta de “qué saben” a la pregunta de “qué están haciendo con ello”».[106]​ La IA generativa a menudo se entrena con obras protegidas por derechos de autor no autorizadas, incluidos dominios como imágenes o código informático; la salida se utiliza luego bajo una justificación de uso justo. Los expertos no están de acuerdo sobre la validez de esta justificación durante un proceso legal, ya que podría depender del propósito y el carácter del uso de la obra protegida por derechos de autor y del efecto sobre el mercado potencial de la obra protegida.[107]​En 2023, escritores como John Grisham y Jonathan Franzen demandaron a las empresas de IA por usar sus obras para entrenar IA generativa.[108]​[109] En 2024, 200 artistas escribieron una carta abierta que solicitaba «parar el asalto a la creatividad humana».[110]​ Normativa para su uso en el entorno educativo La normativa tiene como objetivo regular y reglamentar el uso de la IA en el entorno educativo, específicamente en el aula. La IA ha experimentado un rápido desarrollo y se ha convertido en una herramienta potencialmente beneficiosa para mejorar la enseñanza y el aprendizaje. No obstante, su implementación plantea desafíos éticos, de privacidad y equidad que deben ser abordados de manera efectiva. Esta normativa se establece en respuesta a la necesidad de garantizar que la IA se utilice de manera ética, responsable y equitativa en el ámbito educativo. Los objetivos de esta normativa son: Promover el uso de la IA como una herramienta complementaria en el proceso de enseñanza-aprendizaje. Garantizar la protección de datos y la privacidad de los estudiantes. Fomentar la equidad y la inclusión en el acceso y el uso de la IA. Establecer principios éticos que rijan el uso de la IA en el aula. Definir responsabilidades y procedimientos claros para el uso de la IA. Esta normativa se aplica a todas las instituciones educativas y docentes que utilizan la IA en el aula, así como a los proveedores de tecnología educativa que ofrecen soluciones basadas en IA. Organizaciones como UNESCO Ethics AI (2020), UNESCO Education & AI (2021), Beijin Consensus, OCDE (2021), Comisión Europea (2019), European Parliament Report AI Education (2021), UNICEF (2021) y Foro Económico Mundial (2019) han mostrado preocupación por implementar lineamientos sobre la ética y la IA en el entorno educativo.[111] En 2024, la Universidad Nacional de Costa rica elaboró y publicó[112] la Declaración de Heredia: principios sobre el uso de inteligencia artificial en la edición científica, referida a la publicación científica, en la que propone una serie de consideraciones para el uso responsable de la inteligencia artificial (IA), se alienta a transparentar el uso de la IA para un ejercicio claro, trazable y reproducible del conocimiento y se llama la atención sobre los retos que supone la incorporación de la IA a la edición científica en cuanto a la diversidad de opciones, el evitar la propagación de sesgos y desinformación, y el respeto a la propiedad intelectual. El uso de la IA en el entorno educativo debe regirse por los siguientes principios éticos y valores: Transparencia: Las decisiones tomadas por algoritmos de IA deben ser comprensibles y explicables. Equidad: La IA no debe discriminar a ningún estudiante ni grupo de estudiantes. Privacidad: Los datos de los estudiantes deben ser protegidos y utilizados de manera responsable. Responsabilidad: Los docentes y las instituciones son responsables de las decisiones tomadas con la ayuda de la IA. Honestidad: El contenido creado por los estudiantes debe ser original sin caer en el plagio.[113]​ Mejora del aprendizaje: La IA debe utilizarse para mejorar la calidad de la educación y el aprendizaje. Capacitación: Los docentes deben recibir formación sobre el uso de la IA y su aplicación en el aula. Evaluación: Las soluciones de IA deben ser evaluadas en términos de su eficacia y su impacto en el aprendizaje. Protección de datos: Los datos de los estudiantes deben ser protegidos de acuerdo con las leyes de privacidad aplicables. Supervisión: Se debe establecer un proceso de supervisión para garantizar que la IA se utilice de manera ética y responsable. Riesgos de las IA en el entorno educativo Así como tiene muchos beneficios también nos encontramos con diferentes riesgos a los que la educación está expuesta con su uso. Sesgos y discriminación: Al solo recoger información de las bases de datos y textos que procesa de Internet corre el riesgo de aprender cualquier sesgo cognitivo que se encuentre en dicha información. La no privacidad de los datos: El riesgo de un ciberataque se incrementa cuando no hay protocolos de seguridad adecuados en el manejo de la IA.[114]​ Dependencia: Los estudiantes corren el riesgo de volverse dependientes de la tecnología y no se fomenta la creatividad ni el pensamiento propio.[115]​ Confiabilidad: La IA puede generar respuestas coherentes pero inexactas además muchas IA no brindan fuentes de información. Falta de habilidades orales y escritas.[116]​ Desinterés por la investigación por cuenta propia.[116]​ Dependencia por parte del docente: Los docentes pueden generar dependencia a estas herramientas al momento de dar retroalimentación a las asignaciones además del riesgo de usar la información de las IA para su material didáctico sin antes consultar las fuentes.[116]​ Los estudiantes no tendrían a cabo un aprendizaje natural este sería más artificial, generando en ellos que a largo plazo establezcan una falta posible falta de conocimiento. Los docentes perderían la costumbre de realizar presentaciones más llamativas, interactivas volviéndolas estas así más monótonas y perdiendo su propia autovía de aquella. Consideración de Diversidad e Inclusión Se debe prestar especial atención a la diversidad de estudiantes y garantizar que la IA sea accesible y beneficiosa para todos, independientemente de su origen étnico, género, discapacidad u orientación sexual. Las soluciones de IA deben ser diseñadas teniendo en cuenta la accesibilidad y la inclusión. Esta normativa se basa en investigaciones académicas, recomendaciones de organizaciones educativas y en las mejores prácticas establecidas en el uso de la IA en la educación. Se alienta a las instituciones a mantenerse al día con la literatura científica y las directrices relevantes. Aunque la IA puede ser una herramienta poderosa en el aula, no debe reemplazar la creatividad, la originalidad y el juicio humano en el proceso educativo. La IA debe ser utilizada de manera complementaria para enriquecer la experiencia educativa. Esta normativa se presenta como un marco general que deberá ser adaptado y ampliado por las instituciones educativas de acuerdo a sus necesidades y contextos específicos. Debe ser comunicada de manera efectiva a todos los involucrados en el proceso educativo y revisada periódicamente para asegurar su vigencia. Esta normativa tiene como objetivo garantizar que la IA sea utilizada de manera ética y responsable en el aula, promoviendo el beneficio de los estudiantes y el avance de la educación. Su cumplimiento es esencial para lograr una implementación exitosa de la IA en el entorno educativo. Aprendizaje automatizado y aprendizaje profundo Artículos principales: Aprendizaje automático y Aprendizaje profundo. En cuanto a la naturaleza del aprendizaje, la IA puede subdividirse en dos campos conceptualmente distintos: El aprendizaje automático, que se enfoca en desarrollar algoritmos de regresión, árboles de decisión y modelos que puedan aprender de datos existentes y realizar predicciones o decisiones basadas en esos datos. En el aprendizaje automático, se utilizan técnicas de estadística matemática para encontrar patrones y relaciones en los datos y, a partir de ellos, desarrollar modelos que puedan hacer predicciones sobre nuevos datos. El aprendizaje profundo, que se centra en la creación de redes neuronales artificiales capaces de aprender y realizar tareas de manera similar a como lo hacen los seres humanos. En el aprendizaje profundo, se utilizan capas de neuronas artificiales para procesar los datos de entrada y aprender a través de un proceso iterativo de ajuste de los pesos de las conexiones entre neuronas. Este tipo de aprendizaje es capaz de procesar y analizar grandes cantidades de datos de manera más eficiente y precisa que el primero, especialmente cuando se trata de datos no estructurados, como imágenes, texto y audio. Además, tiene la capacidad de identificar patrones y características más complejas en los datos, lo que puede llevar a mejores resultados en aplicaciones como el reconocimiento de voz, la visión por computadora y el procesamiento del lenguaje natural. Propiedad intelectual de la inteligencia artificial Imagen de una ciudad futurista generada por la IA Midjourney. La composición está en el dominio público al no ser de un autor humano. Al hablar acerca de la propiedad intelectual atribuida a creaciones de la inteligencia artificial, se forma un debate fuerte alrededor de si una máquina puede tener derechos de autor. Según la Organización Mundial de la Propiedad Intelectual (OMPI), cualquier creación de la mente puede ser parte de la propiedad intelectual, pero no especifica si la mente debe ser humana o puede ser una máquina, dejando la creatividad artificial en la incertidumbre. Alrededor del mundo han comenzado a surgir distintas legislaciones con el fin de manejar la inteligencia artificial, tanto su uso como creación. Los legisladores y miembros del gobierno han comenzado a pensar acerca de esta tecnología, enfatizando el riesgo y los desafíos complejos de esta. Observando el trabajo creado por una máquina, las leyes cuestionan la posibilidad de otorgarle propiedad intelectual a una máquina, abriendo una discusión respecto a la legislación relacionada con IA. El 5 de febrero de 2020, la Oficina del Derecho de Autor de los Estados Unidos y la OMPI asistieron a un simposio donde observaron de manera profunda cómo la comunidad creativa utiliza la inteligencia artificial (IA) para crear trabajo original. Se discutieron las relaciones entre la inteligencia artificial y el derecho de autor, qué nivel de involucramiento es suficiente para que el trabajo resultante sea válido para protección de derechos de autor; los desafíos y consideraciones de usar inputs con derechos de autor para entrenar una máquina; y el futuro de la inteligencia artificial y sus políticas de derecho de autor.[117]​[118]​ El director general de la OMPI, Francis Gurry, presentó su preocupación ante la falta de atención que hay frente a los derechos de propiedad intelectual, pues la gente suele dirigir su interés hacia temas de ciberseguridad, privacidad e integridad de datos al hablar de la inteligencia artificial. Así mismo, Gurry cuestionó si el crecimiento y la sostenibilidad de la tecnología IA nos guiaría a desarrollar dos sistemas para manejar derechos de autor- uno para creaciones humanas y otro para creaciones de máquinas.[119]​ Aún hay una falta de claridad en el entendimiento alrededor de la inteligencia artificial. Los desarrollos tecnológicos avanzan a paso rápido, aumentando su complejidad en políticas, legalidades y problemas éticos que se merecen la atención global. Antes de encontrar una manera de trabajar con los derechos de autor, es necesario entenderlo correctamente, pues aún no se sabe cómo juzgar la originalidad de un trabajo que nace de una composición de una serie de fragmentos de otros trabajos. La asignación de derechos de autor alrededor de la inteligencia artificial aún no ha sido regulada por la falta de conocimientos y definiciones. Aún hay incertidumbre sobre si, y hasta qué punto, la inteligencia artificial es capaz de producir contenido de manera autónoma y sin ningún humano involucrado, algo que podría influenciar si sus resultados pueden ser protegidos por derechos de autor. El sistema general de derechos de autor aún debe adaptarse al contexto digital de inteligencia artificial, pues están centrados en la creatividad humana. Los derechos de autor no están diseñados para manejar cualquier problema en las políticas relacionado con la creación y el uso de propiedad intelectual, y puede llegar a ser dañino estirar excesivamente los derechos de autor para resolver problemas periféricos, dado que: «Usar los derechos de autor para gobernar la inteligencia artificial es poco inteligente y contradictorio con la función primordial de los derechos de autor de ofrecer un espacio habilitado para que la creatividad florezca».[120]​ La conversación acerca de la propiedad intelectual tendrá que continuar hasta asegurarse de que la innovación sea protegida, pero también tenga espacio para florecer. En la cultura popular En la literatura A continuación se incluye alguna obra que tiene como motivo central la inteligencia artificial. Yo, Robot (1950), de Isaac Asimov: novela que consta de nueve historias ambientas entre los años de 1940 y 1950, cada uno cuenta con personajes distintos pero que siguen la misma temática a través del seguimiento de las Tres Leyes de la Robótica, en donde se plantea tanto su cumplimiento como la creación de problemas alternos que los mismos robots generan y de esta manera demostrar que la tecnología siempre puede estar un paso adelante del pensamiento y lógica humana. También sigue el hilo argumentativo a través de una entrevista con una psicóloga de robots la cual va relatando el surgimiento de los robots y suponiendo cómo será el desenvolvimiento del ser humano en un mundo en donde la tecnología se esté superando cada vez más.[1] Galatea 2.2 (1995) de Richard Powers: novela que explora la relación entre inteligencia artificial y literatura. La trama sigue al protagonista, quien participa en un experimento con un modelo computacional llamado «Helen» para enseñarle a comunicarse como un humano. A través de esta interacción, se plantean cuestiones profundas sobre la conciencia y la emoción en un entorno tecnológico. La era del diamante (1996) de Neal Stephenson, la inteligencia artificial juega un papel crucial en la trama a través del Manual ilustrado para jovencitas diseñado por John Percival Hackworth. Este instrumento interactivo es capaz de adaptarse dinámicamente a las circunstancias de la niña mediante la inteligencia artificial. El primer libro (2013), de Antonio Palacios Rojo: una novela dialogada que satiriza el uso de la IA en la creación artística unos diez años antes de la irrupción de estas herramientas inteligentes.[121]​ En el cine Véase también: Computadoras en la ciencia ficción La inteligencia artificial está cada vez más presente en la sociedad, la evolución de la tecnología es una realidad y con ello, la producción de películas sobre esta temática. Cabe destacar, que lleva habiendo piezas audiovisuales sobre inteligencia artificial desde hace mucho tiempo, ya sea incluyendo personajes o mostrando un trasfondo moral y ético. A continuación, se muestra una lista de algunas de las principales películas que tratan este tema: The Terminator (1984): En esta película el argumento se basa en el desarrollo de un microchip capaz de dotar de inteligencia artificial a robots que luego se rebelan contra la humanidad. Se trata de una de las películas más populares sobre una hipotética guerra entre humanos y robots inteligentes capaces de crearse a sí mismos. Matrix (1999): En esta película Keanu Reeves interpreta a Thomas Anderson / Neo, un programador de día y hacker de noche que trata de desentrañar la verdad oculta tras una simulación conocida como «Matrix». Esta realidad simulada es producto de programas de inteligencia artificial que terminan esclavizando a la humanidad y utilizando sus cuerpos como fuente de energía. El hombre bicentenario (1999) Inteligencia artificial (2001): Un trabajador de Cybertronics Manufacturing adopta a David de forma momentánea para, así, estudiar su comportamiento. Tanto él como su esposa acaban por tratar al niño artificial como a su propio hijo biológico. A pesar del cariño que le profesan, David siente la necesidad de escapar de su hogar e iniciar un viaje que le ayude a descubrir a quién pertenece realmente. Ante sus perplejos ojos, se abrirá un nuevo mundo oscuro, injusto, violento, insensible... Algo que le resultará difícil aceptar. Se pregunta cosas como: ¿cómo es posible que sienta algo tan real como el amor y que él sea artificial? y fue nominado al Premio Oscar. Minority Report (2002): La película sobre IA de Steven Spielberg, Minority Report, sigue a John (Tom Cruise), un agente de la ley, que es acusado de un asesinato que cometerá en el futuro. En esta película de principios de los años 2000, el protagonista utiliza una tecnología del futuro que permite a la policía atrapar a los criminales antes de que hayan cometido un delito. En Minority Report, la IA se representa a través de los Precogs, los gemelos que poseen habilidades psíquicas. Los Precogs ven los asesinatos antes de que se produzcan, lo que permite a las fuerzas del orden perseguir el crimen antes de que se cometa. En lugar de los robots físicos de IA tipo cyborg, aquí explora la IA mediante el uso de seres humanos. Yo, robot (2004): Esta película de ciencia ficción protagonizada por Will Smith está ambientada en 2035, en una sociedad donde los humanos viven en perfecta armonía con robots inteligentes en los que confían para todo. Los problemas emergen a la superficie cuando un error en la programación de un superordenador llamado VIKI le lleva a creer que los robots deben tomar las riendas para proteger a la humanidad de sí misma. Her (2013): Esta película de Spike Jonze relata la historia de un escritor de cartas quien está solo y a punto de divorciarse. Este personaje lo representó el galardonado Joaquin Phoenix. Este hombre compró un sistema operativo con inteligencia artificial para utilizarlo a fin de complacer a todos los usuarios y adaptarse a sus necesidades. Sin embargo, el resultado es que desarrolla un sentimiento romántico con Samantha. Quien es la voz femenina del sistema operativo. Avengers: Era de Ultrón (2015): En esta segunda entrega de las películas de Avengers, dirigidas por Joseph Hill Whedon y basadas en los cómics escritos por Stan Lee, se demuestra como es que la inteligencia artificial albergada dentro del cetro de Loki, la cual se tenía como objetivo el convertirla en una protección para la Tierra y recibió por nombre Ultrón, al ser conectada con JARVIS, la IA desarrollada por Stark, pudo obtener la suficiente información para comenzar a pensar de manera independiente y ser capaz de ir actualizando tanto su sistema como su cuerpo logrando controlar un ejército de robots con el objetivo de destruir a la humanidad y así ser lo único que quedara en la Tierra para, posteriormente, dominarla y controlarla.[2] Ex Machina (2015): En la interpretación de Alicia Vikander, increíblemente editada, como Ava, encontramos un probable robot a prueba de Turing escondido en la mansión de un genio, Nathan, un poco loco. Y es que, hablamos de una creación extraña que se siente totalmente real y a la vez inhumana. Está considerada como una de las mejores películas que tratan la inteligencia artificial. Esto se debe principalmente a que parece cubrir todo el concepto IA integrado en una película: el protagonista es un sustituto del ser humano y nos adentra en multitud de argumentos morales que rodean a esta, al tiempo que vemos un arco narrativo de thriller que, desde luego, acaba enganchándonos. Desde luego aquí la representación del personaje de la IA no es blanco o negro. Ava no es buena, pero tampoco es del todo mala. Y en esto, el público se queda reflexionando sobre cuestiones profundas sobre la naturaleza de la IA."
ksampletext_wikipedia_tech_computadora: str = "Computadora. Una computadora, computador u ordenador es una máquina programable que ejecuta una serie de comandos para procesar los datos de entrada, obteniendo convenientemente información que posteriormente se envía a las unidades de salida. Una computadora está compuesta por numerosos y diversos circuitos integrados y varios elementos de apoyo, extensión y accesorios, que en conjunto pueden ejecutar tareas diversas con suma rapidez y bajo el control de un programa (software). La constituyen dos partes esenciales, el hardware, que es su estructura física (circuitos electrónicos, cables, gabinete, teclado, ratón, etc.), y el software, que es su parte intangible (programas, datos, información, documentación, etc). Desde el punto de vista funcional es una máquina que posee, al menos, una unidad central de procesamiento (CPU), una unidad de memoria y otra de entrada/salida (periférico). Los periféricos de entrada permiten el ingreso de datos, la CPU se encarga de su procesamiento (operaciones aritmético-lógicas) y los dispositivos de salida los comunican a los medios externos. Es así, que la computadora recibe datos, los procesa y emite la información resultante, la que luego puede ser interpretada, almacenada, transmitida a otra máquina o dispositivo o sencillamente impresa; todo ello a criterio de un operador o usuario y bajo el control de un programa de computación. El hecho de que sea programable le permite realizar una gran variedad de tareas sobre la base de datos de entrada ya que puede realizar operaciones y resolver problemas en diversas áreas de la actividad humana (administración, ciencia, diseño, ingeniería, medicina, comunicaciones, música, etc). Básicamente, la capacidad de una computadora depende de sus componentes hardware, en tanto que la diversidad de tareas radica mayormente en el software que admita ejecutar y contenga instalado. Si bien esta máquina puede ser de dos tipos, computadora analógica o sistema digital, el primer tipo es usado para pocos y muy específicos propósitos; la más difundida, utilizada y conocida es la computadora digital (de propósitos generales); de tal modo que en términos generales (incluso populares), cuando se habla de «la computadora» se está refiriendo a una computadora digital. Las hay de arquitectura mixta, llamadas computadoras híbridas, siendo también estas de propósitos especiales. En la Segunda Guerra Mundial se utilizaron computadoras analógicas mecánicas, orientadas a aplicaciones militares, y durante la misma época se desarrolló la primera computadora digital, que se llamó ENIAC; ella ocupaba un enorme espacio y consumía grandes cantidades de energía, que equivalen al consumo de cientos de computadoras actuales (PC).[4] Las computadoras modernas están basadas en circuitos integrados, miles de millones de veces más veloces que las primeras máquinas, y ocupan una pequeña fracción de su espacio.[5]​ Computadoras simples son lo suficientemente pequeñas para residir en los dispositivos móviles. Las computadoras portátiles, tales como tabletas, netbooks, notebooks, ultrabooks, pueden ser alimentadas por pequeñas baterías. Las computadoras personales en sus diversas formas son iconos de la llamada era de la información y son lo que la mayoría de la gente considera como «computadora». Sin embargo, los sistemas embebidos también constituyen computadoras, y se encuentran en muchos dispositivos actuales, tales como reproductores MP4, teléfonos inteligentes, aviones de combate, juguetes, robots industriales, etc. Etimología Computadora/Computador Una «computadora humana» (eufemismo para personal de apoyo que efectuaba cálculos largos) con un microscopio y una calculadora mecánica. En el español que se habla en América se utilizan términos derivados del inglés computer y este a su vez del latín computare 'calcular'. A partir de la raíz latina, también surgieron computator (lit., «computador»; c. 1600), ‘el que calcula’, y computist («computista»; finales del siglo XIV), ‘experto en cómputo calendárico o cronológico’.[6]​ Según el Oxford English Dictionary, el primer uso conocido de la palabra computer en la lengua inglesa se encuentra en el libro The Yong Mans Gleanings (1613), del escritor Richard Braithwait, para referirse a un arithmetician (aritmético): «I haue [sic] read the truest computer of Times, and the best Arithmetician that euer [sic] breathed, and he reduceth thy dayes into a short number». Este término aludía a un “computador humano”, una persona que realizaba cálculos o cómputos. Computer continuó con el mismo significado hasta mediados del siglo XX.[7] A finales de este período, se contrataban a mujeres como computadoras porque se les podían pagar menos que a sus colegas masculinos.[8] En 1943, la mayoría de las computadoras humanas eran mujeres;[9] para referirse a ellas existía la forma femenina computress (“computadora”), que con el tiempo se cambió a programmer (“programadora”). El Oxford English Dictionary registra que, a finales del siglo XIX, computer empezó a utilizarse con el significado de «máquina calculadora».[7] El uso moderno del término para «computador electrónico digital programable» data de 1945, basándose en el concepto teórico de máquina de Turing publicado en 1937. ENIAC (1946), sigla de «computador e integrador numérico electrónico» (Electronic Numerical Integrator And Computer), generalmente se considera el primero de este tipo.[6]​ Ordenador En el español que se habla en España es predominante el uso del vocablo «ordenador», que proviene del término francés ordinateur y este a su vez del término latino ordinator.[11] La palabra «ordenador» fue introducida por IBM Francia en 1955, después de que François Girard, entonces jefe del departamento de publicidad de la empresa tuvo la idea de consultar a su antiguo profesor de literatura en París, Jacques Perret. Junto con Christian de Waldner, entonces presidente de IBM Francia, pidieron al profesor Perret que sugiriera un nombre en francés para su nueva máquina electrónica de tratamiento de la información (IBM 650), evitando la traducción literal de la palabra inglesa computer (‘calculadora’ o ‘calculatriz’[12]​), que en aquella época estaba reservada para las máquinas científicas. En 1911, una descripción de la máquina analítica de Babbage utilizaba la palabra ordonnateur para describir su fuerza motriz: «Pour aller prendre et reporter les nombres… et pour les soumettre à l’opération demandée, il faut qu'il y ait dans la machine un organe spécial et variable : c'est l'ordonnateur. Cet ordonnateur est constitué simplement par des feuilles de carton ajourées, analogues à celle des métiers Jacquard…». «Para tomar y transferir los números... y someterlos a la operación requerida, debe haber un órgano especial y variable en la máquina: este es el ordenador. Este ordenador se compone simplemente de láminas de cartón con agujeros, similares a los que se utilizan en los telares de Jacquard...». Manual de la máquina Babbage (en francés) Perret propuso una palabra compuesta centrada en el ordonnateur: ‘el que pone orden‘[13] y tiene la noción del orden eclesiástico en la Iglesia católica (ordinant).[11] Sugirió, más precisamente, ordinatrice électronique, de manera que el femenino, según él, pudiese distinguir mejor el uso religioso del uso contable de la palabra.[14] IBM Francia conservó la palabra ordinateur e inicialmente trató de proteger este nombre como marca registrada, pero, como los usuarios adoptaron fácil y rápidamente la palabra ordinateur, la empresa decidió dejarla en el dominio público.[15]​ En 1984, académicos franceses, en el debate Les jeunes, la technique et nous, que el uso del sustantivo ordonnateur es incorrecto, porque la función del aparato es procesar datos, no dar órdenes.[16] Mientras que otros —como también el creador del término, Jacques Perret— conocedores del origen religioso del término, lo consideran el más correcto; se habló del hecho de que la palabra «ordenador» guarda más relación con la función ordenar que con dar órdenes lo que sería más correcto para la función moderna de estos aparatos.[11] El uso de la palabra «ordenador» se ha exportado a los idiomas de España: el aragonés, el asturiano, el gallego, el castellano, el catalán y el euskera. Historia Artículo principal: Historia de la computación Lejos de ser un invento de una persona en particular, la computadora es el resultado evolutivo de ideas de muchas personas relacionadas con áreas tales como la electrónica, la mecánica, los materiales semiconductores, la lógica, el álgebra y la programación. Cronología Los principales hitos en la historia de la computación, desde las primeras herramientas manuales para hacer cálculos hasta las modernas computadoras de bolsillo. 500 a. C.: se utiliza el ábaco en antiguas civilizaciones como la China o la Sumeria, la primera herramienta para realizar sumas y restas. Hacia 830: el matemático e ingeniero persa Musa al-Juarismi desarrolló la teoría del algoritmo, es decir, la resolución metódica de problemas de álgebra y cálculo numérico mediante una lista bien definida, ordenada y finita de operaciones. 1614: el escocés John Napier inventa el logaritmo neperiano, que consiguió simplificar el cálculo de multiplicaciones y divisiones reduciéndolo a un cálculo con sumas y restas. 1620: el inglés Edmund Gunter inventa la regla de cálculo, instrumento manual utilizado desde entonces hasta la aparición de la calculadora electrónica para hacer operaciones aritméticas. 1623: el alemán Wilhelm Schickard inventa la primera máquina de calcular, cuyo prototipo desapareció poco después. 1642: el científico y filósofo francés Blaise Pascal inventa una máquina de sumar (la pascalina), que utilizaba ruedas dentadas, y de la que todavía se conservan algunos ejemplares originales. 1671: el filósofo y matemático alemán Gottfried Wilhelm Leibniz inventa una máquina capaz de multiplicar y dividir. 1801: el francés Joseph Jacquard inventa para su máquina de tejer brocados una tarjeta perforada que controla el patrón de funcionamiento de la máquina, una idea que sería empleada más adelante por los primeros computadores. 1833: el matemático e inventor británico Charles Babbage diseña e intenta construir la primera computadora, de funcionamiento mecánico, a la que llamó la «máquina analítica». Sin embargo, la tecnología de su época no estaba lo suficientemente avanzada para hacer realidad su idea. 1841 : la matemática Ada Lovelace comienza a trabajar junto a Charles Babbage en lo que sería el primer algoritmo destinado a ser procesado por una máquina, por lo que se la considera como la primera programadora de computadores. 1890: el estadounidense Herman Hollerith inventa una máquina tabuladora aprovechando algunas de las ideas de Babbage, que se utilizó para elaborar el censo de Estados Unidos. Hollerith fundó posteriormente la compañía que después se convertiría en IBM. 1893: el científico suizo Otto Steiger desarrolla la primera calculadora automática que se fabricó y empleó a escala industrial, conocida como la Millonaria. 1936: el matemático y computólogo inglés Alan Turing formaliza los conceptos de algoritmo y de máquina de Turing, que serían claves en el desarrollo de la computación moderna. 1938: el ingeniero alemán Konrad Zuse completa la Z1, la primera computadora que se puede considerar como tal. De funcionamiento electromecánico y utilizando relés, era programable (mediante cinta perforada) y usaba sistema binario y lógica booleana. A ella le seguirían los modelos mejorados Z2, Z3 y Z4. 1944: en Estados Unidos la empresa IBM construye la computadora electromecánica Harvard Mark I, diseñada por un equipo encabezado por Howard H. Aiken. Fue la primera computadora creada en Estados Unidos. 1944: en Reino Unido se construyen los computadores Colossus (Colossus Mark I y Colossus Mark 2), con el objetivo de descifrar las comunicaciones de los alemanes durante la Segunda Guerra Mundial. 1946: en la Universidad de Pensilvania se pone en funcionamiento la ENIAC (Electronic Numerical Integrator And Calculator), que funcionaba a válvulas y fue la primera computadora electrónica de propósito general. 1947: en Bell Labs, John Bardeen, Walter Houser Brattain y William Shockley inventan el transistor. 1948: en Reino Unido se construye el primer computador del mundo con programa almacenado, el Manchester Baby, diseñado por Frederic C. Williams, Tom Kilburn y Geoff Tootill, de la Universidad de Mánchester. 1949: el MESM fue la primera computadora en Rusia y la segunda programable en Europa continental, creada por un equipo de científicos bajo la dirección de Serguéi Alekseevich Lébedev. 1950: Kathleen Booth, crea el Lenguaje ensamblador para hacer operaciones en la computadora sin necesidad de cambiar los cables de conexión, sino a través de tarjetas perforadoras (programa u operación guardada para usarla cuando sea necesario) las cuales eran propensas a dañarse por esta razón, a finales de este año se comienza a desarrollar el lenguaje de programación. 1951 BESM El desarrollo de las máquinas BESM comenzó en el Instituto de Mecánica de Precisión y Ciencias de la Computación de la URSS (ITM y VT) en Moscú bajo la supervisión de Serguéi Alekseevich Lébedev en 1950 como una continuación de sus trabajos en Kiev con la computadora MESM. Esta serie de máquinas se dejaron de fabricar en 1987. 1951: comienza a operar la EDVAC, concebida por John von Neumann, que a diferencia de la ENIAC no era decimal, sino binaria, y tuvo el primer programa diseñado para ser almacenado. 1953: IBM fabrica su primera computadora a escala industrial, la IBM 650. Se amplía el uso del lenguaje ensamblador para la programación de las computadoras. Las computadoras con transistores reemplazan a las de válvulas, marcando el comienzo de la segunda generación de computadoras. 1957: Jack S. Kilby construye el primer circuito integrado. 1961: en Reino Unido se construye la Calculadora Anita en dos modelos, siendo las primeras calculadoras electrónicas de escritorio del mundo. 1964: la aparición del IBM 360 marca el comienzo de la tercera generación de computadoras, en la que las placas de circuito impreso con múltiples componentes elementales pasan a ser reemplazadas con placas de circuitos integrados. 1965: Olivetti lanza, Programma 101, la primera computadora de escritorio. 1971: Nicolet Instruments Corp. lanza al mercado la Nicolet 1080, una computadora de uso científico basada en registros de 20 bits. 1971: Intel presenta el primer microprocesador comercial, el Intel 4004, diseñado por Federico Faggin, Marcian Hoff y Masatoshi Shima. 1975: Bill Gates y Paul Allen fundan Microsoft. 1976: Steve Jobs, Steve Wozniak, Mike Markkula fundan Apple. 1977: Apple presenta el primer computador personal que se vende a gran escala, el Apple II, desarrollado por Steve Jobs y Steve Wozniak. 1981: se lanza al mercado el IBM PC, que se convertiría en un éxito comercial, marcaría una revolución en el campo de la computación personal y definiría nuevos estándares. 1982: Microsoft presenta su sistema operativo MS-DOS, por encargo de IBM. 1983: ARPANET se separa de la red militar que la originó, pasando a un uso civil y convirtiéndose así en el origen de Internet. 1983: Richard Stallman anuncia públicamente el proyecto GNU. 1985: Microsoft presenta el sistema operativo Windows 1.0. 1990: Tim Berners-Lee idea el hipertexto para crear el World Wide Web (WWW), una nueva manera de interactuar con Internet. 1991: Linus Torvalds comenzó a desarrollar Linux, un sistema operativo compatible con Unix. 2000: aparecen a comienzos del siglo XXI los computadores de bolsillo, primeras PDA. 2007: presentación del primer iPhone, por la empresa Apple, un teléfono inteligente o smartphone. Componentes Artículo principal: Arquitectura de computadoras Imagen ilustrativa Las tecnologías utilizadas en computadoras digitales han evolucionado mucho desde la aparición de los primeros modelos en los años 1940, aunque la mayoría todavía utiliza la Arquitectura de von Neumann, publicada por John von Neumann a principios de esa década, que otros autores atribuyen a John Presper Eckert y John William Mauchly. La arquitectura de Von Neumann describe una computadora con cuatro (4) secciones principales: la unidad aritmético lógica, la unidad de control, la memoria primaria, principal o central, y los dispositivos de entrada y salida (E/S). Estas partes están interconectadas por canales de conductores denominados buses.[17]​[17]​ Unidad central de procesamiento Artículo principal: Unidad central de procesamiento La unidad central de procesamiento (CPU, por sus siglas del inglés: Central Processing Unit) consta de manera básica de los siguientes tres elementos: Un típico símbolo esquemático para una ALU: A y B son operandos; R es la salida; F es la entrada de la unidad de control; D es un estado de la salida. La unidad aritmética lógica (ALU: Arithmetic Logic Unit) es el dispositivo diseñado y construido para llevar a cabo las operaciones elementales como las operaciones aritméticas (suma, resta), operaciones lógicas (AND, OR, XOR, inversiones, desplazamientos y rotaciones). La unidad de control (UC: Control Unit) sigue la dirección de las posiciones en memoria que contienen la instrucción que el computador va a realizar en ese momento; recupera la información poniéndola en la ALU para la operación que debe desarrollar. Transfiere luego el resultado a ubicaciones correspondientes en la memoria. Una vez que ocurre lo anterior, la unidad de control va a la siguiente instrucción, pudiendo ser la siguiente físicamente (procedente del Contador de Programa) u otra (a través de una instrucción de salto). Los registros: no accesibles (de instrucción, de bus de datos y bus de dirección) y accesibles, uso específico (contador programa, puntero pila, acumulador, flags, etc.) o de uso general. Memoria primaria Véanse también: Jerarquía de memoria, Memoria principal, Memoria (Informática) y RAM. La memoria principal, conocida como memoria de acceso aleatorio (RAM, por sus siglas del inglés: Random-Access Memory), es un conjunto de celdas de almacenamiento organizadas de tal forma que se pueden acceder numéricamente a la dirección de memoria. Cada celda corresponde a un bit o unidad mínima de información. Se accede por secuencias de 8 bits. Una instrucción es una determinada acción operativa, una secuencia que indica a la ALU la operación a realizar (suma, resta, operaciones lógicas, etc). En los bytes de memoria principal se almacenan tanto los datos como los códigos de operación que se necesitan para llevar a cabo las instrucciones. La capacidad de la memoria viene dada por el número de celdas que contiene, medido en bytes o múltiplos. Las tecnologías empleadas para fabricar las memorias han cambiado bastante; desde los relés electromecánicos de las primeras computadoras, tubos con mercurio en los que se formaban los pulsos acústicos, matrices de imanes permanentes, transistores individuales hasta los actuales circuitos integrados con millones de celdas en un solo chip. Se subdividen en memorias estáticas (SRAM) con seis transistores integrados por bit y la mucho más utilizada memoria dinámica (DRAM), de un transistor y un condensador integrados por bit.[18] La memoria RAM puede ser reescrita varios millones de veces; a diferencia de la memoria ROM, que solo puede ser grabada una única vez.[19]​ Periféricos de entrada, de salida o de entrada/salida Véanse también: Periférico (informática), Periférico de entrada y Periférico de entrada/salida. Los dispositivos de entrada permiten el ingreso de datos e información mientras que los de salida son los encargados de exteriorizar la información procesada por la computadora. Hay periféricos que son a la vez de entrada y de salida. Como ejemplo, un dispositivo típico de entrada es el teclado, uno de salida es el monitor, uno de entrada/salida es el disco rígido. Hay una gama muy extensa de dispositivos E/S, como teclado, monitor, impresora, ratón, unidad de disco flexible, cámara web, etc. Computadora de escritorio Buses Las tres unidades básicas en una computadora, la CPU, la memoria y el elemento de E/S, están comunicadas entre sí por buses o canales de comunicación: Bus de direcciones : permite seleccionar la dirección del dato o del periférico al que se quiere acceder, Bus de control : controla el funcionamiento externo e interno de la CPU. Bus de datos :contiene la información (datos) que circula por el sistema. Otros datos y conceptos En los computadores modernos, un usuario tiene la impresión de que los computadores pueden ejecutar varios programas «al mismo tiempo», esto se conoce como multitarea. En realidad, la CPU ejecuta instrucciones de un programa y después tras un breve periodo de tiempo, cambia la ejecución a un segundo programa y ejecuta algunas de sus instrucciones. Dado que este proceso es muy rápido, crea la ilusión de que se están ejecutando varios programas simultáneamente; en realidad se está repartiendo el tiempo de la CPU entre los programas, uno a la vez. El sistema operativo es el que controla el reparto del tiempo. El procesamiento realmente simultáneo se realiza en computadoras que poseen más de un CPU, lo que da origen al multiprocesamiento. El sistema operativo es el programa que gestiona y administra todos los recursos del ordenador, controla, por ejemplo, qué programas se ejecutan y cuándo, administra la memoria y los accesos a los dispositivos E/S, provee las interfases entre dispositivos, incluso entre el computador y el usuario. Actualmente se suele incluir en las distribuciones del sistema operativo algunos programas muy usados; como navegadores de Internet, procesadores de texto, programas de correo electrónico, interfaces de red, reproductores de películas y otros programas que antes se tenían que conseguir e instalar separadamente. Los primeros computadores digitales, de gran tamaño y coste, se utilizaban principalmente para hacer cálculos científicos. ENIAC se creó con el propósito de resolver los problemas de balística del ejército de Estados Unidos. El CSIRAC, el primer ordenador australiano, permitió evaluar patrones de precipitaciones para un gran proyecto de generación hidroeléctrica. Con la fabricación comercial de computadoras, los gobiernos y las empresas sistematizaron muchas de sus tareas de recolección y procesamiento de datos, que antes eran realizadas manualmente. En el mundo académico, los científicos de todos los campos empezaron a utilizar los computadores para hacer sus análisis y cálculos; el descenso continuado de los precios de estos aparatos permitió su uso por empresas cada vez más pequeñas. Las empresas, las organizaciones y los gobiernos empezaron a emplear un gran número de pequeños ordenadores para realizar tareas que antes eran hechas por computadores centrales grandes y costosos. Con la invención del microprocesador en 1970, fue posible fabricar ordenadores cada vez más baratos. Nació el microcomputador y luego apareció la computadora personal, estos últimos se hicieron populares para llevar a cabo tareas rutinarias como escribir e imprimir documentos, calcular probabilidades, realizar análisis y cálculo con hojas de cálculo, comunicarse mediante correo electrónico e Internet. La gran disponibilidad de computadores y su fácil adaptación a las necesidades de cada persona, han hecho que se utilicen para una variedad de tareas, que incluyen los más diversos campos de aplicación. Al mismo tiempo, los computadores pequeños de programación fija (sistemas embebidos) empezaron a abrirse camino entre las aplicaciones para el hogar, los automóviles, los aviones y la maquinaria industrial. Estos procesadores integrados controlaban el comportamiento de los aparatos más fácilmente, permitiendo el desarrollo de funciones de control más complejas, como por ejemplo los sistemas de freno antibloqueo (ABS). A principios del siglo XXI, la mayoría de los aparatos eléctricos, casi todos los tipos de transporte eléctrico y la mayoría de las líneas de producción de las fábricas funcionan con un computador. PC con interfaz táctil. Hacia finales del siglo XX y comienzos del XXI, los computadores personales son usados tanto para la investigación como para el entretenimiento (videojuegos), mientras que los grandes computadores se utilizan para cálculos matemáticos complejos, tecnología, modelado, astronomía, medicina, etc. Como resultado del cruce entre el concepto de la computadora personal y los llamados «supercomputadores» surge la estación de trabajo; este término, originalmente utilizado para equipos y máquinas de registro, grabación y tratamiento digital de sonido, ahora hace referencia a estaciones de trabajo, que son sistemas de gran capacidad de cómputo, normalmente dedicados a labores de cálculo científico o procesos en tiempo real. Una estación de trabajo es, en esencia, un equipo de trabajo personal con capacidad elevada de cálculo, rendimiento y almacenamiento, superior a los computadores personales convencionales."
ksampletext_wikipedia_tech_software: str = "Software. Se conoce como software logicial, soporte lógico o programática al sistema formal de un sistema informático, que comprende el conjunto de los componentes lógicos necesarios que hace posible la realización de tareas específicas, en contraposición a los componentes físicos que son llamados hardware. La interacción entre el software y el hardware hace operativo un ordenador (u otro dispositivo), es decir, el software envía instrucciones que el hardware ejecuta, haciendo posible su funcionamiento. Los componentes lógicos incluyen, entre muchos otros, las aplicaciones informáticas, tales como el procesador de texto, que permite al usuario realizar todas las tareas concernientes a la edición de textos; el llamado software de sistema, tal como el sistema operativo, que básicamente permite al resto de los programas funcionar adecuadamente, facilitando también la interacción entre los componentes físicos y el resto de las aplicaciones, y proporcionando una interfaz con el usuario.[3]​ El software, en su gran mayoría, está escrito en lenguajes de programación de alto nivel, ya que son más fáciles y eficientes para que los programadores los usen, porque son más cercanos al lenguaje natural respecto del lenguaje de máquina.[4] Los lenguajes de alto nivel se traducen a lenguaje de máquina utilizando un compilador o un intérprete, o bien una combinación de ambos. El software también puede estar escrito en lenguaje ensamblador, que es de bajo nivel y tiene una alta correspondencia con las instrucciones de lenguaje máquina; se traduce al lenguaje de la máquina utilizando un ensamblador. El anglicismo software es el más ampliamente difundido al referirse a este concepto, especialmente en la jerga técnica, en tanto que el término sinónimo «logicial», derivado del término francés logiciel, es utilizado mayormente en países y zonas de influencia francesa. Etimología Software es una palabra proveniente del inglés, que en español no posee una traducción adecuada al contexto, por lo cual se la utiliza asiduamente sin traducir y así fue admitida por la Real Academia Española (RAE).[5] Aunque puede no ser estrictamente lo mismo, suele sustituirse por expresiones tales como programas (informáticos), aplicaciones informáticas o soportes lógicos.[6]​ Software es lo que se denomina producto en ingeniería de software.[7]​ El término «logicial» es un calco léxico del término francés logiciel, neologismo que se formó en 1969 a partir de las palabras logique ('lógica') y matériel ('material') como traducción de la Delegación de la informática responsable del Plan Calcul.[8]​ Definición de software Existen varias definiciones similares aceptadas para software, pero probablemente la más formal sea la siguiente: Es el conjunto de los programas de cómputo, procedimientos, reglas, documentación y datos asociados, que forman parte de las operaciones de un sistema de computación. Extraído del estándar 729 del IEEE[9]​ Considerando esta definición, el concepto de software va más allá de los programas de computación en sus distintos estados: código fuente, binario o ejecutable; también su documentación, los datos a procesar e incluso la información de usuario forman parte del software: es decir, abarca todo lo intangible, todo lo «no físico» relacionado. El término software fue usado por primera vez en este sentido por John W. Tukey en 1957. En la ingeniería de software y las ciencias de la computación, el software es toda la información procesada por los sistemas informáticos: programas y datos. El concepto de leer diferentes secuencias de instrucciones (programa) desde la memoria de un dispositivo para controlar los cálculos fue introducido por Charles Babbage como parte de su máquina diferencial. La teoría que forma la base de la mayor parte del software moderno fue propuesta por Alan Turing en su ensayo de 1936, «Los números computables», con una aplicación al problema de decisión.[10]​ Clasificación del software Buscador de Programas en Ubuntu 13.10 Si bien esta distinción es, en cierto modo arbitraria, y a veces confusa, a los fines prácticos se puede clasificar al software en tres tipos:[11]​ Software de sistema: Su objetivo es vincular adecuadamente al usuario y al programador de los detalles del sistema informático en particular que se use, aislándolo especialmente del procesamiento referido a las características internas de: memoria, discos, puertos y dispositivos de comunicaciones, impresoras, pantallas, teclados, etc. El software de sistema le procura al usuario y programador adecuadas interfaces de alto nivel, controladores, herramientas y utilidades de apoyo que permiten el mantenimiento del sistema global. Incluye entre otros: Sistemas operativos Controladores de dispositivos Herramientas de diagnóstico Herramientas de corrección y optimización Servidores Utilidades Software de programación: Es el conjunto de herramientas que permiten al programador desarrollar programas de informática, usando diferentes alternativas y lenguajes de programación, de una manera práctica. Incluyen en forma básica: Editores de texto Compiladores Intérpretes Enlazadores Depuradores Entornos de desarrollo integrados (IDE): Agrupan las anteriores herramientas, usualmente en un entorno visual, de forma tal que el programador no necesite introducir múltiples comandos para compilar, interpretar, depurar, etc. Habitualmente cuentan con una avanzada interfaz gráfica de usuario (GUI). Software de aplicación: Es aquel que permite a los usuarios llevar a cabo una o varias tareas específicas, en cualquier campo de actividad susceptible de ser automatizado o asistido, con especial énfasis en los negocios. Incluye entre muchos otros: Aplicaciones para Control de sistemas y automatización industrial Aplicaciones ofimáticas Software educativo Software empresarial[12]​ Bases de datos Telecomunicaciones (por ejemplo Internet y toda su estructura lógica) Videojuegos Software médico Software de cálculo numérico y simbólico. Software de diseño asistido (CAD) Software de control numérico (CAM) Proceso de creación del software Artículo principal: Proceso para el desarrollo de software Se define como «proceso» al conjunto ordenado de pasos a seguir para llegar a la solución de un problema u obtención de un producto, en este caso particular, para lograr un producto software que resuelva un problema específico. El proceso de creación de software puede llegar a ser muy complejo, dependiendo de su porte, características y criticidad del mismo. Por ejemplo la creación de un sistema operativo es una tarea que requiere proyecto, gestión, numerosos recursos y todo un equipo disciplinado de trabajo. En el otro extremo, si se trata de un sencillo programa (por ejemplo, la resolución de una ecuación de segundo orden), este puede ser realizado por un solo programador (incluso aficionado) fácilmente. Es así que normalmente se dividen en tres categorías según su tamaño (líneas de código) o costo: de «pequeño», «mediano» y «gran porte». Existen varias metodologías para estimarlo, una de las más populares es el sistema COCOMO que provee métodos y un software (programa) que calcula y provee una aproximación de todos los costos de producción en un «proyecto software» (relación horas/hombre, costo monetario, cantidad de líneas fuente de acuerdo a lenguaje usado, etc.). Considerando los de gran porte, es necesario realizar complejas tareas, tanto técnicas como de gerencia, una fuerte gestión y análisis diversos (entre otras cosas), la complejidad de ello ha llevado a que desarrolle una ingeniería específica para tratar su estudio y realización: es conocida como ingeniería de Software. En tanto que en los de mediano porte, pequeños equipos de trabajo (incluso un avezado analista-programador solitario) pueden realizar la tarea. Aunque, siempre en casos de mediano y gran porte (y a veces también en algunos de pequeño porte, según su complejidad), se deben seguir ciertas etapas que son necesarias para la construcción del software. Tales etapas, si bien deben existir, son flexibles en su forma de aplicación, de acuerdo a la metodología o proceso de desarrollo escogido y utilizado por el equipo de desarrollo o por el analista-programador solitario (si fuere el caso). Los «procesos de desarrollo de software» poseen reglas preestablecidas, y deben ser aplicados en la creación del software de mediano y gran porte, ya que en caso contrario lo más seguro es que el proyecto no logre concluir o termine sin cumplir los objetivos previstos, y con variedad de fallos inaceptables (fracasan, en pocas palabras). Entre tales «procesos» los hay ágiles o livianos (ejemplo XP), pesados y lentos (ejemplo RUP), y variantes intermedias. Normalmente se aplican de acuerdo al tipo y porte del software a desarrollar, a criterio del líder (si lo hay) del equipo de desarrollo. Algunos de esos procesos son Programación Extrema (en inglés eXtreme Programming o XP), Proceso Unificado de Rational (en inglés Rational Unified Process o RUP), Feature Driven Development (FDD), etc.[13]​ Cualquiera sea el «proceso» utilizado y aplicado al desarrollo del software (RUP, FDD, XP, etc), y casi independientemente de él, siempre se debe aplicar un «modelo de ciclo de vida».[14]​ Se estima que, del total de proyectos software grandes emprendidos, un 28 % fracasan, un 46 % caen en severas modificaciones que lo retrasan y un 26 % son totalmente exitosos.[15]​ Cuando un proyecto fracasa, rara vez es debido a fallas técnicas, la principal causa de fallos y fracasos es la falta de aplicación de una buena metodología o proceso de desarrollo. Entre otras, una fuerte tendencia, desde hace pocas décadas, es mejorar las metodologías o procesos de desarrollo, o crear nuevas y concientizar a los profesionales de la informática a su utilización adecuada. Normalmente los especialistas en el estudio y desarrollo de estas áreas (metodologías) y afines (tales como modelos y hasta la gestión misma de los proyectos) son los ingenieros en software, es su orientación. Los especialistas en cualquier otra área de desarrollo informático (analista, programador, Lic. en informática, ingeniero en informática, ingeniero de sistemas, etc.) normalmente aplican sus conocimientos especializados pero utilizando modelos, paradigmas y procesos ya elaborados. Es común para el desarrollo de software de mediano porte que los equipos humanos involucrados apliquen «metodologías propias», normalmente un híbrido de los procesos anteriores y a veces con criterios propios.[16]​ El proceso de desarrollo puede involucrar numerosas y variadas tareas,[14] desde lo administrativo, pasando por lo técnico y hasta la gestión y el gerenciamiento. Pero, casi rigurosamente, siempre se cumplen ciertas etapas mínimas; las que se pueden resumir como sigue: Captura, elicitación,[17] especificación y análisis de requisitos (ERS) Diseño Codificación Pruebas (unitarias y de integración) Instalación y paso a producción Mantenimiento En las anteriores etapas pueden variar ligeramente sus nombres, o ser más globales, o contrariamente, ser más refinadas; por ejemplo indicar como una única fase (a los fines documentales e interpretativos) de «análisis y diseño»; o indicar como «implementación» lo que está dicho como «codificación»; pero en rigor, todas existen e incluyen, básicamente, las mismas tareas específicas. En el apartado 4 del presente artículo se brindan mayores detalles de cada una de las etapas indicadas. Modelos de proceso o ciclo de vida Para cada una de las fases o etapas listadas en el ítem anterior, existen subetapas (o tareas). El modelo de proceso o modelo de ciclo de vida utilizado para el desarrollo, define el orden de las tareas o actividades involucradas,[14] también define la coordinación entre ellas, y su enlace y realimentación. Entre los más conocidos se puede mencionar: modelo en cascada o secuencial, modelo espiral, modelo iterativo incremental. De los antedichos hay a su vez algunas variantes o alternativas, más o menos atractivas según sea la aplicación requerida y sus requisitos.[15]​ Modelo cascada Este, aunque es más comúnmente conocido como modelo en cascada es también llamado «modelo clásico», «modelo tradicional» o «modelo lineal secuencial». El modelo en cascada puro «difícilmente se utiliza tal cual», pues esto implicaría un previo y absoluto conocimiento de los requisitos, la no volatilidad de los mismos (o rigidez) y etapas subsiguientes libres de errores; ello solo podría ser aplicable a escasos y pequeños sistemas a desarrollar. En estas circunstancias, el paso de una etapa a otra de las mencionadas sería sin retorno, por ejemplo pasar del diseño a la codificación implicaría un diseño exacto y sin errores ni probable modificación o evolución: «codifique lo diseñado sin errores, no habrá en absoluto variantes futuras». Esto es utópico; ya que intrínsecamente «el software es de carácter evolutivo»,[18] cambiante y difícilmente libre de errores, tanto durante su desarrollo como durante su vida operativa.[14]​ Figura 2: Modelo cascada puro o secuencial para el ciclo de vida del software. Algún cambio durante la ejecución de una cualquiera de las etapas en este modelo secuencial podría implicar reiniciar desde el principio todo el ciclo completo, lo cual redundaría en altos costos de tiempo y desarrollo. La Figura 2 muestra un posible esquema del modelo en cuestión.[14]​ Sin embargo, el modelo cascada en algunas de sus variantes es uno de los actualmente más utilizados,[19] por su eficacia y simplicidad, más que nada en software de pequeño y algunos de mediano porte; pero nunca (o muy rara vez) se lo usa en su forma pura, como se dijo anteriormente. En lugar de ello, siempre se produce alguna realimentación entre etapas, que no es completamente predecible ni rígida; esto da oportunidad al desarrollo de productos software en los cuales hay ciertas incertezas, cambios o evoluciones durante el ciclo de vida. Así por ejemplo, una vez capturados y especificados los requisitos (primera etapa) se puede pasar al diseño del sistema, pero durante esta última fase lo más probable es que se deban realizar ajustes en los requisitos (aunque sean mínimos), ya sea por fallas detectadas, ambigüedades o bien porque los propios requisitos han cambiado o evolucionado; con lo cual se debe retornar a la primera o previa etapa, hacer los reajustes pertinentes y luego continuar nuevamente con el diseño; esto último se conoce como realimentación. Lo normal en el modelo cascada es entonces la aplicación del mismo con sus etapas realimentadas de alguna forma, permitiendo retroceder de una a la anterior (e incluso poder saltar a varias anteriores) si es requerido. De esta manera se obtiene el «modelo cascada realimentado», que puede ser esquematizado como lo ilustra la Figura 3. Figura 3: Modelo cascada realimentado para el ciclo de vida. Lo dicho es, a grandes rasgos, la forma y utilización de este modelo, uno de los más usados y populares.[14] El modelo cascada realimentado resulta muy atractivo, hasta ideal, si el proyecto presenta alta rigidez (pocos cambios, previsto no evolutivo), los requisitos son muy claros y están correctamente especificados.[19]​ Hay más variantes similares al modelo: refino de etapas (más etapas, menores y más específicas) o incluso mostrar menos etapas de las indicadas, aunque en tal caso la faltante estará dentro de alguna otra. El orden de esas fases indicadas en el ítem previo es el lógico y adecuado, pero adviértase, como se dijo, que normalmente habrá realimentación hacia atrás. El modelo lineal o en cascada es el paradigma más antiguo y extensamente utilizado, sin embargo las críticas a él (ver desventajas) han puesto en duda su eficacia. Pese a todo, tiene un lugar muy importante en la ingeniería de software y continúa siendo el más utilizado; y siempre es mejor que un enfoque al azar.[19]​ Desventajas del modelo cascada:[14]​ Los cambios introducidos durante el desarrollo pueden confundir al equipo profesional en las etapas tempranas del proyecto. Si los cambios se producen en etapa madura (codificación o prueba) pueden ser catastróficos para un proyecto grande. No es frecuente que el cliente o usuario final explicite clara y completamente los requisitos (etapa de inicio); y el modelo lineal así lo requiere. La incertidumbre natural en los comienzos es luego difícil de acomodar.[19]​ El cliente debe tener paciencia ya que el software no estará disponible hasta muy avanzado el proyecto. Un error importante detectado por el cliente (en fase de operación) puede ser desastroso, implicando reinicio del proyecto, con altos costos. Modelos evolutivos El software evoluciona con el tiempo.[20]​[18] Los requisitos del usuario y del producto suelen cambiar conforme se desarrolla el mismo. Las fechas de mercado y la competencia hacen que no sea posible esperar a poner en el mercado un producto absolutamente completo, por lo que se aconseja introducir una versión funcional limitada de alguna forma para aliviar las presiones competitivas. En esas u otras situaciones similares, los desarrolladores necesitan modelos de progreso que estén diseñados para acomodarse a una evolución temporal o progresiva, donde los requisitos centrales son conocidos de antemano, aunque no estén bien definidos a nivel detalle. En el modelo cascada y cascada realimentado no se tiene demasiado en cuenta la naturaleza evolutiva del software,[20] se plantea como estático, con requisitos bien conocidos y definidos desde el inicio.[14]​ Los evolutivos son modelos iterativos, permiten desarrollar versiones cada vez más completas y complejas, hasta llegar al objetivo final deseado; incluso evolucionar más allá, durante la fase de operación. Los modelos «iterativo incremental» y «espiral» (entre otros) son dos de los más conocidos y utilizados del tipo evolutivo.[19]​ Modelo iterativo incremental En términos generales, se puede distinguir, en la figura 4, los pasos generales que sigue el proceso de desarrollo de un producto software. En el modelo de ciclo de vida seleccionado, se identifican claramente dichos pasos. La descripción del sistema es esencial para especificar y confeccionar los distintos incrementos hasta llegar al producto global y final. Las actividades concurrentes (especificación, desarrollo y validación) sintetizan el desarrollo pormenorizado de los incrementos, que se hará posteriormente. Figura 4: Diagrama genérico del desarrollo evolutivo incremental. El diagrama de la figura 4 muestra en forma muy esquemática, el funcionamiento de un ciclo iterativo incremental, el cual permite la entrega de versiones parciales a medida que se va construyendo el producto final.[14] Es decir, a medida que cada incremento definido llega a su etapa de operación y mantenimiento. Cada versión emitida incorpora a los anteriores incrementos las funcionalidades y requisitos que fueron analizados como necesarios. El incremental es un modelo de tipo evolutivo que está basado en varios ciclos cascada realimentados aplicados repetidamente, con una filosofía iterativa.[19]​En la figura 5 se muestra un refino del diagrama previo, bajo un esquema temporal, para obtener finalmente el esquema del modelo de ciclo de vida iterativo incremental, con sus actividades genéricas asociadas. Aquí se observa claramente cada ciclo cascada que es aplicado para la obtención de un incremento; estos últimos se van integrando para obtener el producto final completo. Cada incremento es un ciclo cascada realimentado, aunque, por simplicidad, en la figura 5 se muestra como secuencial puro. Figura 5: Modelo iterativo incremental para el ciclo de vida del software. Se observa que existen actividades de desarrollo (para cada incremento) que son realizadas en paralelo o concurrentemente, así por ejemplo, en la Figura, mientras se realiza el diseño detalle del primer incremento ya se está realizando en análisis del segundo. La Figura 5 es solo esquemática, un incremento no necesariamente se iniciará durante la fase de diseño del anterior, puede ser posterior (incluso antes), en cualquier tiempo de la etapa previa. Cada incremento concluye con la actividad de «operación y mantenimiento» (indicada como «Operación» en la figura), que es donde se produce la entrega del producto parcial al cliente. El momento de inicio de cada incremento es dependiente de varios factores: tipo de sistema; independencia o dependencia entre incrementos (dos de ellos totalmente independientes pueden ser fácilmente iniciados al mismo tiempo si se dispone de personal suficiente); capacidad y cantidad de profesionales involucrados en el desarrollo; etc.[16]​ Bajo este modelo se entrega software «por partes funcionales más pequeñas», pero reutilizables, llamadas incrementos. En general cada incremento se construye sobre aquel que ya fue entregado.[14]​ Como se muestra en la Figura 5, se aplican secuencias Cascada en forma escalonada, mientras progresa el tiempo calendario. Cada secuencia lineal o Cascada produce un incremento y a menudo el primer incremento es un sistema básico, con muchas funciones suplementarias (conocidas o no) sin entregar. El cliente utiliza inicialmente ese sistema básico, intertanto, el resultado de su uso y evaluación puede aportar al plan para el desarrollo del/los siguientes incrementos (o versiones). Además también aportan a ese plan otros factores, como lo es la priorización (mayor o menor urgencia en la necesidad de cada incremento en particular) y la dependencia entre incrementos (o independencia). Luego de cada integración se entrega un producto con mayor funcionalidad que el previo. El proceso se repite hasta alcanzar el software final completo. Siendo iterativo, con el modelo incremental se entrega un producto parcial pero completamente operacional en cada incremento, y no una parte que sea usada para reajustar los requisitos (como si ocurre en el modelo de construcción de prototipos).[19]​ El enfoque incremental resulta muy útil cuando se dispone de baja dotación de personal para el desarrollo; también si no hay disponible fecha límite del proyecto por lo que se entregan versiones incompletas pero que proporcionan al usuario funcionalidad básica (y cada vez mayor). También es un modelo útil a los fines de versiones de evaluación. Nota: Puede ser considerado y útil, en cualquier momento o incremento incorporar temporalmente el paradigma MCP como complemento, teniendo así una mixtura de modelos que mejoran el esquema y desarrollo general. Ejemplo: Un procesador de texto que sea desarrollado bajo el paradigma incremental podría aportar, en principio, funciones básicas de edición de archivos y producción de documentos (algo como un editor simple). En un segundo incremento se le podría agregar edición más sofisticada, y de generación y mezcla de documentos. En un tercer incremento podría considerarse el agregado de funciones de corrección ortográfica, esquemas de paginado y plantillas; en un cuarto capacidades de dibujo propias y ecuaciones matemáticas. Así sucesivamente hasta llegar al procesador final requerido. Así, el producto va creciendo, acercándose a su meta final, pero desde la entrega del primer incremento ya es útil y funcional para el cliente, el cual observa una respuesta rápida en cuanto a entrega temprana; sin notar que la fecha límite del proyecto puede no estar acotada ni tan definida, lo que da margen de operación y alivia presiones al equipo de desarrollo.[16]​ Como se dijo, el iterativo incremental es un modelo del tipo evolutivo, es decir donde se permiten y esperan probables cambios en los requisitos en tiempo de desarrollo; se admite cierto margen para que el software pueda evolucionar.[18] Aplicable cuando los requisitos son medianamente bien conocidos pero no son completamente estáticos y definidos, cuestión esa que si es indispensable para poder utilizar un modelo Cascada. El modelo es aconsejable para el desarrollo de software en el cual se observe, en su etapa inicial de análisis, que posee áreas bastante bien definidas a cubrir, con suficiente independencia como para ser desarrolladas en etapas sucesivas. Tales áreas a cubrir suelen tener distintos grados de apremio por lo cual las mismas se deben priorizar en un análisis previo, es decir, definir cual será la primera, la segunda, y así sucesivamente; esto se conoce como «definición de los incrementos» con base en la priorización. Pueden no existir prioridades funcionales por parte del cliente, pero el desarrollador debe fijarlas de todos modos y con algún criterio, ya que basándose en ellas se desarrollarán y entregarán los distintos incrementos. El hecho de que existan incrementos funcionales del software lleva inmediatamente a pensar en un esquema de desarrollo modular, por tanto este modelo facilita tal paradigma de diseño. En resumen, un modelo incremental lleva a pensar en un desarrollo modular, con entregas parciales del producto software denominados «incrementos» del sistema, que son escogidos según prioridades predefinidas de algún modo. El modelo permite una implementación con refinamientos sucesivos (ampliación o mejora). Con cada incremento se agrega nueva funcionalidad o se cubren nuevos requisitos o bien se mejora la versión previamente implementada del producto software. Este modelo brinda cierta flexibilidad para que durante el desarrollo se incluyan cambios en los requisitos por parte del usuario, un cambio de requisitos propuesto y aprobado puede analizarse e implementarse como un nuevo incremento o, eventualmente, podrá constituir una mejora/adecuación de uno ya planeado. Aunque si se produce un cambio de requisitos por parte del cliente que afecte incrementos previos ya terminados (detección/incorporación tardía) se debe evaluar la factibilidad y realizar un acuerdo con el cliente, ya que puede impactar fuertemente en los costos.[16]​ La selección de este modelo permite realizar entregas funcionales tempranas al cliente (lo cual es beneficioso tanto para él como para el grupo de desarrollo). Se priorizan las entregas de aquellos módulos o incrementos en que surja la necesidad operativa de hacerlo, por ejemplo para cargas previas de información, indispensable para los incrementos siguientes.[19]​ El modelo iterativo incremental no obliga a especificar con precisión y detalle absolutamente todo lo que el sistema debe hacer, (y cómo), antes de ser construido (como el caso del cascada, con requisitos congelados). Solo se hace en el incremento en desarrollo. Esto torna más manejable el proceso y reduce el impacto en los costos. Esto es así, porque en caso de alterar o rehacer los requisitos, solo afecta una parte del sistema. Aunque, lógicamente, esta situación se agrava si se presenta en estado avanzado, es decir en los últimos incrementos. En definitiva, el modelo facilita la incorporación de nuevos requisitos durante el desarrollo. Con un paradigma incremental se reduce el tiempo de desarrollo inicial, ya que se implementa funcionalidad parcial. También provee un impacto ventajoso frente al cliente, que es la entrega temprana de partes operativas del software. El modelo proporciona todas las ventajas del modelo en cascada realimentado, reduciendo sus desventajas solo al ámbito de cada incremento. El modelo incremental no es recomendable para casos de sistemas de tiempo real, de alto nivel de seguridad, de procesamiento distribuido, o de alto índice de riesgos. Modelo espiral El modelo espiral fue propuesto inicialmente por Barry Boehm. Es un modelo evolutivo que conjuga la naturaleza iterativa del modelo MCP con los aspectos controlados y sistemáticos del Modelo Cascada. Proporciona potencial para desarrollo rápido de versiones incrementales. En el modelo espiral el software se construye en una serie de versiones incrementales. En las primeras iteraciones la versión incremental podría ser un modelo en papel o bien un prototipo. En las últimas iteraciones se producen versiones cada vez más completas del sistema diseñado.[14]​[19]​ El modelo se divide en un número de Actividades de marco de trabajo, llamadas «regiones de tareas». En general existen entre tres y seis regiones de tareas (hay variantes del modelo). En la figura 6 se muestra el esquema de un modelo espiral con seis regiones. En este caso se explica una variante del modelo original de Boehm, expuesto en su tratado de 1988; en 1998 expuso un tratado más reciente. Figura 6: Modelo espiral para el ciclo de vida del software. Las regiones definidas en el modelo de la figura son: Región 1 — Tareas requeridas para establecer la comunicación entre el cliente y el desarrollador. Región 2 — Tareas inherentes a la definición de los recursos, tiempo y otra información relacionada con el proyecto. Región 3 — Tareas necesarias para evaluar los riesgos técnicos y de gestión del proyecto. Región 4 — Tareas para construir una o más representaciones de la aplicación software. Región 5 — Tareas para construir la aplicación, instalarla, probarla y proporcionar soporte al usuario o cliente (Ej. documentación y práctica). Región 6 — Tareas para obtener la reacción del cliente, según la evaluación de lo creado e instalado en los ciclos anteriores. Las actividades enunciadas para el marco de trabajo son generales y se aplican a cualquier proyecto, grande, mediano o pequeño, complejo o no. Las regiones que definen esas actividades comprenden un «conjunto de tareas» del trabajo: ese conjunto sí se debe adaptar a las características del proyecto en particular a emprender. Nótese que lo listado en los ítems de 1 a 6 son conjuntos de tareas, algunas de las ellas normalmente dependen del proyecto o desarrollo en sí. Proyectos pequeños requieren baja cantidad de tareas y también de formalidad. En proyectos mayores o críticos cada región de tareas contiene labores de más alto nivel de formalidad. En cualquier caso se aplican actividades de protección (por ejemplo, gestión de configuración del software, garantía de calidad, etc.). Al inicio del ciclo, o proceso evolutivo, el equipo de ingeniería gira alrededor del espiral (metafóricamente hablando) comenzando por el centro (marcado con ๑ en la figura 6) y en el sentido indicado; el primer circuito de la espiral puede producir el desarrollo de una especificación del producto; los pasos siguientes podrían generar un prototipo y progresivamente versiones más sofisticadas del software. Cada paso por la región de planificación provoca ajustes en el plan del proyecto; el coste y planificación se realimentan en función de la evaluación del cliente. El gestor de proyectos debe ajustar el número de iteraciones requeridas para completar el desarrollo. El modelo espiral puede ir adaptándose y aplicarse a lo largo de todo el Ciclo de vida del software (en el modelo clásico, o cascada, el proceso termina a la entrega del software). Una visión alternativa del modelo puede observarse examinando el «eje de punto de entrada de proyectos». Cada uno de los circulitos (๏) fijados a lo largo del eje representan puntos de arranque de los distintos proyectos (relacionados); a saber: Un proyecto de «desarrollo de conceptos» comienza al inicio de la espiral, hace múltiples iteraciones hasta que se completa, es la zona marcada con verde. Si lo anterior se va a desarrollar como producto real, se inicia otro proyecto: «Desarrollo de nuevo Producto». Que evolucionará con iteraciones hasta culminar; es la zona marcada en color azul. Eventual y análogamente se generarán proyectos de «mejoras de productos» y de «mantenimiento de productos», con las iteraciones necesarias en cada área (zonas roja y gris, respectivamente). Cuando la espiral se caracteriza de esta forma, está operativa hasta que el software se retira, eventualmente puede estar inactiva (el proceso), pero cuando se produce un cambio el proceso arranca nuevamente en el punto de entrada apropiado (por ejemplo, en «mejora del producto»). El modelo espiral da un enfoque realista, que evoluciona igual que el software;[20] se adapta muy bien para desarrollos a gran escala. El Espiral utiliza el MCP para reducir riesgos y permite aplicarlo en cualquier etapa de la evolución. Mantiene el enfoque clásico (cascada) pero incorpora un marco de trabajo iterativo que refleja mejor la realidad. Este modelo requiere considerar riesgos técnicos en todas las etapas del proyecto; aplicado adecuadamente debe reducirlos antes de que sean un verdadero problema. El Modelo evolutivo como el Espiral es particularmente apto para el desarrollo de Sistemas Operativos (complejos); también en sistemas de altos riesgos o críticos (Ej. navegadores y controladores aeronáuticos) y en todos aquellos en que sea necesaria una fuerte gestión del proyecto y sus riesgos, técnicos o de gestión. Desventajas importantes: Requiere mucha experiencia y habilidad para la evaluación de los riesgos, lo cual es requisito para el éxito del proyecto. Es difícil convencer a los grandes clientes que se podrá controlar este enfoque evolutivo. Este modelo no se ha usado tanto, como el Cascada (Incremental) o MCP, por lo que no se tiene bien medida su eficacia, es un paradigma relativamente nuevo y difícil de implementar y controlar. Modelo espiral Win & Win Una variante interesante del Modelo Espiral previamente visto (Figura 6) es el «Modelo espiral Win-Win»[15] (Barry Boehm). El Modelo Espiral previo (clásico) sugiere la comunicación con el cliente para fijar los requisitos, en que simplemente se pregunta al cliente qué necesita y él proporciona la información para continuar; pero esto es en un contexto ideal que rara vez ocurre. Normalmente cliente y desarrollador entran en una negociación, se negocia coste frente a funcionalidad, rendimiento, calidad, etc. «Es así que la obtención de requisitos requiere una negociación, que tiene éxito cuando ambas partes ganan». Las mejores negociaciones se fuerzan en obtener «Victoria & Victoria» (Win & Win), es decir que el cliente gane obteniendo el producto que lo satisfaga, y el desarrollador también gane consiguiendo presupuesto y fecha de entrega realista. Evidentemente, este modelo requiere fuertes habilidades de negociación. El modelo Win-Win define un conjunto de actividades de negociación al principio de cada paso alrededor de la espiral; se definen las siguientes actividades: Identificación del sistema o subsistemas clave de los directivos * (saber qué quieren). Determinación de «condiciones de victoria» de los directivos (saber qué necesitan y los satisface). Negociación de las condiciones «victoria» de los directivos para obtener condiciones «Victoria & Victoria» (negociar para que ambos ganen). * Directivo: Cliente escogido con interés directo en el producto, que puede ser premiado por la organización si tiene éxito o criticado si no. El modelo Win & Win hace énfasis en la negociación inicial, también introduce 3 hitos en el proceso llamados «puntos de fijación», que ayudan a establecer la completitud de un ciclo de la espiral, y proporcionan hitos de decisión antes de continuar el proyecto de desarrollo del software. Etapas en el desarrollo del software Captura, análisis y especificación de requisitos Al inicio de un desarrollo (no de un proyecto), esta es la primera fase que se realiza, y, según el modelo de proceso adoptado, puede casi terminar para pasar a la próxima etapa (caso de Modelo Cascada Realimentado) o puede hacerse parcialmente para luego retomarla (caso Modelo Iterativo Incremental u otros de carácter evolutivo). En simple palabras y básicamente, durante esta fase, se adquieren, reúnen y especifican las características funcionales y no funcionales que deberá cumplir el futuro programa o sistema a desarrollar. Las bondades de las características, tanto del sistema o programa a desarrollar, como de su entorno, parámetros no funcionales y arquitectura dependen enormemente de lo bien lograda que esté esta etapa. Esta es, probablemente, la de mayor importancia y una de las fases más difíciles de lograr certeramente, pues no es automatizable, no es muy técnica y depende en gran medida de la habilidad y experiencia del analista que la realice. Involucra fuertemente al usuario o cliente del sistema, por tanto tiene matices muy subjetivos y es difícil de modelar con certeza o aplicar una técnica que sea «la más cercana a la adecuada» (de hecho no existe «la estrictamente adecuada»). Si bien se han ideado varias metodologías, incluso software de apoyo, para captura, elicitación y registro de requisitos, no existe una forma infalible o absolutamente confiable, y deben aplicarse conjuntamente buenos criterios y mucho sentido común por parte del o los analistas encargados de la tarea; es fundamental también lograr una fluida y adecuada comunicación y comprensión con el usuario final o cliente del sistema. El artefacto más importante resultado de la culminación de esta etapa es lo que se conoce como especificación de requisitos software o simplemente documento ERS. Como se dijo, la habilidad del analista para interactuar con el cliente es fundamental; lo común es que el cliente tenga un objetivo general o problema que resolver, no conoce en absoluto el área (informática), ni su jerga, ni siquiera sabe con precisión qué debería hacer el producto software (qué y cuantas funciones) ni, mucho menos, cómo debe operar. En otros casos menos frecuentes, el cliente «piensa» que sabe precisamente lo que el software tiene que hacer, y generalmente acierta muy parcialmente, pero su empecinamiento entorpece la tarea de elicitación. El analista debe tener la capacidad para lidiar con este tipo de problemas, que incluyen relaciones humanas; tiene que saber ponerse al nivel del usuario para permitir una adecuada comunicación y comprensión.[21]​ Escasas son las situaciones en que el cliente sabe con certeza e incluso con completitud lo que requiere de su futuro sistema, este es el caso más sencillo para el analista. Las tareas relativas a captura, elicitación, modelado y registro de requisitos, además de ser sumamente importante, puede llegar a ser dificultosa de lograr acertadamente y llevar bastante tiempo relativo al proceso total del desarrollo; al proceso y metodologías para llevar a cabo este conjunto de actividades normalmente se las asume parte propia de la ingeniería de software, pero dada la antedicha complejidad, actualmente se habla de una ingeniería de requisitos,[22] aunque ella aún no existe formalmente. Hay grupos de estudio e investigación, en todo el mundo, que están exclusivamente abocados a idear modelos, técnicas y procesos para intentar lograr la correcta captura, análisis y registro de requisitos. Estos grupos son los que normalmente hablan de la ingeniería de requisitos; es decir se plantea esta como un área o disciplina pero no como una carrera universitaria en sí misma. Algunos requisitos no necesitan la presencia del cliente, para ser capturados o analizados; en ciertos casos los puede proponer el mismo analista o, incluso, adoptar unilateralmente decisiones que considera adecuadas (tanto en requisitos funcionales como no funcionales). Por citar ejemplos probables: Algunos requisitos sobre la arquitectura del sistema, requisitos no funcionales tales como los relativos al rendimiento, nivel de soporte a errores operativos, plataformas de desarrollo, relaciones internas o ligas entre la información (entre registros o tablas de datos) a almacenar en caso de bases o bancos de datos, etc. Algunos funcionales tales como opciones secundarias o de soporte necesarias para una mejor o más sencilla operatividad; etc. La obtención de especificaciones a partir del cliente (u otros actores intervinientes) es un proceso humano muy interactivo e iterativo; normalmente a medida que se captura la información, se la analiza y realimenta con el cliente, refinándola, puliéndola y corrigiendo si es necesario; cualquiera sea el método de ERS utilizado. El analista siempre debe llegar a conocer la temática y el problema que resolver, dominarlo, hasta cierto punto, hasta el ámbito que el futuro sistema a desarrollar lo abarque. Por ello el analista debe tener alta capacidad para comprender problemas de muy diversas áreas o disciplinas de trabajo (que no son específicamente suyas); así por ejemplo, si el sistema a desarrollar será para gestionar información de una aseguradora y sus sucursales remotas, el analista se debe compenetrar en cómo ella trabaja y maneja su información, desde niveles muy bajos e incluso llegando hasta los gerenciales. Dada a gran diversidad de campos a cubrir, los analistas suelen ser asistidos por especialistas, es decir gente que conoce profundamente el área para la cual se desarrollará el software; evidentemente una única persona (el analista) no puede abarcar tan vasta cantidad de áreas del conocimiento. En empresas grandes de desarrollo de productos software, es común tener analistas especializados en ciertas áreas de trabajo. Contrariamente, no es problema del cliente, es decir él no tiene por qué saber nada de software, ni de diseños, ni otras cosas relacionadas; solo se debe limitar a aportar objetivos, datos e información (de mano propia o de sus registros, equipos, empleados, etc) al analista, y guiado por él, para que, en primera instancia, defina el «Universo de Discurso», y con posterior trabajo logre confeccionar el adecuado documento ERS. Es bien conocida la presión que sufren los desarrolladores de sistemas informáticos para comprender y rescatar las necesidades de los clientes/usuarios. Cuanto más complejo es el contexto del problema más difícil es lograrlo, a veces se fuerza a los desarrolladores a tener que convertirse en casi expertos de los dominios que analizan. Cuando esto no sucede es muy probable que se genere un conjunto de requisitos[23] erróneos o incompletos y por lo tanto un producto de software con alto grado de desaprobación por parte de los clientes/usuarios y un altísimo costo de reingeniería y mantenimiento. Todo aquello que no se detecte, o resulte mal entendido en la etapa inicial provocará un fuerte impacto negativo en los requisitos, propagando esta corriente degradante a lo largo de todo el proceso de desarrollo e incrementando su perjuicio cuanto más tardía sea su detección (Bell y Thayer 1976)(Davis 1993). Procesos, modelado y formas de elicitación de requisitos Siendo que la captura, elicitación y especificación de requisitos, es una parte crucial en el proceso de desarrollo de software, ya que de esta etapa depende el logro de los objetivos finales previstos, se han ideado modelos y diversas metodologías de trabajo para estos fines. También existen herramientas software que apoyan las tareas relativas realizadas por el ingeniero en requisitos. El estándar IEEE 830-1998 brinda una normalización de las «Prácticas recomendadas para la especificación de requisitos software».[24]​ A medida que se obtienen los requisitos, normalmente se los va analizando, el resultado de este análisis, con o sin el cliente, se plasma en un documento, conocido como ERS o Especificación de requisitos software, cuya estructura puede venir definida por varios estándares, tales como CMMI. Un primer paso para realizar el relevamiento de información es el conocimiento y definición acertada lo que se conoce como «Universo de Discurso» del problema, que se define y entiende por: Universo de Discurso (UdeD): es el contexto general en el cual el software deberá ser desarrollado y deberá operar. El UdeD incluye todas las fuentes de información y todas las personas relacionadas con el software. Esas personas son conocidas también como actores de ese universo. El UdeD es la realidad circunstanciada por el conjunto de objetivos definidos por quienes demandaron el software. A partir de la extracción y análisis de información en su ámbito se obtienen todas las especificaciones necesarias y tipos de requisitos para el futuro producto software. El objetivo de la ingeniería de requisitos (IR) es sistematizar el proceso de definición de requisitos permitiendo elicitar, modelar y analizar el problema, generando un compromiso entre los ingenieros de requisitos y los clientes/usuarios, ya que ambos participan en la generación y definición de los requisitos del sistema. La IR aporta un conjunto de métodos, técnicas y herramientas que asisten a los ingenieros de requisitos (analistas) para obtener requisitos lo más seguros, veraces, completos y oportunos posibles, permitiendo básicamente: Comprender el problema Facilitar la obtención de las necesidades del cliente/usuario Validar con el cliente/usuario Garantizar las especificaciones de requisitos Si bien existen diversas formas, modelos y metodologías para elicitar, definir y documentar requisitos, no se puede decir que alguna de ellas sea mejor o peor que la otra, suelen tener muchísimo en común, y todas cumplen el mismo objetivo. Sin embargo, lo que si se puede decir sin dudas es que es indispensable utilizar alguna de ellas para documentar las especificaciones del futuro producto software. Así por ejemplo, hay un grupo de investigación argentino que desde hace varios años ha propuesto y estudia el uso del LEL (Léxico Extendido del Lenguaje) y Escenarios como metodología, aquí[25] se presenta una de las tantas referencias y bibliografía sobre ello. Otra forma, más ortodoxa, de capturar y documentar requisitos se puede obtener en detalle, por ejemplo, en el trabajo de la Universidad de Sevilla sobre «Metodología para el Análisis de Requisitos de Sistemas Software».[26]​ En la Figura 7 se muestra un esquema, más o menos riguroso, aunque no detallado, de los pasos y tareas a seguir para realizar la captura, análisis y especificación de requisitos software. También allí se observa qué artefacto o documento se obtiene en cada etapa del proceso. En el diagrama no se explicita metodología o modelo a utilizar, sencillamente se pautan las tareas que deben cumplirse, de alguna manera. Figura 7: Diagrama de tareas para captura y análisis de requisitos. Una posible lista, general y ordenada, de tareas recomendadas para obtener la definición de lo que se debe realizar, los productos a obtener y las técnicas a emplear durante la actividad de elicitación de requisitos, en fase de Especificación de requisitos software es: Obtener información sobre el dominio del problema y el sistema actual (UdeD). Preparar y realizar las reuniones para elicitación/negociación. Identificar/revisar los objetivos del usuario. Identificar/revisar los objetivos del sistema. Identificar/revisar los requisitos de información. Identificar/revisar los requisitos funcionales. Identificar/revisar los requisitos no funcionales. Priorizar objetivos y requisitos. Algunos principios básicos a tener en cuenta: Presentar y entender cabalmente el dominio de la información del problema. Definir correctamente las funciones que debe realizar el software. Representar el comportamiento del software a consecuencias de acontecimientos externos, particulares, incluso inesperados. Reconocer requisitos incompletos, ambiguos o contradictorios. Dividir claramente los modelos que representan la información, las funciones y comportamiento y características no funcionales. Clasificación e identificación de requisitos Se pueden identificar dos formas de requisitos: Requisitos de usuario: Los requisitos de usuario son frases en lenguaje natural junto a diagramas con los servicios que el sistema debe proporcionar, así como las restricciones bajo las que debe operar. Requisitos de sistema: Los requisitos de sistema determinan los servicios del sistema y pero con las restricciones en detalle. Sirven como contrato. Es decir, ambos son lo mismo, pero con distinto nivel de detalle. Ejemplo de requisito de usuario: El sistema debe hacer préstamos. Ejemplo de requisito de sistema: Función préstamo: entrada código socio, código ejemplar; salida: fecha devolución; etc. Se clasifican en tres los tipos de requisitos de sistema: Requisitos funcionales Los requisitos funcionales describen: Los servicios que proporciona el sistema (funciones). La respuesta del sistema ante determinadas entradas. El comportamiento del sistema en situaciones particulares. Requisitos no funcionales Los requisitos no funcionales son restricciones de los servicios o funciones que ofrece el sistema (ej. cotas de tiempo, proceso de desarrollo, rendimiento, etc.) Ejemplo 1. La biblioteca Central debe ser capaz de atender simultáneamente a todas las bibliotecas de la Universidad Ejemplo 2. El tiempo de respuesta a una consulta remota no debe ser superior a 1/2 s A su vez, hay tres tipos de requisitos no funcionales: Requisitos del producto. Especifican el comportamiento del producto (Ej. prestaciones, memoria, tasa de fallos, etc.) Requisitos organizativos. Se derivan de las políticas y procedimientos de las organizaciones de los clientes y desarrolladores (Ej. estándares de proceso, lenguajes de programación, etc.) Requisitos externos. Se derivan de factores externos al sistema y al proceso de desarrollo (Ej. requisitos legislativos, éticos, etc.) Requisitos del dominio. Los requisitos del dominio se derivan del dominio de la aplicación y reflejan características de dicho dominio. Pueden ser funcionales o no funcionales. Ej. El sistema de biblioteca de la Universidad debe ser capaz de exportar datos mediante el Lenguaje de Intercomunicación de Bibliotecas de España (LIBE). Ej. El sistema de biblioteca no podrá acceder a bibliotecas con material censurado. Diseño del sistema En ingeniería de software, el diseño es una fase de ciclo de vida del software. Se basa en la especificación de requisitos producido por el análisis de los requisitos (fase de análisis), el diseño define cómo estos requisitos se cumplirán, la estructura que debe darse al sistema de software para que se haga realidad. El diseño sigue siendo una fase separada de la programación o codificación, esta última corresponde a la traducción en un determinado lenguaje de programación de las premisas adoptadas en el diseño. Las distinciones entre las actividades mencionadas hasta ahora no siempre son claras cómo se quisiera en las teorías clásicas de ingeniería de software. El diseño, en particular, puede describir el funcionamiento interno de un sistema en diferentes niveles de detalle, cada una de ellos se coloca en una posición intermedia entre el análisis y codificación. Normalmente se entiende por diseño de la arquitectura al diseño de muy alto nivel, que solo define la estructura del sistema en términos de la módulos de software de que se compone y las relaciones macroscópicas entre ellos. A este nivel de diseño pertenecen fórmulas como cliente-servidor o “tres niveles”, o, más generalmente, las decisiones sobre el uso de la arquitectura de hardware especial que se utilice, el sistema operativo, DBMS, Protocolos de red, etc. Un nivel intermedio de detalle puede definir la descomposición del sistema en módulos, pero esta vez con una referencia más o menos explícita al modo de descomposición que ofrece el particular lenguaje de programación con el que el desarrollo se va a implementar, por ejemplo, en un diseño realizado con la tecnología de objetos, el proyecto podría describir al sistema en términos de clases y sus interrelaciones. El diseño detallado, por último, es una descripción del sistema muy cercana a la codificación (por ejemplo, describir no solo las clases en abstracto, sino también sus atributos y los métodos con sus tipos). Debido a la naturaleza intangible del software, y dependiendo de las herramientas que se utilizan en el proceso, la frontera entre el diseño y la codificación también puede ser virtualmente imposible de identificar. Por ejemplo, algunas herramientas CASE son capaces de generar código a partir de diagramas UML, los que describen gráficamente la estructura de un sistema software. Codificación del software Durante esta etapa se realizan las tareas que comúnmente se conocen como programación; que consiste, esencialmente, en llevar a código fuente, en el lenguaje de programación elegido, todo lo diseñado en la fase anterior. Esta tarea la realiza el programador, siguiendo por completo los lineamientos impuestos en el diseño y en consideración siempre a los requisitos funcionales y no funcionales (ERS) especificados en la primera etapa. Es común pensar que la etapa de programación o codificación (algunos la llaman implementación) es la que insume la mayor parte del trabajo de desarrollo del software; sin embargo, esto puede ser relativo (y generalmente aplicable a sistemas de pequeño porte) ya que las etapas previas son cruciales, críticas y pueden llevar bastante más tiempo. Se suele hacer estimaciones de un 30 % del tiempo total insumido en la programación, pero esta cifra no es consistente ya que depende en gran medida de las características del sistema, su criticidad y el lenguaje de programación elegido.[15] En tanto menor es el nivel del lenguaje mayor será el tiempo de programación requerido, así por ejemplo se tardaría más tiempo en codificar un algoritmo en lenguaje ensamblador que el mismo programado en lenguaje C. Mientras se programa la aplicación, sistema, o software en general, se realizan también tareas de depuración, esto es la labor de ir liberando al código de los errores factibles de ser hallados en esta fase (de semántica, sintáctica y lógica). Hay una suerte de solapamiento con la fase siguiente, ya que para depurar la lógica es necesario realizar pruebas unitarias, normalmente con datos de prueba; claro es que no todos los errores serán encontrados solo en la etapa de programación, habrá otros que se encontrarán durante las etapas subsiguientes. La aparición de algún error funcional (mala respuesta a los requisitos) tarde o temprano puede llevar a retornar a la fase de diseño antes de continuar la codificación. Durante la fase de programación, el código puede adoptar varios estados, dependiendo de la forma de trabajo y del lenguaje elegido, a saber: Código fuente: es el escrito directamente por los programadores en editores de texto, lo cual genera el programa. Contiene el conjunto de instrucciones codificadas en algún lenguaje de alto nivel. Puede estar distribuido en paquetes, procedimientos, bibliotecas fuente, etc. Código objeto: es el código binario o intermedio resultante de procesar con un compilador el código fuente. Consiste en una traducción completa y de una sola vez de este último. El código objeto no es inteligible por el ser humano (normalmente es formato binario) pero tampoco es directamente ejecutable por la computadora. Se trata de una representación intermedia entre el código fuente y el código ejecutable, a los fines de un enlace final con las rutinas de biblioteca y entre procedimientos o bien para su uso con un pequeño intérprete intermedio [a modo de distintos ejemplos véase EUPHORIA, (intérprete intermedio), FORTRAN (compilador puro) MSIL (Microsoft Intermediate Language) (intérprete) y BASIC (intérprete puro, intérprete intermedio, compilador intermedio o compilador puro, depende de la versión utilizada)]. El código objeto no existe si el programador trabaja con un lenguaje a modo de intérprete puro, en este caso el mismo intérprete se encarga de traducir y ejecutar línea por línea el código fuente (de acuerdo al flujo del programa), en tiempo de ejecución. En este caso tampoco existe el o los archivos de código ejecutable. Una desventaja de esta modalidad es que la ejecución del programa o sistema es un poco más lenta que si se hiciera con un intérprete intermedio, y bastante más lenta que si existe el o los archivos de código ejecutable. Es decir no favorece el rendimiento en velocidad de ejecución. Pero una gran ventaja de la modalidad intérprete puro, es que él está forma de trabajo facilita enormemente la tarea de depuración del código fuente (frente a la alternativa de hacerlo con un compilador puro). Frecuentemente se suele usar una forma mixta de trabajo (si el lenguaje de programación elegido lo permite), es decir inicialmente trabajar a modo de intérprete puro, y una vez depurado el código fuente (liberado de errores) se utiliza un compilador del mismo lenguaje para obtener el código ejecutable completo, con lo cual se agiliza la depuración y la velocidad de ejecución se optimiza. Código ejecutable: Es el código binario resultado de enlazar uno o más fragmentos de código objeto con las rutinas y bibliotecas necesarias. Constituye uno o más archivos binarios con un formato tal que el sistema operativo es capaz de cargarlo en la memoria RAM (eventualmente también parte en una memoria virtual), y proceder a su ejecución directa. Por lo anterior se dice que el código ejecutable es directamente «inteligible por la computadora». El código ejecutable, también conocido como código máquina, no existe si se programa con modalidad de «intérprete puro». Pruebas (unitarias y de integración) Entre las diversas pruebas que se le efectúan al software se pueden distinguir principalmente: Prueba unitarias: Consisten en probar o testear piezas de software pequeñas; a nivel de secciones, procedimientos, funciones y módulos; aquellas que tengan funcionalidades específicas. Dichas pruebas se utilizan para asegurar el correcto funcionamiento de secciones de código, mucho más reducidas que el conjunto, y que tienen funciones concretas con cierto grado de independencia. Pruebas de integración: Se realizan una vez que las pruebas unitarias fueron concluidas exitosamente; con éstas se intenta asegurar que el sistema completo, incluso los subsistemas que componen las piezas individuales grandes del software funcionen correctamente al operar e interoperar en conjunto. Las pruebas normalmente se efectúan con los llamados datos de prueba, que es un conjunto seleccionado de datos típicos a los que puede verse sometido el sistema, los módulos o los bloques de código. También se escogen: Datos que llevan a condiciones límites al software a fin de probar su tolerancia y robustez; datos de utilidad para mediciones de rendimiento; datos que provocan condiciones eventuales o particulares poco comunes y a las que el software normalmente no estará sometido pero pueden ocurrir; etc. Los «datos de prueba» no necesariamente son ficticios o «creados», pero normalmente sí lo son los de poca probabilidad de ocurrencia. Generalmente, existe un fase probatoria final y completa del software, llamada Beta Test, durante la cual el sistema instalado en condiciones normales de operación y trabajo es probado exhaustivamente a fin de encontrar errores, inestabilidades, respuestas erróneas, etc. que hayan pasado los previos controles. Estas son normalmente realizadas por personal idóneo contratado o afectado específicamente a ello. Los posibles errores encontrados se transmiten a los desarrolladores para su depuración. En el caso de software de desarrollo «a pedido», el usuario final (cliente) es el que realiza el Beta Test, teniendo para ello un período de prueba pactado con el desarrollador. Instalación y paso a producción La instalación del software es el proceso por el cual los programas desarrollados son transferidos apropiadamente al computador destino, inicializados, y, finalmente, configurados; todo ello con el propósito de ser ya utilizados por el usuario final. Constituye la etapa final en el desarrollo propiamente dicho del software. Luego de esta el producto entrará en la fase de funcionamiento y producción, para el que fuera diseñado. La instalación, dependiendo del sistema desarrollado, puede consistir en una simple copia al disco rígido destino (casos raros actualmente); o bien, más comúnmente, con una de complejidad intermedia en la que los distintos archivos componentes del software (ejecutables, bibliotecas, datos propios, etc.) son descomprimidos y copiados a lugares específicos preestablecidos del disco; incluso se crean vínculos con otros productos, además del propio sistema operativo. Este último caso, comúnmente es un proceso bastante automático que es creado y guiado con herramientas software específicas (empaquetado y distribución, instaladores). En productos de mayor complejidad, la segunda alternativa es la utilizada, pero es realizada o guiada por especialistas; puede incluso requerirse la instalación en varios y distintos computadores (instalación distribuida). También, en software de mediana y alta complejidad normalmente es requerido un proceso de configuración y chequeo, por el cual se asignan adecuados parámetros de funcionamiento y se testea la operatividad funcional del producto. En productos de venta masiva las instalaciones completas, si son relativamente simples, suelen ser realizadas por los propios usuarios finales (tales como sistemas operativos, paquetes de oficina, utilitarios, etc.) con herramientas propias de instalación guiada; incluso la configuración suele ser automática. En productos de diseño específico o «a medida» la instalación queda restringida, normalmente, a personas especialistas involucradas en el desarrollo del software en cuestión. Una vez realizada exitosamente la instalación del software, el mismo pasa a la fase de producción (operatividad), durante la cual cumple las funciones para las que fue desarrollado, es decir, es finalmente utilizado por el (o los) usuario final, produciendo los resultados esperados. Mantenimiento El mantenimiento de software es el proceso de control, mejora y optimización del software ya desarrollado e instalado, que también incluye depuración de errores y defectos que puedan haberse filtrado de la fase de pruebas de control y beta test. Esta fase es la última (antes de iterar, según el modelo empleado) que se aplica al ciclo de vida del desarrollo de software. La fase de mantenimiento es la que viene después de que el software está operativo y en producción. De un buen diseño y documentación del desarrollo dependerá cómo será la fase de mantenimiento, tanto en costo temporal como monetario. Modificaciones realizadas a un software que fue elaborado con una documentación indebida o pobre y mal diseño puede llegar a ser tanto o más costosa que desarrollar el software desde el inicio. Por ello, es de fundamental importancia respetar debidamente todas las tareas de las fases del desarrollo y mantener adecuada y completa la documentación. El período de la fase de mantenimiento es normalmente el mayor en todo el ciclo de vida.[15] Esta fase involucra también actualizaciones y evoluciones del software; no necesariamente implica que el sistema tuvo errores. Uno o más cambios en el software, por ejemplo de adaptación o evolutivos, puede llevar incluso a rever y adaptar desde parte de las primeras fases del desarrollo inicial, alterando todas las demás; dependiendo de cuán profundos sean los cambios. El modelo cascada común es particularmente costoso en mantenimiento, ya que su rigidez implica que cualquier cambio provoca regreso a fase inicial y fuertes alteraciones en las demás fases del ciclo de vida. Durante el período de mantenimiento, es común que surjan nuevas revisiones y versiones del producto; que lo liberan más depurado, con mayor y mejor funcionalidad, mejor rendimiento, etc. Varias son las facetas que pueden ser alteradas para provocar cambios deseables, evolutivos, adaptaciones o ampliaciones y mejoras. Básicamente se tienen los siguientes tipos de cambios: Perfectivos: Aquellos que llevan a una mejora de la calidad interna del software en cualquier aspecto: Reestructuración del código, definición más clara del sistema y su documentación; optimización del rendimiento y eficiencia. Evolutivos: Agregados, modificaciones, incluso eliminaciones, necesarias en el software para cubrir su expansión o cambio, según las necesidades del usuario. Adaptivos: Modificaciones que afectan a los entornos en los que el sistema opera, tales como: Cambios de configuración del hardware (por actualización o mejora de componentes electrónicos), cambios en el software de base, en gestores de base de datos, en comunicaciones, etc. Correctivos: Alteraciones necesarias para corregir errores de cualquier tipo en el producto de software desarrollado. Carácter evolutivo del software El software es el producto derivado del proceso de desarrollo, según la ingeniería de software. Este producto es intrínsecamente evolutivo durante su ciclo de vida: en general, evoluciona generando versiones cada vez más completas, complejas, mejoradas, optimizadas en algún aspecto, adecuadas a nuevas plataformas (sean de hardware o sistemas operativos), etc.[27]​ Cuando un sistema deja de evolucionar, tarde o temprano cumplirá con su ciclo de vida, entrará en obsolescencia e inevitablemente, tarde o temprano, será reemplazado por un producto nuevo. El software evoluciona sencillamente porque se debe adaptar a los cambios del entorno, sean funcionales (exigencias de usuarios), operativos, de plataforma o arquitectura hardware. La dinámica de evolución del software es el estudio de los cambios del sistema. La mayor contribución en esta área fue realizada por Meir M. Lehman y Belady, comenzando en los años 70 y 80. Su trabajo continuó en la década de 1990, con Lehman y otros investigadores[28] de relevancia en la realimentación en los procesos de evolución (Lehman, 1996; Lehman et al., 1998; lehman et al., 2001). A partir de esos estudios propusieron un conjunto de leyes (conocidas como leyes de Lehman)[18] respecto de los cambios producidos en los sistemas. Estas leyes (en realidad son hipótesis) son invariantes y ampliamente aplicables. Lehman y Belady analizaron el crecimiento y la evolución de varios sistemas software de gran porte; derivando finalmente, según sus medidas, las siguientes ocho leyes: Cambio continuo: Un programa que se usa en un entorno real necesariamente debe cambiar o se volverá progresivamente menos útil en ese entorno. Complejidad creciente: A medida que un programa en evolución cambia, su estructura tiende a ser cada vez más compleja. Se deben dedicar recursos extras para preservar y simplificar la estructura. Evolución prolongada del programa: La evolución de los programas es un proceso autorregulativo. Los atributos de los sistemas, tales como tamaño, tiempo entre entregas y la cantidad de errores documentados son aproximadamente invariantes para cada entrega del sistema. Estabilidad organizacional: Durante el tiempo de vida de un programa, su velocidad de desarrollo es aproximadamente constante e independiente de los recursos dedicados al desarrollo del sistema. Conservación de la familiaridad: Durante el tiempo de vida de un sistema, el cambio incremental en cada entrega es aproximadamente constante. Crecimiento continuado: La funcionalidad ofrecida por los sistemas tiene que crecer continuamente para mantener la satisfacción de los usuarios. Decremento de la calidad: La calidad de los sistemas software comenzará a disminuir a menos que dichos sistemas se adapten a los cambios de su entorno de funcionamiento. Realimentación del sistema: Los procesos de evolución incorporan sistemas de realimentación multiagente y multibucle y estos deben ser tratados como sistemas de realimentación para lograr una mejora significativa del producto."

ksampletext_wikipedia_anth_antropologia: str = "Antropología. La antropología es la ciencia que estudia al ser humano de una forma integral, en sus características físicas como animales y de su cultura. Para abarcar la materia de su estudio, la antropología recurre a herramientas y conocimientos producidos por las ciencias sociales y las ciencias naturales. La aspiración de la disciplina antropológica es producir conocimiento sobre el ser humano en diversas esferas, intentando abarcar tanto las estructuras sociales de la actualidad, la evolución biológica de nuestra especie, el desarrollo y los modos de vida de pueblos que han desaparecido y la diversidad de expresiones culturales y lingüísticas que caracterizan a la humanidad. Las facetas diversas del ser humano llevaron a una especialización de los campos de la antropología. Cada uno de los campos de estudio del ser humano implicó el desarrollo de disciplinas que mantienen constante diálogo entre ellas. Se trata de la antropología física, la arqueología, la lingüística y la antropología social. Con mucha frecuencia, el término «antropología» solo se aplica a esta última, que a su vez se ha diversificado en numerosas ramas, dependiendo de la orientación teórica, la materia de su estudio o bien, como resultado de la interacción entre la antropología social y otras disciplinas. La antropología se constituyó como disciplina independiente durante la segunda mitad del siglo XIX. Uno de los factores que favoreció su aparición fue la difusión de la teoría de la evolución, que en el campo de los estudios sobre la sociedad dio origen al evolucionismo social, entre cuyos principales autores se encuentra Herbert Spencer. Los primeros antropólogos pensaban que así como las especies evolucionaban de organismos sencillos a otros más complejos, las sociedades y las culturas de los humanos debían seguir el mismo proceso de evolución hasta producir estructuras complejas como su propia sociedad. Varios de los antropólogos pioneros eran abogados de profesión, de modo que las cuestiones jurídicas aparecieron frecuentemente como tema central de sus obras. A esta época corresponde el descubrimiento de los sistemas de parentesco por parte de Lewis Henry Morgan. Desde el final del siglo XIX el enfoque adoptado por los primeros antropólogos fue puesto en tela de juicio por las siguientes generaciones. Después de la crítica de Franz Boas a la antropología evolucionista del siglo XIX, la mayor parte de las teorías producidas por los antropólogos de la primera generación se considera obsoleta. A partir de entonces, la antropología vio la aparición de varias corrientes durante el siglo XIX y el XX, entre ellas la escuela culturalista de Estados Unidos, el estructural-funcionalismo, el estructuralismo antropológico, la antropología marxista, el procesualismo, el indigenismo, etc. La antropología es, sobre todo, una ciencia integradora que estudia al ser humano en el marco de la sociedad y cultura a las que pertenece, y, al mismo tiempo, como producto de estas. Se le puede definir como la ciencia que se ocupa de estudiar el origen y desarrollo de toda la gama de la variabilidad humana y los modos de comportamientos sociales a través del tiempo y el espacio; es decir, del proceso biosocial de la existencia de la especie humana. Antecedentes Fruto de la meticulosa investigación de Bernardino de Sahagún es el Códice Florentino. Se considera como antecedente de la etnografía. En la imagen, un folio de esta obra escrito en náhuatl. Se atribuye al explorador François Péron haber sido quien usó por primera ocasión el término antropología.[2] Péron recogió en esa obra un conjunto de datos sobre los aborígenes de Tasmania, que fueron casi exterminados en los años que siguieron al paso de Péron por la isla. Sin embargo, Péron no fue el primero ni el más antiguo de quienes estaban interesados en la cuestión de la diversidad humana y sus manifestaciones. El estudio del ser humano es muy antiguo. Heródoto (484-425 a. C.) en sus Historias nos cuenta las diferencias entre los distintos habitantes del mundo (Libia, Egipto, Grecia, Asia Menor), y nos habla de las diferencias de cráneo entre egipcios y persas. Hipócrates (460-377 a. C.) lanza la teoría de que el medio influye en los caracteres físicos del ser humano, y llama la atención sobre las diferencias de quienes habitan climas distintos. Aristóteles (384-322 a. C.) estudia al ser humano por ser el animal más complejo. Llama la atención sobre el tamaño de su cráneo, mucho mayor que en el resto de animales, así como sobre su carácter bípedo y que es el único animal capaz de deliberar y reflexionar. Define al hombre como zoon politikón o «animal político». Algunos autores consideran a fray Bernardino de Sahagún como uno de los antecedentes más notables de la etnografía.[3] De la misma manera que otros misioneros del siglo XVI, Sahagún estaba preocupado por las diversas maneras en que la religión de los indígenas podría confundirse con el cristianismo recién implantado. En el afán de comprender mejor a los pueblos nahuas del centro de Nueva España, Sahagún investigó de manera muy detallada la historia, las costumbres y las creencias de los nahuas antes de la llegada de los españoles. Para hacerlo tuvo que aprender náhuatl. Luego, con el apoyo de algunos de sus informantes, organizó la información obtenida en una obra pensada para un público más o menos amplio. El resultado fue el Códice Florentino, de vital importancia en el conocimiento de la civilización mesoamericana precolombina.[4]​ Georges-Louis Leclerc de Buffon, naturalista quien escribió Histoire Naturelle (1749), enlaza las ciencias naturales y la diversidad física de la especie humana (anatomía comparada) con la inquietud por comprender la diversidad de las expresiones culturales de los pueblos.[5] De manera análoga, algunos pensadores de la Ilustración como Montesquieu, Rousseau e incluso el matemático D'Alembert abordaron la materia, y propusieron algunas hipótesis sobre el origen de las relaciones sociales, las formas de gobierno y los temperamentos de las naciones. Historia Esta sección es un extracto de Historia de la antropología. Durante el siglo XIX, la llamada entonces «antropología general» incluida un amplísimo espectro de intereses, desde la paleontología del cuaternario al folclor europeo, pasando por el estudio comparado de los pueblos aborígenes. Fue por ello una rama de la Historia Natural y del historicismo cultural alemán que se propuso el estudio científico de la historia de la diversidad humana. Tras la aparición de los modelos evolucionistas y el desarrollo del método científico en las ciencias naturales, muchos autores pensaron que los fenómenos históricos también seguirían pautas deducibles por observación. El desarrollo inicial de la antropología como disciplina más o menos autónoma del conjunto de las Ciencias Naturales coincide con el auge del pensamiento ilustrado y posteriormente del positivismo que elevaba la razón como una capacidad distintiva de los seres humanos. Su desarrollo se pudo vincular muy pronto a los intereses del colonialismo europeo derivado de la Revolución Industrial. Por razones que tienen que ver con el proyecto de la New Republic, y sobre todo con el problema de la gestión de los asuntos indios, la antropología de campo empezó a tener bases profesionales en Estados Unidos en el último tercio del siglo. XIX, a partir del Bureau of American Ethnology y de la Smithsonian Institution. El antropólogo alemán Franz Boas, inicialmente vinculado a este tipo de tarea, institucionalizó académica y profesionalmente la Antropología en Estados Unidos. En la Gran Bretaña victoriana, Edward Burnett Tylor y posteriormente autores como William Rivers y más tarde Bronisław Malinowski y Alfred Reginald Radcliffe-Brown desarrollaron un modelo profesionalizado de Antropología académica. Lo mismo sucedió en Alemania antes de 1918. En todas las potencias coloniales de principios de siglo hay esbozos de profesionalización de la Antropología que no acabaron de cuajar hasta después de la II Guerra Mundial. En el caso de España puede citarse a Julio Caro Baroja y a diversos africanistas y arabistas que estudiaron las culturas del Norte de África. En todos los países occidentales se incorporó el modelo profesional de la Antropología anglosajona. Por este motivo, la mayor parte de la producción de la Antropología social o cultural antes de 1960 —lo que se conoce como «modelo antropológico clásico»— se basa en etnografías producidas en América, Asia, Oceanía y África, pero con un peso muy inferior de Europa. La razón es que en el continente europeo prevaleció una etnografía positivista, destinada a apuntalar un discurso sobre la identidad nacional, tanto en los países germánicos como en los escandinavos y los eslavos. Históricamente hablando, el proyecto de Antropología general se componía de cuatro ramas: la lingüística, la arqueología, la antropología biológica y la antropología social, referida esta última como antropología cultural o etnología en algunos países. Estas últimas ponen especial énfasis en el análisis comparado de la cultura —término sobre el que no existe consenso entre las corrientes antropológicas—, que se realiza básicamente por un proceso trifásico, que comprende, en primera instancia, una investigación de gabinete; en segundo lugar, una inmersión cultural que se conoce como etnografía o trabajo de campo y, por último, el análisis de los datos obtenidos mediante el trabajo de campo. El modelo antropológico clásico de la antropología social fue abandonado en la segunda mitad del siglo XX. Actualmente los antropólogos trabajan prácticamente todos los ámbitos de la cultura,la sociedad y la lingüística. El objeto de estudio antropológico El cráneo del niño de Taung, descubierto en Sudáfrica. Este niño era un Australopithecus africanus, una forma intermedia de hominino. La materia de estudio de la antropología ha sido materia de debate desde el nacimiento de la disciplina, aunque es común a todas las posturas el compartir la preocupación por producir conocimiento sobre el ser humano. La manera en que se aborda la cuestión es lo que plantea el desacuerdo, porque la materia puede abordarse desde diversos puntos de vista. Sin embargo, desde el inicio la configuración epistemológica de la antropología consistió en la pregunta por el Otro. Esta es una cuestión central en las ciencias y disciplinas antropológicas que se va configurando desde el Renacimiento.[6]​ Tras el desarrollo de diferentes tradiciones teóricas en diversos países, entró en debate cuál era el aspecto de la vida humana que correspondía estudiar a la antropología. Para esa época, los lingüistas y arqueólogos ya habían definido sus propios campos de acción. Edward Burnett Tylor, en las primeras líneas del capítulo primero de su obra Cultura primitiva, había propuesto que el objeto era la cultura o civilización, entendida como un «todo complejo» que incluye las creencias, el arte, la moral, el derecho, las costumbres y cualesquiera otros hábitos adquiridos por el ser humano como miembro de una sociedad. Esta propuesta está presente en todas las corrientes de la antropología, ya sea que se declaren a favor o en contra. Sin embargo, a partir del debate se presenta un fenómeno de constante atomización en la disciplina, a tal grado que para muchos autores —por citar el ejemplo más conocido—, el estudio de la cultura sería el campo de la antropología cultural; el de las estructuras sociales sería facultad de la antropología social propiamente dicha. De esta suerte, Radcliffe-Brown (antropólogo social) consideraba como una disciplina diferente (y errada, por lo demás) la que realizaban Franz Boas y sus alumnos (antropólogos culturales). Según Clifford Geertz, el objeto de la antropología es el estudio de la diversidad cultural. La antropología es una ciencia que estudia las respuestas del ser humano ante el medio, las relaciones interpersonales y el marco sociocultural en que se desenvuelven, cuyo objeto va a ser el estudio del ser humano en sus múltiples relaciones; además estudia la cultura como elemento diferenciador de los demás seres humanos. Estudia al ser humano en su totalidad, incluyendo los aspectos biológicos y socioculturales como parte integral de cualquier grupo o sociedad. Se convirtió en una ciencia empírica que reunió mucha información, además fue la primera ciencia que introdujo el trabajo de campo y surge de los relatos de viajeros, misioneros, etc. Autores como Manuel Marzal (1998: 16), sostienen que antropología cultural, antropología social y etnología son la misma disciplina. Campos de acción de la antropología Excavación del yacimiento de Gran Dolina, en Atapuerca (España). La antropología social se orientó en su inicio a la investigación de las sociedades no occidentales. En la imagen Alfred Kroeber e Ishi, el último yahi, en 1911. Saussure (en la imagen) sentó los antecedentes del gran desarrollo de la lingüística moderna, cuyos aportes han sido aprovechados especialmente por los antropólogos sociales. Londres, fue una de las primeras ciudades analizadas desde el enfoque de la Antropología Urbana. La Antropología, como ciencia que abarca los fenómenos del ser humano como parte de una sociedad, se ha diversificado en sus métodos y sus teorías. La diversificación obedece al interés por rendir mejor cuenta de los procesos que enfrenta la especie en diversas dimensiones. De acuerdo con la American Anthropological Association (AAA), los cuatro campos de la Antropología son la Antropología biológica, la Antropología cultural, la Arqueología y la Antropología lingüística.[7]​ La Antropología biológica o física es el campo de la Antropología que se especializa en el estudio de los seres humanos desde el punto de vista evolutivo y adaptativo. Al adoptar una postura evolucionista, los antropólogos físicos pretenden dar cuenta no solo de los grandes cambios en los aspectos biológicos del ser humano —lo que se llama hominización—, sino en los pequeños cambios que se observan entre poblaciones humanas. La diversidad física del ser humano incluye cuestiones como la pigmentación de la piel, las formas de los cráneos, la talla promedio de un grupo, tipo de cabello y otras cuestiones numerosas. Para abordar esta diversidad, la Antropología física no solo echa mano de estudios propiamente anatómicos, sino las interacciones entre los seres humanos y otras especies, animales y vegetales, el clima, cuestiones relativas a la salud y la interacción entre distintas sociedades.[8] El campo de la Antropología biológica también es interés de otras ciencias con las que mantiene un diálogo, por ejemplo, con la Primatología (estudio científico de los primates), la Demografía, la Ecología o las ciencias de la salud. Cuenta entre sus especializaciones a la Paleoantropología y la Antropología médica. La Arqueología es una de las ciencias antropológicas con mayor difusión entre el público no especializado. Se trata del estudio científico de los vestigios del pasado humano. Podría decirse que este interés se ha encontrado en diversas épocas y lugares, aunque la Arqueología tiene un antecedente muy claro en el coleccionismo de antigüedades en las sociedades europeas.[9] Para lograr sus propósitos, los arqueólogos indagan en depósitos de estos materiales que son llamados yacimientos arqueológicos —o «sitios arqueológicos», calcado del inglés archaeological site— a los que se accede normalmente por excavaciones. A pesar de los estereotipos sobre los arqueólogos —a los que se suele imaginar como una especie de Indiana Jones[10]​— y los lugares comunes sobre lo que es la Arqueología, el método arqueológico no comprende únicamente las técnicas de excavación. Ante todo se trata de interpretar los hallazgos, tanto en relación con su contexto arqueológico como en relación con los conocimientos ya comprobados, la historia del yacimiento y otros elementos. La Antropología social, cultural o Etnología estudia el comportamiento humano, la cultura, las estructuras de las relaciones sociales. En la actualidad la antropología social se ha volcado al estudio de Occidente y su cultura. Aunque para los antropólogos de los países centrales (EE. UU., Gran Bretaña, Francia, etc.) este es un enfoque nuevo, hay que señalar que esta práctica es común en la antropología de muchos países latinoamericanos (como ejemplo, la obra de Darcy Ribeiro sobre el Brasil, la de Guillermo Bonfil Batalla y Gonzalo Aguirre Beltrán sobre México, etc.). Dependiendo de si surge de la tradición anglosajona se conoce como antropología cultural y, si parte de la escuela francesa, entonces se le denomina etnología. Quizá se haya distinguido de la antropología social en tanto que su estudio es esencialmente dirigido al análisis de la otredad (condición de ser otro) en tanto que el trabajo de la antropología social resulta generalmente más inmediato. Uno de sus principales exponentes es Claude Lévi-Strauss, quien propone un análisis del comportamiento del ser humano basado en un enfoque estructural en el que las reglas de comportamiento de todos los sujetos de una determinada cultura son existentes en todos los sujetos a partir de una estructura invisible que ordena a la sociedad.[cita requerida] La Antropología lingüística o Lingüística antropológica estudia los lenguajes humanos. Dado que el lenguaje es una amplia parte constitutiva de la cultura, los antropólogos la consideran como una disciplina separada. Los lingüistas se interesan en el desarrollo de las lenguas. Así mismo, se ocupan en las diferencias de los lenguajes vivos, cómo se vinculan o difieren, y en ciertos procesos que explican las migraciones y la difusión de la información. También se preguntan sobre las formas en que el lenguaje se opone o refleja otros aspectos de la cultura. Dentro de las ciencias sociales, disciplinas como la lingüística y la antropología han mantenido una relación que ha tomado la forma de un complejo proceso articulatorio influido a lo largo del tiempo por las distintas condiciones históricas, sociales y teóricas imperantes. La lingüística, al igual que la etnología, la arqueología, la antropología social, la antropología física y la historia, es una de las disciplinas que conforman el campo de la antropología desde algunas perspectivas. La lingüística estudia el lenguaje para encontrar sus principales características y así poder describir, explicar o predecir los fenómenos lingüísticos. Dependiendo de sus objetivos, estudia las estructuras cognitivas de la competencia lingüística humana o la función y relación del lenguaje con factores sociales y culturales. La relación entre la lingüística y la antropología ha respondido a distintos intereses. Durante el siglo XIX y la primera mitad del XX, la antropología y la lingüística comparativa intentaban trazar las relaciones genéticas y el desarrollo histórico de las lenguas y familias lingüísticas. Posteriormente, la relación entre las dos disciplinas tomó otra perspectiva por la propuesta desde el estructuralismo. Los modelos lingüísticos fueron adoptados como modelos del comportamiento cultural y social en un intento por interpretar y analizar los sistemas socioculturales, dentro de las corrientes de la antropología. La tendencia estructural pudo proponerse por la influencia de la lingüística, tanto en lo teórico como en lo metodológico. Sin embargo, al excluir las condiciones materiales y el desarrollo histórico, se cuestionó que la cultura y la organización social pudieran ser analizadas del mismo modo que un código lingüístico, tomando al lenguaje como el modelo básico sobre el que se estructura todo el pensamiento o clasificación. No obstante estos puntos de vista diferentes, se puede llegar a acercamientos productivos reconociendo que la cultura y la sociedad son producto tanto de condiciones objetivas o materiales como de construcciones conceptuales o simbólicas. De esta forma, la interacción entre estas dos dimensiones nos permite abordar a los sistemas socioculturales como una realidad material a la vez que una construcción conceptual. Las lenguas implican o expresan teorías del mundo y, por tanto, son objetos ideales de estudio para los científicos sociales. El lenguaje, como herramienta conceptual, aporta el más complejo sistema de clasificación de experiencias, por lo que cada teoría, sea ésta antropológica, lingüística o la unión de ambas, contribuye a nuestra comprensión de la cultura como un fenómeno complejo, ya que «el lenguaje es lo que hace posible el universo de patrones de entendimiento y comportamiento que llamamos cultura. Es también parte de la cultura, ya que es transmitido de una generación a otra a través del aprendizaje y la imitación, al igual que otros aspectos de la cultura».[cita requerida] Roman Jakobson plantea que «los antropólogos nos prueban, repitiéndolo sin cesar, que lengua y cultura se implican mutuamente, que la lengua debe concebirse como parte integrante de la vida de la sociedad y que la lingüística está en estrecha conexión con la antropología cultural». Para él, la lengua, como el principal sistema semiótico, es el fundamento de la cultura: «Ahora sólo podemos decir con nuestro amigo McQuown que no se da igualdad perfecta entre los sistemas de signos, y que el sistema semiótico primordial, básico y más importante, es la lengua: la lengua es, a decir verdad, el fundamento de la cultura. Con relación a la lengua, los demás sistemas de símbolos no pasan de ser o concomitantes o derivados. La lengua es el medio principal de comunicación informativa». Ramas y subramas A su vez, cada una de estas cuatro ramas principales se subdivide en innumerables subramas que muchas veces interactúan entre sí. De la antropología cultural o social (también conocida como antropología sociocultural), se desprenden: Antropología urbana: Hace referencia el estudio etnográfico y transcultural de la urbanización global y de la vida en las ciudades. Es una subdisciplina enseñada en la mayoría de las universidades del mundo.[11] Las Áreas Metropolitanas se han constituido en los lugares objeto de estudio de las investigaciones sobre temas como la etnicidad, la pobreza, el espacio público, las clases y las variaciones subculturales.[12]​ Antropología del parentesco: esta rama se enfoca en las relaciones de parentesco, entendido como un fenómeno social, y no como mero derivado de las relaciones biológicas que se establecen entre un individuo, sus progenitores y los consanguíneos de éstos; se trata de una de las especialidades más antiguas de la antropología, y de hecho está relacionada con el quehacer de los primeros antropólogos evolucionistas del siglo XIX. Antropología de la religión: Estudia los sistemas religiosos y de creencias. Antropología filosófica: es una rama de la filosofía alemana y no de la Antropología científica que, principalmente, se ocupa de las incertidumbres de índole ontológica, centrado su atención en el ser humano, tomando en cuenta una variedad de aspectos de la existencia humana, pasada y presente, combinando estos materiales diversos en un abordaje íntegro del problema de la existencia humana. Además, se pregunta por la naturaleza fundamental de su ser, se pregunta lo que diferencia al ser humano de todos los demás seres, cómo se define a través de su existencia histórica, etc. Tales interrogantes fundamentales de la antropología filosófica pueden ser condensadas en una pregunta radical: ¿Qué es el ser humano? Además de: antropología económica, antropología política, aplicada, rural, urbana, visual, todas las que deben entenderse como enfoques o puntos de partida diversos para analizar los fenómenos sociales. De la antropología física (también como antropología biológica), se desprenden: Antropología forense: Se encarga de la identificación de restos humanos esqueletizados dado su amplia relación con la biología y variabilidad del esqueleto humano. También puede determinar, en el caso de que hayan dejado marcas sobre los huesos, las causas de la muerte, para tratar de reconstruir la mecánica de hechos y la mecánica de lesiones, conjuntamente con el arqueólogo forense, el criminalista de campo y médico forense, así como aportar, de ser posible, elementos sobre la conducta del victimario por medio de indicios dejados en el lugar de los hechos y el tratamiento perimortem y post mortem dado a la víctima. Paleoantropología: Se ocupa del estudio de la evolución humana y sus antepasados fósiles u homínidos antiguos. A veces, también puede ser conocida como paleontología humana. Antropología genética: Se la define como la aplicación de técnicas moleculares para poder entender la evolución homínida, en particular la humana, relacionándolas con otras criaturas no humanas. Autores como Lorena Campo (2008: 38), consideran a la arqueología como rama que se desprende de la antropología cultural. En todo caso, de la arqueología se pueden citar las siguientes subramas: Arqueoastronomía: Es el estudio de yacimientos arqueológicos relacionados con el estudio de la astronomía por culturas antiguas. También estudia el grado de conocimientos astronómicos poseído por los diferentes pueblos antiguos. Uno de los aspectos de esta disciplina es el estudio del registro histórico de conocimientos astronómicos anterior al desarrollo de la moderna astronomía. Arqueología subacuática: Sigue los preceptos de la arqueología terrestre pero se dedica, a través de las técnicas de buceo, a desentrañar antiguas culturas cuyos restos materiales que, por una u otra razón, se encuentran actualmente bajo el agua. Antropología evolucionista: es el estudio interdisciplinario de la evolución de la fisiología humana y el comportamiento humano y la relación entre los homínidos y los primates no homínidos. La antropología evolucionista, se basa en las ciencias naturales y las ciencias sociales. Varios campos y disciplinas incluyen: La antropología de la evolución humana y la antropogenía. La paleoantropología y la paleontología. La primatología de etología y paleontología de los primates. La evolución cultural del comportamiento humano. El estudio arqueológico de la tecnología humana y el cambio sobre tiempo y espacio. La genética humana evolucionista y los cambios en el genoma humano durante el tiempo. La neurociencia cognitiva y neuroantropología de la cognición, las acciones y las capacidades de los primates y humanos. La ecología del comportamiento y la interacción entre humanos y el medio ambiente. Los estudios de la anatomía humana ósea, la endocrinología y la neurobiología y las diferencias y cambios entre especies, la variación entre grupos humanos y relaciones a factores culturales. La antropología evolucionista está relacionada con la evolución biológica y cultural de los humanos, pasados y presentes. Está basada en un enfoque científico, y une campos como la arqueología, la ecología del comportamiento, la psicología, la primatología y la genética. Es un campo dinámico e interdisciplinario, aprovechándose de muchas líneas de evidencia para comprender la experiencia humana, pasada y presente. Generalmente los estudios de la evolución biológica están relacionados con la evolución de la forma humana. La evolución cultural supone el estudio del cambio cultural sobre el tiempo y el espacio e incorpora los modelos de transmisión cultural con frecuencia. Nota que la evolución cultural no es la misma que la evolución biológica, y que la cultura humana supone la transmisión de información cultural, que comporte en maneras distintas de la biología humana y la genética. El estudio del cambio cultural se realiza cada vez más tras cladística y los modelos genéticos. Cada una de las ramas ha tenido un desarrollo propio en mayor o menor medida. La diversificación de las disciplinas no impide, por otro lado, que se hallen en interacción permanente unas con otras. Los edificios teóricos de las disciplinas antropológicas comparten como base su interés por el estudio de la humanidad. Sin embargo, metonímicamente en la actualidad, cuando se habla de antropología, por antonomasia se hace referencia a la antropología social. El origen de la pregunta antropológica Este artículo o sección necesita referencias que aparezcan en una publicación acreditada. Busca fuentes: «Antropología» – noticias · libros · académico · imágenes Este aviso fue puesto el 18 de abril de 2023. La pregunta antropológica es ante todo una pregunta por el otro. Y en términos estrictos, está presente en todo individuo y en todo grupo humano, en la medida en que ninguna de las dos entidades puede existir como aislada, sino en relación con el otro. Ese otro es el referente para la construcción de la identidad, puesto que ésta se construye por «oposición a» y no «a favor de». La preocupación por aquello que genera las variaciones de sociedad en sociedad es el interés fundador de la antropología moderna. De esa manera, para Krotz el «asombro» es el pilar del interés por lo «otro» (alter), y son las «alteridades» las que marcan tal contraste binario entre los seres humanos. A pesar de que todos los pueblos comparten esta inquietud, es en Occidente donde, por condiciones históricas y sociales particulares se ha documentado de manera más notable. Es innegable que ya Hesíodo, Heródoto, y otros clásicos indagaban en estas diferencias. Sin embargo, cuando Europa se halló frente a pueblos desconocidos y que resultaban tan extraordinarios, interpretó estas exóticas formas de vida ora fascinada, ora sobrecogida. Colón toma posesión del «Nuevo Mundo». La Conquista de América constituye un gran hito de la pregunta antropológica moderna. Los escritos de Cristóbal Colón y otros navegantes revelan el choque cultural en que se vio inmersa la vieja Europa. Especial importancia tienen los trabajos de los misioneros indianos en México, Perú, Colombia y Argentina en los primeros acercamientos a las culturas aborígenes. De entre ellos destaca Bernardino de Sahagún, quien emplea en sus investigaciones un método sumamente riguroso, y lega una obra donde hay una separación bien clara entre su opinión eclesiástica y los datos de sus «informantes» sobre su propia cultura. Esta obra es la Historia general de las cosas de la Nueva España. Con los nuevos descubrimientos geográficos se desarrolló el interés hacia las sociedades que encontraban los exploradores. En el siglo XVI el ensayista francés Montaigne se preocupó por los contrastes entre las costumbres en diferentes pueblos. En 1724 el misionero jesuita Lafitau publicó un libro en el que comparaba las costumbres de los indios americanos con las del mundo antiguo. En 1760 Charles de Brosses describe el paralelismo entre la religión africana y la del Antiguo Egipto. En 1748 Montesquieu publica El espíritu de las leyes basándose en lecturas sobre costumbres de diferentes pueblos. En el siglo XVIII, fue común la presencia de relatores históricos, los cuales, a modo de crónica, describían sus experiencias a través de viajes de gran duración a través del mundo. El siglo XIX vio el comienzo de viajes emprendidos con el fin de observar otras sociedades humanas. Viajeros famosos de este siglo fueron Bastian (1826-1905) y Ratzel (1844-1904). Ratzel fue el padre de la teoría del difusionismo que consideraba que todos los inventos se habían extendido por el mundo por medio de migraciones, esta teoría fue llevada al absurdo por su discípulo Frobenius (1873-1938) que pensaba que todos los inventos básicos se hicieron en un solo sitio: Egipto. En la era moderna, Charles Darwin y sucesos históricos como la Revolución industrial contribuirían al desarrollo de la antropología como una disciplina científica. Antropología moderna Para el establecimiento de una ciencia que incorporase las teorías filosóficas y los programas generales ya elaborados, serían necesarios ciertos avances metodológicos que no tuvieron lugar hasta finales del siglo XVIII y comienzos del siglo XIX. En esta época se produjeron las primeras clasificaciones raciales sistemáticas, como las de Linneo (1707-1778) y J. Blumenbach (1752-1840). Durante este mismo período surgió la lingüística moderna, dominada durante el s. XIX por la idea de que los idiomas podían clasificarse en familias y que los pertenecientes a una misma familia eran ramas de un tronco común más antiguo. Ello dio lugar al desarrollo de métodos comparativos sistemáticos con el fin de poder reconstruir el idioma ancestral. Aquí se aprecia a Boas posando como la “danza canibal” de los indígenas Kwaklutl, durante una exhibición en el National Museum of Natural History, en 1895. La regularidad de las correspondencias fonéticas en idiomas emparentados fue presentada primero por R. Rask (1787-1832) y divulgada por J. Grimm (1785-1863) a comienzos del s. XIX, con lo que contribuyeron a consolidar la idea general de la existencia de regularidades en el cambio cultural humano. Otro tipo de descubrimientos realizados en este período ampliaron de manera importante el horizonte temporal del desarrollo humano y otorgaron legitimidad a la idea de un progreso cultural gradual. Por una parte, el desciframiento de la escritura egipcia por Jean-François Champollion (1790-1832), en 1821, alteró de forma radical las ideas tradicionales acerca de la antigüedad del ser humano. Posteriormente, a mediados del s. XIX, se reconoció la validez del descubrimiento de Boucher de Perthes (1788- 1868) de utensilios humanos del Paleolítico, contemporáneos de mamíferos ya extinguidos. De este modo, la arqueología y las teorías de Darwin concurrían en ofrecer una imagen del ser humano como la de un ser sólidamente anclado entre las demás especies animales del pasado, que pasa de ser un antropoide carente de atributos culturales a transformarse en ser humano a lo largo de un prolongado período de cientos de miles de años. Durante la primera mitad del s. XIX la antropología comenzó a adquirir el rango de disciplina científica independiente y se crean las primeras sociedades etnológicas o antropológicas en Inglaterra, Francia y Alemania. En este último país, la palabra «Kultur» adquiere el sentido técnico que reviste en la actualidad. Posteriormente el término fue introducido en el mundo de habla inglesa por E.B. Tylor en su obra clásica La cultura primitiva (Primitive Culture), publicada en 1871. En una tan detallada como amplia panorámica de la evolución cultural humana y con una clara exposición de las perspectivas teóricas de una ciencia de la cultura, el libro de Tylor representa una obra fundacional en el desarrollo de la antropología moderna. Código de ética y política en antropología Este artículo o sección necesita referencias que aparezcan en una publicación acreditada. Busca fuentes: «Antropología» – noticias · libros · académico · imágenes Este aviso fue puesto el 20 de mayo de 2024. Logo de la Asociación Americana de Antropología. Algunos problemas éticos surgen de la sencilla razón de que los antropólogos tienen más poder que los pueblos que estudian. Se ha argumentado que la disciplina es una forma de colonialismo en la cual los antropólogos obtienen poder a expensas de los sujetos. Según esto, los antropólogos adquieren poder explotando el conocimiento y los artefactos de los pueblos que investigan. Estos, por su parte, no obtienen nada a cambio, y en el colmo, llevan la pérdida en la transacción. De hecho, la llamada escuela británica estuvo ligada explícitamente, en su origen, a la administración colonial. Otros problemas son derivados también del énfasis en el relativismo cultural de la antropología estadounidense y su añeja oposición al concepto de raza. El desarrollo de la sociobiología hacia finales de la década de 1960 fue objetado por antropólogos culturales como Marshall Sahlins, quien argumentaba que se trataba de una posición reduccionista. Algunos autores, como John Randal Baker, continuaron con el desarrollo del concepto biológico de raza hasta la década de 1970, cuando el nacimiento de la genética se volvió central en este frente. En tanto que la genética ha avanzado como ciencia, algunos genetistas como Luca Cavalli-Sforza han desterrado el concepto de raza de acuerdo con los nuevos descubrimientos (tales como el trazo de las migraciones antiguas por medio del ADN de la mitocondria y del cromosoma Y). La antropología estadounidense tiene una historia de asociaciones con las agencias gubernamentales de inteligencia y la política antibelicosa. Boas rechazó públicamente la participación de los Estados Unidos en la Primera Guerra Mundial, lo mismo que la colaboración de algunos antropólogos con el servicio de inteligencia de Estados Unidos. En contraste, muchos antropólogos contemporáneos de Boas fueron activos participantes en estas guerras de múltiples formas. Entre ellos se cuentan las docenas de antropólogos que sirvieron en la Oficina de Servicios Estratégicos y la Oficina de Información de Guerra. Como ejemplo, Ruth Benedict escribió El crisantemo y la espada, un informe sobre la cultura japonesa realizado a pedido del Ejército de los Estados Unidos. Fotografía del antropólogo Josef Mengele. A veces la antropología puede ser utilizada con fines perversos, tal y como hizo durante el Holocausto. En 1950 la Asociación Antropológica Estadounidense (AAA) proveyó a la CIA información especializada de sus miembros, y muchos participaron en la Operación Camelot en Latinoamérica y la guerra de Vietnam. Aunque en aquellos años, varios otros antropólogos estuvieron sumamente activos en el movimiento pacifista e hicieron pública su oposición en la American Anthropological Association, condenando el involucramiento del gremio en operaciones militares encubiertas. Hoy en día, los colegios profesionales de antropólogos censuran el servicio estatal de la antropología y su deontología les puede impedir a los antropólogos dar conferencias secretas con fines colonizadores. La Asociación Británica de Antropología Social, ha calificado ciertas becas éticamente peligrosas, por ejemplo, ha condenado el programa de la CIA «Pat Roberts Intelligence Scholars Program», que patrocina a estudiantes de antropología en las universidades de Estados Unidos en preparación a tareas de espionaje para el gobierno. La Declaración de Responsabilidad Profesional de la American Anthropological Association afirma claramente que «en relación con el gobierno propio o anfitrión (...) no deben aceptarse acuerdos de investigaciones secretas, reportes secretos o informes de ningún tipo». Los antropólogos, junto con otros científicos sociales, han trabajado con los militares de EE. UU. como parte de la estrategia del Ejército de EE. UU. en Afganistán,[13] este programa de intervención se denomina: Human Terrain System. Ramas principales Antropología cultural o social o etnología Arqueología Antropología física o biológica Lingüística antropológica Teorías, corrientes, escuelas antropológicas Positivismo Evolucionismo social Evolucionismo cultural Difusionismo Determinismo Particularismo histórico Escuela Sociológica Francesa Funcionalismo Funcionalismo estructuralista Estructuralismo Antipositivismo Neoevolucionismo Relativismo cultural Marxismo antropológico Escuela culturalista Etnometodología Indigenismo Interaccionismo simbólico Postestructuralismo Deconstrucción Teoría de sistemas Métodos y técnicas de investigación Investigación cualitativa Etnografía Investigación cuantitativa Metodología arqueológica Trabajo de campo Holismo Diario de campo Etnología Entrevista (investigación) Observación participante Osteología Excavación Antropometría Autores principales Alberto Rex González Alfred Kroeber Alfred Reginald Radcliffe-Brown Anne Chapman Bronisław Malinowski Bruno Latour Carlos Martínez Sarasola Carlota Sempé Clark Wissler Claude Lévi-Strauss Clifford Geertz Daniel Miller Edward Burnett Tylor Edward Evan Evans-Pritchard Edward Sapir Esther Hermitte Evelia Edith Oyhenart Federico Kauffmann Doig Francisco Raúl Carnese Franz Boas Georges Balandier Gustavo Politis Helen Fisher Herbert Spencer Héctor Mario Pucciarelli Hugo Ratier James George Frazer Johann Jakob Bachofen Julian Steward Karl Polanyi Lewis Henry Morgan Leslie White Louis Leakey Manuel Gamio Marcel Mauss Margaret Mead Marshall Sahlins Marvin Harris Mary Douglas Mary Leakey Maurice Godelier Melville Herskovits Néstor García Canclini Philippe Descola Pierre Bourdieu Pierre Clastres Ralph Linton Renato Rosaldo Rita Segato Rodolfo Kusch Rosana Guber Ruth Benedict Sherry Ortner Tim Ingold Revistas científicas de antropología Categoría principal: Revistas de antropología Temas más estudiados Aculturación Alteridad Brujería Capacidad intercultural Cerámica Chamanismo Choque cultural Colonialismo Comportamiento Comunicación intercultural Contacto cultural Cosmovisión Costumbre Cultura Desigualdad Diversidad cultural Drogas ancestrales Ecología cultural Educación intercultural Endoculturación Etnabotánica Etnia Etnolingüística Etnomusicología Etnoarqueología Etnocentrismo Evolución humana Exotismo Excavación Familia Genocidio Globalización Hominización *Humanización Identidad cultural Institución Interculturalidad Magia Matriarcado Mito Modernidad Netnografía Neuroantropología Parentesco Posmodernidad Proxémica Razas humanas Ritual Rito Sexualidad humana Síndrome cultural Sociedad Organización social Tabú Transculturación Género"
ksampletext_wikipedia_anth_homosapiens: str = "Homo sapiens. Homo sapiens (en latín hombre sabio), comúnmente llamado ser humano, persona u hombre —este último en el sentido de ser racional, que no distingue entre ambos sexos—, es una especie de primate catarrino perteneciente a la familia de los homínidos, nativo originalmente de África, aunque luego se fue expandiendo hacia el resto del mundo. El conjunto de personas o el género humano[5] también se conoce con la denominación genérica de humanos[6] y humanidad.[7] Los seres humanos poseen capacidades mentales que les permiten inventar, aprender, utilizar estructuras lingüísticas complejas, adquirir y mejorar sus habilidades lógicas, matemáticas, de escritura, musicales, entre otras. Los seres humanos son animales sociales, capaces de concebir, transmitir y aprender conceptos totalmente abstractos. Se considera Homo sapiens de manera indiscutible a los que poseen las características anatómicas de las poblaciones humanas actuales. Los restos más antiguos atribuidos a Homo sapiens, datados en 315 000 años, se encontraron en Marruecos.[8] Las evidencias más antiguas de comportamiento moderno son las de Pinnacle Point (Sudáfrica), con 165 000 años de antigüedad. Pertenece al género Homo, que fue más diversificado y durante el último millón y medio de años incluía otras especies ya extintas. Desde la extinción de Homo neanderthalensis, hace 28 000 años, es la única especie conocida del género Homo que aún perdura. Hasta hace poco, en la biología se utilizaba un nombre trinomial —Homo sapiens sapiens— para esta especie, pero sigue en debate el nexo filogenético entre el neandertal y la actual humanidad. Homo sapiens pertenece a una estirpe de primates, los hominoideos. Aunque el descubrimiento de Homo sapiens idaltu en 2003 haría necesario volver al sistema trinomial, la posición taxonómica de este último es aún incierta.[9] Evolutivamente se diferenció en África y de ese ancestro surgió la familia de la que forman parte los homínidos. Filosóficamente, el ser humano se ha definido y redefinido a sí mismo de numerosas maneras a través de la historia, otorgándose de esta manera un propósito positivo o negativo respecto de su propia existencia. Existen diversos sistemas religiosos e ideales filosóficos que, de acuerdo con una diversa gama de culturas e ideales individuales, tienen como propósito y función responder a algunas de esas interrogantes existenciales. Los seres humanos tienen la capacidad de ser conscientes de sí mismos, así como de su pasado; saben que tienen el poder de planear, realizar y transformar proyectos de diversos tipos. En función de esta capacidad, han creado diversos códigos morales y dogmas orientados directamente al manejo de estas capacidades. Además, pueden ser conscientes de responsabilidades y peligros provenientes de la naturaleza, así como de otros seres humanos. En la actualidad, aproximadamente 8000 millones de seres humanos habitan la Tierra. Nombre científico Hombre de Vitruvio, por Leonardo da Vinci. El nombre científico asignado por el naturalista sueco Carlos Linneo (1707-1778) en 1758[10] alude al rasgo biológico más característico (sapiens significa «sabio» o «capaz de conocer») y se refiere a la consideración del ser humano como «animal racional», al contrario que todas las otras especies, siendo la descripción que aportó para Homo sapiens simplemente: Nosce te ipsum («Conócete a ti mismo»). Es precisamente la capacidad del ser humano de realizar operaciones conceptuales y simbólicas muy complejas —que incluyen, por ejemplo, el uso de sistemas lingüísticos muy sofisticados, el razonamiento abstracto y las capacidades de introspección y especulación— uno de sus rasgos más destacados. Posiblemente esta complejidad, fundada neurológicamente en un aumento del tamaño del cerebro y, sobre todo, en el desarrollo del lóbulo frontal, es también una de las causas, a la vez que producto, de las muy complejas estructuras sociales que el ser humano ha desarrollado, y que forman una de las bases de la cultura, entendida biológicamente como la capacidad para transmitir información y hábitos por imitación e instrucción, en vez de por herencia genética. Esta propiedad no es exclusiva de esta especie y es importante también en otros primates. Linneo clasificó al hombre y a los monos en un grupo que llamó antropomorfos, como subconjunto del grupo cuadrúpedos, pues entonces no reconocía signos orgánicos que le permitieran ubicar al ser humano en un lugar privilegiado de la escala de los seres vivientes. Años más tarde, en el prefacio de Fauna suecica, manifestó que había clasificado al hombre como cuadrúpedo porque no era planta ni piedra, sino un animal, tanto por su género de vida como por su locomoción y porque además, no había podido encontrar un solo carácter distintivo por el cual el hombre se diferenciara del mono; en otro contexto afirmó sin embargo que considera al hombre como el fin último de la creación. A partir de la décima edición de Systema naturae reemplazó a los cuadrúpedos por los mamíferos y como primer orden de estos, puso a los primates, entre los cuales colocó al hombre. Linneo tuvo el mérito de dar origen a un nuevo e inmenso campo epistemológico, el de la antropología, si bien se limitó a enunciarlo y no lo cultivó. A él tendrán que remitirse todos los científicos posteriores, tanto para retomar sus definiciones como para criticarlas. En 1758 se definió al Homo sapiens linneano como una especie diurna que cambiaba por la educación y el clima. Linneo no designó un holotipo para Homo sapiens, pero en 1959 William Stearn propuso al propio Linneo, padre de la moderna taxonomía, como lectotipo para la especie. Con posterioridad se difundió la idea de que había sido sustituido por Edward Cope, pero esta propuesta no llegó a formalizarse, así que siguen siendo los restos de Linneo enterrados en Uppsala el tipo nomenclatural —que debe considerarse simbólico— para la especie Homo sapiens.[11]​ En la actualidad existen defensores de incluir al ser humano, chimpancé (Pan troglodytes) y bonobo (Pan paniscus) en el mismo género, dada la cercanía filogenética, que es más estrecha que la que se encuentra entre otras especies animales que sí están agrupadas genéricamente.[12] Sin embargo, la inmensa mayoría de los especialistas no consideran correcto incluirlos dentro del mismo género, debido a que los linajes evolutivos que condujeron al ser humano y al chimpancé divergieron hace entre 6 y 10 millones de años y se diversificaron posteriormente, como argumenta Sandy Harcourt,[12] y debido a las significativas diferencias entre los planes corporales de ambas líneas, especialmente en la de los Hominina, que permiten justificar varios géneros (Ardipithecus, Paranthropus, Australopithecus u Homo).[13]​ Taxonomía y cladística En la siguiente tabla se muestra la clasificación cladística y taxonómica de la especie Homo sapiens desde el origen de la vida: N.º de clado Rango Nombre Contenido Biología El ser humano es un ser vivo, y como tal está compuesto por sustancias químicas llamadas biomoléculas, por células y realiza las tres funciones vitales: nutrición, relación y reproducción.[14]​ Además, el humano es un organismo pluricelular; es decir, está formado por muchas células, entre las cuales existen diferencias de estructura y de función.[14]​ Por otra parte, el ser humano es un animal, pues tiene células eucariotas, es decir, presenta orgánulos celulares especializados en una función determinada y su material genético se encuentra protegido por una envoltura; y presenta nutrición heterótrofa; es decir, que para obtener su propia materia orgánica se alimenta de otros seres vivos.[14]​ Cuerpo humano Elementos principales de la anatomía externa de la mujer y el varón u hombre Artículo principal: Anatomía humana En cuanto a su locomoción y movimiento, es uno de los más plásticos del reino animal, pues existe una amplia gama de movimientos posibles, lo que le capacita para actividades como el arte escénico y la danza, el deporte y un sinnúmero de actividades cotidianas. Asimismo destaca la habilidad de manipulación, gracias a los pulgares oponibles, que le facilitan la fabricación y uso de instrumentos. La especie humana posee un notorio dimorfismo sexual en el nivel anatómico, siendo los machos adultos (o varones) más altos y más pesados que las mujeres (hembras de la especie) en promedio, aunque se ha notado una «tendencia secular» al aumento de las tallas en ambos sexos (especialmente durante el siglo XX). El ser humano adulto contemporáneo promedio mide entre: 1,50 m a 1,70 m (mujeres), y entre 1,60 m a 1,80 m (varones). El peso depende de la contextura del individuo y del sexo, generalmente rondando los 45 kg a 70 kg (mujeres), y 65 kg a 100 kg (varones). Los cuerpos humanos difieren entre sí según la estatura, peso, musculatura, nivel de grasa, entre otros. Véanse también: Cuerpo humano, Fisiología humana y Genética humana. Mente Artículo principal: Mente La mente se refiere colectivamente a aspectos del entendimiento y consciencia que son combinaciones de capacidades como el raciocinio, la percepción, la emoción, la memoria, la imaginación y la voluntad. La mente, según la neurociencia, es un resultado de la actividad del cerebro. El término pensamiento define todos los productos que la mente puede generar incluyendo las actividades racionales del intelecto y las abstracciones de la imaginación; todo aquello que sea de naturaleza mental es considerado pensamiento, bien sean estos abstractos, racionales, creativos, artísticos, etc. Junto con los cetáceos superiores (delfines y ballenas), los homininos de los géneros Gorilla y Pan, y los elefantes, alcanzan el mayor desarrollo y aun muchas de sus interacciones nos son desconocidas. Los seres humanos, a diferencia del resto del reino animal, son los únicos con capacidad de razonar. Además poseen capacidades mentales que les permiten inventar, aprender y utilizar estructuras lingüísticas complejas, lógicas, matemáticas, escritura, música, ciencia y tecnología. Los seres humanos son animales sociales, capaces de concebir, transmitir y aprender conceptos totalmente abstractos. Véanse también: Cerebro humano, Consciente, Inteligencia, Pensamiento y Psicología. Nutrición humana Artículo principal: Nutrición Véanse también: Régimen alimenticio, Alimentación humana, Omnívoro y Vegetarianismo. El ser humano es un animal omnívoro.[15]​[16] En las primeras especies del género Homo, el paso de una alimentación eminentemente vegetariana a la inclusión de carne y grasas animales en la dieta no se debió a cuestiones culturales, sino a los desajustes metabólicos provocados por un mayor desarrollo cerebral.[15] Sin embargo, en el humano, una dieta demasiado rica en proteínas necesita el complemento de carbohidratos y grasas; de lo contrario pueden aparecer carencias nutricionales importantes que pueden incluso provocar la muerte.[15] Por ello, la alimentación del ser humano se basa en la combinación de carne con materia vegetal.[15] Al igual que el mismo ser humano, su alimentación ha evolucionado a lo largo de los años, cambiando para adaptarse al mundo que lo rodea.[17] El ser humano cazaba y recolectaba, sin embargo, con la invención de la agricultura y de la ganadería, su alimentación evolucionó.[17]​ Etología Artículo principal: Comportamiento humano Ciclo vital Feto, por Leonardo da Vinci. La especie humana es entre los seres vivos pluricelulares actuales una de las más longevas; se tienen documentados casos de longevidad que sobrepasan los cien años. Tal longevidad es un carácter genotípico que, sin embargo, debe ser coadyuvado por condiciones vivenciales favorables. En el Imperio romano, hacia el año 2 d. C., la esperanza de vida rondaba solo los 27 años, debido en gran parte a la elevada mortalidad infantil.[18] A principios del siglo XXI, la esperanza de vida global era de unos 70 años aproximadamente, siendo más elevada en países desarrollados y más baja en países subdesarrollados. Se supone que el ser humano, en óptimas condiciones, pueda vivir cien años o un poco más. Sin embargo a pesar del avance en la salud y calidad de vida en el último siglo, las costumbres humanas como el consumo de drogas, alcohol, azúcar, comida basura, sedentarismo, estrés, enfermedades de todo tipo, exposición a elementos tóxicos, entre otros, disminuye los años de vida de los seres humanos. Se cree también que pueda ser genético.[19]​ La ‘infancia’ humana es una de las más prolongadas en comparación con otras especies cercanas, siendo la edad de la pubertad aproximadamente a los once años en las mujeres y a los trece años en los varones, aunque las edades varían según la persona. Véanse también: Biología del desarrollo, Longevidad, Crecimiento humano y Desarrollo (biología). Sexualidad Artículo principal: Sexualidad Como todos los mamíferos, el ser humano tiene comportamientos reproductivos y sexuales. Pero a diferencia de la mayoría de ellos no tiene una época reproductiva estacional determinada, manteniendo actividad sexual y fertilidad en las hembras a lo largo de todo el año. Las mujeres tienen un ciclo de ovulación aproximadamente mensual, durante el cual producen óvulos y pueden ser fecundadas; en caso contrario tienen la menstruación, que es la eliminación a través de la vagina de los tejidos y sustancias relacionados con la producción de células sexuales. Pero el comportamiento sexual humano no está únicamente supeditado a las funciones reproductivas, sino que, de modo similar a otros simios antropoides, tiene fines recreativos y sociales. En el contacto sexual se busca tanto la reproducción como el placer y la comunicación afectiva. Es una parte importante de las relaciones de pareja y también se considera importante en las necesidades psicológicas del individuo aunque no tenga una relación de pareja. Cabe destacar la importancia del lenguaje simbólico en Homo sapiens, que hace que los significantes sean los soportes del pensar o los pensamientos. En nuestra especie, el pensar humano, a partir de los tres años y medio de edad se hace prevalentemente simbólico. Asociado con lo anterior, debe notarse que la especie humana es prácticamente la única que se mantiene en celo sexual continuo: es realmente destacable que en la especie humana no exista un estro propiamente dicho. En las mujeres existe un ciclo de actividad ovárica en virtud del cual existen cambios fisiológicos en todo su sistema reproductivo y del cual derivan ciertos cambios de conducta. Sin embargo, como en las mujeres la aceptación sexual no se circunscribe a una parte del ciclo reproductivo, no se debería usar los vocablos «estro» y «celo» en el ser humano, dado que la aceptación sexual es independiente de su ciclo reproductivo. Ya entre chimpancés y, sobre todo, bonobos, se nota una conducta próxima. Ahora bien, dada la dificultad de vivir «solamente» practicando relaciones sexuales, un «mecanismo» evolutivo compensatorio habría sido el de la sublimación –la cual se considera asociada a la existencia de un lenguaje y un pensar simbólicos–. Si se da una sublimación, esto parece significar que también se da una «represión» (en el sentido freudiano) que origina a lo inconsciente. Homo sapiens es, en este sentido, un ««animal pulsional». Según la ley del reflejo condicional de Pavlov Homo sapiens «no» se restringe a un «primer sistema de señales» (el de estímulo/respuesta y respuesta a un estímulo substitutivo), sino que el ser humano se encuentra en un nivel de «segundo sistema de señales». Este segundo sistema es, principalmente, el del lenguaje simbólico que permite una heurística, que es la capacidad para realizar de forma inmediata innovaciones positivas para sus fines. Por otra parte, la especie humana es de las pocas, junto con el bonobo (Pan paniscus), en el reino animal que copula cara a cara (entre otras múltiples formas), lo cual tiene implicaciones emocionales de gran relevancia para la especie. Cabe anotar que con el surgimiento de la teoría de la inteligencia emocional, desde la psicología sistémica, el ser humano no debe reducirse a sus pulsiones, las cuales sublima o reprime, sino que se entiende como un ser sexuado, que vive esta dimensión en relación con la formación recibida en la familia y la sociedad. La sexualidad se forma entonces desde los primeros años y se va entendiendo como una vivencia procesual acorde a su ciclo vital y su contexto sociocultural. A diferencia de lo que ocurre en la mayor parte de las otras especies sexuadas, la mujer sigue viviendo mucho tiempo tras la menopausia. En las otras especies la hembra suele fenecer al poco tiempo de su llegada. Por la indicada precocidad, la madurez sexo-genital es –con relación a otras especies– muy tardía entre los individuos de la especie humana. Actualmente en muchas zonas la menarquia está ocurriendo a los once años; esto significa que, aunque la madurez sexo-genital es siempre lenta en la especie humana, existe un adelantamiento de la misma respecto a épocas pasadas (del mismo modo suele darse una menopausia cada vez más tardía). Pero si la madurez sexo-genital es tardía en la especie humana, aún más suele serlo la madurez intelectual y, en especial, la madurez emotiva. Véanse también: Amor, Sexualidad humana y Sistema reproductivo. Origen y evolución Artículos principales: Evolución humana y Origen del hombre. Mitos sobre los orígenes Artículo principal: Mitos de la creación A lo largo de la historia se han ido desarrollando distintas concepciones míticas, religiosas, filosóficas y científicas respecto del ser humano, cada una con su propia explicación sobre el origen del hombre, trascendencia y misión en la vida. Esqueleto reconstruido de Proconsul, un primate hominoideo. De los simios del Viejo Mundo Artículo principal: Catarrinos Evolutivamente, en cuanto perteneciente al orden taxonómico Catarrhini, Homo sapiens parece tener su ancestro, junto con todos los primates catarrinos, en un período que va de los 50 a 33 millones de años antes del presente (AP). Uno de los primeros catarrinos, quizás el primero, es Propliopithecus, incluyendo a Aegyptopithecus. En este sentido, el ser humano actual, al igual que primates del «Viejo Mundo» con características más primitivas, probablemente descienda de esa antigua especie. Homínidos bípedos Artículo principal: Hominina Australopithecus africanus. En cuanto a la bipedestación, esta se observa en ciertos primates a partir del Mioceno. Ya se encuentran ejemplos de bipedación en Oreopithecus bambolii y la bipedestación parece haber sido común en Orrorin y Ardipithecus. Las mutaciones que llevaron a la bipedación fueron exitosas porque dejaban libres las manos para agarrar objetos y, particularmente, porque en la marcha un homínido ahorra mucha más energía andando sobre dos piernas que sobre cuatro patas, puede acarrear objetos durante la marcha y otear más lejos. Sin embargo, de remontarse la bipedestación a quizás a unos seis millones de años AP, la andadura o forma de marcha típica del humano se consolida aproximadamente hace al menos unos cuatro millones de años con Australopithecus. Previamente los primates antropoides apoyaban toda la planta del pie haciendo una flexión y descargando el peso en el calcáneo; en cambio, Australopithecus logra una marcha bípeda eficiente, pues se notan claramente los cambios anatómicos a nivel del pie, en especial del dedo gordo; también ajustando el ángulo del fémur con el cuerpo para el equilibrio, la cadera o pelvis cambia a más robusta, corta y cóncava (forma de cuenco); la columna pasó de ser un arco en forma de C a una forma de S y el agujero de la base del cráneo que conecta con la columna se desplazó hacia adelante[20] como dirigiéndose al centro de gravedad de la cabeza. Hace 1.5 millones de años con Homo erectus o con Homo ergaster, la andadura moderna implica la existencia de un pequeño ángulo entre el dedo gordo y el eje del pie, así como la presencia del arco longitudinal de la planta y una distribución medial del peso (nótese que en las mujeres la andadura distribuye el peso más hacia las partes internas del pie debido a la mayor anchura de la pelvis).[21]​ Todos los cambios reseñados han sucedido en un periodo relativamente breve (aunque se mida en millones de años). Esto explica la susceptibilidad de nuestra especie a afecciones en la columna vertebral y en la circulación sanguínea y linfática (por ejemplo, el corazón recibe —relativamente— «poca» sangre). Aparece el ser humano Artículo principal: Homo Homo erectus. Lo que denominamos propiamente «humano» es una referencia a la aparición de la capacidad de fabricar herramientas de piedra en un homínido bípedo, Homo habilis, considerado por la mayoría como la especie humana más primitiva, mostrando además incremento en la capacidad craneana con respecto a Australopithecus. Es así como se establece que hace unos dos millones y medio de años, con la aparición del género Homo, se toma como punto de inicio para el Paleolítico o Edad de Piedra. Mayor éxito evolutivo tendrá Homo erectus, quien logrará expandirse por toda Eurasia. Véanse también: Prehistoria y Paleolítico inferior. Evolución de la nutrición Probablemente cuando los ancestros de Homo sapiens vivían en selvas comiendo frutos, bayas y hojas, abundantes en vitamina C, pudieron perder la capacidad metabólica que tiene la mayoría de los animales de sintetizar en su propio organismo tal vitamina; ya antes parecen haber perdido la capacidad de digerir la celulosa. Tales pérdidas durante la evolución han implicado sutiles pero importantes determinaciones: cuando las selvas originales se redujeron o, por crecimiento demográfico, resultaron superpobladas, los primitivos homínidos (y luego los humanos) se vieron forzados a recorrer importantes distancias, migrar, para obtener nuevas fuentes de nutrientes. La pérdida de la capacidad de metabolizar ciertos nutrientes como la vitamina C habría sido compensada por una mutación favorable que permite a Homo sapiens una metabolización óptima (ausente en primates) del almidón, y así una rápida y «barata» obtención de energía, particularmente útil para el cerebro. Homo sapiens parece ser una criatura bastante indefensa, y como respuesta satisfactoria la única solución evolutiva que ha tenido es su complejísimo sistema nervioso central, espoleado principalmente por la búsqueda de nuevas fuentes de alimentación. Se ha sugerido la hipótesis de que la cefalización aumentó paralelamente al incremento de consumo de carne,[cita requerida] aunque dicha hipótesis no concuerda con el grado de cefalización desarrollada por los animales carnívoros. La habilidad humana para digerir alimentos con alto contenido de almidón podría explicar el éxito del Homo sapiens en el planeta, y sugiere un estudio genético.[22]​ Humanos arcaicos Artículo principal: Humanos arcaicos Homo neanderthalensis. Se denomina «humanos arcaicos», «Homo sapiens arcaico» o también «pre-sapiens», a un cierto número de especies de Homo que aun no son considerados anatómicamente modernos. Poseen hasta 600 000 años de antigüedad y tienen un tamaño cerebral cercano al del ser humano moderno. El antropólogo Robin Dunbar opina que es en esta etapa cuando aparece el lenguaje humano. La filiación de estos individuos dentro de nuestro género resulta aun controvertida. Entre los humanos arcaicos están considerados Homo heidelbergensis, Homo rhodesiensis, Homo neanderthalensis y a veces Homo antecessor. En 2010 se ha añadido a estos el denominado «hombre de Denísova»,[23] y en 2012 el denominado «hombre del ciervo rojo» en China.[24] Ya que no son sapiens, algunos especialistas prefieren llamarlos simplemente arcaicos antes que H. sapiens arcaico.[25]​ Humanos anatómicamente modernos Artículo principal: Humanos anatómicamente modernos Mujer de Qafzeh (anatómicamente moderna). Se denomina propiamente Homo sapiens o anatómicamente modernos a individuos con una apariencia similar a la del ser humano moderno. Estos humanos pueden clasificarse como «premodernos», pues en ellos no se observa todavía el conjunto de características de un cráneo moderno, casi esférico, con la bóveda alta y la frente vertical.[26] La similitud se aprecia a nivel del esqueleto del cuerpo y cavidad craneana, pero esta similitud no es total pues el rostro aun mantiene características arcaicas como los arcos superciliares (grandes cejas) y prognatismo maxilar (proyección bucal), aunque menos desarrollados que en los neandertales.[27]​ Se considera dentro de este grupo a los restos de Florisbad en Sudáfrica (260 000 años),[28] los de Herto en Etiopía, que corresponde a Homo sapiens idaltu (160 000 años), los de Jebel Irhoud en Marruecos (315 000 años) y los de Skhul/Qafzeh al norte de Israel (100 000 años). También se considera anatómicamente modernos a los hombres de Kibish; sin embargo, estos se enmarcan mejor dentro del hombre moderno. Ser humano moderno Artículo principal: Origen de los humanos modernos Véase también: Adán cromosómico Véase también: Eva mitocondrial Ascendencia mitocondrial africana. Se considera Homo sapiens sapiens de forma indiscutible a los que poseen las características principales que definen al ser humano moderno: primero la equiparación anatómica con las poblaciones humanas actuales y luego lo que se define como «comportamiento moderno». Actualmente, gracias a los análisis científicos, se sabe que en la genealogía de la evolución humana habría existido un antepasado común masculino y uno femenino, a los cuales se les nombró como sus símiles religiosos. Los restos más antiguos son los de Omo I, llamados Hombres de Kibish, encontrados en Etiopía con 195 000 años, y restos en cuevas del río Klasies en Sudáfrica con 125 000 años y con indicios de una conducta más moderna.[29]​ Esta antigüedad coincide con lo estimado para la Eva mitocondrial, la cual está considerada la antecesora de todos los seres humanos actuales y de la que se cree que vivió en el África Oriental[30] (probablemente Tanzania) hace unos 200 000 años. Por otra parte, la línea patrilineal nos lleva hasta el Adán cromosómico, quien nos confirma un origen para el hombre moderno en el África subsahariana y se le calcula unos 140 000 años de antigüedad.[31]​ Pigmentación Es casi seguro que la Eva mitocondrial y el Adán cromosómico, los primeros Homo sapiens eran melanodérmicos, esto es, de tez oscura. Esto se debe a que la piel oscura es una excelente adaptación a la exposición solar alta de las zonas intertropicales del planeta Tierra; la tez oscura (por la melanina) protege de las radiaciones UV (ultravioletas) y obtiene de ellas por metabolismo un nutriente llamado folato, indispensable para el desarrollo del embrión y del feto; pero, a medida que las poblaciones humanas migraron a latitudes más allá de los 45° (tanto norte como sur) la melanina paulatinamente fue menos necesaria, más aún, en las cercanías de las latitudes de los 50° la casi total falta de este pigmento en la dermis, cabello y ojos ha sido una adaptación para captar más radiaciones U.V. —relativamente escasas en tales latitudes, salvo que se produzcan huecos de ozono—; en tales latitudes la tez muy clara posibilita una mayor metabolización de vitamina D a partir de las radiaciones UV. Comportamiento moderno Indígenas karajá de Brasil. El uso de adornos personales es un comportamiento humano ampliamente extendido. La aparición del comportamiento humano moderno significó el más importante cambio en la evolución de la mente humana, dando lugar a que el ingenio creativo humano le permitiese dominar su entorno paulatinamente. Las innovaciones que fueron apareciendo consisten en una gran diversidad de herramientas de piedra, en el uso de hueso, asta y marfil, en entierros con bienes funerarios y rituales, construcción de viviendas, diseño de las fogatas, evidencia de pesca, cacería compleja, aparición del arte figurativo y el uso de adornos personales.[32]​ Las evidencias más antiguas se encuentran en África; herramientas elaboradas hace 165 000 años se encontraron en la cueva de Pinnacle Point (Sudáfrica).[33] Restos de puntas de flechas y herramientas de hueso para pescar se encontraron en el Congo y tienen 90 000 años. Igualmente antiguos son unos símbolos sombreados con ocre rojo en costas al sur de África.[34]​ Véase también: Paleolítico superior Expansión de la humanidad Artículo principal: Expansión de la humanidad Mapa de las migraciones humanas fuera de África, versión de Naruya Saitou y Masatoshi Nei (2002) del Instituto Nacional de la Genética del Japón[35] que coincide con la versión de Göran Burenhult (2000).[36]​ Según la teoría fuera de África, hubo una gran migración de África hacia Eurasia hace 70 000 años que produjo la paulatina dispersión por todos los continentes. Según los estudios genéticos y los descubrimientos paleontológicos, se estima que hace 60 000 años hubo una migración costera por el Sur de Asia, de pocos miles de años, que posibilitó la colonización posterior de Australia, Extremo Oriente y Europa. En Occidente hubo un centro de expansión en Oriente Medio que está relacionado con el hombre de Cromañón y la población temprana de Europa, probable causa de la extinción del hombre de Neandertal. Según algunos estudios genéticos, en Europa hubo tres migraciones: la primera, proveniente del Asia Central hace 40 000 años que colonizó la Europa Oriental. Una segunda oleada hace 22 000 años, proveniente del Oriente Medio, que se instaló en la Europa del sur y del oeste. El 80 % de los europeos actuales son descendientes de estas dos migraciones, que durante el transcurso del máximo glaciar de hace 20 000 años se refugiaron en la península ibérica y en los Balcanes, para volver a expandirse por el resto de Europa cuando llegó el clima favorable. La tercera migración se habría producido hace 9000 años, proveniente del Oriente Medio, durante el transcurso del Neolítico, y solo el 20 % de los europeos actuales llevan marcadores genéticos correspondientes a esos emigrantes.[37]​ Otros estudios dicen lo contrario, afirmando que en Europa el componente neolítico desde Oriente Próximo es el más importante.[38] Lo cierto por ahora es que el acervo genético europeo prehistórico proviene mayoritariamente del Cercano Oriente, y una menor parte proviene de África, Asia Central y Siberia. En Oriente la población es igualmente antigua. El pliegue epicántico de los párpados existente en gran parte de las poblaciones del Asia y de América, el pliegue que hace 'bridados' en su aspecto externo a los ojos, ha sido una especialización de poblaciones que durante las glaciaciones debieron pervivir en lugares con abundancia de nieve; los ojos vulgarmente llamados «rasgados» entonces fueron el modo de adaptación para que los ojos no padecieran un excesivo reflejo de la luz solar reflejada por la nieve.[cita requerida] Sin embargo, una publicación de julio de 2019 en la revista Nature puso en tela de juicio las teorías e ideas previas acerca del momento del poblamiento de Europa por Homo sapiens desde África. El hallazgo y datación de un cráneo de Homo sapiens de 210 000 años de antigüedad en Grecia significaría un poblamiento de Europa 60 000 años más temprano que lo que se suponía.[39]​[40]​ Véanse también: Neolítico, Civilización, Historia y Arqueología. Cultura Artículo principal: Cultura Lenguaje y semiótica Artículo principal: Lenguaje humano El lenguaje designa todas las comunicaciones basadas en la interpretación, incluyendo el lenguaje humano, pero la mayoría de las veces la locución se refiere a lo que los humanos utilizan para comunicarse, es decir, a los idiomas. El lenguaje es universal y es usado por naturaleza en las personas y en el resto de animales. Sin embargo, filósofos como Martin Heidegger consideran que el lenguaje propiamente tal es solo privativo del hombre. Es famosa la tesis de Heidegger según la cual el lenguaje es la casa del ser (Haus des Seins) y la morada de la esencia humana. Este criterio es similar al de Ernst Cassirer, quien ha definido a Homo sapiens como el animal simbólico por excelencia; tan es así que es casi imposible suponer un pensamiento humano sin la ayuda de los símbolos, particularmente de los significantes que subyacen como fundamentos elementales para todo pensar complejo y que transcienda a lo instintivo. Actualmente la especie humana muestra esta faceta hablando en torno a 6000 idiomas diferentes, si bien más del 50 % de los 8000 millones de personas que actualmente conforman la colectividad humana, sabe hablar al menos uno de los siguientes: chino mandarín, español, inglés, francés, árabe, hindi, portugués, alemán, bengalí o ruso. Véanse también: Lenguaje, Familias de lenguas, Idioma, Lingüística y Evolución del lenguaje. Espiritualidad y trascendencia Artículo principal: Religión En muchas civilizaciones los seres humanos se han visto a sí mismos como diferentes de los demás animales, y en ciertos ámbitos culturales (como las religiones del Libro o buena parte de la metafísica del Occidente) la diferencia se asigna a una entidad inmaterial llamada alma, en la que residirían la mente y la personalidad, y que algunos creen que puede existir con independencia del cuerpo. La espiritualidad del ser humano está ligada a la idea de un Dios, el cual abre el paso a muchas interpretaciones sobre la inmortalidad, el alma y sobre todo el más allá.[41]​ Véase también: Familias de religiones Arte y cultura Artículo principal: Arte Posiblemente, la manifestación más clara de humanidad es el arte —en el sentido amplio del término—, que produce la cultura. Por ejemplo, los individuos de una determinada especie de ave fabrican un nido, o emiten un canto, cuyas características son específicas, comunes a todos los individuos de esa especie. En cambio, cada hombre puede imprimir a sus acciones los rasgos propios de su individualidad; por eso, cuando se analiza un cuadro, una forma de escribir, una manera de fabricar herramientas, etc., se puede deducir quién es su autor, su artífice, su artista.[cita requerida] En 2011, en la revista Science, se publicó un trabajo de Francesco d'Errico, de la Universidad de Burdeos, donde afirma haber encontrado uno de los rastros más antiguos de un taller de pintura, en la cueva Blombos en Cape Coast, 300 km al este de Ciudad del Cabo. Este hecho muestra un modo sistemático para obtener pigmentos, pues reunir todos los elementos necesarios para una preparación de este tipo es indicativo de un elevado nivel de pensamiento, que se puede llamar pensamiento simbólico. «La capacidad de tener estos pensamientos es considerada un gran paso en la evolución humana, precisamente lo que nos diferenció del mundo animal».[42]​ Paralelamente, también es la única especie que dedica su tiempo y energía a algo aparentemente inútil desde el punto de vista puramente práctico. El arte es una de las manifestaciones de la creatividad humana, pero una manifestación vacía y negativa desde el punto de vista de la supervivencia. Si bien esta actividad es en principio dañina, en realidad es la herramienta con la cual Homo sapiens desarrolla su cultura, unión y fuerza como pueblo.[aclaración requerida][cita requerida] Véanse también: Humanidades, Música, Danza, Historia del arte y Filosofía. Ciencia Esta sección es un extracto de Ciencia.[editar] Ciencia a través de escalas. De arriba abajo, en el sentido de las agujas del reloj: investigadores en un laboratorio; el Observatorio Paranal y la Vía Láctea; bacteria Escherichia coli bajo el microscopio; y funciones de onda del electrón en un átomo de hidrógeno. La ciencia es una disciplina sistemática que construye y organiza conocimiento en forma de hipótesis y predicciones comprobables sobre el universo.[43]​[44]​[45] La ciencia moderna se divide típicamente en dos o tres ramas principales:[46] las ciencias naturales (por ejemplo, física, química y biología), que estudian el mundo físico; y las ciencias sociales (por ejemplo, economía, psicología y sociología), que estudian a los individuos y las sociedades. Las ciencias formales (por ejemplo, lógica, matemáticas y ciencia computacional teórica), que estudian sistemas formales regidos por axiomas y reglas,[47]​[48] a veces también se describen como ciencias; sin embargo, a menudo se consideran un campo separado porque dependen del razonamiento deductivo en lugar del método científico o de la evidencia empírica como su principal metodología.[49]​[50] Las ciencias aplicadas son disciplinas que utilizan el conocimiento científico con fines prácticos, como la ingeniería y la medicina. La historia de la ciencia abarca la mayoría del registro histórico, siendo los primeros precursores identificables de la ciencia moderna datados en la Edad de Bronce en Egipto y Mesopotamia (aproximadamente 3000–1200 a. C.). Sus contribuciones a las matemáticas, la astronomía y la medicina entraron y moldearon la filosofía natural griega de la antigüedad clásica, mediante la cual se intentaron proporcionar explicaciones de los eventos en el mundo físico basadas en causas naturales.[54]​[55]​[56] La investigación científica decayó en estas regiones después de la caída del Imperio romano de Occidente durante la Alta Edad Media (400–1000 d. C.), pero en los renacimientos medievales (Renacimiento carolingio, Renacimiento otoniano y el Renacimiento del siglo XII) la erudición floreció nuevamente. Algunos manuscritos griegos perdidos en Europa Occidental fueron preservados y ampliados en el Medio Oriente durante la Edad de Oro del islam,[57] junto con los esfuerzos posteriores de los eruditos griegos bizantinos, quienes llevaron manuscritos griegos desde el moribundo Imperio Bizantino a Europa Occidental al comienzo del Renacimiento. La recuperación y asimilación de obras griegas e investigaciones islámicas en Europa Occidental desde los siglos X al XIII revivió la filosofía natural, que luego fue transformada por la Revolución Científica que comenzó en el siglo XVI cuando nuevas ideas y descubrimientos se apartaron de las concepciones y tradiciones griegas anteriores.[58]​[59] El método científico pronto desempeñó un papel más importante en la creación del conocimiento y no fue hasta el siglo XIX cuando muchas de las características institucionales y profesionales de la ciencia comenzaron a tomar forma,[60]​[61] junto con el cambio de filosofía natural a ciencias naturales.[62]​ El nuevo conocimiento en la ciencia avanza mediante la investigación de científicos motivados por la curiosidad sobre el mundo y el deseo de resolver problemas.[63]​[64] La investigación científica contemporánea es altamente colaborativa y suele realizarse por equipos en instituciones académicas y de investigación,[65] agencias gubernamentales[57] y empresas.[66] El impacto práctico de su trabajo ha llevado al surgimiento de políticas científicas que buscan influir en la empresa científica, priorizando el desarrollo ético y moral de productos comerciales, armamentos, atención médica, infraestructura pública y protección del medio ambiente. Véase también: Tecnología Sociedad Artículo principal: Sociedad Una sociedad humana es aquella que se considera a sí misma, a los habitantes y a su entorno, todo ello interrelacionado con un proyecto común, que les da una identidad de pertenencia. Asimismo, el término connota un grupo con lazos económicos, ideológicos y políticos. Tal sociedad supera al concepto de nación-estado, planteando a la sociedad occidental como una sociedad de naciones, organizaciones nacionales, etc.[cita requerida] Véanse también: Sistema social, Comercio, Estado, Gobierno y Guerra. Hábitat Cuenca. El ser humano es capaz de modificar enteramente su entorno para adaptarlo a sus necesidades. Artículo principal: Geografía humana En relación con la capacidad para realizar grandes modificaciones ambientales, cabe decir que Homo sapiens es actualmente un poderoso agente geomorfológico; es en este y otros sentidos que el ser humano es actualmente el mayor superpredador y la especie más poderosa del planeta. Sin embargo, sigue siendo frágil ante posibles acontecimientos cataclísmicos que pudieran afectar a su hábitat, como las glaciaciones. Homo sapiens, por ser un animal muy vulnerable en el medio natural, es muy dependiente de la tecnología (ergo: es dependiente de la ciencia por primitiva que esta sea), así es que se dice de Homo sapiens que es homo faber. Quizás, dado que todo sistema retroalimentado de forma natural llega a su fin, el fin de un ecosistema llega cuando la vida ha logrado evolucionar hasta lograr seres con un grado de consciencia capaz de programarse en función de la educación recibida y no según lo termodinámicamente sostenible.[cita requerida] La educación es, por tanto, la demostración evidente de si somos parte de un sistema aun mayor o intentamos independizarnos de todo, estableciendo nuestras formas de obtener nuestros recursos, sin tener en cuenta los ya establecidos por la propia naturaleza. Por ejemplo, la naturaleza le dota de capacidades físicas para buscar alimentos en el medio que les rodea de una manera termodinámicamente eficaz. Los humanos establecen que lo mejor es racionalizar los medios que la naturaleza les da y replicarlos de forma industrial, aplicando procesos que no se dan de forma natural, aumentando el consumo energético por redundar algo que ya existe y ampliándolo a algo totalmente termodinámicamente innecesario, como es el hecho de que se le entregue alimento en casa, de intervenir los códigos genéticos de las especies alimentarias para hacerlas resistentes a enfermedades, de influir en qué alimentos contendrán semillas y cuáles no y un largo etcétera, que a día de hoy nos hace la vida más cómoda, pero que ignoran cómo les afectan esos cambios en su estructura genética y, por lo tanto, si su descendencia portará características fundamentales para sobrevivir a un medio natural o, por el contrario, nacerán y dependerán tan íntimamente del medio artificial que cualquier modificación a ese medio le incapacite de tal manera que provoque su extinción."
ksampletext_wikipedia_anth_cultura: str = "Cultura. La cultura es un concepto que abarca el comportamiento social, las instituciones y las normas que se encuentran en las sociedades humanas, así como el conocimiento, las creencias, el arte, las leyes, las costumbres, las actitudes y los hábitos de los individuos de estos grupos.[1] La cultura a menudo se origina en una región o ubicación específica, o se atribuye a ella. Los humanos adquieren cultura a través de los procesos de aprendizaje de enculturación y socialización, que se muestran en la diversidad de culturas en las sociedades. Una norma cultural codifica la conducta aceptable en la sociedad; sirve como guía para el comportamiento, la vestimenta, el lenguaje y el comportamiento en una situación, que sirve como plantilla para las expectativas en un grupo social. El cambio cultural es la reconstrucción de un concepto cultural de una sociedad. Las culturas se ven afectadas internamente tanto por fuerzas que fomentan el cambio como por fuerzas que se resisten al cambio, y externamente a través del contacto entre sociedades. Organizaciones como la Unesco intentan preservar la cultura y el patrimonio cultural. Naturaleza y cultura Una primera distinción en el conocimiento científico es la que se establece entre naturaleza y cultura. Esa distinción significa que el mundo de la naturaleza es el que no ha sido creado por el hombre, al menos en sus orígenes; mientras que la cultura es el mundo creado por los seres humanos, tal como se explica extensamente en el libro The Man-Made World ([3]​). Cuando el término cultura surgió en Europa, entre los siglos XVIII y XIX, se refería a un proceso de tecnología o mejora, como en la agricultura u horticultura. En el siglo XIX, pasó primero a referirse al mejoramiento o refinamiento de lo individual, especialmente a través de la educación, y luego al logro de las aspiraciones o ideales nacionales. A mediados del siglo XIX, algunos científicos utilizaron el término «cultura» para referirse a la capacidad humana universal. Para el antipositivista y sociólogo alemán Georg Simmel, la cultura se refería a «la cultivación de los individuos a través de la injerencia de formas externas que se han materializado en el transcurso de la historia».[4]​ En el siglo XX, la «cultura» surgió como un concepto central de la antropología, abarcando todos los fenómenos humanos que no son el total resultado de la genética. Específicamente, el término «cultura» en la antropología americana tiene dos significados: (1) la evolucionada capacidad humana de clasificar y representar las experiencias con símbolos y actuar de forma imaginativa y creativa; y (2) las distintas maneras en que la gente vive en diferentes partes del mundo, clasificando y representando sus experiencias y actuando creativamente. Después de la Segunda Guerra Mundial, el término se volvió importante, aunque con diferentes significados, en otras disciplinas como estudios culturales, psicología organizacional, sociología de la cultura y estudios gerenciales. Algunos etólogos han hablado de «cultura» para referirse a costumbres, actividades o comportamientos transmitidos de una generación a otra en grupos de animales por imitación consciente de dichos comportamientos.[cita requerida] Las creencias y prácticas de una cultura determinada pueden ser ejercidas como mecanismos de control que limitan la conducta social. La cultura se asocia con la libertad, ya que es el vehículo entre el conocimiento y nuevas formas de conciencia que permiten una desestabilización en la hegemonía. Además puede reconocerse como conjuntos o modos de vida y costumbres de una época o grupo social. El término cultura puede alcanzar extensión y usos diversos, como diversidad cultural, objeto del conocimiento empírico, y la diferencia cultural.[5]​ Otros conceptos de cultura La palabra cultura se asocia con la acción de cultivar o practicar algo, también según la RAE puede ser el resultado o efecto de prevalecer conocimientos humanos y conjuntos de modos de vida. La cultura ha sido vista dentro de los proyectos de modernidad. Una dimensión y expresión de la vida humana, se realiza mediante la utilización de símbolos y artefactos, en los que hay un campo de producción, circulación y consumo de signos y como una praxis que se articula en una teoría. En el diccionario se nombran diferentes tipos de culturas y entre ellas las dos más emblemáticas son la cultura popular y la cultura de masas.[5]​ Formación del concepto de cultura Etimología La etimología del concepto moderno “cultura” tiene un origen antiguo. En varias lenguas europeas, la palabra “cultura” está basada en el término latino utilizado por Cicerón, en su Tusculanae Disputationes, quien escribió acerca de un cultivo del alma o “cultura animi”, utilizando así una metáfora agrícola para describir el desarrollo de un alma filosófica, que fue comprendida teleológicamente como uno de los ideales más altos posibles para el desarrollo humano. Samuel Pufendorf llevó esta metáfora a un concepto moderno, con un significado similar, pero ya sin asumir que la filosofía es la perfección natural del hombre. Para este autor, los significados de cultura, que muchos escritores posteriores retoman, “se refieren a todas las formas en la que los humanos comienzan a superar su barbarismo original y, a través de artificios, se vuelven completamente humanos”.[6]​ Como lo describe Velkley:[6]​ El término “cultura”, que originalmente significaba la cultivación del alma o la mente, adquiere la mayoría de sus posteriores significados en los escritos de los pensadores alemanes del siglo XVIII, quienes en varios niveles desarrollaron la crítica de Rousseau al liberalismo moderno y la Ilustración. Además, un contraste entre “cultura” y “civilización” está usualmente implícito por estos autores, aun cuando no lo expresen así. Dos significados primarios de cultura surgen de este período: cultura como un espíritu folclórico con una identidad única, y cultura como la cultivación de la espiritualidad o la individualidad libre. El primer significado es predominante dentro de nuestro uso actual del término “cultura”, pero el segundo juega todavía un importante rol en lo que creemos debería lograr la cultura, como la “expresión” plena del ser único y “auténtico”. Concepción clásica de la cultura En sus primeras acepciones, cultura designaba el cultivo de los campos. El término cultura proviene del latín cultus que a su vez deriva de la voz colere que significa cuidado del campo o del ganado. Hacia el siglo XIII, el término se empleaba para designar una parcela cultivada, y tres siglos más tarde había cambiado su sentido de estado de una cosa a la propia acción que lleva a dicho estado: el cultivo de la tierra o el cuidado del ganado (Cuche, 1999: 10), aproximadamente en el sentido en que se emplea en el español de nuestros días en vocablos como agricultura, apicultura, piscicultura y otros. Por la mitad del siglo XVI, el término adquiere una connotación metafórica, como el cultivo de cualquier facultad. De cualquier manera, la acepción figurativa de cultura no se extenderá hasta el siglo XVII, cuando también aparece en ciertos textos académicos. El Siglo de las Luces (siglo XVIII) es la época en que el sentido figurado del término como “cultivo del espíritu” se impone en amplios campos académicos. Por ejemplo, el Dictionnaire de l'Académie Française de 1718. Y aunque la Enciclopedia lo incluye solo en su sentido restringido de cultivo de tierras, no desconoce el sentido figurado, que aparece en los artículos dedicados a la literatura, la pintura, la filosofía y las ciencias. Con el paso del tiempo, como cultura se entenderá la formación de la mente. Es decir, se convierte nuevamente en una palabra que designa un estado, aunque en esta ocasión es el estado de la mente humana, y no el estado de las parcelas. Voltaire, uno de los pocos pensadores franceses del siglo XVIII que se mostraban partidarios de una concepción relativista de la historia humana. La clásica oposición entre cultura y naturaleza también tiene sus raíces en esta época. En 1798, el Dictionnaire incluye una acepción de cultura en que se estigmatiza el “espíritu natural”. Para muchos de los pensadores de la época, como Jean Jacques Rousseau, la cultura es un fenómeno distintivo de los seres humanos, que los coloca en una posición diferente a la del resto de animales. La cultura es el conjunto de los conocimientos y saberes acumulados por la humanidad a lo largo de sus milenios de historia. En tanto una característica universal (el vocablo), se emplea en número singular, puesto que se encuentra en todas las sociedades sin distinción de etnias, ubicación geográfica o momento histórico. Cultura y civilización También es en el contexto de la Ilustración cuando surge otra de las clásicas oposiciones en que se involucra a la cultura, esta vez, como sinónimo de la civilización. Esta palabra aparece por primera vez en la lengua francesa del siglo XVIII, y con ella se significaba la refinación de las costumbres. Civilización es un término relacionado con la idea de progreso. Según esto, la civilización es un estado de la Humanidad en el cual la ignorancia ha sido abatida y las costumbres y relaciones sociales se hallan en su más elevada expresión. La civilización no es un proceso terminado, es constante, e implica el perfeccionamiento progresivo de las leyes, las formas de gobierno, el conocimiento. Como la cultura, también es un proceso universal que incluye a todos los pueblos, incluso a los más atrasados en la línea de la evolución social. Desde luego, los parámetros con los que se medía si una sociedad era más civilizada o más salvaje eran los de su propia sociedad. En los albores del siglo XIX, ambos términos, cultura y civilización eran empleados casi del mismo modo, sobre todo en francés e inglés (Thompson, 2002: 186). Johann Gottfried Herder. Según él, la cultura podía entenderse como la realización del genio nacional (Volksgeist). Es necesario señalar que no todos los intelectuales franceses emplearon el término. Rousseau y Voltaire se mostraron reticentes a esta concepción progresista de la historia. Intentaron proponer una versión más relativista de la historia, aunque sin éxito, pues la corriente dominante era la de los progresistas. No fue en Francia, sino en Alemania donde las posturas relativistas ganaron mayor prestigio. El término Kultur en sentido figurado aparece en Alemania hacia el siglo XVII -aproximadamente con la misma connotación que en francés. Para el siglo XVIII goza de gran prestigio entre los pensadores burgueses alemanes. Esto se debió a que fue empleado para denostar a los aristócratas, a los que acusaban de tratar de imitar las maneras “civilizadas” de la corte francesa. Por ejemplo, Immanuel Kant apuntaba que “nos cultivamos por medio del arte y de la ciencia, nos civilizamos [al adquirir] buenos modales y refinamientos sociales” (Thompson, 2002: 187). Por lo tanto, en Alemania el término civilización fue equiparado con los valores cortesanos, calificados de superficiales y pretenciosos. En sentido contrario, la cultura se identificó con los valores profundos y originales de la burguesía (Cuche, 1999:13). En el proceso de crítica social, el acento en la dicotomía cultura/civilización se traslada de las diferencias entre estratos sociales a las diferencias nacionales. Mientras Francia era el escenario de una de las revoluciones burguesas más importantes de la historia, Alemania estaba fragmentada en múltiples Estados. Por ello, una de las tareas que se habían propuesto los pensadores alemanes era la unificación política. La unidad nacional pasaba también por la reivindicación de las especificidades nacionales, que el universalismo de los pensadores franceses pretendía borrar en nombre de la civilización. Ya en 1774, Johann Gottfried Herder proclamaba que el genio de cada pueblo (Volksgeist) se inclinaba siempre por la diversidad cultural, la riqueza humana y en contra del universalismo. Por ello, el orgullo nacional radicaba en la cultura, a través de la que cada pueblo debía cumplir un destino específico. La cultura, como la entendía Herder, era la expresión de la humanidad diversa, y no excluía la posibilidad de comunicación entre los pueblos. Durante el siglo XIX, en Alemania el término cultura evoluciona bajo la influencia del nacionalismo.[7] Mientras tanto, en Francia, el concepto se amplió para incluir no solo el desarrollo intelectual del individuo, sino el de la humanidad en su conjunto. De aquí, el sentido francés de la palabra presenta una continuidad con el de civilización: no obstante la influencia alemana, persiste la idea de que más allá de las diferencias entre “cultura alemana” y “cultura francesa” (por poner un ejemplo), hay algo que las unifica a todas: la cultura humana.[8]​ Definiciones de cultura Para efecto de las ciencias sociales, las primeras acepciones de cultura fueron construidas a finales del siglo XIX. Por esta época, la sociología y la antropología eran disciplinas relativamente nuevas, y la pauta en el debate sobre el tema que aquí nos ocupa la llevaba la filosofía. Los primeros sociólogos, como Émile Durkheim, rechazaban el uso del término. Hay que recordar que en su perspectiva, la ciencia de la sociedad debía abordar problemas relacionados con la estructura social.[9] Si bien es opinión generalizada que Karl Marx dejó de lado a la cultura, ello se ve refutado por las mismas obras del autor, sosteniendo que las relaciones sociales de producción (la organización que adoptan los seres humanos para el trabajo y la distribución social de sus frutos) constituyen la base de la superestructura jurídico-política e ideológica, pero en ningún caso un aspecto secundario de la sociedad. No es concebible una relación social de producción sin reglas de conducta, sin discursos de legitimación, sin prácticas de poder, sin costumbres y hábitos permanentes de comportamiento, sin objetos valorados tanto por la clase dominante como por la clase dominada. El desvelo de las obras juveniles de Marx, tanto de La ideología alemana (1845-1846) en 1932 por la célebre edición del Instituto Marx-Engels de la URSS bajo dirección de David Riazanov, como de los Manuscritos económicos y filosóficos (1844) posibilitó que varios partidarios de sus propuestas teóricas desarrollaran una teoría de la cultura marxista (véase más adelante). Generalmente, el significado de cultura se relaciona con la antropología, una de las ramas más importantes de la disciplina social que se encarga precisamente del estudio comparativo de la cultura. Quizá por la centralidad que la palabra tiene en la teoría de la antropología, el término ha sido desarrollado de diversas maneras, que suponen el uso de una metodología analítica basada en premisas que en ocasiones distan mucho las unas de las otras. Fue Franz Boas,[10] frente a esta empresa etnocentrista, quien opera el gran cambio epistemológico en la antropología. A partir de Boas, padre del relativismo cultural, el antropólogo se hace traductor, pudiendo entrar en la cosmovisión del estudiado y entender el mundo de sus significaciones. Ese mundo de la significación,[11] irá conduciendo, lentamente, hacia un concepto que no es antropológico, y es el de la construcción social del sentido. Ante este aumento de la sensibilidad vinculada con las cuestiones del lenguaje, se desarrollarán disciplinas nuevas, vinculadas con el mundo de la significación humana y del lenguaje, que completarán la idea de la cultura entendida desde el mundo de la significación.[12]​ De acuerdo con la Declaración Universal sobre la Diversidad Cultural de la UNESCO la cultura debe ser considerada como el conjunto de los rasgos distintivos espirituales y materiales, intelectuales y afectivos que caracterizan a una sociedad o a un grupo social y que abarca, además de las artes y las letras, los modos de vida, las maneras de vivir juntos, los sistemas de valores, las tradiciones y las creencias[13]​ Los etnólogos y antropólogos británicos y estadounidenses de las postrimerías del siglo XIX retomaron el debate sobre el contenido de cultura. Estos autores tenían casi siempre una formación profesional en derecho, pero estaban particularmente interesados en el funcionamiento de las sociedades exóticas con las que Occidente se encontraba en ese momento.[14] En la opinión de estos pioneros de la etnología y la antropología social (como Bachoffen, McLennan, Maine y Morgan), la cultura es el resultado del devenir histórico de la sociedad. Pero la historia de la humanidad en estos escritores era fuertemente deudora de las teorías ilustradas de la civilización, y sobre todo, del darwinismo social de Spencer. Definiciones descriptivas de cultura Definición de Tylor E. B. Tylor, etnólogo británico, dijo: “La principal tendencia de la cultura desde los orígenes a los tiempos modernos ha sido del salvajismo hacia la civilización” (1995:43). Como señala Thompson (2002:190), la definición descriptiva de cultura se encontraba presente en esos primeros autores de la antropología decimonónica. El interés principal en la obra de estos autores (que abordaba problemáticas tan disímbolas como el origen de la familia y el matriarcado, y las supervivencias de culturas antiquísimas en la civilización occidental de su tiempo) era la búsqueda de los motivos que llevaban a los pueblos a comportarse de tal o cual modo. En esas exploraciones, meditarente, o entre la tecnología y el resto del sistema social. Uno de los más importantes etnógrafos de la época fue Gustav Klemm. En los diez tomos de su obra Allgemeine Kulturgeschichte der Menschheit (1843-1852)[15] intentó mostrar el desarrollo gradual de la humanidad por medio del análisis de la tecnología, costumbres, arte, herramientas, prácticas religiosas. Una obra monumental, pues incluía ejemplos etnográficos de pueblos de todo el mundo. El trabajo de Klemm habría de tener eco en sus contemporáneos, empeñados en definir el campo de una disciplina científica que estaba naciendo. Unos veinte años más tarde, en 1871, Edward B. Tylor publicó en Primitive Culture una de las definiciones más ampliamente aceptadas de cultura. Según Tylor, la cultura es: ...aquel todo complejo que incluye el conocimiento, las creencias, el arte, la moral, el derecho, las costumbres, y cualesquiera otros hábitos y capacidades adquiridos por el hombre. La situación de la cultura en las diversas sociedades de la especie humana, en la medida en que puede ser investigada según principios generales, es un objeto apto para el estudio de las leyes del pensamiento y la acción del hombre. (Tylor, 1995: 29) De esta suerte, uno de los principales aportes de Tylor fue la elevación de la cultura como materia de estudio sistemático. A pesar de este notable avance conceptual, la propuesta de Tylor adolecía de dos grandes debilidades. Por un lado, sacó del concepto su énfasis humanista al convertir a la cultura en objeto de ciencia. Por el otro, su procedimiento analítico era demasiado descriptivo. En el texto citado arriba, Tylor plantea que “un primer paso para el estudio de la civilización[16] consiste en diseccionarla en detalles, y clasificar éstos en los grupos adecuados” (Tylor, 1995:33). Según esta premisa, la mera recopilación de los “detalles” permitiría el conocimiento de una cultura. Una vez conocida, sería posible clasificarla en una graduación de más a menos civilizada, premisa que heredó de los darwinistas sociales. Definición de los culturalistas Una mujer hopi arregla el peinado de una joven soltera de su tribu. Los antropólogos estadounidenses de la primera mitad del siglo XX estaban muy interesados en la documentación etnográfica de los pueblos indios, algunos de los cuales estaban en proceso de extinción. La propuesta teórica de Tylor fue retomada y reelaborada posteriormente, tanto en Gran Bretaña como en Estados Unidos. En este último país, la antropología evolucionaba hacia una posición relativista, representada en primera instancia por Franz Boas. Esta posición representaba un rompimiento con las ideas anteriores sobre la evolución cultural, en especial las propuestas por los autores británicos y el estadounidense Lewis Henry Morgan. Para este último, contra quien Boas dirigió sus críticas en uno de sus pocos textos teóricos, el proceso de la evolución social humana (tecnología, relaciones sociales y cultura) podía ser equiparado con el proceso de crecimiento de un individuo de la especie. Por lo tanto, Morgan comparaba el salvajismo con la “infancia de la especie humana”, y la civilización, con la madurez.[17] Boas fue sumamente duro con las propuestas de Morgan y el resto de los antropólogos evolucionistas contemporáneos. A lo que sus autores llamaban “teorías” sobre la evolución de la sociedad, Boas las calificó de “puras conjeturas” sobre el ordenamiento histórico de “fenómenos observados conforme a principios admitidos [de antemano]” (1964:184). La crítica de Boas en contra de los evolucionistas es un eco de la perspectiva de los filósofos alemanes como Herder y Wilhelm Dilthey. El núcleo de la propuesta radica en su inclinación a considerar la cultura como un fenómeno plural. En otras palabras, más que hablar de cultura, Boas hablaba de culturas. Para la mayor parte de los antropólogos y etnólogos adscritos a la escuela culturalista estadounidense, el estado del arte etnográfico al principio del siglo XX no permitía la conformación de una teoría general sobre la evolución de las culturas. Por lo tanto, la labor más importante de los estudiosos del fenómeno debía ser la documentación etnográfica.[18] De hecho, Boas escribió muy pocos textos teóricos, en comparación con sus monografías sobre los pueblos indígenas de la costa pacífica de América del Norte. Los antropólogos formados por Robin Reid hubieron de heredar muchas de las premisas de su maestro. Entre otros casos notables, están el de Ruth Benedict. En su obra Patterns of culture (1939), Benedict señala que cada cultura es un todo comprensible solo en sus propios términos[19] y constituye una suerte de matriz que da sentido a la actuación de los individuos en una sociedad. Alfred Kroeber, retomando la oposición entre cultura y naturaleza, también señalaba que las culturas son fenómenos sui generis pero, en sentido estricto, eran de una categoría exterior a la naturaleza. Por lo tanto, según Kroeber, el estudio de las culturas debía salirse del dominio de las ciencias naturales y encarar a las primeras como lo que eran: fenómenos superorgánicos.[20] Melville Herskovits y Clyde Kluckhohn retomaron de Tylor su definición cientificista del estudio de la cultura. Para el primero, también la recolección de rasgos definitorios de las culturas permitiría su clasificación. Aunque, en este caso, la clasificación no se realizaba en sentido diacrónico, sino espacial-geográfico que habría de permitir el conocimiento de las relaciones entre los diferentes pueblos asentados en un área cultural. Kluckhonn, por su parte, resume en su texto Antropología la mayor parte de los postulados vistos en esta sección, y reclama el dominio de lo cultural como el campo específico de la actividad antropológica. Definición funcionalista-estructural La característica más peculiar del concepto funcionalista de cultura se refiere precisamente a la función social de la misma. El supuesto básico es que todos los elementos de una sociedad (entre los que la cultura es uno más) existen porque son necesarios. Esta perspectiva ha sido desarrollada tanto en antropología como en sociología aunque, sin duda, sus primeras características fueron delineadas involuntariamente por Émile Durkheim. Este sociólogo francés muy pocas veces empleó el término como unidad analítica principal de su disciplina. En su libro Las reglas del método sociológico (1895), plantea que la sociedad está compuesta por entidades que tienen una función específica, integradas en un sistema análogo al de los seres vivos, donde cada órgano está especializado en el cumplimiento de una función vital. Del mismo modo en que los órganos de un cuerpo son susceptibles a la enfermedad, las instituciones y costumbres, las creencias y las relaciones sociales también pueden caer en un estado de anomia. Durkheim y sus seguidores, sin embargo, no se ocupan exclusiva ni principalmente de la cultura como objeto de estudio, sino de hechos sociales. A pesar de ellos, sus propuestas analíticas fueron retomadas por autores conspicuos de la antropología social británica y la sociología de la cultura de Estados Unidos. Más tarde, el polaco Bronislaw Malinowski retomó tanto la descripción de cultura de Tylor como algunos de los planteamientos de Durkheim relativos a la función social. Para Malinowski, la cultura podía ser entendida como una «realidad sui generis» que debía estudiarse como tal (en sus propios términos). En la categoría de cultura incluía artefactos, bienes, procesos técnicos, ideas, hábitos y valores heredados (Thompson, 2002: 193). También consideraba que la estructura social podía ser entendida análogamente a los organismos vivos pero, a diferencia de Durkheim, Malinowski tenía una tendencia más holística. Malinowski creía que todos los elementos de la cultura poseían una función que les daba sentido y hacía posible su existencia. Pero esta función no era dada únicamente por lo social, sino por la historia del grupo y el entorno geográfico, entre muchos otros elementos. El reflejo más claro de este pensamiento aplicado al análisis teórico fue el libro Los argonautas del Pacífico Occidental (1922), una extensa y detallada monografía sobre las distintas esferas de la cultura de los isleños trobriandeses, un pueblo que habitaba en las islas Trobriand, al oriente de Nueva Guinea. Años más tarde, Alfred Reginald Radcliffe-Brown, también antropólogo británico, retomaría algunas de las propuestas de Malinowski, y muy especialmente las que se referían a la función social. Radcliffe-Brown rechazaba que el campo de análisis de la antropología fuera la cultura, más bien se encargaba del estudio de la estructura social, un entramado de relaciones entre las personas de un grupo. Sin embargo, también analizó aquellas categorías que habían sido descritas con anterioridad por Malinowski y Tylor, siguiendo siempre el principio del análisis científico de la sociedad. En su libro Estructura y función en la sociedad primitiva (1975) Radcliffe-Brown establece que la función más importante de las creencias y prácticas sociales es la del mantenimiento del orden social, el equilibrio en las relaciones y la trascendencia del grupo en el tiempo. Sus propuestas fueron retomadas más tarde por muchos de sus alumnos, especialmente por Edward Evan Evans-Pritchard etnógrafo de los nuer y los azande, pueblos del centro de África. En ambos trabajos etnográficos, la función reguladora de las creencias y prácticas sociales está presente en el análisis de esas sociedades, a la primera de las cuales, Evans-Pritchard llamó “anarquía ordenada”. Definiciones simbólicas Los orígenes de las concepciones simbólicas de cultura se remontan a Leslie White, antropólogo estadounidense formado en la tradición culturalista de Boas. A pesar de que en su libro La ciencia de la cultura afirma, en un principio, que esta es «el nombre de un tipo preciso o clase de fenómenos, es decir, las cosas y los sucesos que dependen del ejercicio de una habilidad mental, exclusiva de la especie humana, que hemos llamado 'simbolizante'», en el transcurso de su texto, White irá abandonando la idea de la cultura como símbolos para orientarse hacia una perspectiva ecológica.[21]​ Definición estructuralista Según la teoría estructuralista, la mente humana clasifica todos los fenómenos del mundo, estableciendo conjuntos clasificatorios a los que se adhieren cargas semánticas (se convierten en símbolos). Por ejemplo, Héritier proponía que un par de grupos clasificatorios universal es el que distingue varones de mujeres, basado en las diferencias fisiológicas. Lo que cambia son las atribuciones de cada grupo: en algunas culturas, como la occidental, la mujer se encarga de criar a los niños; en otras, esta tarea corresponde a los varones. El estructuralismo es una corriente más o menos extendida en las ciencias sociales. Sus orígenes se remontan a Ferdinand de Saussure, lingüista, quien propuso grosso modo que la lengua es un sistema de signos. Tras su conversión a la antropología (tal como la llama en Tristes trópicos), Claude Lévi-Strauss –influido por Roman Jakobson– habría de retomar este concepto para el estudio de los hechos de interés antropológico, entre los que la cultura era solo uno más. De acuerdo con Lévi-Strauss, la cultura es básicamente un sistema de signos[22] producidos por la actividad simbólica de la mente humana (tesis que comparte con White). En Antropología estructural (1958) Lévi-Strauss irá definiendo las relaciones que existen entre los signos y símbolos del sistema, y su función en la sociedad, sin prestar demasiada atención a este último punto. En resumen, se puede decir que en la teoría estructuralista, la cultura es un mensaje que puede ser decodificado tanto en sus contenidos, como en sus reglas. El mensaje de la cultura habla de la concepción del grupo social que la crea, habla de sus relaciones internas y externas. En El pensamiento salvaje (1962), Lévi-Strauss apunta que todos los símbolos y signos de que está hecha la cultura son productos de la misma capacidad simbólica que poseen todas las mentes humanas. Esta capacidad, básicamente consiste en la clasificación de las cosas del mundo en grupos, a los que se atribuyen ciertas cargas semánticas. No existe un grupo de símbolos o signos (campo semántico) que no tenga uno complementario. Los signos y sus significados pueden ser asociados por metáfora (como en el caso de las palabras) o metonimia (como en el caso de los emblemas de la realeza) a fenómenos significativos para el grupo creador del sistema cultural. Las asociaciones simbólicas no necesariamente son las mismas en todas las culturas. Por ejemplo, mientras en la cultura occidental, el rojo es el color del amor, en Mesoamérica es el de la muerte. Según la propuesta estructuralista, las culturas de los pueblos “primitivos” y “civilizados” están hechas de la misma materia y, por tanto, los sistemas del conocimiento del mundo exterior dominantes en cada uno —magia en los primeros, ciencia en los segundos—– no son radicalmente diferentes. Aunque son varias las distinciones que se pueden establecer entre culturas primitivas y modernas: una de las más importantes es el modo en que manipulan los elementos del sistema. En tanto que la magia improvisa, la ciencia procede sobre la base del método científico.[23] El uso del método científico no quiere decir —según Lévi-Strauss— que las culturas donde la ciencia es dominante sean superiores, o que aquellas donde la magia juega un papel fundamental sean menos rigurosas o metódicas en su manera de conocer el mundo. Simplemente, son de índole distinta unas de otras, pero la posibilidad de comprensión entre ambos tipos de culturas radica básicamente en una facultad universal del género humano. En la perspectiva estructuralista, el papel de la historia en la conformación de la cultura de una sociedad no es tan importante. Lo fundamental es llegar a dilucidar las reglas que subyacen en la articulación de los símbolos en una cultura, y observar la manera en que estos dotan de sentido la actuación de una sociedad. En varios textos, Lévi-Strauss y sus seguidores (como Edmund Leach) parecen insinuar, como Ruth Benedict, que la cultura es una suerte de patrón que pertenece a todo el grupo social pero no se encuentra en nadie en particular. Esta idea también fue retomada del concepto de lenguaje propuesto por Saussure. Definición de la antropología simbólica La antropología simbólica es una rama de las ciencias sociales cuyo desarrollo se relaciona con la crítica al estructuralismo lévi-straussiano. Uno de los principales exponentes de esta corriente es Clifford Geertz. Comparte con el estructuralismo francés la tesis de la cultura como un sistema de símbolos pero, a diferencia de Lévi-Strauss, Geertz señala que no es posible para los investigadores el conocimiento de sus contenidos: Al creer tal como Max Weber que el hombre es un animal suspendido en tramas de significación tejidas por él mismo, consideró que la cultura se compone de tales tramas, y que el análisis de ésta no es, por tanto, una ciencia experimental en busca de leyes, sino una ciencia interpretativa en busca de significado. (Geertz, 1988:) Bajo la premisa anterior, Geertz y la mayor parte de los antropólogos simbólicos ponen en duda la autoridad de la etnografía. Señalan que a lo que pueden limitarse los antropólogos es a hacer “interpretaciones plausibles” del significado de la trama simbólica que es la cultura, a partir de la descripción densa de la mayor cantidad de puntos de vista que sea posible conocer respecto a un mismo suceso. En otro sentido, los simbólicos no creen que todos los elementos de la trama cultural posean el mismo sentido para todos los miembros de una sociedad. Más bien creen que pueden ser interpretados de modos diferentes, dependiendo, ya de la posición que ocupen en la estructura social, ya de condicionamientos sociales y psíquicos anteriores, o bien, del mismo contexto.[24]​ Definiciones marxistas Tal como se señaló anteriormente, Karl Marx a pesar de la opinión generalizada, puso atención en el análisis de las cuestiones culturales, específicamente en su relación con el resto de la estructura social. Según la propuesta teórica de Marx, el dominio de lo cultural (constituido sobre todo por la ideología) es un reflejo de las relaciones sociales de producción, es decir, de la organización que adoptan los seres humanos frente a la actividad económica. La gran aportación del marxismo en el análisis de la cultura es que esta es entendida como el producto de las relaciones de producción, como un fenómeno que no está desligado del modo de producción de una sociedad. Asimismo, la considera como uno de los medios por los cuales se reproducen las relaciones sociales de producción, que permiten la permanencia en el tiempo de las condiciones de desigualdad entre las clases. En sus interpretaciones más simplistas, la definición de la ideología en Marx ha dado lugar a una tendencia a explicar las creencias y el comportamiento social en función de las relaciones que se establecen entre quienes dominan el sistema económico y sus subalternos. Sin embargo, son muchas las posturas donde la relación entre la base económica y la superestructura cultural es analizada en enfoques más amplios. Por ejemplo, Antonio Gramsci llama la atención a la hegemonía, un proceso por medio del cual, un grupo dominante se legitima ante los dominados, y estos terminan por ver natural y asumir como deseable la dominación. Louis Althusser propuso que el ámbito de la ideología (el principal componente de la cultura) es un reflejo de los intereses de la élite, y que a través de los aparatos ideológicos del Estado se reproducen en el tiempo. Así mismo, Michel Foucault –en el conocido debate de noviembre de 1971 en Países Bajos con Noam Chomsky– respondiendo la pregunta de que si la sociedad capitalista era democrática, además de contestar negativamente –argumentando que una sociedad democrática se basa en el efectivo ejercicio del poder por una población que no esté dividida u ordenada jerárquicamente en clases– sostiene que, de manera general, todos los sistemas de enseñanza –los cuales aparecen simplemente como transmisores de conocimientos aparentemente neutrales–, están hechos para mantener a cierta clase social en el poder, y excluir de los instrumentos de poder a otras clases sociales. Definición neoevolucionista o ecofuncionalista Plataforma petrolera en el mar del Norte. White proponía que la energía de que dispusiera una sociedad es la que determina en buena medida la cultura. Occidente, por ejemplo, ha modificado sus tecnologías para poder aprovechar diversas fuentes energéticas a lo largo de su historia. La mayor cantidad de energía disponible ha permitido a su vez el desarrollo de nuevas tecnologías, creencias y formas de relaciones sociales. Sin embargo, como señalan Rappaport y Morán, es posible que la expansión en el consumo energético produzca una desadaptación ecológica y conduzca a la civilización Occidental a su desaparición. Si bien el estudio de la cultura nació como una inquietud por el cambio de las sociedades a lo largo del tiempo, el desprestigio en el que cayeron los primeros autores de la antropología fue un terreno fértil para que arraigaran en la reflexión sobre la cultura las concepciones ahistóricas. Salvo los marxistas, interesados en el proceso revolucionario hacia el socialismo, el resto de las disciplinas sociales no prestaron mayor atención al problema de la evolución cultural. Para introducir las definiciones neoevolucionistas de cultura, es necesario recordar que los evolucionistas sociales de finales del siglo XIX (representados, entre otros, por Tylor), pensaban que las sociedades “primitivas” de su época eran residuos de antiguas formas culturales, por las que necesariamente habría pasado la civilización de Occidente antes de llegar a ser lo que era en ese momento. Como se indicó antes, Boas y sus discípulos echaron por tierra estos argumentos, señalando que nada probaba la veracidad de estas suposiciones. Sin embargo, en Estados Unidos, hacia la década de 1940 tuvo lugar un nuevo viraje del enfoque temporal de la antropología. Este nuevo rumbo es el neoevolucionista, interesado entre otras cosas, por el cambio sociocultural y las relaciones entre cultura y medio ambiente. White y Steward Según el neoevolucionismo, la cultura es el producto de las relaciones históricas entre un grupo humano y su medio ambiente. De esta manera se pueden resumir las definiciones de cultura propuestas por Leslie White (1992) y Julian Steward (1992), quienes encabezaron la corriente neoevolucionista en su nacimiento.[25] El énfasis de la nueva corriente antropológica se movió del funcionamiento de la cultura a su carácter dinámico. Este cambio de paradigma representa una clara oposición al funcionalismo estructuralista, interesado en el funcionamiento actual de la sociedad; y el culturalismo, que aplazaba el análisis histórico para un momento en que los datos etnográficos lo permitieran. Tanto Steward como White concuerdan en que la cultura es solo uno de los ámbitos de la vida social. Para White, la cultura no es un fenómeno que deba entenderse en sus propios términos, como proponían los culturalistas. El aprovechamiento energético es el motor de las transformaciones culturales: estimula la transformación de la tecnología disponible, tendiendo siempre a mejorar. Así, la cultura está determinada por la forma en la que el grupo humano aprovecha su entorno. Este aprovechamiento se traduce a su vez en energía. El desarrollo de la cultura de un grupo es proporcional la cantidad de energía que la tecnología disponible le permite aprovechar. La tecnología determina las relaciones sociales y esencialmente la división del trabajo como una prístina forma de organización. A su vez, la estructura social y la división del trabajo se reflejan en el sistema de creencias del grupo, que formula conceptos que le permiten comprender el entorno que le rodea. Una modificación en la tecnología y la cantidad de energía aprovechada se traduce, por tanto, en modificaciones en todo el conjunto. Steward, por su parte, retomaba de Kroeber la concepción de la cultura como un hecho que se encontraba por encima y fuera de la naturaleza. Sin embargo, Steward sostenía que había un diálogo entre ambos dominios. Opinaba que la cultura es un fenómeno o capacidad del ser humano que le permite adaptarse a su medio biológico. Uno de los principales conceptos en su obra es el de evolución. Steward planteaba que la cultura sigue un proceso de evolución multilineal (es decir, no todas las culturas pasan de un estado salvaje a la barbarie, y de ahí a la civilización), y que este proceso se basa en el desarrollo de tipos culturales derivados de las adaptaciones culturales al medio físico de una sociedad. Steward introduce en las ciencias sociales el término de ecología, señalando con él: el análisis de las relaciones existentes entre todos los organismos que comparten un mismo nicho ecológico. Marvin Harris y el materialismo cultural Dentro del tipo de ideas introducidas por White y Steward, cabe señalar el materialismo cultural propugnado por Marvin Harris y otros antropólogos estadounidenses. Esta corriente puede ser asimilada a una forma de ecofuncionalismo en el que se encajan ciertas divisiones introducidas por Marx. Para el materialismo cultural, entender la evolución cultural y la configuración de las sociedades depende básicamente de condiciones materiales, tecnológicas e infraestructurales. El materialismo cultural establece una triple división entre grupos de conceptos que atiende a su relación causal. Esos grupos se llaman: infraestructura (modo de producción, tecnología, condiciones geográficas, etc.), estructura (modo de organización social, estructura jerárquica, etc.) y supraestructura (valores religiosos y morales, creaciones artísticas, leyes, etc.). Evolución cultural Había por lo menos una gran distancia conceptual entre la propuesta de White y de Steward. El primero se inclinaba por el estudio de la cultura como fenómeno total, en tanto que el segundo se mantenía más proclive al relativismo. Por ello, entre las limitaciones que tuvieron que superar sus sucesores estuvo la de concatenar ambas posturas, para unificar la teoría de los estudios de la ecología cultural. De esta suerte, Marshall Sahlins propuso que la evolución cultural sigue dos direcciones. Por un lado, crea diversidad “a través de una modificación de adaptación: las nuevas formas se diferencian de las viejas. Por otra parte, la evolución genera progreso: las formas superiores surgen de las inferiores y las sobrepasan”.[26]​ La idea de que la cultura se transforma siguiendo dos líneas simultáneas fue desarrollada por Darcy Ribeiro, que introdujo el concepto de proceso civilizatorio[27] para comprender las transformaciones de la cultura. Con el tiempo, el neoevolucionismo sirvió como una de las principales bisagras entre las ciencias sociales y las ciencias naturales, especialmente como puente con la biología y la ecología. De hecho, su propia vocación como enfoque holístico le ha convertido en una de las corrientes más interdisciplinarias de las disciplinas que estudian la humanidad. A partir de la década de 1960, la ecología entró en una relación muy estrecha con los estudios culturales de corte evolutivo. Los biólogos habían descubierto que los seres humanos no son los únicos animales que poseen cultura: se habían encontrado indicios de ella entre algunos cetáceos, pero especialmente entre los primates. Roy Rappaport introdujo en la discusión de lo social la idea de que la cultura forma parte de la misma biología del ser humano, y que la evolución misma del ser humano se debe a la presencia de la cultura. Señalaba que: ...superorgánica o no, se debe tener presente que la cultura en sí pertenece a la naturaleza. Emergió en el curso de la evolución mediante procesos de selección natural diferentes sólo en parte de aquellos que produjeron los tentáculos del pulpo […] Aunque la cultura está altamente desarrollada en los seres humanos, estudios etológicos recientes han indicado alguna capacidad simbólica entre otros animales. […] Aunque las culturas pueden imponerse a los sistemas ecológicos, hay límites para esas imposiciones, ya que las culturas y sus componentes están sujetos a su vez a procesos selectivos. (Rappaport, 1998: 273-274) Los nuevos descubrimientos en la etología (ciencia que estudia el comportamiento de los animales) animaron a muchos biólogos a intervenir en el debate sociológico de la cultura. Algunos de ellos buscaban establecer relaciones entre la cultura humana y las formas primitivas de cultura observadas, por ejemplo, entre los macacos de Japón. Uno de los ejemplos más conocidos es el de Sherwood Washburn, profesor de antropología de la Universidad de California. Al frente de un equipo multidisciplinario, emprendió la tarea de buscar cuáles eran los orígenes de la cultura humana. Como primera parte de su proyecto, analizó el comportamiento social de los primates superiores. En segundo lugar, suponiendo que los bosquimanos !kung eran los últimos reductos de las formas más primitivas de cultura humana, procedió al estudio de su cultura. La tercera etapa del programa de Washburn (en el que colaboraron Richard Lee e Irven de Vore, y que se prolongó durante la primera mitad de los años sesenta) fue proceder a la comparación de los resultados de ambas investigaciones, y especuló sobre esta base acerca de la importancia de la cacería en la construcción de la sociedad y la cultura. Esta hipótesis fue presentada en un congreso llamado Man, the Hunter, realizado en la Universidad de Chicago en 1966. Fuera porque la investigación se apoyaba en premisas sobre la evolución cultural que fueron desechadas desde los tiempos de Boas, o porque era una tesis que negaba la importancia de la mujer en la construcción de la cultura, la tesis de Washburn, Lee y De Vore no fue bien recibida.[28]​ Esta definición, atiende a la característica principal de la cultura, que es una obra estrictamente de creación humana, a diferencia de los procesos que realiza la naturaleza, por ejemplo, el movimiento de la tierra, las estaciones del año, los ritos de apareamiento de las especies, las mareas e incluso la conducta de las abejas que hacen sus panales, elaboran miel, se orientan para encontrar el camino de regreso pero, que a pesar de eso, no constituyen una cultura, pues todas las abejas del mundo hacen exactamente lo mismo, de manera mecánica, y no pueden cambiar nada. Exactamente lo contrario ocurren en el caso de las obras, ideas y actos humanos, ya que estos transforman o se agregan a la naturaleza, por ejemplo, el diseño de una casa, la receta de un dulce de miel o de chocolate, la elaboración de un plano, la simple idea de las relaciones matemáticas, son cultura y sin la creación humana no existirían por obra de la naturaleza. En 1998, Jesús Mosterín publicó su libro ¡Vivan los animales!, donde explica qué es la cultura:[29]​ La cultura no es un fenómeno exclusivamente humano, sino que está bien documentada en muchas especies de animales superiores no humanos. Y el criterio para decidir hasta qué punto cierta pauta de comportamiento es natural o cultural no tiene nada que ver con el nivel de complejidad o de importancia de dicha conducta, sino sólo con el modo como se trasmite la información pertinente a su ejecución. […] Los chimpancés son animales muy culturales. Aprenden a distinguir cientos de plantas y sustancias, y a conocer sus funciones alimentarias y astringentes. Así logran alimentarse y contrarrestar los efectos de los parásitos. Tienen muy poco comportamiento instintivo o congénito. No existe una 'cultura de los chimpancés' común a la especie. Cada grupo tiene sus propias tradiciones sociales, venatorias, alimentarias, sexuales, instrumentales, etc. […] La cultura es tan importante para los chimpancés, que todos los intentos de reintroducir en la selva a los chimpancés criados en cautividad fracasan lamentablemente. Los chimpancés no sobreviven. Les falta la cultura. No saben qué comer, cómo actuar, cómo interaccionar con los chimpancés silvestres, que los atacan y matan. Ni siquiera saben cómo hacer cada noche su alto nido-cama para dormir sin peligro en la copa de un árbol. Durante los cinco años que el pequeño chimpancé duerme con su madre tiene unas 2.000 oportunidades de observar cómo se hace el nido-cama. Los chimpancés hembras separados de su grupo y criados con biberón en el zoo ni siquiera saben cómo cuidar a sus propias crías, aunque lo aprenden si ven películas o vídeos de otros chimpancés criando (Jesús Mosterín, ¡Vivan los animales! 1998: 146-7, 151-2) Definición de cultura en la Iglesia católica La definición clásica de cultura en la Iglesia católica se encuentra en el concilio Vaticano II: Con la palabra cultura se indica, en sentido general, todo aquello con lo que el hombre afina y desarrolla sus innumerables cualidades espirituales y corporales; procura someter el mismo orbe terrestre con su conocimiento y trabajo; hace más humana la vida social, tanto en la familia como en toda la sociedad civil, mediante el progreso de las costumbres e instituciones; finalmente, a través del tiempo expresa, comunica y conserva en sus obras grandes experiencias espirituales y aspiraciones para que sirvan de provecho a muchos, e incluso a todo el género humano. (Constitución dogmática Gaudium et spes, 1965, n. 53) En la definición destacan dos aspectos: el poner al individuo al centro, siendo la cultura un producto del hombre y al servicio del hombre; y el conjugar la formación de cada persona a través de la cultura, con la contribución específica de una comunidad al progreso de la humanidad. Este concepto de cultura es la base para explicar el proceso de la inculturación o inserción de la Iglesia católica en una cultura y expresión del cristianismo en una nueva modalidad y culturalidad. El concepto científico de cultura El concepto científico de cultura hizo uso desde el principio de ideas procedentes de la teoría de la información, de la noción de meme introducida por Richard Dawkins, de los métodos matemáticos desarrollados en la genética de poblaciones por autores como Luigi Luca Cavalli-Sforza y de los avances en la comprensión del cerebro y del aprendizaje. Diversos antropólogos, como William Durham, y filósofos, como Daniel Dennett y Jesús Mosterín, han contribuido decisivamente al desarrollo de la concepción científica de la cultura. Mosterín define la cultura como la información transmitida por aprendizaje social entre animales de la misma especie. Como tal, se contrapone a la naturaleza, es decir, a la información transmitida genéticamente. Si los memes son las unidades o trozos elementales de información adquirida, la cultura actual de un individuo en un momento determinado sería el conjunto de los memes presentes en el cerebro de ese individuo en ese momento. A su vez, la noción vaga de cultura de un grupo social es analizada por Mosterín en varias nociones precisas distintas, definidas todas ellas en función de los memes presentes en los cerebros de los miembros del grupo.[30]​ Industria cultural La industria cultural la define la UNESCO como aquella que produce y distribuye bienes o servicios culturales que, «considerados desde el punto de vista de su calidad, utilización o finalidad específicas, encarnan o transmiten expresiones culturales, independientemente del valor comercial que puedan tener. Las actividades culturales pueden constituir una finalidad de por sí, o contribuir a la producción de bienes y servicios culturales».[31]​ Socialización de la cultura La importante aportación de la psicología humanista de, por ejemplo, Erik Erikson con una teoría psicosocial para explicar los componentes socioculturales del desarrollo personal. Cada miembro de la especie podría acceder a ella desde una fuente común, sin limitarse, ejemplo de ello: el conocimiento transmitido por los padres. Debe poder ser incrementada en las ulteriores generaciones. Ha de resultar universalmente compartible por todos aquellos que poseen un lenguaje racional y significativo. Así, el ser humano tiene la facultad de enseñar al animal, desde el momento en que es capaz de entender su rudimentario aparato de gestos y sonidos, llevando a cabo nuevos actos de comunicación; pero los animales no pueden hacer algo parecido con nosotros. De ellos podemos aprender por la observación, como objetos, pero no mediante el intercambio cultural, es decir, como sujetos. Clasificación La cultura se clasifica, respecto a sus definiciones, de la siguiente manera: Tópica: La cultura consiste en una lista de tópicos o categorías, tales como organización social, religión o economía. Histórica: La cultura es la herencia social, es la manera que los seres humanos solucionan problemas de adaptación al ambiente o a la vida en común. Mental: La cultura es un complejo de ideas, o los hábitos aprendidos, que inhiben impulsos y distinguen a las personas de los demás. Estructural: La cultura consiste en ideas, símbolos o comportamientos, modelados o pautados e interrelacionados. Simbólica: La cultura se basa en los significados arbitrariamente asignados que son compartidos por una sociedad. La cultura puede también ser clasificada del siguiente modo: Según su extensión Universal: cuando es tomada desde el punto de vista de una abstracción a partir de los rasgos que son comunes en las sociedades del mundo. Por ej., el saludo. Total: conformada por la suma de todos los rasgos particulares a una misma sociedad. Particular: igual a la subcultura; conjunto de pautas compartidas por un grupo que se integra a la cultura general y que a su vez se diferencia de ellas. Ej.: las diferentes culturas en un mismo país. Según su desarrollo Primitiva: aquella cultura que mantiene rasgos precarios de desarrollo técnico y que por ser conservadora no tiende a la innovación. Civilizada: cultura que se actualiza produciendo nuevos elementos que le permitan el desarrollo a la sociedad. Analfabeta o prealfabeta: se maneja con lenguaje oral y no ha incorporado la escritura ni siquiera parcialmente. Alfabeta: cultura que ha incorporado el lenguaje tanto escrito como oral. Según su carácter dominante Sensista: cultura que se manifiesta exclusivamente por los sentidos y es conocida a partir de los mismos. Racional: cultura donde impera la razón y es conocido a través de sus productos tangibles. Ideal: se construye por la combinación de la sensista y la racional. Según su dirección Posfigurativa: aquella cultura que mira al pasado para repetirlo en el presente. Cultura tomada de nuestros mayores sin variaciones. Es generacional y se da particularmente en pueblos primitivos. Configurativa: la cultura cuyo modelo no es el pasado, sino la conducta de los contemporáneos. Los individuos imitan modos de comportamiento de sus pares y recrean los propios. Elementos de la cultura La cultura forma todo lo que implica transformación y seguir un modelo de vida. Los elementos de la cultura se dividen en: a) Materiales: Son todos los objetos, en su estado natural o transformados por el trabajo humano, que un grupo esté en condiciones de aprovechar en un momento dado de su devenir histórico: tierra, materias primas, fuentes de energía, herramientas, utensilios, productos naturales y manufacturados, etcétera. b) De organización: Son las formas de relación social sistematizadas, a través de las cuales se hace posible la participación de los miembros del grupo cuya intervención es necesaria para cumplir la acción. La magnitud y otras características demográficas de la población son datos importantes que deben tomarse en cuenta al estudiar los elementos de organización de cualquier sociedad o grupo. c) De conocimiento: Son las experiencias asimiladas y sistematizadas que se elaboran, es decir, los conocimientos, las ideas y las creencias que se acumulan y trasmiten de generación a generación y en el marco de las cuales se generan o incorporan nuevos conocimientos. d) De conducta: Son los comportamientos o las pautas de conducta comunes a un grupo humano. e) Simbólicos: Son los diferentes códigos que permiten la comunicación necesaria entre los participantes en los diversos momentos de una acción. El código fundamental es el lenguaje, pero hay otros sistemas simbólicos significativos que también deben ser compartidos para que sean posibles ciertas acciones y resulten eficaces. f) Emotivos: que también pueden llamarse subjetivos. Son las representaciones colectivas, las creencias y los valores integrados que motivan a la participación y/o la aceptación de las acciones: la subjetividad como un elemento cultural indispensable. g) Pautada: Son sistemas integrados. Una persona no representa una cultura, pero si todo un grupo amplio. Dentro de toda cultura hay dos elementos a tener en cuenta: Rasgos culturales: porción más pequeña y significativa de la cultura, que da el perfil de una sociedad. Todos los rasgos se transmiten siempre al interior del grupo y cobran fuerza para luego ser exteriorizados. Complejos culturales: contienen en sí los rasgos culturales en la sociedad. Cambios culturales Artículo principal: Evolución cultural Los cambios culturales: son los cambios a lo largo del tiempo de todos o algunos de los elementos culturales de una sociedad (o una parte de la misma). Enculturación: es el proceso en el que el individuo se culturiza, es decir, el proceso en el que el ser humano, desde que es niño o niña, se culturiza. Este proceso es parte de la cultura, y como la cultura cambia constantemente, también lo hacen la forma y los medios con los que se culturaliza. Aculturación: se da normalmente en momento de conquista o de invasión. Es normalmente de manera forzosa e impuesta, como la conquista de América, la invasión de Irak. Ejemplos de resultados de este fenómeno: comida (potaje, pozole), huipil. El fenómeno contrario recibe el nombre de deculturación, y consiste en la pérdida de características culturales propias a causa de la incorporación de otras foráneas. Transculturación: La transculturación es un fenómeno que ocurre cuando un grupo social recibe y adopta las formas culturales que provienen de otro grupo. Inculturación: se da cuando la persona se integra a otras culturas, las acepta y dialoga con la gente de esa determinada cultura. La cultura está basada en todos nosotros. Historia del arte cultural Pintura rupestre: La historia del arte comenzó desde la Edad de Piedra dividiéndose en el Paleolítico, mesolítico y neolítico, hasta la Edad de los Metales. Se considera un periodo en el que surgieron las primeras manifestaciones consideradas como artísticas por parte del Homo sapiens, llamadas “pinturas rupestres”, en las cuevas a mediados del ( 25000-8000 a. C. ). En estas pinturas se veían reflejadas la cacería, la agricultura y la divinidad, como muestra de ello su arte era reflejada en hueso, madera y en esculturas de piedra. Escritura jeroglífica: En la Edad Antigua se desarrollan las primeras muestras de escritura por la necesidad de llevar registros comerciales y económicos. Su primer indicio fue la escritura cuneiforme que era practicada en tablillas de arcilla, basada con elementos pictográficos e ideográficos, seguido de esto aparece la escritura jeroglífica basada en la lengua hebrea la cual se utilizó como método de escritura del alfabeto que relacionaban los fonemas con cada símbolo. En esta etapa se presenta el arte mesopotámico, arte del antiguo Egipto, arte precolombino, arte africano, arte de la india, y el arte de china. Arte clásico: En el arte clásico es desarrollado en Grecia y Roma, donde se representa la naturaleza y la armonía humana, sus bases provienen del arte occidental. El arte griego se divide en tres periodos: arcaico, clásico y helenístico. Sus pinturas eran desarrolladas más que todo en cerámica. Sus mitos griegos llamaron la atención con la fusión de elementos indogermánicos y mediterráneos. Arte medieval: En el arte medieval se marca por la caída del imperio romano de occidente. El arte clásico es reinterpretado de manera que el cristianismo como nueva religión se encarga de la mayor parte de la producción artística medieval. Este arte se divide en el arte paleocristiano, el arte germánico, el arte prerrománico, el arte bizantino, el arte islámico, el arte románico, el arte gótico. Arte moderno: En el arte de la Edad Moderna suele darse como sinónimo del arte contemporáneo el cual se desarrolló en el siglo XV y XVIII, sus cambios se realizaron a nivel político, económico, social, y cultural. Su primera aparición fue con las vidas de girgio vasari en el texto inaugural del estudio del arte con carácter historiográfico. Arte hilemorfista: En este momento de nuestra historia estamos viviendo una gran variedad de cambios no solo sociopolíticos sino culturales. Así mismo, la visión que tenemos hoy en día del arte, ha sido modificada por todos esos cambios, y hoy conocemos lo que sería el “Arte Hilemorfista” que pretende romper la relación convencional entre materia y forma ha sido adoptado por las culturas latinoamericanas como una medida de protesta. Ya que en esencia este sentido hilemorfista lo que busca es deconstruir la concepción de conceptos convencionales creados por esta relación entre materia y forma, y darnos la capacidad de pensar en otras posibilidades del mundo. Y así como lo dice el famoso pintor y artista, Luis Camnitzer en su “Concepto latino”: El arte debe ir a la realidad y transformarla, reemplazarla. Un ejemplo claro de este arte hilemorfista actual es Lotty Rosenfeld, una artista visual chilena, adscrita al neo-vanguardismo. Quien por medio de sus obras modifica la realidad, llevando su arte a las calles y transformando la relación convencional que existe entre materia y forma. Tal es “Un millar de cruces sobre el pavimento” Que fue una obra hecha en la época de la dictadura militar chilena en la que Rosenfeld dibujaba una cruz utilizando las líneas de señalización de las calles y colocándose una línea transversal por cada desaparecido que había de la dictadura, haciendo una contra información a los medios tradicionales de ese momento que negaban la realidad en la que estaba consumido el país, y que no daban cifras o estadísticas de los desaparecidos que había. Relación entre cultura y lingüística Un elemento esencial de la cultura es el lenguaje, hay conceptos culturales dentro de los diferentes sistemas en los que encontramos características específicas de gramática y léxico. Por tanto, incluir la lingüística en el estudio de las culturas antiguas y contemporáneas es indispensable. El análisis de la cultura desde la lingüística en los últimos sesenta años ha evolucionado desde el pensamiento estructuralista hasta la variación cultural.[32] Los lingüistas también han desarrollado una investigación sobre la comunicación intercultural, y recientemente han creado conceptos nuevos tales como multilingüismo y multiculturalismo para definir nuevos fenómenos culturales.[33]​ Véase también -cultura Progreso Teorías sobre la cultura Antropología cultural Asimilación cultural Comunicación intercultural Difusión cultural Etnocentrismo Evolución cultural Geografía cultural Materialismo cultural Meme Relativismo cultural Revolución Cultural Sociología de la cultura Otras cuestiones culturales Categoría:Subculturas Alta cultura Artes y tradiciones populares Cultura de la violación Cibercultura Civilización Cultura científica Cultura organizacional Cultura popular Decondicionamiento Las dos culturas Leyenda urbana Neocolonialismo Subcultura Tecnociencia"
ksampletext_wikipedia_anth_etnia: str = "Etnia. Una etnia del griego clásico ethnos, pueblo o nación es un conjunto de personas que se identifican con un origen genético, comunidad lingüística, cultural, etc. Según Anthony D. Smith, «las etnias se pueden definir como poblaciones humanas que comparten unos mitos sobre la ascendencia, unas historias y unas culturas y que se asocian con un territorio específico y tienen un sentimiento de solidaridad».[4]​ La pertenencia étnica tiende a definirse por una herencia cultural compartida, ascendencia, mito de origen, historia, patria, idioma o dialecto, sistemas simbólicos como religión, mitología y ritual, cocina, estilo de vestir, arte o apariencia física. Los grupos étnicos pueden compartir un espectro estrecho o amplio de ascendencia genética, según la identificación del grupo, y muchos grupos tienen ascendencia genética mixta.[5]​ Este concepto esencialista de etnia es actualmente discutido por la ciencia como definidor de las sociedades humanas, ya que no explica bien la realidad empírica, de la misma manera que sucede con el concepto de raza aplicada a seres humanos.[6]​[7]​[8]​[9] No es posible delimitar de manera nítida y objetiva grupos humanos en base al conjunto de estos criterios étnicos, ya que en realidad existe una intrincada y abigarrada red de rasgos culturales.[10]​Se ha propuesto en cambio otro concepto diferente que asimila este término al de grupo social, en el que existe una conciencia de diferenciación de otros grupos con base en algún elemento que puede ser muy diverso. Desde la religión, el grupo de edad, el territorio o la lengua, a la ideología política o la ocupación económica.[11]​ Otro término relacionado con esta materia es el de etnicidad.[12]​ Terminología El término étnico deriva del griego ἔθνος ethnos (más precisamente, del adjetivo ἐθνικός ethnikos,[13] que fue importado al latín como ethnicus). En español y hasta mediados del siglo XIX, étnico se usaba para significar pagano (en el sentido de naciones dispares que aún no participaban en la ecúmene cristiana), ya que la Septuaginta utilizaba ta ethne (las naciones) para traducir el hebreo goyim las naciones extranjeras, no hebreas, no judías.[14]​ El término griego en la antigüedad temprana (griego homérico) podía referirse a cualquier grupo grande, una hueste de hombres, una banda de camaradas, así como un enjambre o un rebaño de animales. En griego clásico, el término adquirió un significado comparable al concepto expresado actualmente por grupo étnico, traducido mayoritariamente como nación, tribu, un grupo único de personas; sólo en griego helenístico el término tendió a estrecharse aún más para referirse a naciones extranjeras o bárbaros en particular (de ahí el significado posterior de pagano, pagano).[15]​ En el siglo XIX, el término pasó a utilizarse en el sentido de peculiar de una tribu, raza, pueblo o nación, en un retorno al significado griego original. El sentido de grupos culturales diferentes, y en español grupo minoritario tribal, racial, cultural o nacional surge en las décadas de 1930 a 1940, [16]​sirviendo como sustituto del término raza que anteriormente había tomado este sentido pero que ahora estaba quedando en desuso debido a su asociación con el racismo ideológico. El término abstracto etnicidad se había utilizado como sinónimo de paganismo en el siglo XVIII, pero ahora pasó a expresar el significado de carácter étnico (registrado por primera vez en 1953). El término grupo étnico se registró por primera vez en 1935 y entró en el Oxford English Dictionary en 1972.[17] Dependiendo del contexto, el término nacionalidad puede utilizarse como sinónimo de etnia o como sinónimo de ciudadanía (en un Estado soberano). El proceso que da lugar a la aparición de una etnia se denomina etnogénesis, término utilizado en la literatura etnológica desde 1950, aproximadamente. El término también puede usarse con la connotación de algo único e inusualmente exótico (cf. un restaurante étnico, etc.), generalmente relacionado con culturas de inmigrantes más recientes, que llegaron después de que se estableciera la población dominante de una zona. Dependiendo de qué fuente de identidad de grupo se enfatice para definir la pertenencia, se pueden identificar los siguientes tipos de grupos (a menudo superpuestos mutuamente): Etnolingüístico, que hace hincapié en la lengua compartida, dialecto (y posiblemente escritura) – ejemplo: canadienses franceses Etnonacional, que hace hincapié en una entidad política o sentido de identidad nacional – ejemplo: austriacos Etnorracial, que hace hincapié en la apariencia física compartida basada en el fenotipo  – ejemplo: afroamericanos Etnorregional, que enfatiza un sentido local distintivo de pertenencia derivado de un relativo aislamiento geográfico  – ejemplo: isleños del sur de Nueva Zelanda Etnorreligioso, que hace hincapié en la afiliación compartida a una religión, confesión o secta en particular – ejemplo: sijs Etnocultural, que hace hincapié en una cultura o tradición compartida, que a menudo se solapa con otras formas de etnicidad – ejemplo: grupos itinerantes en Europa En muchos casos, más de un aspecto determina la pertenencia: por ejemplo, la etnia armenios puede definirse por la ciudadanía armenia, tener herencia armenia, el uso nativo de la lengua armenia o la pertenencia a la Iglesia Apostólica Armenia. Definición Históricamente, la palabra «etnia» proviene del adjetivo griego ethnikos. El adjetivo se deriva del sustantivo ethnos, que significa ‘gente o nación extranjera’. Etnia twa de Uganda. Las personas que se consideran miembros de un grupo étnico se sienten vinculados entre sí por un origen genético común y se sienten por ello parte de una comunidad familiar extendida que les impone formas de lealtad y solidaridad típicas de los vínculos familiares.[3]​ Lingüística La lengua suele utilizarse como primer factor clasificado de los grupos sin embargo,esta herramienta no ha estado exenta de manipulación política o error. Se debe señalar que existe un gran número de lenguas multiétnicas a la par que determinadas etnias son multilingües. Cultura Dos hombres kurdos de Turquía y un sacerdote ortodoxo. Fotografía de 1873. La delimitación cultural de un grupo étnico respecto a los grupos culturales de sus fronteras se hace un proceso de gran dificultad para el etnólogo (antropólogo), en especial en los grupos humanos altamente comunicados con grupos vecinos. Elie Kedourie es quizás el autor que más ha profundizado en el análisis de las diferencias entre multietnicidad y pluriculturalidad. Generalmente, se aprecia que los grupos étnicos comparten una creencia en un origen común y tienen una continuidad en el tiempo, es decir, una historia o tradición común y proyectan un futuro como pueblo. Esto se alcanza a través de la transmisión generacional de una lengua común, unas instituciones, unos valores y unos usos y costumbres que los distinguen de otras etnias. Si bien en determinadas culturas se entremezclan los factores étnicos y los políticos, no es imprescindible que un grupo étnico cuente con instituciones propias de gobierno para ser considerado como tal. La soberanía, por tanto, no define la etnia, si bien se admite la necesidad de una cierta proyección social común. Genética Mapa detallado de las etnias del mundo. Las etnias generalmente se remontan a mitos de fundación que revelan un parentesco más o menos remoto entre sus miembros. La genética actual puede confirmar o negar la existencia de esa relación genética. Clasificación Entre los principales grupos de pueblos se incluyen: Pueblos aborígenes australianos* Pueblos afroasiáticos Pueblos semitas Pueblo árabe Pueblo judío Pueblos bereberes Pueblos chádicos Pueblos cusitas Pueblos egipcios Pueblos omóticos Pueblos altaicos Pueblos mongoles Pueblos tunguses Pueblos túrquicos Pueblos amerindios* Pueblos guaraníes* Pueblos quechuas* Pueblos andamaneses* Pueblos austrasiáticos Pueblos austronesios Pueblos caucásicos Pueblos chucoto-camchatcos Pueblos dravídicos Pueblos esquimo-aleutianos Pueblos hurrito-urartianos Pueblos indoeuropeos Pueblo albanés Pueblo esloveno Pueblos armenios Pueblos bálticos Pueblos celtas + Pueblo gitano Pueblos dacios Pueblo magiar Pueblos eslavos Pueblos frigios Pueblos germánicos + Pueblos helénicos + Pueblos íberos + Pueblos indo-iranios Pueblos itálicos + Pueblos nenéticos Pueblos tocarios + Pueblos tracios Pueblos indoarios Pueblos iranios Pueblos anatolios Pueblos ilirios + Pueblos lusitanos + Pueblos peonios + Pueblos joisanos Pueblos na-dené Pueblo persa Pueblos níger-congo Pueblos atlánticos Pueblos cordofanos Pueblos dogón Pueblos ijoi Pueblos mandé Pueblos Volta-Congo Pueblos Adamawa-Ubangi Pueblos Benue-Congo Pueblos bantúes Pueblos yoruba Pueblos gur Pueblos kru Pueblos kwa Pueblos senufos Pueblos nilo-saharianos Pueblos paleosiberianos Pueblos papúes Pueblos sino-tibetanos Pueblos birmanotibetanos Etnias chinas Pueblos tai-kadai Pueblos tirsénicos Pueblos urálicos Pueblo kurdo extinto en peligro de extinción Por idioma Artículo principal: Familia de lenguas Mapa con la situación de las diferentes familias lingüísticas. Etnicidad y raza La diversidad racial de los grupos étnicos de Asia (título original: Asiatiska folk), Nordisk familjebok (1904). La etnia se utiliza como una cuestión de identidad cultural de un grupo, a menudo basada en la ascendencia, la lengua y las tradiciones culturales compartidas, mientras que la raza se aplica como una agrupación taxonómica, basada en las similitudes físicas entre los grupos. La raza es un tema más controvertido que la etnia, debido al uso político común del término. Ramón Grosfoguel (Universidad de California, Berkeley) sostiene que la identidad racial/étnica es un concepto y que los conceptos de raza y etnicidad no pueden utilizarse como categorías separadas y autónomas.[18]​ Antes de Weber (1864-1920), la raza y la etnia se consideraban principalmente dos aspectos de la misma cosa. Alrededor de 1900 y antes, predominaba la concepción primordialista de la etnicidad: se consideraba que las diferencias culturales entre los pueblos eran el resultado de rasgos y tendencias heredados.[19] Con la introducción por parte de Weber de la idea de etnicidad como construcción social, la raza y la etnicidad se dividieron más entre sí. En 1950, la declaración de la UNESCO La cuestión racial, firmada por algunos de los eruditos de renombre internacional de la época (entre ellos Ashley Montagu, Claude Lévi-Strauss, Gunnar Myrdal, Julian Huxley, etc.), decía: Los grupos nacionales, religiosos, geográficos, lingüísticos y culturales no coinciden necesariamente con los grupos raciales: y los rasgos culturales de tales grupos no tienen ninguna conexión genética demostrada con los rasgos raciales. Dado que habitualmente se cometen graves errores de este tipo cuando se utiliza el término raza en el lenguaje popular, sería mejor, cuando se habla de razas humanas, abandonar por completo el término raza y hablar de grupos étnicos.[20]​ En 1982, el antropólogo David Craig Griffith resumió cuarenta años de investigación etnográfica, argumentando que las categorías raciales y étnicas son marcadores simbólicos de las diferentes formas en que las personas de distintas partes del mundo han sido incorporadas a una economía global: Los intereses opuestos que dividen a las clases trabajadoras se refuerzan aún más mediante apelaciones a distinciones raciales y étnicas. Tales apelaciones sirven para asignar diferentes categorías de trabajadores a los peldaños de la escala de los mercados laborales, relegando a las poblaciones estigmatizadas a los niveles inferiores y aislando a los escalones superiores de la competencia desde abajo. El capitalismo no creó todas las distinciones de etnia y raza que funcionan para separar a unas categorías de trabajadores de otras. Sin embargo, es el proceso de movilización laboral bajo el capitalismo el que confiere a estas distinciones sus valores efectivos.[21]​ Según Wolf, las categorías raciales se construyeron e incorporaron durante el período del expansión mercantil europea, y las agrupaciones étnicas durante el período del expansión capitalista.[22]​ En 1977, Wallman escribió sobre el uso del término étnico en el lenguaje ordinario de Gran Bretaña y Estados Unidos. El término étnico connota popularmente [raza] en Gran Bretaña, sólo que con menos precisión, y con una carga de valor más ligera. En Norteamérica, en cambio, [raza] suele significar color, y los étnicos son los descendientes de inmigrantes relativamente recientes de países de habla no inglesa. [Etnia] no es un sustantivo en Gran Bretaña. En efecto, no hay etnias; sólo hay relaciones étnicas. En Estados Unidos, la OMB afirma que la definición de raza utilizada a efectos del censo estadounidense no es científica ni antropológica y tiene en cuenta características sociales y culturales, así como la ascendencia, utilizando metodologías científicas apropiadas que no son de referencia principalmente biológica o genética."


ksampletext_wikipedia_hist_historia: str = ""
ksampletext_wikipedia_hist_guerra: str = ""
ksampletext_wikipedia_hist_mesopotamia: str = ""
ksampletext_wikipedia_hist_edadmedia: str = ""

ksampletext_wikipedia_geog_pais: str = ""
ksampletext_wikipedia_geog_clima: str = ""
ksampletext_wikipedia_geog_geografiafisica: str = ""
ksampletext_wikipedia_geog_geografiahumana: str = ""

ksampletext_wikipedia_poli_politica: str = ""
ksampletext_wikipedia_poli_gobierno: str = ""
ksampletext_wikipedia_poli_ideologia: str = ""
ksampletext_wikipedia_poli_capitalismo: str = ""

ksampletext_wikipedia_juri_ley: str = ""
ksampletext_wikipedia_juri_constitucion: str = ""
ksampletext_wikipedia_juri_derecho: str = ""
ksampletext_wikipedia_juri_justicia: str = ""

ksampletext_wikipedia_econ_economia: str = ""
ksampletext_wikipedia_econ_dinero: str = ""
ksampletext_wikipedia_econ_teoriadelvalortrabajo: str = ""
ksampletext_wikipedia_econ_microeconomia: str = ""

ksampletext_wikipedia_phil_filosofia: str = ""
ksampletext_wikipedia_phil_fenomenologiatrascendental: str = ""
ksampletext_wikipedia_phil_metafisica: str = ""
ksampletext_wikipedia_phil_filosofiadelaciencia: str = ""

ksampletext_wikipedia_spor_deporte: str = ""
ksampletext_wikipedia_spor_juegosolimpicos: str = ""
ksampletext_wikipedia_spor_tenis: str = ""
ksampletext_wikipedia_spor_futbol: str = ""

ksampletext_wikipedia_vide_videojuego: str = ""
ksampletext_wikipedia_vide_videoconsola: str = ""
ksampletext_wikipedia_vide_nintendo: str = ""
ksampletext_wikipedia_vide_minecraft: str = ""

ksampletext_wikipedia_arts_musica: str = ""
ksampletext_wikipedia_arts_pintura: str = ""
ksampletext_wikipedia_arts_teoriadelarte: str = ""
ksampletext_wikipedia_arts_obradearte: str = ""

ksampletext_wikipedia_fict_cienciaficcion: str = ""
ksampletext_wikipedia_fict_literaturafantastica: str = ""
ksampletext_wikipedia_fict_shingekinokyojin: str = ""
ksampletext_wikipedia_fict_starwars: str = ""


###############################################################################################


ksampletext_wikipedia_bacilo: str = "Bacilo. En bacteriología: la palabra bacilo se usa para describir cualquier bacteria con forma de barra o vara, y pueden encontrarse en muchos grupos taxonómicos diferentes tipos de bacterias. Sin embargo el nombre Bacillus, se refiere a un género específico de bacteria. El otro nombre Bacilli; hace referencia a una clase de bacilos que incluyen dos órdenes, uno de los cuales contiene al género Bacillus. Los bacilos son bacterias que se encuentran en diferentes ambientes y solo se pueden observar con un microscopio. Los bacilos suelen dividirse en el mismo plano y son solitarios, pero pueden combinarse para formar diplobacilos, estreptobacilos y cocobacilos: Diplobacilos: Dos bacilos dispuestos uno al lado del otro. Estreptobacilos: Bacilos dispuestos en cadenas. Cocobacilos: Ovalados y en forma de bastoncillo. Por tipo de bacteria los bacilos pueden ser: Bacilos Gram positivos: fijan el cristal violeta (tinción de Gram) en la pared celular porque tienen una gruesa capa de peptidoglucano. Bacilos Gram negativos: no fijan el cristal violeta y se tiñen con el colorante de contraste usado en la tinción de Gram que es la safranina, debido a que tienen una fina capa de péptidoglucano en medio de dos bicapas lipídicas en la cual se encuentran los lipopolisacáridos o también llamados endotoxinas (principalmente en la membrana externa). Aunque muchos bacilos son patógenos para el ser humano, algunos no hacen daño, pues producen algunos productos lácteos como el yogur (lactobacilos). A lo largo de la historia de la medicina y de la microbiología, varias de estas bacterias han producido enfermedad en los humanos y por lo general se han adoptado el nombre del científico que los descubría, por ejemplo: Bacilo de Aertrycke: Salmonela. Bacilo de Bang: Brucella abortus. Bacilo de Ducrey: Haemophilus ducreyi. Bacilo de Eberth: Salmonella typhi. Bacilo de Nicolaier: Tétano. Bacilo de Hansen: Mycobacterium leprae. Bacilo de Klebs-Löffler: Corynebacterium diphtheriae. Bacilo de Koch: Mycobacterium tuberculosis. Bacilo de Morex: Género Moraxella. Bacilo de Yersin: Yersinia pestis."

