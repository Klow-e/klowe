

# klowe/sampletexts.py


###############################################################################################


def KCleanWikiText(stext: str) -> str:
    """
    Cleans a copied Wikipedia text to be easily pasted in a file.
    """
    s: list[str] = stext.split("\n")
    s: list[str] = [i.replace('\"', "") for i in s if len(i) > 2 and "{" not in i and "}" not in i]
    s: list[str] = [i.replace("	", " ").replace("​ ", " ").replace("\u200b", " ").replace("—", ",") for i in s]
    s: list[str] = [i.replace("'", "").replace("‘", "").replace("’", "").replace("  ", " ") for i in s]
    o: str = " ".join(s)
    for i in [f"[{i}]" for i in range(600)]: o = o.replace(i, "")
    o: str = o.replace("  ", " ").removesuffix("[").strip().replace("\n\n", "").removesuffix(".")
    o += "."
    return o


def KSampleTexts() -> list[str]:
    """
    Returns a list of every variable that starts with 'ksampletext_', that's to say, every sample text in KlowE.
    """
    kst: list[str] = [str(i) for i in globals() if i.startswith("ksampletext_")]
    return kst


###############################################################################################


ksampletext_wikipedia_math_matematicas: str = "Matemáticas. Las matemáticas o, también, la matemática (del latín mathematĭca, y a la vez del griego, transliterado como, derivado de, lo que se comprende) es una ciencia formal que estudia los patrones, propiedades, estructuras y relaciones presentes en sistemas lógicos y abstractos creados por los humanos, conceptos tales como cantidad, forma, espacio y número se podrían considerar como el objeto de estudio de la matemática. Descripción Las ciencias naturales han hecho un uso extensivo de la matemática para explicar diversos fenómenos observables, tal como lo expresó Eugene Paul Wigner (Premio Nobel de Física en 1963): «El primer punto es que la enorme utilidad de las matemáticas en las ciencias naturales es algo que roza lo misterioso y que no tiene una explicación racional. En segundo lugar, es precisamente esta extraña utilidad de los conceptos matemáticos lo que plantea la cuestión de la unicidad de nuestras teorías físicas.» «El milagro de la adecuación del lenguaje de las matemáticas para la formulación de las leyes de la física es un don maravilloso que no comprendemos ni merecemos.» Galileo Galilei, en la misma línea, lo había expresado así: «La filosofía está escrita en este enorme libro, que está continuamente abierto ante nuestros ojos (digo en el nuevo idioma), pero uno no puede entenderlo primero, uno no aprende a entender el idioma y a conocer los caracteres en que está escrito. Está escrito en lenguaje matemático, y los caracteres son triángulos, círculos y otras figuras geométricas, sin las cuales es imposible entender una palabra; sin éstos es un vano vagar por un oscuro laberinto.» Mediante la abstracción y el uso de la lógica en el razonamiento, la matemática ha evolucionado basándose en el cálculo y las mediciones, junto con el estudio sistemático de la forma y el movimiento de los objetos físicos. Las matemáticas, desde sus comienzos, han tenido un fin práctico. Las explicaciones que se apoyaban en la lógica aparecieron por primera vez con la matemática helénica, especialmente con los Elementos de Euclides. La matemática siguió desarrollándose, con continuas interrupciones, hasta que en el Renacimiento las innovaciones matemáticas interactuaron con los nuevos descubrimientos científicos. Como consecuencia, hubo una aceleración en la investigación que continúa hasta la actualidad. Hoy día, la matemática se usa en todo el mundo como una herramienta esencial en muchos campos, entre los que se encuentran las ciencias naturales, las ciencias aplicadas, las humanidades, la medicina y las ciencias sociales, e incluso disciplinas que, aparentemente, no están vinculadas con ella, como la música (por ejemplo, en cuestiones de resonancia armónica, Cuerda vibrante, etc.) y la literatura. Las matemáticas aplicadas, rama de la matemática destinada a la aplicación del conocimiento matemático a otros ámbitos, inspiran y hacen uso de los nuevos descubrimientos matemáticos y, en ocasiones, conducen al desarrollo de nuevas disciplinas. Los matemáticos también participan en la matemática pura, sin tener en cuenta sus aplicaciones, aunque estas suelen ser descubiertas con el paso del tiempo. Historia Artículo principal: Historia de las matemáticas Las matemáticas son una de las ciencias más antiguas. Floreció primero antes de la antigüedad en Mesopotamia, en cuanto a la geometría India y China, y más tarde en la antigüedad en Grecia y el helenismo. De ahí data la orientación hacia la tarea de «demostración puramente lógica» y la primera axiomatización, a saber, la geometría euclidiana. En la Edad Media sobrevivió de forma independiente en el primer humanismo de las universidades y en el mundo árabe. A principios de la era moderna, François Viète introdujo variables y René Descartes inauguró un enfoque computacional de la geometría mediante el uso de coordenadas. La consideración de las tasas de cambio (fluxión) así como la descripción de las tangentes y la determinación de los contenidos de las superficies (cuadratura) condujeron al cálculo infinitesimal de Gottfried Wilhelm Leibniz e Isaac Newton. La mecánica de Newton y su ley de la gravitación fueron también una fuente de orientación de problemas matemáticos como el problema de los tres cuerpos en los siglos siguientes. Otro de los principales problemas de la primera época moderna fue la solución de ecuaciones algebraicas cada vez más complicadas. Para hacer frente a esto, Niels Henrik Abel y Évariste Galois desarrollaron el concepto de grupo, que describe las relaciones entre las simetrías de un objeto. El álgebra más reciente y, en particular, la geometría algebraica pueden considerarse como una profundización de estas investigaciones. Una idea entonces nueva en el intercambio de cartas entre Blaise Pascal y Pierre de Fermat en 1654 acerca del problema de los juegos de azar, aunque existían otras soluciones discutibles como las de Cardano, quien intentó matematizarlas. Pierre-Simon Laplace hace un recuento de los diferentes logros hasta 1812 cuando publica su Ensayo filosófico sobre las posibilidades. Las nuevas ideas y métodos conquistaron muchos campos. Pero durante siglos, la teoría clásica de la probabilidad se dividió en escuelas separadas. Los intentos de definir explícitamente el término «probabilidad» solo tuvieron éxito para casos especiales. Solo la publicación del libro de texto de Andrei Kolmogorov en 1933 Los fundamentos de la Teoría de la Probabilidad completó el desarrollo de los fundamentos de la teoría moderna de la probabilidad. En el transcurso del siglo XIX, el cálculo infinitesimal encontró su forma actual de rigor gracias a los trabajos de Augustin-Louis Cauchy y Karl Weierstrass. La teoría de conjuntos desarrollada por Georg Cantor hacia finales del siglo XIX es también indispensable en la matemática actual, aunque las paradojas del concepto ingenuo de conjuntos dejaron claro, en un primer momento, la incierta base sobre la que se asentaban las matemáticas. El desarrollo de la primera mitad del siglo XX estuvo influenciado por la publicación de los problemas de Hilbert. Uno de los problemas intentaba axiomatizar completamente las matemáticas; al mismo tiempo, se hicieron grandes esfuerzos de abstracción, es decir, el intento de reducir los objetos a sus propiedades esenciales. Así, Emmy Noether desarrolló los fundamentos del álgebra moderna, Felix Hausdorff desarrolló la topología general como el estudio de los espacios topológicos, Stefan Banach desarrolló probablemente el concepto más importante del análisis funcional, el espacio de Banach que lleva su nombre. Un nivel de abstracción aún mayor, un marco común para la consideración de construcciones similares de diferentes áreas de las matemáticas, fue finalmente creado por la introducción de la teoría de categorías por Samuel Eilenberg y Saunders Mac Lane. Introducción Etimología La palabra «matemática» (del griego μαθηματικά mathēmatiká, «cosas que se aprenden») viene del griego antiguo μάθημα (máthēma), que quiere decir «campo de estudio o instrucción». Las matemáticas requieren un esfuerzo de instrucción o aprendizaje, refiriéndose a áreas del conocimiento que solo pueden entenderse tras haber sido instruido en las mismas, como la astronomía. «El arte matemática» (μαθηματική τέχνη, mathēmatikḗ tékhnē) se contrapondría en esto a la música, «el arte de las musas» (μουσική τέχνη, mousikē téchnē), que sería un arte, como la poesía, retórica y similares, que se puede apreciar directamente, «que se puede entender sin haber sido instruido». Aunque el término ya era usado por los pitagóricos (matematikoi) en el siglo VI a. C., alcanzó su significado más técnico y reducido de «estudio matemático» en los tiempos de Aristóteles (siglo IV a. C.). Su adjetivo es μαθηματικός (mathēmatikós), «relacionado con el aprendizaje», lo cual, de manera similar, vino a significar «matemático». En particular, μαθηματική τέχνη (mathēmatikḗ tékhnē; en latín ars mathematica), significa «el arte matemática». La forma más usada es el plural matemáticas (cuyo acortamiento, en algunos países, es «mates»), que tiene el mismo significado que el singular y viene de la forma latina mathematica (Cicerón), basada en el plural en griego τα μαθηματικά (ta mathēmatiká), usada por Aristóteles y que significa, a grandes rasgos, «todas las cosas matemáticas». Algunos autores, sin embargo, hacen uso de la forma singular del término; tal es el caso de Bourbaki, en el tratado Elementos de matemática (Élements de mathématique, 1940), destaca la uniformidad de este campo aportada por la visión axiomática moderna, aunque también hace uso de la forma plural como en Éléments dhistoire des mathématiques (1969), posiblemente sugiriendo que es Bourbaki quien finalmente realiza la unificación de las matemáticas. Así mismo, en el escrito LArchitecture des mathématiques (1948) plantea el tema en la sección «¿Matemáticas, singular o plural?» donde defiende la unicidad conceptual de la matemática aunque hace uso de la forma plural en dicho escrito. Algunas definiciones de matemática Establecer definiciones claras y precisas es el fundamento de la matemática, aunque encontrar una definición única para ella es improbable. Se muestran algunas reflexiones de reconocidos autores: René Descartes: «Y considerando esto más atentamente al cabo se nota que solamente aquellas en las que se estudia cierto orden y medida hacen referencia a la Mathesis, y que no importa si tal medida ha de buscarse en los números, en las figuras, en los astros, en los sonidos o en cualquier otro objeto;» Carl Friedrich Gauss: «El matemático se abstrae totalmente de la naturaleza de los objetos y el contenido de sus relaciones; se preocupa únicamente por la enumeración y la comparación de las relaciones entre ellos [...]» David Hilbert: «[...] nos lleva a una concepción de las matemáticas que considera a éstas como un inventario de fórmulas a las que corresponden, en primer lugar, expresiones concretas de enunciados finitistas y a las que se añaden, en segundo lugar, otras fórmulas que carecen de todo significado y que constituyen los objetos ideales de nuestra teoría.» Benjamin Peirce: «La matemática es la ciencia que extrae conclusiones necesarias.» Bertrand Russell: Trató de probar «que toda la Matemática pura trabaja exclusivamente con conceptos definibles en función de un número muy pequeño de conceptos lógicos fundamentales, y de que todas las proposiciones se pueden deducir de un número muy pequeño de principios lógicos fundamentales.» John David Barrow: «En el fondo, matemáticas es el nombre que le damos al conjunto de todos los patrones e interrelaciones posibles. Algunos de esos patrones están entre formas, otros están en secuencias de números, mientras que otros son relaciones más abstractas entre estructuras. La esencia de las matemáticas radica en las relaciones entre cantidades y cualidades. Por lo tanto, son las relaciones entre los números, no los números en sí mismos, las que constituyen el foco de interés de los matemáticos modernos.» Epistemología y controversia sobre la matemática como ciencia El carácter epistemológico y científico de la matemática ha sido ampliamente discutido. En la práctica, la matemática se emplea para estudiar relaciones cuantitativas, estructuras, relaciones geométricas y las magnitudes variables. Los matemáticos buscan patrones, formulan nuevas conjeturas e intentan alcanzar la verdad matemática mediante deducciones rigurosas. Estas les permiten establecer los axiomas y las definiciones apropiados para dicho fin. Algunas definiciones clásicas restringen las matemáticas al razonamiento sobre cantidades, aunque solo una parte de la matemática actual usa números, predominando el análisis lógico de construcciones abstractas no cuantitativas. Existe cierta discusión acerca de si los objetos matemáticos, como los números y puntos, realmente existen o simplemente provienen de la imaginación humana. El matemático Benjamin Peirce definió las matemáticas como «la ciencia que señala las conclusiones necesarias». Por otro lado: «cuando las leyes de la matemática se refieren a la realidad, no son exactas; cuando son exactas, no se refieren a la realidad». Albert Einstein Se ha discutido el carácter científico de las matemáticas debido a que sus procedimientos y resultados poseen una firmeza e inevitabilidad inexistentes en otras disciplinas como pueden ser la física, la química o la biología. Así, la matemática sería tautológica, infalible y a priori, mientras que otras, como la geología o la fisiología, serían falibles y a posteriori. Son estas características lo que hace dudar de colocarse en el mismo rango que las disciplinas antes citadas pese a las afirmaciones como las de John Stuart Mill quien sostenía en 1843: «En realidad, las leyes de los números son verdades físicas provenientes de la observación.» Así, los matemáticos pueden descubrir nuevos procedimientos para resolver integrales o teoremas, pero se muestran incapaces de descubrir un suceso que ponga en duda el Teorema de Pitágoras o cualquier otro, como sí sucede constantemente con las ciencias de la naturaleza. El teorema de Pitágoras es uno de los enunciados más conocidos y antiguos de las matemáticas. Un ábaco, instrumento para efectuar operaciones aritméticas sencillas (sumas, restas y también multiplicaciones), fue muy utilizado en otros tiempos. La matemática puede ser entendida como ciencia; si es así debiera señalarse su objeto y su método. Sin embargo, algunos plantean que la matemática es un lenguaje formal, seguro, eficiente, aplicable al entendimiento de la naturaleza, tal como indicó Galileo; además muchos fenómenos de carácter social, otros de carácter biológico o geológico, pueden ser estudiados mediante la aplicación de ecuaciones diferenciales, cálculo de probabilidades o teoría de conjunto. Precisamente, el avance de la física y de la química ha exigido la invención de nuevos conceptos, instrumentos y métodos en la matemática, sobre todo en el análisis real, análisis complejo y el análisis matricial. Aspectos formales, metodológicos y estéticos La inspiración, las matemáticas puras, aplicadas y la estética Isaac Newton (1643-1727), comparte con Leibniz la autoría del desarrollo del cálculo integral y diferencial. Es muy posible que el arte de calcular haya sido desarrollado antes incluso que la escritura, relacionado fundamentalmente con la contabilidad y la administración de bienes, el comercio, en la agrimensura y, posteriormente, en la astronomía. Actualmente, todas las ciencias aportan problemas que son estudiados por matemáticos, al mismo tiempo que aparecen nuevos problemas dentro de las propias matemáticas. Por ejemplo, el físico Richard Feynman propuso la integral de caminos como fundamento de la mecánica cuántica, combinando el razonamiento matemático y el enfoque de la física, pero todavía, no se ha logrado una definición plenamente satisfactoria en términos matemáticos. Igualmente, la teoría de cuerdas, una teoría científica en desarrollo que trata de unificar las cuatro fuerzas fundamentales de la física, sigue inspirando a las más modernas matemáticas. Algunas matemáticas solo son relevantes en el área en la que estaban inspiradas y son aplicadas para otros problemas en ese campo. Sin embargo, a menudo las matemáticas inspiradas en un área concreta resultan útiles en muchos ámbitos, y se incluyen dentro de los conceptos matemáticos generales aceptados. El notable hecho de que incluso la matemática más pura habitualmente tiene aplicaciones prácticas es lo que Eugene Paul Wigner ha definido como «la irrazonable eficacia de las matemáticas en las Ciencias Naturales». Como en la mayoría de las áreas de estudio, la explosión de los conocimientos en la era científica ha llevado a la especialización de las matemáticas. Hay una importante distinción entre las matemáticas puras y las matemáticas aplicadas. La mayoría de los matemáticos que se dedican a la investigación se centran únicamente en una de estas áreas y, a veces, la elección se realiza cuando comienzan su licenciatura. Varias áreas de las matemáticas aplicadas se han fusionado con otras áreas tradicionalmente fuera de las matemáticas y se han convertido en disciplinas independientes, como pueden ser la estadística, la investigación de operaciones o la informática. Aquellos que sienten predilección por las matemáticas, consideran que prevalece un aspecto estético que define a la mayoría de las matemáticas. Muchos matemáticos hablan de la elegancia de la matemática, su intrínseca estética y su belleza interna. En general, uno de sus aspectos más valorados es la simplicidad. Hay belleza en una simple y contundente demostración, como la demostración de Euclides de la existencia de infinitos números primos, y en un elegante análisis numérico que acelera el cálculo, así como en la transformada rápida de Fourier. Godfrey Harold Hardy en A Mathematicians Apology (Apología de un matemático) expresó la convicción de que estas consideraciones estéticas son, en sí mismas, suficientes para justificar el estudio de las matemáticas puras. Los matemáticos con frecuencia se esfuerzan por encontrar demostraciones de los teoremas que son especialmente elegantes, el excéntrico matemático Paul Erdős se refiere a este hecho como la búsqueda de pruebas de El Libro en el que Dios ha escrito sus demostraciones favoritas. La popularidad de la matemática recreativa es otra señal que nos indica el placer que produce resolver las preguntas matemáticas. Notación, lenguaje y rigor Artículo principal: Notación matemática Leonhard Euler. Probablemente el más prolífico matemático de todos los tiempos. La mayor parte de la notación matemática que se utiliza hoy en día no se inventó hasta el siglo XVIII. Antes de eso, las matemáticas eran escritas con palabras, un minucioso proceso que limitaba el avance matemático. En el siglo XVIII, Euler, fue responsable de muchas de las notaciones empleadas en la actualidad. La notación moderna hace que las matemáticas sean mucho más fácil para los profesionales, pero para los principiantes resulta complicada. La notación reduce las matemáticas al máximo, hace que algunos símbolos contengan una gran cantidad de información. Al igual que la notación musical, la notación matemática moderna tiene una sintaxis estricta y codifica la información que sería difícil de escribir de otra manera. El símbolo de infinito en diferentes tipografías. El lenguaje matemático también puede ser difícil para los principiantes. Palabras tales como o y solo si tienen significados más precisos que en lenguaje cotidiano. Además, palabras como abierto y cuerpo tienen significados matemáticos muy concretos. La jerga matemática, o lenguaje matemático, incluye términos técnicos como homeomorfismo o integrabilidad. La razón que explica la necesidad de utilizar la notación y la jerga es que el lenguaje matemático requiere más precisión que el lenguaje cotidiano. Los matemáticos se refieren a esta precisión en el lenguaje y en la lógica como el «rigor». El rigor es una condición indispensable que debe tener una demostración matemática. Los matemáticos quieren que sus teoremas a partir de los axiomas sigan un razonamiento sistemático. Esto sirve para evitar teoremas erróneos, basados en intuiciones falibles, que se han dado varias veces en la historia de esta ciencia. El nivel de rigor previsto en las matemáticas ha variado con el tiempo: los griegos buscaban argumentos detallados, pero en tiempos de Isaac Newton los métodos empleados eran menos rigurosos. Los problemas inherentes de las definiciones que Newton utilizaba dieron lugar a un resurgimiento de un análisis cuidadoso y a las demostraciones oficiales del siglo XIX. Ahora, los matemáticos continúan apoyándose entre ellos mediante demostraciones asistidas por ordenador. Un axioma se interpreta tradicionalmente como una «verdad evidente», pero esta concepción es problemática. En el ámbito formal, un axioma no es más que una cadena de símbolos, que tiene un significado intrínseco solo en el contexto de todas las fórmulas derivadas de un sistema axiomático. La matemática como ciencia Carl Friedrich Gauss, apodado el «príncipe de los matemáticos», se refería a la matemática como «la reina de las ciencias». Carl Friedrich Gauss se refería a la matemática como «la reina de las ciencias». Tanto en el latín original Scientiārum Regīna, así como en alemán Königin der Wissenschaften, la palabra ciencia debe ser interpretada como (campo de) conocimiento. Si se considera que la ciencia es el estudio del mundo físico, entonces las matemáticas, o por lo menos las matemáticas puras, no son una ciencia. Muchos filósofos creen que las matemáticas no son experimentalmente falsables y, por ende, no son una ciencia según la definición de Karl Popper. No obstante, en la década de 1930 una importante labor en la lógica matemática demuestra que las matemáticas no pueden reducirse a la lógica y Karl Popper llegó a la conclusión de que «la mayoría de las teorías matemáticas son, como las de física y biología, hipotético-deductivas. Por lo tanto, las matemáticas puras se han vuelto más cercanas a las ciencias naturales cuyas hipótesis son conjeturas, así ha sido hasta ahora». Otros pensadores, en particular Imre Lakatos, han solicitado una versión de Falsacionismo para las propias matemáticas. Una visión alternativa es que determinados campos científicos (como la física teórica) son matemáticas con axiomas que pretenden corresponder a la realidad. De hecho, el físico teórico, John Michael Ziman, propone que la ciencia es «conocimiento público» y, por tanto, incluye a las matemáticas. En cualquier caso, las matemáticas tienen mucho en común con distintos campos de las ciencias físicas, especialmente la exploración de las consecuencias lógicas de las hipótesis. La intuición y la experimentación también desempeñan un papel importante en la formulación de conjeturas tanto en las matemáticas como en las otras ciencias. Las matemáticas experimentales siguen ganando representación dentro de las matemáticas. El cálculo y simulación están jugando un papel cada vez mayor tanto en las ciencias como en las matemáticas, atenuando la objeción de que las matemáticas no se sirven del método científico. En 2002 Stephen Wolfram propuso, en su libro Un nuevo tipo de ciencia, que la matemática computacional merece ser explorada empíricamente como un campo científico. Las opiniones de los matemáticos sobre este asunto son muy variadas. Muchos matemáticos consideran que llamar a su campo ciencia es minimizar la importancia de su perfil estético, además supone negar su historia dentro de las siete artes liberales. Otros consideran que hacer caso omiso de su conexión con las ciencias supone ignorar la evidente conexión entre las matemáticas y sus aplicaciones en la ciencia y la ingeniería, que ha impulsado considerablemente el desarrollo de las matemáticas. Otro asunto de debate, que guarda cierta relación con el anterior, es si la matemática fue creada (como el arte) o descubierta (como la ciencia). Este es uno de los muchos temas de incumbencia de la filosofía de las matemáticas. Los premios matemáticos se mantienen generalmente separados de sus equivalentes en la ciencia. El más prestigioso premio dentro de las matemáticas es la Medalla Fields, fue instaurado en 1936 y se concede cada cuatro años. A menudo se le considera el equivalente del Premio Nobel para la ciencia. Otros premios son el Premio Wolf en matemática, creado en 1978, que reconoce los logros en vida de los matemáticos, y el Premio Abel, otro gran premio internacional, que se introdujo en 2003. Estos dos últimos se conceden por un excelente trabajo, que puede ser una investigación innovadora o la solución de un problema pendiente en un campo determinado. Una famosa lista de esos 23 problemas sin resolver, denominada los «Problemas de Hilbert», fue recopilada en 1900 por el matemático alemán David Hilbert. Esta lista ha alcanzado gran popularidad entre los matemáticos y, al menos, nueve de los problemas ya han sido resueltos. Una nueva lista de siete problemas fundamentales, titulada «Problemas del milenio», se publicó en 2000. La solución de cada uno de los problemas será recompensada con 1 millón de dólares. Curiosamente, tan solo uno (la hipótesis de Riemann) aparece en ambas listas. Ramas de estudio de las matemáticas Artículo principal: Áreas de las matemáticas La Sociedad Matemática Americana distingue unas 5.000 ramas distintas de matemática. En una subdivisión escolarizada de la matemática se distinguen cinco áreas de estudio básicas: la cantidad, la estructura, el espacio, el cambio y la variabilidad que se corresponden con la aritmética, el álgebra, la geometría, el cálculo, la probabilidad y estadística. Como señalaba Richard Courant «Es posible seguir una ruta directa a partir de los elementos fundamentales hasta puntos avanzados» para que puedan divisarse las directrices de la matemática como ciencia. Además, hay ramas de las matemáticas conectadas a otros campos, por ejemplo la lógica, teoría de conjuntos y las matemáticas aplicadas entre muchas otras tal como indica la Sociedad Matemática Americana. Véase también: Categoría:Áreas de las matemáticas Matemática pura Artículo principal: Matemáticas puras Cantidad Números naturales Enteros Números racionales Números reales Números complejos Estructura Combinatoria Teoría de números Teoría de grupos Teoría de grafos Teoría del orden Álgebra Espacio Geometría Trigonometría Geometría diferencial Topología Geometría fractal Teoría de la medida Cambio Cálculo Cálculo vectorial Ecuaciones diferenciales Sistemas dinámicos Teoría del caos Análisis complejo Matemática aplicada Artículo principal: Matemáticas aplicadas El concepto «matemática aplicada» se refiere a aquellos métodos y herramientas matemáticas que pueden ser utilizados en el análisis o resolución de problemas pertenecientes al área de las ciencias básicas o aplicadas. Muchos métodos matemáticos han resultado efectivos en el estudio de problemas en física, química, biología, medicina, ciencias sociales, ingeniería, economía, finanzas, ecología entre otras. Sin embargo, una posible diferencia es que en matemática aplicada se procura el desarrollo de la matemática «hacia afuera», es decir su aplicación o transferencia hacia el resto de las áreas. Y en menor grado «hacia dentro» o sea, hacia el desarrollo de la matemática misma. Este último sería el caso de la matemática pura o matemática elemental. La matemática aplicada se usa con frecuencia en distintas áreas tecnológicas para modelado, simulación y optimización de procesos o fenómenos, como el túnel de viento o el diseño de experimentos. Estadística y ciencias de la decisión La estadística es la rama de la matemática que estudia la variabilidad, así como el proceso aleatorio que la genera siguiendo leyes de probabilidad. Es un conocimiento fundamental para la investigación científica en algunos campos de la tecnología, como informática e ingeniería, y de las ciencias fácticas, como economía, genética, sociología, psicología, medicina, contabilidad, etc. En ocasiones, estas áreas de conocimiento necesitan aplicar técnicas estadísticas durante su proceso de investigación factual, con el fin de obtener nuevos conocimientos basados en la experimentación y en la observación, precisando para ello recolectar, organizar, presentar y analizar un conjunto de datos numéricos y, a partir de ellos y de un marco teórico, hacer las inferencias apropiadas. Se consagra en forma directa al gran problema universal de cómo tomar decisiones inteligentes y acertadas en condiciones de incertidumbre. La estadística descriptiva sirve como fuente de instrucción en los niveles básicos de estadística aplicada a las ciencias fácticas y, por tanto, los conceptos manejados y las técnicas empleadas suelen ser presentadas de la forma más simple y clara posibles. Matemática computacional."
ksampletext_wikipedia_math_calculo: str = "Cálculo. En general el término cálculo (del latín calculus, piedrecita, usado para contar o como ayuda al calcular) hace referencia al resultado correspondiente a la acción de calcular. Calcular, por su parte, consiste en realizar las operaciones necesarias para prever el resultado de una acción previamente concebida, o conocer las consecuencias que se pueden derivar de unos datos previamente conocidos. No obstante, el uso más común del término «cálculo» es el lógico-matemático. Desde esta perspectiva, el cálculo consiste en un procedimiento mecánico o algoritmo, mediante el cual podemos conocer las consecuencias que se derivan de las variables previamente conocidas debidamente formalizadas y simbolizadas. Cálculo como razonamiento y cálculo lógico-matemático Ejemplo de aplicación de un cálculo algebraico a la resolución de un problema, según la interpretación de una teoría física. La expresión del cálculo algebraico Pero si interpretamos Al mismo tiempo, según dicha teoría, sirve para resolver el problema de calcular cuántos kilómetros ha recorrido un coche que circula de Madrid a Barcelona a una velocidad constante de 60 km/h durante 4 horas de recorrido. 240 kilómetros recorridos = 60 km/h x 4 h Las dos acepciones del cálculo (la general y la restringida) arriba definidas están íntimamente ligadas. El cálculo es una actividad natural y primordial en el hombre, que comienza en el mismo momento en que empieza a relacionar unas cosas con otras en un pensamiento o discurso. El cálculo lógico natural como razonamiento es el primer cálculo elemental del ser humano. El cálculo en sentido lógico-matemático aparece cuando se toma conciencia de esta capacidad de razonar y trata de formalizarse. Por lo tanto, podemos distinguir dos tipos de operaciones: Operaciones orientadas hacia la consecución de un fin, como prever, programar, conjeturar, estimar, precaver, prevenir, proyectar, configurar, etc. que incluyen en cada caso una serie de complejas actividades y habilidades tanto de pensamiento como de conducta. En su conjunto dichas actividades adquieren la forma de argumento o razones que justifican una finalidad práctica o cognoscitiva. Operaciones formales como algoritmo que se aplica bien directamente a los datos conocidos o a los esquemas simbólicos de la interpretación lógico-matemática de dichos datos; las posibles conclusiones, inferencias o deducciones de dicho algoritmo son el resultado de la aplicación de reglas estrictamente establecidas de antemano. Resultado que es: Conclusión de un proceso de razonamiento. Resultado aplicable directamente a los datos iniciales (resolución de problemas). Modelo de relaciones previamente establecido como teoría científica y significativo respecto a determinadas realidades (Creación de modelos científicos). Mero juego formal simbólico de fundamentación, creación y aplicación de las reglas que constituyen el sistema formal del algoritmo (Cálculo lógico-matemático, propiamente dicho). Dada la importancia que históricamente ha adquirido la actividad lógico-matemática en la cultura humana el presente artículo se refiere a este último sentido. De hecho la palabra, en su uso habitual, casi queda restringida a este ámbito de aplicación; para algunos, incluso, queda reducida a un solo tipo de cálculo matemático, pues en algunas universidades se llamaba «Cálculo» a una asignatura específica de cálculo matemático (como puede ser el cálculo infinitesimal, análisis matemático, cálculo diferencial e integral, etc.). En un artículo general sobre el tema no puede desarrollarse el contenido de lo que supone el cálculo lógico-matemático en la actualidad. Aquí se expone solamente el fundamento de sus elementos más simples, teniendo en cuenta que sobre estas estructuras simples se construyen los cálculos más complejos tanto en el aspecto lógico como en el matemático. Historia del cálculo Artículo principal: Historia del cálculo De la Antigüedad Reconstrucción de un ábaco romano. Un ábaco moderno. El término «cálculo» procede del latín calculus, piedrecita que se mete en el calzado y que produce molestia. Precisamente, tales piedrecitas ensartadas en tiras constituían el ábaco romano que, junto con el suanpan chino, constituyen las primeras máquinas de calcular en el sentido de contar. Los antecedentes de procedimiento de cálculo, como algoritmo, se encuentran en los que utilizaron los geómetras griegos, Eudoxo en particular, en el sentido de llegar por aproximación de restos cada vez más pequeños, a una medida de figuras curvas; así como Diofanto precursor del álgebra. Se considera que Arquímedes fue uno de los matemáticos más grandes de la antigüedad y, en general, de toda la historia. Usó el método exhaustivo para calcular el área bajo el arco de una parábola con el sumatorio de una serie infinita, y dio una aproximación extremadamente precisa del número Pi. También definió la espiral que lleva su nombre, fórmulas para los volúmenes de las superficies de revolución y un ingenioso sistema para expresar números muy largos. La consideración del cálculo como una forma de razonamiento abstracto aplicado en todos los ámbitos del conocimiento se debe a Aristóteles, quien en sus escritos lógicos fue el primero en formalizar y simbolizar los tipos de razonamientos categóricos (silogismos). Este trabajo sería completado más tarde por los estoicos, los megáricos, la Escolástica. Los algoritmos actuales del cálculo aritmético, utilizados universalmente, son fruto de un largo proceso histórico. De vital importancia son las aportaciones de Muhammad ibn al-Juarismi en el siglo IX; En el siglo XIII, Fibonacci introduce en Europa la representación de los números arábigos del sistema decimal. Se introdujo el 0, ya de antiguo conocido en la India y se construye definitivamente el sistema decimal de diez cifras con valor posicional. La escritura antigua de números en Babilonia, en Egipto, en Grecia o en Roma, hacía muy difícil un procedimiento mecánico de cálculo. El sistema decimal fue muy importante para el desarrollo de la contabilidad de los comerciantes de la Baja Edad Media, en los inicios del capitalismo. El concepto de función por tablas ya era practicado de antiguo pero adquirió especial importancia en la Universidad de Oxford en el siglo XIV. La idea de un lenguaje o algoritmo capaz de determinar todas las verdades, incluidas las de la fe, aparecen en el intento de Raimundo Lulio en su Ars Magna A fin de lograr una operatividad mecánica se confeccionaban unas tablas a partir de las cuales se podía generar un algoritmo prácticamente mecánico. Este sistema de tablas ha perdurado en algunas operaciones durante siglos, como las tablas de logaritmos, o las funciones trigonométricas; las tablas venían a ser como la calculadora de hoy día; un instrumento imprescindible de cálculo. Las amortizaciones de los créditos en los bancos, por ejemplo, se calculaban a partir de tablas elementales hasta que se produjo la aplicación de la informática en el tercer tercio del siglo XX. A finales de la Edad Media la discusión entre los partidarios del ábaco y los partidarios del algoritmo se decantó claramente por estos últimos. De especial importancia es la creación del sistema contable por partida doble recomendado por Luca Pacioli fundamental para el progreso del capitalismo en el Renacimiento. Renacimiento El sistema que usamos actualmente fue introducido por Luca Pacioli en 1494, el cual fue creado y desarrollado para responder a la necesidad de la contabilidad en los negocios de la burguesía renacentista. El desarrollo del álgebra (con la introducción de un sistema de símbolos por un lado, y la resolución de problemas por medio de las ecuaciones) vino de la mano de los grandes matemáticos de la época renacentista como Tartaglia, Stevin, Cardano o Vieta y fue esencial para el planteamiento y solución de los más diversos problemas que surgieron en la época, que dieron como consecuencia los grandes descubrimientos que hicieron posible el progreso científico que surgiría en el siglo XVII. Siglos XVII y XVIII Página del artículo de Leibniz Explication de lArithmétique Binaire, 1703/1705 En el siglo XVII el cálculo conoció un enorme desarrollo siendo los autores más destacados Descartes, Pascal y, finalmente, Leibniz y Newton con el cálculo infinitesimal que en muchas ocasiones ha recibido simplemente, por absorción, el nombre de cálculo. El concepto de cálculo formal en el sentido de algoritmo reglado para el desarrollo de un razonamiento y su aplicación al mundo de lo real, adquiere una importancia y desarrollo enorme respondiendo a una necesidad de establecer relaciones matemáticas entre diversas medidas, esencial para el progreso de la ciencia física que, debido a esto, es tomada como nuevo modelo de Ciencia frente a la especulación tradicional filosófica, por el rigor y seguridad que ofrece el cálculo matemático. Cambia así el sentido tradicional de la Física como filosofía de la naturaleza y toma el sentido de ciencia que estudia los cuerpos materiales, en cuanto materiales. A partir de entonces el propio sistema de cálculo permite establecer modelos sobre la realidad física, cuya comprobación experimental supone la confirmación de la teoría como sistema. Es el momento de la consolidación del llamado método científico cuyo mejor exponente es en aquel momento la Teoría de la Gravitación Universal y las leyes de la Mecánica de Newton. Siglos XIX y XX George Boole Durante el siglo XIX y XX el desarrollo científico y la creación de modelos teóricos fundados en sistemas de cálculo aplicables tanto en mecánica como en electromagnetismo y radioactividad, etc., así como en astronomía fue impresionante. Las geometrías no euclidianas encuentran aplicación en modelos teóricos de astronomía y física. El mundo deja de ser un conjunto de infinitas partículas que se mueven en un espacio-tiempo absoluto y se convierte en un espacio de configuración o espacio de fases de La lógica asimismo sufrió una transformación radical. La formalización simbólica fue capaz de integrar las leyes lógicas en un cálculo matemático, hasta el punto que la distinción entre razonamiento lógico-formal y cálculo matemático viene a considerarse como meramente utilitaria. En la segunda mitad del siglo XIX y primer tercio del XX, a partir del intento de formalización de todo el sistema matemático, Frege, y de matematización de la lógica, (Bolzano, Boole, Whitehead, Russell) fue posible la generalización del concepto como cálculo lógico. Se lograron métodos muy potentes de cálculo, sobre todo a partir de la posibilidad de tratar como «objeto» conjuntos de infinitos elementos, dando lugar a los números transfinitos de Cantor. Mediante el cálculo la lógica encuentra nuevos desarrollos como lógicas modales y lógicas polivalentes. Los intentos de axiomatizar el cálculo como cálculo perfecto por parte de Hilbert y Poincaré, llevaron, como consecuencia de diversas paradojas (Cantor, Russell, etc.) a nuevos intentos de axiomatización, Axiomas de Zermelo-Fraenkel y a la demostración de Gödel de la imposibilidad de un sistema de cálculo perfecto: consistente, decidible y completo en 1931, de grandes implicaciones lógicas, matemáticas y científicas. Actualidad En la actualidad, el cálculo en su sentido más general, en tanto que cálculo lógico interpretado matemáticamente como sistema binario, y físicamente hecho material mediante la lógica de circuitos electrónicos, ha adquirido una dimensión y desarrollo impresionante por la potencia de cálculo conseguida por los ordenadores, propiamente máquinas computadoras. La capacidad y velocidad de cálculo de estas máquinas hace lo que humanamente sería imposible: millones de operaciones por segundo. El cálculo así utilizado se convierte en un instrumento fundamental de la investigación científica por las posibilidades que ofrece para la modelización de las teorías científicas, adquiriendo especial relevancia en ello el cálculo numérico. Cálculo infinitesimal: breve reseña Artículo principal: Cálculo infinitesimal El cálculo infinitesimal, llamado por brevedad «cálculo», tiene su origen en la antigua geometría griega. Demócrito calculó el volumen de pirámides y conos considerándolos formados por un número infinito de secciones de grosor infinitesimal (infinitamente pequeño). Eudoxo y Arquímedes utilizaron el «método de agotamiento» o exhaución para encontrar el área de un círculo con la exactitud finita requerida mediante el uso de polígonos regulares inscritos de cada vez mayor número de lados. En el periodo tardío de Grecia, el neoplatónico Pappus de Alejandría hizo contribuciones sobresalientes en este ámbito. Sin embargo, las dificultades para trabajar con números irracionales y las paradojas de Zenón de Elea impidieron formular una teoría sistemática del cálculo en el periodo antiguo. En el siglo XVII, Cavalieri y Torricelli ampliaron el uso de los infinitesimales, Descartes y Fermat utilizaron el álgebra para encontrar el área y las tangentes (integración y derivación en términos modernos). Fermat e Isaac Barrow tenían la certeza de que ambos cálculos estaban relacionados, aunque fueron Newton (hacia 1660), en Inglaterra y Leibniz en Alemania (hacia 1670) quienes demostraron que los problemas del área y la tangente son inversos, lo que se conoce como teorema fundamental del cálculo. Leibniz es el creador del simbolismo de la derivada, diferencial y la ∫ estilizada para la integración, en vez de la I de Bernoulli. Usó el nombre de cálculo diferencial y el nombre de cálculo integral propuso Juan Bernoulli, que sustituyó al nombre de cálculo sumatorio de Leibniz. La simbología de Leibniz impulsó el avance del cálculo en Europa continental. El descubrimiento de Newton, a partir de su teoría de la gravitación universal, fue anterior al de Leibniz, pero el retraso en su publicación aún provoca controversias sobre quién de los dos fue el primero. Newton utilizó el cálculo en mecánica en el marco de su tratado «Principios matemáticos de filosofía natural», obra científica por excelencia, llamando a su método de «fluxiones». Leibniz utilizó el cálculo en el problema de la tangente a una curva en un punto, como límite de aproximaciones sucesivas, dando un carácter más filosófico a su discurso. Sin embargo, terminó por adoptarse la notación de Leibniz por su versatilidad. En el siglo XVIII aumentó considerablemente el número de aplicaciones del cálculo, pero el uso impreciso de las cantidades infinitas e infinitesimales, así como la intuición geométrica, causaban todavía confusión y duda sobre sus fundamentos. De hecho, la noción de límite, central en el estudio del cálculo, era aún vaga e imprecisa en ese entonces. Uno de sus críticos más notables fue el filósofo George Berkeley. En el siglo XIX el trabajo de los analistas matemáticos sustituyeron esas vaguedades por fundamentos sólidos basados en cantidades finitas: Bolzano y Cauchy definieron con precisión los conceptos de límite en términos de épsilon-delta y de derivada, Cauchy y Riemann hicieron lo propio con las integrales, y Dedekind y Weierstrass con los números reales. Fue el periodo de la fundamentación del cálculo. Por ejemplo, se supo que las funciones diferenciables son continuas y que las funciones continuas son integrables, aunque los recíprocos son falsos. En el siglo XX, el análisis no convencional, legitimó el uso de los infinitesimales, al mismo tiempo que la aparición de las computadoras ha incrementado las aplicaciones y velocidad del cálculo. Actualmente, el cálculo infinitesimal tiene un doble aspecto: por un lado, se ha consolidado su carácter disciplinario en la formación de la sociedad culta del conocimiento, destacando en este ámbito textos propios de la disciplina como el de Louis Leithold, el de Earl W. Swokowski, el de Denis G. Zill o el de James Stewart, entre muchos otros; por otro su desarrollo como disciplina científica que ha desembocado en ámbitos tan especializados como el cálculo fraccional, la teoría de funciones analíticas de variable compleja o el análisis matemático. El éxito del cálculo ha sido extendido con el tiempo a las ecuaciones diferenciales, al cálculo de vectores, al cálculo de variaciones, al análisis complejo y a las topología algebraica y topología diferencial entre muchas otras ramas. El desarrollo y uso del cálculo ha tenido efectos muy importantes en casi todas las áreas de la vida moderna: es fundamento para el cálculo numérico aplicado en casi todos los campos técnicos y/o científicos cuya principal característica es la continuidad de sus elementos, en especial en la física. Prácticamente todos los desarrollos técnicos modernos como la construcción, aviación, transporte, meteorología, etc., hacen uso del cálculo. Muchas fórmulas algebraicas se usan hoy en día en balística, calefacción, refrigeración, etc. Como complemento del cálculo, en relación con sistemas teóricos o físicos cuyos elementos carecen de continuidad, se ha desarrollado una rama especial conocida como Matemática discreta. Recientemente, se ha desarrollado el Cálculo Fraccional de Conjuntos (en inglés, Fractional Calculus of Sets o FCS) como una metodología derivada del Cálculo Fraccional. Esta metodología, mencionada por primera vez en el artículo Sets of Fractional Operators and Numerical Estimation of the Order of Convergence of a Family of Fractional Fixed-Point Methods, tiene como objetivo caracterizar y organizar los elementos del cálculo fraccional mediante el uso de conjuntos, aprovechando la variedad de operadores fraccionales disponibles en la literatura. Actualmente, el cálculo fraccional carece de una definición unificada de lo que constituye una derivada fraccional. En consecuencia, cuando no es necesario especificar explícitamente la forma de una derivada fraccional, típicamente se denota de la siguiente manera: Los operadores fraccionales tienen varias representaciones, pero una de sus propiedades fundamentales es que recuperan los resultados del cálculo tradicional a medida que Denotando y lim Cálculo lógico Artículo principal: Cálculo lógico El cálculo lógico es un sistema de reglas de inferencia o deducción de un enunciado a partir de otro u otros. El cálculo lógico requiere un conjunto consistente de axiomas y unas reglas de inferencia; su propósito es poder deducir algorítmicamente proposiciones lógicas verdaderas a partir de dichos axiomas. La inferencia es una operación lógica que consiste en obtener una proposición lógica como conclusión a partir de otra(s) (premisas) mediante la aplicación de reglas de inferencia. Informalmente interpretamos que alguien infiere ,o deduce, T de R si acepta que si R tiene valor de verdad V, entonces, necesariamente, T tiene valor de verdad V. Sin embargo, en el enfoque moderno del cálculo lógico no es necesario acudir al concepto de verdad, para construir el cálculo lógico. Los hombres en nuestra tarea diaria, utilizamos constantemente el razonamiento deductivo. Partimos de enunciados empíricos ,supuestamente verdaderos y válidos, para concluir en otro enunciado que se deriva de aquellos, según las leyes de la lógica natural. La lógica, como ciencia formal, se ocupa de analizar y sistematizar dichas leyes, fundamentarlas y convertirlas en las reglas que permiten la transformación de unos enunciados ,premisas- en otros -conclusiones, con objeto de convertir las operaciones en un algoritmo riguroso y eficaz, que garantiza que dada la verdad de las premisas, la conclusión es necesariamente verdadera. Al aplicar las reglas de un cálculo lógico a los enunciados de un argumento mediante la simbolización adecuada como fórmulas o expresiones bien formadas (EBF) del cálculo, construimos un modelo o sistema deductivo. En ese contexto, las reglas de formación de fórmulas definen la sintaxis de un lenguaje formal de símbolos no interpretados, es decir, sin significado alguno; y las reglas de transformación del sistema permiten transformar dichas expresiones en otras equivalentes; entendiendo por equivalentes que ambas tienen siempre y de forma necesaria el mismo valor de verdad. Dichas transformaciones son meramente tautologías. Un lenguaje formal que sirve de base para el cálculo lógico está formado por varias clases de entidades: Un conjunto de elementos primitivos. Dichos elementos pueden establecerse por enumeración, o definidos por una propiedad tal que permita discernir sin duda alguna cuándo un elemento pertenece o no pertenece al sistema. Un conjunto de reglas de formación de «expresiones bien formadas» (EBF) que permitan en todo momento establecer, sin forma de duda, cuándo una expresión pertenece al sistema y cuándo no. Un conjunto de reglas de transformación de expresiones, mediante las cuales partiendo de una expresión bien formada del cálculo podremos obtener una nueva expresión equivalente y bien formada que pertenece al cálculo. Cuando en un cálculo así definido se establecen algunas expresiones determinadas como verdades primitivas o axiomas, decimos que es un sistema formal axiomático. Un cálculo así definido si cumple al mismo tiempo estas tres condiciones decimos que es un Cálculo Perfecto: Es consistente: No es posible que dada una expresión bien formada del sistema, ƒ, y su negación, no – ƒ, sean ambas teoremas del sistema. No puede haber contradicción entre las expresiones del sistema. Decidible: Dada cualquier expresión bien formada del sistema podemos encontrar un método que nos permita decidir mediante una serie finita de operaciones si dicha expresión es o no es un teorema del sistema. Completo: Cuando dada cualquier expresión bien formada del sistema, podemos establecer la demostración matemática o prueba de que es un teorema del sistema. La misma lógica-matemática ha demostrado que tal sistema de cálculo perfecto «no es posible» (véase el Teorema de Gödel). Sistematización de un cálculo de deducción natural Reglas de formación de fórmulas I. Una letra enunciativa (con o sin subíndice) es una EBF. II. Si A es una EBF, ¬ A también lo es. III. Si A es una EBF y B también, entonces A ∧ B; A ∨ B; A → B; A ↔ B, también lo son. IV. Ninguna expresión es una fórmula del Cálculo sino en virtud de I, II, III. Notas: A, B, … con mayúsculas están utilizadas como metalenguaje en el que cada variable expresa cualquier proposición, atómica (p,q,r,s, …) o molecular (p ∧ q), (p ∨ q), …309>100 A, B, … son símbolos que significan variables; ¬, ∧, ∨, →, ↔, son símbolos constantes. Existen diversas formas de simbolización. Utilizamos aquí la de uso más frecuente en España. Reglas de transformación de fórmulas 1) Regla de sustitución (R.T.1): Dada una tesis EBF del cálculo, en la que aparecen variables de enunciados, el resultado de sustituir una, algunas o todas esas variables por expresiones bien formadas (EBF) del cálculo, será también una tesis EBF del cálculo. Y ello con una única restricción, si bien muy importante: cada variable ha de ser sustituida siempre que aparece y siempre por el mismo sustituto. Veamos el ejemplo: 1 [(p ∧ q) ∨ r] → t ∨ s Transformación 2 A ∨ r → B Donde A = (p ∧ q); y donde B = (t ∨ s) 3 C → B Donde C = A ∨ r O viceversa 1 C → B Transformación 2 A ∨ r → B Donde A ∨ r = C 3 [(p ∧ q) ∨ r] → t ∨ s Donde (p ∧ q) = A; y donde (t ∨ s) = B 2) Regla de separación (R.T.2): Si X es una tesis EBF del sistema y lo es también X → Y, entonces Y es una tesis EBF del sistema. Esquemas de inferencia Sobre la base de estas dos reglas, siempre podremos reducir un argumento cualquiera a la forma: [A ∧ B ∧ C … ∧ N] → Y lo que constituye un esquema de inferencia en el que una vez conocida la verdad de cada una de las premisas A, B, … N y, por tanto, de su producto, podemos obtener la conclusión Y con valor de verdad V, siempre y cuando dicho esquema de inferencia sea una ley lógica, es decir su tabla de verdad nos muestre que es una tautología. Por la regla de separación podremos concluir Y, de forma independiente como verdad. Dada la poca operatividad de las tablas de verdad, el cálculo se construye como una cadena deductiva aplicando a las premisas o a los teoremas deducidos las leyes lógicas utilizadas como reglas de transformación, como se expone en cálculo lógico. El lenguaje natural como modelo de un cálculo lógico Naturalmente el cálculo lógico es útil porque puede tener aplicaciones, pero ¿en qué consisten o cómo se hacen tales aplicaciones? Podemos considerar que el lenguaje natural es un modelo de C si podemos someterlo, es decir, aplicarle una correspondencia en C. Para ello es necesario someter al lenguaje natural a un proceso de formalización de tal forma que podamos reducir las expresiones lingüísticas del lenguaje natural a EBF de un cálculo mediante reglas estrictas manteniendo el sentido de verdad lógica de dichas expresiones del lenguaje natural. Esto es lo que se expone en cálculo lógico. Las diversas formas en que tratemos las expresiones lingüísticas formalizadas como proposiciones lógicas dan lugar a sistemas diversos de formalización y cálculo: Cálculo proposicional o cálculo de enunciados Cuando se toma la oración simple significativa del lenguaje natural con posible valor de verdad o falsedad como una proposición atómica, como un todo sin analizar. Cálculo como lógica de clases Cuando se toma la oración simple significativa del lenguaje natural con posible valor de verdad o falsedad como resultado del análisis de la oración como una relación de individuos o posibles individuos que poseen o no poseen una propiedad común determinada como pertenecientes o no pertenecientes a una clase natural o a un conjunto como individuos. Cálculo de predicados o cuantificacional Cuando se toma la oración simple significativa del lenguaje natural con posible valor de verdad o falsedad como resultado del análisis de la misma de forma que una posible función predicativa (P), se predica de unos posibles sujetos variables (x) [tomados en toda su posible extensión: (Todos los x); o referente a algunos indeterminados: (algunos x)], o de una constante individual existente (a). Cálculo como lógica de relaciones Cuando se toma la oración simple significativa con posible valor de verdad propio, verdadero o falso, como resultado del análisis de la oración como una relación R que se establece entre un sujeto y un predicado. La simbolización y formación de EBFs en cada uno de esos cálculos, así como las reglas de cálculo se trata en cálculo lógico."
ksampletext_wikipedia_math_algebra: str = "Álgebra. El álgebra (del árabe) es la rama de la matemática que estudia la combinación de elementos de estructuras abstractas acorde a ciertas reglas. Originalmente esos elementos podían ser interpretados como números o cantidades, por lo que el álgebra en cierto modo fue originalmente una generalización y extensión de la aritmética. En el álgebra moderna existen áreas del álgebra que en modo alguno pueden considerarse extensiones de la aritmética (álgebra abstracta, álgebra homológica, álgebra exterior, etc.). El álgebra elemental difiere de la aritmética en el uso de abstracciones, como el empleo de letras para representar números que son desconocidos o que pueden tomar muchos valores. Por ejemplo, en La palabra álgebra también se utiliza en ciertas formas especializadas. Un tipo especial de objeto matemático en el álgebra abstracta se llama álgebra, y la palabra se usa, por ejemplo, en las frases álgebra lineal y topología algebraica. Etimología La palabra álgebra proviene del y cálculo de datos del título del libro de principios del siglo, La ciencia del restablecimiento y el equilibrio por el matemático y astrónomo persa Muḥammad ibn Mūsā al-Khwārizmī. En su obra, el término al-jabr se refería a la operación de mover un término de un lado de una ecuación al otro, المقابلة al-muqābala equilibrar se refería a añadir términos iguales a ambos lados. Acortada a simplemente algeber o álgebra en latín, la palabra acabó entrando en la lengua inglesa durante el siglo XV, ya sea desde el español, el italiano o el latín medieval. Originalmente se refería al procedimiento quirúrgico de fijar huesos rotos o dislocados. El significado matemático se registró por primera vez (en inglés) en el siglo XVI. Introducción A diferencia de la aritmética elemental, que trata de los números y las operaciones fundamentales, en álgebra -para lograr la generalización- se introducen además símbolos (usualmente letras) para representar parámetros (variables o coeficientes), o cantidades desconocidas (incógnitas); las expresiones así formadas son llamadas «fórmulas algebraicas» y expresan una regla o un principio general. El álgebra conforma una de las grandes áreas de las matemáticas, junto a la teoría de números, la geometría y el análisis. Página del libro Kitāb al-mukhtaṣar fī ḥisāb al-ŷabr wa-l-muqābala, de Al-Juarismi La palabra «álgebra» proviene del vocablo árabe الجبر al-ŷabar (en árabe dialectal por asimilación progresiva se pronunciaba [alŷɛbɾ], de donde derivan los términos de las lenguas europeas), que se traduce como restauración o reposición, reintegración. Deriva del tratado escrito alrededor del año 820 e. c. por el matemático y astrónomo persa Muhammad ibn Musa al-Jwarizmi (conocido como Al Juarismi), titulado Al-kitāb al-mukhtaṣar fī ḥisāb al-ŷarabi waˀl-muqābala (Compendio de cálculo por reintegración y comparación), el cual proporcionaba operaciones simbólicas para la solución sistemática de ecuaciones lineales y cuadráticas. Muchos de sus métodos derivan del desarrollo de la matemática en el islam medieval, destacando la independencia del álgebra como una disciplina matemática independiente de la geometría y de la aritmética. Puede considerarse al álgebra como el arte de hacer cálculos del mismo modo que en aritmética, pero con objetos matemáticos no-numéricos. El adjetivo «algebraico» denota usualmente una relación con el álgebra, como por ejemplo en estructura algebraica. Por razones históricas, también puede indicar una relación con las soluciones de ecuaciones polinomiales, números algebraicos, extensión algebraica o expresión algebraica. Conviene distinguir entre: Álgebra elemental es la parte del álgebra que se enseña generalmente en los cursos de matemáticas. Álgebra abstracta es el nombre dado al estudio de las «estructuras algebraicas» propiamente. El álgebra usualmente se basa en estudiar las combinaciones de cadenas finitas de signos y, mientras que análisis matemático requiere estudiar límites y sucesiones de una cantidad infinita de elementos. Historia del álgebra Véase también: Historia de la matemática El álgebra en la antigüedad Las raíces del álgebra pueden rastrearse hasta la antigua matemática babilónica, que había desarrollado un avanzado sistema aritmético con el que fueron capaces de hacer cálculos en una forma algorítmica. Con el uso de este sistema lograron encontrar fórmulas y soluciones para resolver problemas que, en la actualidad, suelen resolverse mediante ecuaciones lineales, ecuaciones de segundo grado y ecuaciones indeterminadas. En contraste, la mayoría de los egipcios de esta época, y la mayoría de los matemáticos griegos y chinos del primer milenio antes de Cristo, normalmente resolvían tales ecuaciones por métodos geométricos, tales como los descritos en el Papiro de Rhind, Los Elementos de Euclides y Los nueve capítulos sobre el arte matemático. Papiro de Ahmes; datado entre 2000 al 1800 a. e. c. Papiro de Ahmes; datado entre 2000 al 1800 a. e. c. Elementos de Euclides, ca. Elementos de Euclides, ca. 300 a. e. c. Las nueve lecciones del arte matemático; compilado durante siglos II y III a. e. c. Las nueve lecciones del arte matemático; compilado durante siglos II y III a. e. c. Arithmetica; escrito por Diofanto alrededor de 280 de nuestra era Véase también: Matemática helénica Los matemáticos de la Antigua Grecia introdujeron una importante transformación al crear un álgebra de tipo geométrico, en donde los «términos» eran representados mediante los «lados de objetos geométricos», usualmente líneas a las cuales asociaban letras. Los matemáticos helénicos Herón de Alejandría y Diofanto así como también los matemáticos indios como Brahmagupta, siguieron las tradiciones de Egipto y Babilonia, si bien la Arithmetica de Diofanto y el Brahmasphutasiddhanta de Brahmagupta se hallan a un nivel de desarrollo mucho más alto. Por ejemplo, la primera solución aritmética completa (incluyendo al cero y soluciones negativas) para las ecuaciones cuadráticas fue descrita por Brahmagupta en su libro Brahmasphutasiddhanta. Más tarde, los matemáticos árabes y musulmanes desarrollarían métodos algebraicos a un grado mucho mayor de sofisticación. Diofanto (siglo III), algunas veces llamado «el pádre del álgebra», fue un matemático alejandrino, autor de una serie de libros intitulados Arithmetica. Estos textos tratan de las soluciones a las ecuaciones algebraicas. Influencia árabe Véase también: Matemática en el islam medieval Los babilonios y Diofanto utilizaron sobre todo métodos especiales ad hoc para resolver ecuaciones, la contribución de Al-Khwarizmi fue fundamental; resuelve ecuaciones lineales y cuadráticas sin el simbolismo algebraico, números negativos o el cero, por lo que debe distinguir varios tipos de >jab. El matemático persa Omar Khayyam desarrolló la geometría algebraica y encontró la solución geométrica de la ecuación cúbica. Otro matemático persa, Sharaf Al-Din al-Tusi, encontró la solución numérica y algebraica a diversos casos de ecuaciones cúbicas; también desarrolló el concepto de función. Los matemáticos indios Mahavirá y Bhaskara II, el matemático persa Al-Karaji, y el matemático chino Zhu Shijie, resolvieron varios casos de ecuaciones de grado tres, cuatro y cinco, así como ecuaciones polinómicas de orden superior mediante métodos numéricos. Edad Moderna Durante la Edad Moderna europea tienen lugar numerosas innovaciones, y se alcanzan resultados que claramente superan los resultados obtenidos por los matemáticos árabes, persas, indios o griegos. Parte de este estímulo viene del estudio de las ecuaciones polinómicas de tercer y cuarto grado. Las soluciones para ecuaciones polinómicas de segundo grado ya era conocida por los matemáticos babilónicos cuyos resultados se difundieron por todo el mundo antiguo. El descrubrimiento del procedimiento para encontrar soluciones algebraicas de tercer y cuarto orden se dieron en la Italia del siglo XVI. También es notable que la noción de determinante fue descubierta por el matemático japonés Kowa Seki en el siglo XVII, seguido por Gottfried Leibniz diez años más tarde, con el fin de resolver sistemas de ecuaciones lineales simultáneas utilizando matrices. Entre los siglos XVI y XVII se consolidó la noción de número complejo, con lo cual la noción de álgebra empezaba a apartarse de cantidades medibles. Gabriel Cramer también hizo un trabajo sobre matrices y determinantes en el siglo XVIII. También Leonhard Euler, Joseph-Louis Lagrange, Adrien-Marie Legendre y numerosos matemáticos del siglo XVIII hicieron avances notables en álgebra. Siglo XIX El álgebra abstracta se desarrolló en el siglo XIX, inicialmente centrada en lo que hoy se conoce como teoría de Galois y en temas de la constructibilidad. Los trabajos de Gauss generalizaron numerosas estructuras algebraicas. La búsqueda de una fundamentación matemática rigurosa y una clasificación de los diferentes tipos de construcciones matemáticas llevó a crear áreas del álgebra abstracta durante el siglo XIX absolutamente independientes de nociones aritméticas o geométricas (algo que no había sucedido con el álgebra de los siglos anteriores). Áreas de matemáticas con la palabra álgebra en su nombre Algunas áreas de las matemáticas que entran en la clasificación de álgebra abstracta tienen la palabra álgebra en su nombre; el álgebra lineal es un ejemplo. Otras no: La teoría de grupos, la teoría de anillos y la teoría de campos son ejemplos. En esta sección, enumeramos algunas áreas de las matemáticas con la palabra álgebra en el nombre. Álgebra elemental, la parte del álgebra que se suele enseñar en los cursos elementales de matemáticas. Álgebra abstracta, en la que se definen e investigan las estructuras algebraicas como grupos, anillos y campos. Álgebra lineal, en la que se estudian las propiedades específicas de las ecuaciones lineales, los espacios vectoriales y las matrices. Álgebra de Boole, una rama del álgebra que abstrae el cálculo con los valor de verdades falso y verdadero. Álgebra conmutativa, el estudio de los anillos conmutativos. Álgebra computacional, la implementación de métodos algebraicos como algoritmos y programas de ordenador. Álgebra homológica, el estudio de las estructuras algebraicas fundamentales para el estudio de los espacios topológicos. Álgebra universal, en la que se estudian propiedades comunes a todas las estructuras algebraicas. Teoría de números algebraicos, en la que se estudian las propiedades de los números desde un punto de vista algebraico. Geometría algebraica, una rama de la geometría, que en su forma primitiva especifica las curvas y superficies como soluciones de ecuaciones polinómicas. Combinatoria algebraica, en la que se utilizan métodos algebraicos para estudiar cuestiones combinatorias. Álgebra relacional: conjunto de relaciones finitas que son cerradas bajo ciertos operadores. Muchas estructuras matemáticas se llaman álgebras: Álgebra sobre un cuerpo o más generalmente álgebra sobre un anillo. Muchas clases de álgebras sobre un campo o sobre un anillo tienen un nombre específico: Álgebra asociativa Álgebra no asociativa Álgebra de Lie Álgebra de Hopf C*-álgebra Álgebra simétrica Álgebra exterior Álgebra tensorial En teoría de la medida, σ-álgebra Álgebra sobre un conjunto En teoría de categorías Álgebra F y co-álgebra F Álgebra T En lógica, Álgebra de relación, un álgebra booleana residuada expandida con una involución llamada conversa. Álgebra booleana, un complementado del Retículo distributivo. Álgebra de Heyting Notación algebraica Notación matemática de raíz cuadrada de x Consiste en que los números se emplean para representar cantidades conocidas y determinadas. Las letras se emplean para representar toda clase de cantidades, ya sean conocidas o desconocidas. Las cantidades conocidas se expresan por las primeras letras del alfabeto: a, b, c, d, … Las cantidades desconocidas se representan por las últimas letras del alfabeto: u, v, w, x, y, z. Los signos empleados en álgebra son tres clases: Signos de operación, signos de relación y signos de agrupación. Signos de operación En álgebra se verifican con las cantidades las mismas operaciones que en aritmética: suma, resta, multiplicación, elevación a potencias y extracción de raíces, que se indican con los principales signos de aritmética excepto el signo de multiplicación. En lugar del signo × suele emplearse un punto entre los factores y también se indica a la multiplicación colocando los factores entre paréntesis. Así a⋅b y (a)(b) equivale a a × b. Signos de relación Se emplean estos signos para indicar la relación que existe entre dos cantidades. Los principales son: =, que se lee igual a. Así, a=b se lee “a igual a b”. >, que se lee mayor que. Así, x + y > m se lee “x + y mayor que m”. <, que se lee menor que. Así, a < b + c se lee “a menor que b + c”. Signos de agrupación Signos y símbolos más comunes Los signos y símbolos son utilizados en el álgebra ,y en general en teoría de conjuntos y álgebra de conjuntos, con los que se constituyen ecuaciones, matrices, series, etc. Sus letras son llamadas variables, ya que se usa esa misma letra en otros problemas y su valor va variando. Aquí algunos ejemplos: Signos y símbolos Expresión Uso + Además de expresar adición, también es usada para expresar operaciones binarias c o k Expresan términos constantes Primeras letras del abecedario a, b, c, … Se utilizan para expresar cantidades conocidas Últimas letras del abecedario …, x, y, z Se utilizan para expresar incógnitas n Expresa cualquier número (1, 2, 3, 4, …, n) Exponentes y subíndices Expresar cantidades de la misma especie, de diferente magnitud. Simbología de Conjuntos Símbolo Descripción ∈ Es un elemento del conjunto o pertenece al conjunto. ∉ No es un elemento del conjunto o no pertenece al conjunto. ⎜ Tal que n (C) Cardinalidad del conjunto C U Conjunto Universo Φ Conjunto vacío ⊆ Subconjunto de ⊂ Subconjunto propio de ⊄ No es subconjunto propio de > Mayor que < Menor que ≥ Mayor o igual que ≤ Menor o igual que ∩ Intersección de conjuntos ∪ Unión de Conjuntos A Complemento del conjunto A = Símbolo de igualdad ≠ No es igual a … El conjunto continúa ⇔ Si y solo si ¬ (en algunos ocasiones ∼) No, negación lógica (es falso que) ∧ Y ∨ O Lenguaje algebraico Lenguaje algebraico Lenguaje común Lenguaje algebraico Un número cualquiera m Un número cualquiera aumentado en siete m + 7 La diferencia de dos números cualesquiera f - q El doble de un número excedido en cinco 2x + 5 La división de un número entero entre su antecesor x/(x-1) La mitad de un número d/2 El cuadrado de un número y^2 La media de la suma de dos números (b+c)/2 Las dos terceras partes de un número disminuidos en cinco es igual a 12. 2/3 (x-5) = 12 Tres números naturales consecutivos. x, x + 1, x + 2. La parte mayor de 1200, si la menor es w 1200 - w El cuadrado de un número aumentado en siete b2 + 7 Las tres quintas partes de un número más la mitad de su consecutivo equivalen a tres. 3/5 p + 1/2 (p+1) = 3 El producto de un número con su antecesor equivalen a 30. x(x-1) = 30 El cubo de un número más el triple del cuadrado de dicho número x3 + 3x2 Estructura algebraica Artículo principal: Estructura algebraica En matemáticas, una estructura algebraica es un conjunto de elementos con unas propiedades operacionales determinadas; es decir, lo que define a la estructura del conjunto son las operaciones que se pueden realizar con los elementos de dicho conjunto y las propiedades matemáticas que dichas operaciones poseen. Un objeto matemático constituido por un conjunto no vacío y algunas leyes de composición interna definida en él es una estructura algebraica. Las estructuras algebraicas más importantes son."
ksampletext_wikipedia_math_numero: str = "Número. Un número es un objeto matemático utilizado para contar (cantidades), medir (magnitudes) y etiquetar. Los números más sencillos, que utilizamos todos en la vida cotidiana, son los números naturales: 1, 2, 3, etc. Se denotan mediante Los números desempeñan un papel esencial en las ciencias empíricas, ya que permiten cuantificar y describir fenómenos observables. No solo los números naturales son utilizados, sino también diversos conjuntos numéricos desarrollados a lo largo de la historia de las matemáticas. El conjunto de los números enteros (representado por Desde la antigüedad, se conoce la existencia de números que no pueden expresarse como fracciones de enteros. Por ejemplo, la longitud de la diagonal de un cuadrado de lado unidad es Con el tiempo, se introdujeron otros tipos de números para ampliar el alcance del análisis matemático, como los imaginarios, complejos y trascendentes, que permiten describir y resolver fenómenos de creciente complejidad en diversas ramas de la ciencia y la ingeniería. Nótese que la teoría de números es una rama de las matemáticas que se ocupa de los enteros (no de números en general). El origen de los números es que los primeros números que el hombre inventó fueron los números naturales, los cuales se utilizaban y se utilizan para contar elementos de un conjunto finito, ya que se procede a enumerar dichos elementos de una manera ordenada seleccionándolos uno tras otro a la vez que se le atribuye a cada uno un número. Tipos de números Clasificación de los números. Los números más conocidos son los números naturales. Denotados mediante Otro tipo de números ampliamente usados son números fraccionarios, los cuales representan tanto cantidades inferiores a una unidad, como números mixtos (un conjunto de unidades más una parte inferior a la unidad). Los números fraccionarios pueden ser expresados siempre como cocientes de enteros. El conjunto de todos los números fraccionarios es el conjunto de los números racionales (que usualmente se define para que incluya tanto a los racionales positivos, como a los racionales negativos y el cero). Este conjunto de números se designa como Los números racionales permiten resolver gran cantidad de problemas prácticos, pero desde los antiguos griegos se conoce que ciertas relaciones geométricas (la diagonal de un cuadrado de lado unidad) son números no enteros que tampoco son racionales. Igualmente, la solución numérica de una ecuación polinómica cuyos coeficientes son números racionales, usualmente es un número no racional. Puede demostrarse que cualquier número irracional puede representarse como una sucesión de Cauchy de números racionales que se aproximan a un límite numérico. El conjunto de todos los números racionales y los irracionales (obtenidos como límites de sucesiones de Cauchy de números racionales) es el conjunto de los números reales Uno de los problemas de los números reales es que no forman un cuerpo algebraicamente cerrado, por lo que ciertos problemas no tienen solución planteados en términos de números reales. Esa es una de las razones por las cuales se introdujeron los números complejos Al margen de los números reales y complejos, claramente conectados con problemas de las ciencias naturales, existen otros tipos de números que generalizan aún más y extienden el concepto de número de una manera más abstracta y responden más a creaciones deliberadas de matemáticos. La mayoría de estas generalizaciones del concepto de número se usan solo en matemáticas, aunque algunos de ellos han encontrado aplicaciones para resolver ciertos problemas físicos. Entre ellos están los números hipercomplejos, que incluyen a los cuaterniones, útiles para representar rotaciones en un espacio de tres dimensiones, y generalizaciones de estos, como octoniones y los sedeniones. A un nivel un poco más abstracto también se han ideado conjuntos de números capaces de tratar con cantidades infinitas e infinitesimales, como los hiperreales Lista de los tipos de números existentes La teoría de los números trata básicamente de las propiedades de los números naturales y los enteros, mientras que las operaciones del álgebra y el cálculo permiten definir la mayor parte de los sistemas numéricos, entre los cuales están: Números naturales Número primo Números compuestos Números perfectos Números enteros Números negativos Números pares Números impares Números racionales Números reales Números irracionales Números algebraicos Números trascendentes Extensiones de los números reales Números complejos Números hipercomplejos Cuaterniones Octoniones Números hiperreales Números superreales Números surreales Números usados en teoría de conjuntos Números ordinales Números cardinales Números transfinitos Estructura de los sistemas numéricos En álgebra abstracta y análisis matemático un sistema numérico se caracteriza por una: Estructura algebraica, usualmente un anillo conmutativo o cuerpo matemático (en el caso no conmutativo son un álgebra sobre un cuerpo y en el caso de los números naturales solo un monoide conmutativo). Estructura de orden, usualmente un conjunto ordenado, en el caso de los números naturales, enteros, racionales y reales se trata de conjuntos totalmente ordenados, aunque los números complejos e hipercomplejos solo son conjuntos parcialmente ordenados. Los reales además son un conjunto bien ordenado y con un orden denso. Estructura topológica, los conjuntos numéricos numerables usualmente son conjuntos disconexos, sobre los que se considera la topología discreta, mientras que sobre los conjuntos no numerables se considera una topología que los hace adecuados para el análisis matemático. Otra propiedad interesante de muchos conjuntos numéricos es que son representables mediante diagramas de Hasse, diagramas de Euler y diagramas de Venn, pudiéndose tomar una combinación de ambos en un diagrama de Euler-Venn con la forma característica de cuadrilátero y además pudiéndose representar internamente un diagrama de Hasse (es una recta). Tanto históricamente como conceptualmente, los diversos conjuntos numéricos, desde el más simple de los números naturales, hasta extensiones transcendentes de los números reales y complejos, elaboradas mediante la teoría de modelos durante el siglo XX, se construyen desde una estructura más simple hasta otra más compleja. Números naturales especiales El estudio de ciertas propiedades que cumplen los números ha producido una enorme cantidad de tipos de números, la mayoría sin un interés matemático específico. Se pueden encuadrar dentro de la matemática recreativa. A continuación se indican algunos: Perfecto: número igual a la suma de sus divisores (incluyendo el 1). Ejemplo: 6 = 1 + 2 + 3. Sheldon: el número 73, es el 21° número primo, que al multiplicar 7 × 3 = 21; Y al dar la vuelta a sus dígitos da 37 que es el 12° número primo. Narcisista: número de n dígitos que resulta ser igual a la suma de las potencias de orden n de sus dígitos. Ejemplo: 153 = 1³ + 5³ + 3³. Omirp: número primo que al invertir sus dígitos da otro número primo. Ejemplo: 1597 y 7951 son primos. Vampiro: número que es el producto de dos números obtenidos a partir de sus dígitos. Ejemplo: 2187 = 27 × 81. Hamsteriano: Su estructura aritmética N= (a×b)2-1, donde a y b son primos los dos, la suma de sus divisores sobrepasa N, y la cantidad de sus divisores es > a×b/2; va como ejemplo: 1224 = (5×7)2-1 Pitagórico: una terna pitagórica son tres números que cumplen las siguientes condiciones: el cuadrado de uno de ellos, más el cuadrado de otro, es igual al cuadrado del tercero, por ejemplo: (3, 4, 5) ya que 32 + 42 = 9 + 16 = 25 = 52 Una vez entendido el problema de la naturaleza y la clasificación de los números, surge otro, más práctico, pero que condiciona todo lo que se va a hacer con ellos: la manera de escribirlos. El sistema que se ha impuesto universalmente es la numeración posicional, gracias al invento del cero, con una base constante. Más formalmente, en Los fundamentos de la aritmética, Gottlob Frege (1848-1925) realiza una definición de «número», la cual fue tomada como referencia por muchos matemáticos (entre ellos Bertrand Russell [1872-1870], cocreador de Principia mathematica): «n» es un número, es entonces la definición de «que existe un concepto “F” para el cual “n” aplica», que a su vez se ve explicado como que «n» es la extensión del concepto «equinumerable con» para «F», y dos conceptos son equinumerables si existe una relación «uno a uno» (véase que no se utiliza el símbolo «1» porque no está definido aún) entre los elementos que lo componen (es decir, una biyección en otros términos). Véase también que Frege, tanto como cualquier otro matemático, se ve inhabilitado para definir al número como la expresión de una cantidad, porque la simbología matemática no hace referencia necesaria a la numerabilidad, y el hecho de «cantidad» referiría a algo numerable, mientras que números se adoptan para definir la cardinalidad de, por ejemplo, los elementos que se encuentran en el intervalo abierto (0, 1), que contiene innumerables elementos (el continuo). Peano, antes de establecer sus cinco proposiciones sobre los números naturales, explícita que supone sabida una definición (quizás debido a su «obviedad») de las palabras o conceptos cero, sucesor y número. De esta manera postula: 0 es un número natural el sucesor de todo número es un número dos números diferentes no tienen el mismo sucesor 0 no es el sucesor de ningún número y la propiedad inductiva Sin embargo, si uno define el concepto cero como el número 100, y el concepto número como los números mayores a 100, entonces las cinco proposiciones mencionadas anteriormente aplican, no a la idea que Peano habría querido comunicar, sino a su formalización. La definición de número se encuentra por ende no totalmente formalizada, aunque se encuentre un acuerdo mayoritario en adoptar la definición enunciada por Frege. Historia del concepto de número Hueso de Ishango. Cognitivamente el concepto de número está asociado a la habilidad de contar y comparar cuál de dos conjuntos de entidades similares tiene mayor cantidad de elementos. Las primeras sociedades humanas se encontraron muy pronto con el problema de determinar cuál de dos conjuntos era «mayor» que otro, o de conocer con precisión cuántos elementos formaban una colección de cosas. Esos problemas podían ser resueltos simplemente contando. La habilidad de contar del ser humano, no es un fenómeno simple, aunque la mayoría de culturas tienen sistemas de cuenta que llegan como mínimo a centenares, algunos pueblos con una cultura material simple, solo disponen de términos para los números 1, 2 y 3 y usualmente usan el término «muchos» para cantidades mayores, aunque cuando es necesario usan recursivamente expresiones traducibles como «3 más 3 y otros 3». El conteo se debió iniciar mediante el uso de objetos físicos (tales como montones de piedras) y de marcas de cuenta, como las encontradas en huesos tallados: el de Lebombo, con 29 muescas grabadas en un hueso de babuino, tiene unos 37 000 años de antigüedad y otro hueso de lobo encontrado en la antigua Checoslovaquia, con 57 marcas dispuestas en cinco grupos de 11 y dos sueltas, se ha estimado en unos 30 000 años de antigüedad. Ambos casos constituyen una de las más antiguas marcas de cuenta conocidas habiéndose sugerido que pudieran estar relacionadas con registros de fases lunares. En cuanto al origen ordinal algunas teorías lo sitúan en rituales religiosos. Los sistemas numerales de la mayoría de familias lingüísticas reflejan que la operación de contar estuvo asociado al conteo de dedos (razón por la cual los sistemas de base decimal y vigesimal son los más abundantes), aunque está testimoniado el empleo de otras bases numéricas. El paso hacia los símbolos numerales, al igual que la escritura, se ha asociado a la aparición de sociedades complejas con instituciones centralizadas constituyendo artificios burocráticos de contabilidad en registros impositivos y de propiedades. Su origen estaría en primitivos símbolos con diferentes formas para el recuento de diferentes tipos de bienes como los que se han encontrado en Mesopotamia inscritos en tablillas de arcilla que a su vez habían venido a sustituir progresivamente el conteo de diferentes bienes mediante fichas de arcilla (constatadas al menos desde el 8000 a. C.) Los símbolos numerales más antiguos encontrados se sitúan en las civilizaciones mesopotámicas usándose como sistema de numeración ya no solo para la contabilidad o el comercio sino también para la agrimensura o la astronomía como, por ejemplo, registros de movimientos planetarios. En conjunto, desde hace 5000 años la mayoría de las civilizaciones han contado como lo hacemos hoy aunque la forma de escribir los números (si bien todos representan con exactitud los naturales) ha sido muy diversa. Básicamente la podemos clasificar en tres categorías: Sistemas de notación aditiva. Acumulan los símbolos de todas las unidades, decenas, centenas, …, necesarios hasta completar el número. Aunque los símbolos pueden ir en cualquier orden, adoptaron siempre una determinada posición (de más a menos). De este tipo son los sistemas de numeración: egipcio, hitita, cretense, romano, griego, armenio y judío. Sistemas de notación híbrida. Combinan el principio aditivo con el multiplicativo. En los anteriores 500 se representa con 5 símbolos de 100, en estos se utiliza la combinación del 5 y el 100. El orden de las cifras es ahora fundamental (estamos a un paso del sistema posicional). De este tipo son los sistemas de numeración: chino clásico, asirio, armenio, etíope y maya. Este último utilizaba símbolos para el 1, el 5 y el 0. Siendo este el primer uso documentado del cero tal como lo conocemos hoy (año 36 a. C.) ya que el de los babilonios solo se utilizaba entre otros dígitos. Sistemas de notación posicional. La posición de las cifras nos indica si son unidades, decenas, centenas, …, o en general la potencia de la base. Solo tres culturas además de la india lograron desarrollar un sistema de este tipo: el sistema chino (300 a. C.) que no disponía de 0, el sistema babilónico (2000 a. C.) con dos símbolos, de base 10 aditivo hasta el 60 y posicional (de base 60) en adelante, sin 0 hasta el 300 a. C. Las fracciones unitarias egipcias (Papiro de Ahmes/Rhind) Artículo principal: Fracción egipcia En este papiro adquirido por Alexander Henry Rhind (1833-1863) en 1858, cuyo contenido data del 2000 al 1800 a. C. además del sistema de numeración antes descrito nos encontramos con su tratamiento de las fracciones. No consideran las fracciones en general, solo las fracciones unitarias (inversas de los naturales 1/20) que se representan con un signo oval encima del número, la fracción 2/3 que se representa con un signo especial y en algunos casos fracciones del tipo Al ser un sistema sumativo la notación es: 1+1/2+1/4. La operación fundamental es la suma y nuestras multiplicaciones y divisiones se hacían por «duplicaciones» y «mediaciones», por ejemplo 69×19=69×(16+2+1), donde 16 representa 4 duplicaciones y 2 una duplicación. Fracciones sexagesimales babilónicas (documentos cuneiformes) En las tablillas cuneiformes de la dinastía Hammurabi (1800-1600 a. C.) aparece el sistema posicional, antes referido, extendido a las fracciones, pero XXX vale para Para calcular recurrían, como nosotros antes de disponer de máquinas, a las numerosas tablas que disponían: De multiplicar, de inversos, de cuadrados y cubos, de raíces cuadradas y cúbicas, de potencias sucesivas de un número dado no fijo, etc. Por ejemplo, para calcular Realizaban las operaciones de forma parecida a hoy, la división multiplicando por el inverso (para lo que utilizan sus tablas de inversos). En la tabla de inversos faltan los de 7 y 11 que tienen una expresión sexagesimal infinitamente larga. Sí están, pero no se percataron del desarrollo periódico. Descubrimiento de los inconmensurables Las circunstancias y la fecha de este descubrimiento son inciertas, aunque se atribuye a la escuela pitagórica (se utiliza el teorema de Pitágoras). Aristóteles (384-322 a. C.) menciona una demostración de la inconmensurabilidad de la diagonal de un cuadrado con respecto a su lado basada en la distinción entre lo par y lo impar. La reconstrucción que realiza C. Boyer es: Sean d:diagonal, s:lado y d/s racional, que podremos escribirlo como La teoría pitagórica de todo es número quedó seriamente dañada. El problema lo resolvería Eudoxo de Cnido (408-355 a. C.) tal como nos indica Euclides en el libro V de Los elementos. Para ello estableció el axioma de Arquímedes: «Dos magnitudes tienen una razón si se puede encontrar un múltiplo de una de ellas que supere a la otra» (excluye el 0). Después, en la definición 5, da la famosa formulación de Eudoxo: «Dos magnitudes están en la misma razón En el libro Historia de la matemática (1985), de J. P. Colette, se hace la observación de que esta definición está muy próxima a la de número real que dará Dedekind (1831-1916), divide las fracciones en las Creación del cero Artículo principal: Cero En cualquier sistema de numeración posicional surge el problema de la falta de unidades de determinado orden. Por ejemplo, en el sistema babilónico el número Hacia el siglo III a. C., en Grecia, se comenzó a representar la nada mediante una o que significa oudos vacío, y que no dio origen al concepto de cero como existe hoy en día. La idea del cero como concepto matemático parece haber surgido en la India antes que en ningún otro lugar. La única notación ordinal del Viejo Mundo fue la sumeria, donde el cero se representaba por un vacío. En América, la primera expresión conocida del sistema de numeración vigesimal prehispánico data del siglo III a. C. Se trata de una estela olmeca tardía, la cual ya contaba tanto con el concepto de orden como el de cero. Los mayas inventaron cuatro signos para el cero; los principales eran: el corte de un caracol para el cero matemático, y una flor para el cero calendárico (que implicaba no la ausencia de cantidad, sino el cumplimiento de un ciclo). Números negativos Brahmagupta, en el 628 de nuestra era, considera las dos raíces de las ecuaciones cuadráticas, aunque una de ellas sea negativa o irracional. De hecho en su obra es la primera vez que aparece sistematizada la aritmética (+, -, *, /, potencias y raíces) de los números positivos, negativos y el cero, que él llamaba los bienes, las deudas y la nada. Así, por ejemplo, para el cociente, establece: Positivo dividido por positivo, o negativo dividido por negativo, es afirmativo. Cifra dividido por cifra es nada (0/0=0). Positivo dividido por negativo es negativo. Negativo dividido por afirmativo es negativo. Positivo o negativo dividido por cifra es una fracción que la tiene por denominador (a/0=¿?) No solo utilizó los negativos en los cálculos, sino que los consideró como entidades aisladas, sin hacer referencia a la geometría. Todo esto se consiguió gracias a su despreocupación por el rigor y la fundamentación lógica, y su mezcla de lo práctico con lo formal. Sin embargo, el tratamiento que hicieron de los negativos cayó en el vacío, y fue necesario que transcurrieran varios siglos (hasta el Renacimiento) para que fuese recuperado. Al parecer, los chinos también poseían la idea de número negativo, y estaban acostumbrados a calcular con ellos utilizando varillas negras para los negativos y rojas para los positivos. Transmisión del sistema indo-arábigo a Occidente Varios autores del siglo XIII contribuyeron a esta difusión, destacan Alexandre de Villedieu (1225), Sacrobosco (circa 1195, o 1200-1256) y sobre todo Leonardo de Pisa (1180-1250). Este último, conocido como Fibonacci, viajó por Oriente y aprendió de los árabes el sistema posicional hindú. Escribió un libro, El Liber abaci, que trata en el capítulo I la numeración posicional, en los cuatro siguientes las operaciones elementales, en los capítulos VI y VII las fracciones: comunes, sexagesimales y unitarias (¡no usa los decimales, principal ventaja del sistema!), y en el capítulo XIV los radicales cuadrados y cúbicos. También contiene el problema de los conejos que da la serie: No aparecen los números negativos, que tampoco consideraron los árabes, debido a la identificación de número con magnitud (¡obstáculo que duraría siglos!). A pesar de la ventaja de sus algoritmos de cálculo, se desataría por diversas causas una lucha encarnizada entre abacistas y algoristas, hasta el triunfo final de estos últimos. Las fracciones continuas Pietro Antonio Cataldi (1548-1626), aunque con ejemplos numéricos, desarrolla una raíz cuadrada en fracciones continuas como hoy: Queremos calcular Siendo así los números irracionales aceptados con toda normalidad, pues se les podía aproximar fácilmente mediante números racionales. Primera formulación de los números complejos Los números complejos eran en pocos casos aceptados como raíces o soluciones de ecuaciones (M. Stifel (1487-1567), S. Stevin (1548-1620)) y por casi ninguno como coeficientes). Estos números se llamaron inicialmente ficticii ficticios (el término imaginario usado actualmente es reminiscente de estas reticencias a considerarlos números respetables). A pesar de esto G. Cardano (1501-1576) conoce la regla de los signos y R. Bombelli (1526-1573) las reglas aditivas a través de haberes y débitos, pero se consideran manipulaciones formales para resolver ecuaciones, sin entidad al no provenir de la medida o el conteo. Cardano en la resolución del problema dividir 10 en dos partes tales que su producto valga 40 obtiene como soluciones En la resolución de ecuaciones cúbicas con la fórmula de Cardano-Tartaglia, aunque las raíces sean reales, aparecen en los pasos intermedios raíces de números negativos. En esta situación Bombelli dice en su Álgebra que tuvo lo que llamó una idea loca, esta era que los radicales podían tener la misma relación que los radicandos y operar con ellos, tratando de eliminarlos después. En un texto posterior en 20 años utiliza p.d.m. Generalización de las fracciones decimales Aunque se encuentra un uso más que casual de las fracciones decimales en la Arabia medieval y en la Europa renacentista, y ya en 1579 Vieta (1540-1603) proclamaba su apoyo a éstas frente a las sexagesimales, y las aceptaban los matemáticos que se dedicaban a la investigación, su uso se generalizó con la obra que Simon Stevin publicó en 1585 De Thiende (La Disme). En su definición primera dice que la Disme es una especie de aritmética que permite efectuar todas las cuentas y medidas utilizando únicamente números naturales. En las siguientes define nuestra parte entera: cualquier número que vaya el primero se dice comienzo y su signo es (0), (primera posición decimal 1/10). El siguiente se dice primera y su signo es (1) (segunda posición decimal 1/100). El siguiente se dice segunda (2). Es decir, los números decimales que escribe: 0,375 como 3(1)7(2)5(3), o 372,43 como 372(0)4(1)3(2). Añade que no se utiliza ningún número roto (fracciones), y el número de los signos, exceptuando el 0, no excede nunca a 9. Esta notación la simplificó Joost Bürgi (1552-1632) eliminando la mención al orden de las cifras y sustituyéndolo por un «.» en la parte superior de las unidades 372·43, poco después Magini (1555-1617) usó el «.» entre las unidades y las décimas: 372.43, uso que se generalizaría al aparecer en la Constructio de Napier (1550-1617) de 1619. La «,» también fue usada a comienzos del siglo XVII por el holandés Willebrord Snellius: 372,43. El principio de inducción matemática Artículo principal: Inducción matemática Su antecedente es un método de demostración, llamado inducción completa, por aplicación reiterada de un mismo silogismo que se extiende indefinidamente y que usó Maurolyco (1494-1575) para demostrar que la suma de los primeros Si El hecho de que entonces De manera intuitiva se entiende la inducción como un efecto dominó. Suponiendo que se tiene una fila infinita de fichas de dominó, el paso base equivale a tirar la primera ficha; por otro lado, el paso inductivo equivale a demostrar que si alguna ficha se cae, entonces la ficha siguiente también se caerá. La conclusión es que se pueden tirar todas las fichas de esa fila. La interpretación geométrica de los números complejos Esta interpretación suele ser atribuida a Gauss (1777-1855) que hizo su tesis doctoral sobre el teorema fundamental del álgebra, enunciado por primera vez por Harriot y Girard en 1631, con intentos de demostración realizados por DAlembert, Euler y Lagrange, demostrando que las pruebas anteriores eran falsas y dando una demostración correcta primero para el caso de coeficientes, y después de complejos. También trabajó con los números enteros complejos que adoptan la forma La representación gráfica de los números complejos había sido descubierta ya por Caspar Wessel (1745-1818) pero pasó desapercibida, y así el plano de los números complejos se llama «plano de Gauss» a pesar de no publicar sus ideas hasta 30 años después. Desde la época de Girard (mitad siglo XVII) se conocía que los números reales se pueden representar en correspondencia con los puntos de una recta. Al identificar ahora los complejos con los puntos del plano los matemáticos se sentirán cómodos con estos números, ver es creer. Descubrimiento de los números trascendentes La distinción entre números irracionales algebraicos y trascendentes data del siglo XVIII, en la época en que Euler demostró que Liouville (1809-1882) demostró en 1844 que todos los números de la forma Hermite (1822-1901) en una memoria Sobre la función exponencial de 1873 demostró la trascendencia de Lindeman (1852-1939) en la memoria Sobre el número El problema 7 de Hilbert (1862-1943) que plantea si Teorías de los irracionales Hasta mediados del siglo XIX los matemáticos se contentaban con una comprensión intuitiva de los números y sus sencillas propiedades no son establecidas lógicamente hasta el siglo XIX. La introducción del rigor en el análisis puso de manifiesto la falta de claridad y la imprecisión del sistema de los números reales, y exigía su estructuración lógica sobre bases aritméticas. Bolzano había hecho un intento de construir los números reales basándose en sucesiones de números racionales, pero su teoría pasó desapercibida y no se publicó hasta 1962. Hamilton hizo un intento, haciendo referencia a la magnitud tiempo, a partir de particiones de números racionales: si cuando y si cuando pero no desarrolló más su teoría. Pero en el mismo año 1872 cinco matemáticos, un francés y cuatro alemanes, publicaron sus trabajos sobre la aritmetización de los números reales: Charles Meray (1835-1911) en su obra Nouveau précis danalyse infinitesimale define el número irracional como un límite de sucesiones de números racionales, sin tener en cuenta que la existencia misma del límite presupone una definición del número real. Hermann Heine (1821-1881) publicó, en el Journal de Crelle en 1872, su artículo «Los elementos de la teoría de funciones», donde proponía ideas similares a las de Cantor, teoría que en conjunto se llama actualmente «teorema de Heine-Cantor». Richard Dedekind (1831-1916) publica su Stetigkeit und irrationale zahlen. Su idea se basa en la continuidad de la recta real y en los agujeros que hay si solo consideramos los números racionales. En la sección dedicada al «dominio R» enuncia un axioma por el que se establece la continuidad de la recta: «cada punto de la recta divide los puntos de ésta en dos clases tales que cada punto de la primera se encuentra a la izquierda de cada punto de la segunda clase, entonces existe un único punto que produce esta división». Esta misma idea la utiliza en la sección «creación de los números irracionales» para introducir su concepto de «cortadura». Bertrand Russell apuntaría después que es suficiente con una clase, pues esta define a la otra. Georg Cantor (1845-1918). Define los conceptos de: sucesión fundamental, sucesión elemental, y límite de una sucesión fundamental, y partiendo de ellos define el número real. Karl Weierstrass (1815-1897). No llegó a publicar su trabajo, continuación de los de Bolzano, Abel y Cauchy, pero fue conocido por sus enseñanzas en la Universidad de Berlín. Su caracterización basada en los «intervalos encajados», que pueden contraerse a un número racional pero no necesariamente lo hacen, no es tan generalizable como las anteriores, pero proporciona fácil acceso a la representación decimal de los números reales. Álgebras hipercomplejas La construcción de obtención de los números complejos a partir de los números reales, y su conexión con el grupo de transformaciones afines en el plano sugirió a algunos matemáticos otras generalizaciones similares conocidas como números hipercomplejos. En todas estas generalizaciones los números complejos son un subconjunto de estos nuevos sistemas numéricos, aunque estas generalizaciones tienen la estructura matemática de álgebra sobre un cuerpo, pero en ellos la operación de multiplicación no es conmutativa. Teoría de conjuntos Artículo principal: Teoría de conjuntos La teoría de conjuntos sugirió muchas y variadas formas de extender los números naturales y los números reales de formas diferentes a como los números complejos extendían al conjunto de los números reales. El intento de capturar la idea de conjunto con un número no finito de elementos llevó a la aritmética de números transfinitos que generalizan a los naturales, pero no a los números enteros. Los números transfinitos fueron introducidos por Georg Cantor hacia 1873. Los números hiperreales usados en el análisis no estándar generalizan a los reales pero no a los números complejos (aunque admiten una complejificación que generalizaría también a los números complejos). Aunque parece los números hiperreales no proporcionan resultados matemáticos interesantes que vayan más allá de los obtenibles en el análisis real, algunas demostraciones y pruebas matemáticas parecen más simples en el formalismo de los números hiperreales, por lo que no están exentos de importancia práctica. Socialmente Los números naturales por la necesidad de contar. Los números fraccionarios por la necesidad de medir partes de un todo, y compartir. Los enteros negativos por fenómenos de doble sentido: izquierda-derecha, arriba-abajo, pérdida-ganancia. Los números reales por la necesidad de medir segmentos. Los números complejos por exigencias de resolver ecuaciones algebraicas, como el caso de la cúbicas o de x2 + 1 = 0. Sistemas de representación de los números Cifra, dígito y numeral Artículo principal: Cifra (matemática) Una de las formas más frecuentes de representar números por escrito consiste en un «conjunto finito de símbolos» o dígitos que, adecuadamente combinados, permiten formar cifras que funcionan como representaciones de números (cuando una secuencia específica de signos se emplea para representar un número se la llama numeral, aunque una cifra también puede representar simplemente un código identificativo). Base numérica Artículo principal: Base (aritmética) Tanto las lenguas naturales como la mayor parte de sistemas de representación de números mediante cifras, usan un inventario finito de unidades para expresar una cantidad mucho mayor de números. Una manera importante de lograr eso es el uso de una base aritmética en esos sistemas un número se expresa en general mediante suma o multiplicación de números. Los sistemas puramente aritméticos recurren a bases donde cada signo recibe una interpretación diferente según su posición. Así en el siguiente numeral arábigo (base 10): 13568 El <8> por estar en última posición representa unidades, el <6> representa decenas, el <5> centenas, el <3> millares y el <1> decenas de millares. Es decir, ese numeral representara el número: 13568 13568 Muchas lenguas del mundo usan una base decimal, igual que el sistema arábigo, aunque también es frecuente que las lenguas usen sistemas vigesimales (base 20). De hecho la idea de usar un número finito de dígitos o signos para representar números arbitrariamente grandes funciona para cualquier base b, donde b es un número entero mayor o igual que 2. Los ordenadores frecuentemente usan para sus operaciones la base binaria (b = 2), y para ciertos usos también se emplea la base octal (b = 8 ) o hexadecimal (b = 16). La base coincide con el número de signos primarios, si un sistema posicional tiene b símbolos primarios que designaremos por Designará al número: Números en las lenguas naturales Artículo principal: Numeral (lingüística) Las lenguas naturales usan nombres o numerales para los números frecuentemente basados en el contaje mediante dedos, razón por la cual la mayoría de las lenguas usan sistemas de numeración en base 10 (dedos de las manos) o base 20 (dedos de manos y pies), aunque también existen algunos sistemas exóticos que emplean otras bases."

ksampletext_wikipedia_phys_mecanicacuantica: str = "Mecánica cuántica. La mecánica cuántica es la rama de la física que estudia la naturaleza a escalas espaciales pequeñas, los sistemas atómicos, subatómicos, sus interacciones con la radiación electromagnética y otras fuerzas, en términos de cantidades observables. Se basa en la observación de que todas las formas de energía se liberan en unidades discretas o paquetes llamados cuantos. Las partículas con esta propiedad pueden pertenecer a dos tipos distintos: fermiones o bosones. Algunos de estos últimos están ligados a una -interacción fundamental (por ejemplo, el fotón pertenece a la electromagnética). Sorprendentemente, la teoría cuántica solo permite normalmente cálculos probabilísticos o estadísticos de las características observadas de las partículas elementales, entendidos en términos de funciones de onda. La ecuación de Schrödinger desempeña, en la mecánica cuántica, el papel que las leyes de Newton y la conservación de la energía desempeñan en la mecánica clásica. Es decir, la predicción del comportamiento futuro de un sistema dinámico y es una ecuación de onda en términos de una función de onda la que predice analíticamente la probabilidad precisa de los eventos o resultados. En teorías anteriores de la física clásica, la energía era tratada únicamente como un fenómeno continuo, en tanto que la materia se supone que ocupa una región muy concreta del espacio y que se mueve de manera continua. Según la teoría cuántica, la energía se emite y se absorbe en cantidades discretas y minúsculas. Un paquete individual de energía, llamado cuanto, en algunas situaciones se comporta como una partícula de materia. Por otro lado, se encontró que las partículas exponen algunas propiedades ondulatorias cuando están en movimiento y ya no son vistas como localizadas en una región determinada, sino más bien extendidas en cierta medida. La luz u otra radiación emitida o absorbida por un átomo solo tiene ciertas frecuencias (o longitudes de onda), como puede verse en la línea del espectro asociado al elemento químico representado por tal átomo. La teoría cuántica demuestra que tales frecuencias corresponden a niveles definidos de los cuantos de luz, o fotones, y es el resultado del hecho de que los electrones del átomo solo pueden tener ciertos valores de energía permitidos. Cuando un electrón pasa de un nivel permitido a otro, una cantidad de energía es emitida o absorbida, cuya frecuencia es directamente proporcional a la diferencia de energía entre los dos niveles. La mecánica cuántica surge tímidamente en los inicios del siglo XX dentro de las tradiciones más profundas de la física para dar una solución a problemas para los que las teorías conocidas hasta el momento habían agotado su capacidad de explicar, como la llamada catástrofe ultravioleta en la radiación de cuerpo negro predicha por la física estadística clásica y la inestabilidad de los átomos en el modelo atómico de Rutherford. La primera propuesta de un principio propiamente cuántico se debe a Max Planck en 1900, para resolver el problema de la radiación de cuerpo negro, que fue duramente cuestionado, hasta que Albert Einstein lo convierte en el principio que exitosamente pueda explicar el efecto fotoeléctrico. Las primeras formulaciones matemáticas completas de la mecánica cuántica no se alcanzan hasta mediados de la década de 1920, sin que hasta el día de hoy se tenga una interpretación coherente de la teoría, en particular del problema de la medición. El formalismo de la mecánica cuántica se desarrolló durante la década de 1920. En 1924, Louis de Broglie propuso que, al igual que las ondas de luz presentan propiedades de partículas, como ocurre en el efecto fotoeléctrico, las partículas, también presentan propiedades ondulatorias. Dos formulaciones diferentes de la mecánica cuántica se presentaron después de la sugerencia de Broglie. En 1926, la mecánica ondulatoria de Erwin Schrödinger implica la utilización de una entidad matemática, la función de onda, que está relacionada con la probabilidad de encontrar una partícula en un punto dado en el espacio. En 1925, la mecánica matricial de Werner Heisenberg no hace mención alguna de las funciones de onda o conceptos similares, pero ha demostrado ser matemáticamente equivalente a la teoría de Schrödinger. Un descubrimiento importante de la teoría cuántica es el principio de incertidumbre, enunciado por Heisenberg en 1927, que pone un límite teórico absoluto en la precisión de ciertas mediciones. Como resultado de ello, la asunción clásica de los científicos de que el estado físico de un sistema podría medirse exactamente y utilizarse para predecir los estados futuros tuvo que ser abandonada. Esto supuso una revolución filosófica y dio pie a numerosas discusiones entre los más grandes físicos de la época. La mecánica cuántica propiamente dicha no incorpora a la relatividad en su formulación matemática. La parte de la mecánica cuántica que incorpora elementos relativistas de manera formal para abordar diversos problemas se conoce como mecánica cuántica relativista o ya, en forma más correcta y acabada, teoría cuántica de campos (que incluye a su vez a la electrodinámica cuántica, cromodinámica cuántica y teoría electrodébil dentro del modelo estándar) y más generalmente, la teoría cuántica de campos en espacio-tiempo curvo. La única interacción elemental que no se ha podido cuantizar hasta el momento ha sido la interacción gravitatoria. Este problema constituye entonces uno de los mayores desafíos de la física del siglo XXI. La mecánica cuántica se combinó con la teoría de la relatividad en la formulación de Paul Dirac de 1928, lo que, además, predijo la existencia de antipartículas. Otros desarrollos de la teoría incluyen la estadística cuántica, presentada en una forma por Einstein y Bose (la estadística de Bose-Einstein) y en otra forma por Dirac y Enrico Fermi (la estadística de Fermi-Dirac); la electrodinámica cuántica, interesada en la interacción entre partículas cargadas y los campos electromagnéticos, su generalización, la teoría cuántica de campos y la electrónica cuántica. La mecánica cuántica proporciona el fundamento de la fenomenología del átomo, de su núcleo y de las partículas elementales (lo cual requiere necesariamente el enfoque relativista). También su impacto en teoría de la información, criptografía y química ha sido decisivo entre esta misma. Contexto histórico La mecánica cuántica es, cronológicamente hablando, la última de las grandes ramas de la física. Se formuló a principios del siglo XX, casi al mismo tiempo que la teoría de la relatividad, aunque el grueso de la mecánica cuántica se desarrolló a partir de 1920 (siendo la teoría de la relatividad especial de 1905 y la teoría general de la relatividad de 1915). Además al advenimiento de la mecánica cuántica existían diversos problemas no resueltos en la electrodinámica clásica. El primero de estos problemas era la emisión de radiación de cualquier objeto en equilibrio, llamada radiación térmica, que es la que proviene de la vibración microscópica de las partículas que lo componen. Usando las ecuaciones de la electrodinámica clásica, la energía que emitía esta radiación térmica tendía al infinito, si se suman todas las frecuencias que emitía el objeto, con ilógico resultado para los físicos. También la estabilidad de los átomos no podía ser explicada por el electromagnetismo clásico, y la noción de que el electrón fuera o bien una partícula clásica puntual o bien una cáscara esférica de dimensiones finitas resultaban igualmente problemáticas para esto. Radiación electromagnética El problema de la radiación electromagnética de un cuerpo negro fue uno de los primeros problemas resueltos en el seno de la mecánica cuántica. Es en el seno de la mecánica estadística donde surgen por primera vez las ideas cuánticas en 1900. Al físico alemán Max Planck se le ocurrió un artificio matemático: si en el proceso aritmético se sustituía la integral de esas frecuencias por una suma no continua (discreta), se dejaba de obtener infinito como resultado, con lo que se eliminaba el problema; además, el resultado obtenido concordaba con lo que después era medido. Fue Max Planck quien entonces enunció la hipótesis de que la radiación electromagnética es absorbida y emitida por la materia en forma de «cuantos» de luz o fotones de energía cuantizados introduciendo una constante estadística, que se denominó constante de Planck. Su historia es inherente al siglo XX, ya que la primera formulación cuántica de un fenómeno fue dada a conocer por el mismo Planck el 14 de diciembre de 1900 en una sesión de la Sociedad Física de la Academia de Ciencias de Berlín. La idea de Planck habría permanecido muchos años solo como hipótesis sin verificar por completo si Albert Einstein no la hubiera retomado, proponiendo que la luz, en ciertas circunstancias, se comporta como partículas de energía (los cuantos de luz o fotones) en su explicación del efecto fotoeléctrico. Fue Albert Einstein quien completó en 1905 las correspondientes leyes del movimiento su teoría especial de la relatividad, demostrando que el electromagnetismo era una teoría esencialmente no mecánica. Culminaba así lo que se ha dado en llamar física clásica, es decir, la física no-cuántica. Usó este punto de vista llamado por él «heurístico», para desarrollar su teoría del efecto fotoeléctrico, publicando esta hipótesis en 1905, lo que le valió el Premio Nobel de Física de 1921. Esta hipótesis fue aplicada también para proponer una teoría sobre el calor específico, es decir, la que resuelve cuál es la cantidad de calor necesaria para aumentar en una unidad la temperatura de la unidad de masa de un cuerpo. El siguiente paso importante se dio hacia 1925, cuando Louis De Broglie propuso que cada partícula material tiene una longitud de onda asociada, inversamente proporcional a su masa, y a su velocidad. Así quedaba establecida la dualidad onda/materia. Poco tiempo después Erwin Schrödinger formuló una ecuación de movimiento para las «ondas de materia», cuya existencia había propuesto De Broglie y varios experimentos sugerían que eran reales. La mecánica cuántica introduce una serie de hechos contraintuitivos que no aparecían en los paradigmas físicos anteriores; con ella se descubre que el mundo atómico no se comporta como esperaríamos. Los conceptos de incertidumbre o cuantización son introducidos por primera vez aquí. Además la mecánica cuántica es la teoría científica que ha proporcionado las predicciones experimentales más exactas hasta el momento, a pesar de estar sujeta a las probabilidades. Inestabilidad de los átomos clásicos El segundo problema importante que la mecánica cuántica resolvió a través del modelo de Bohr, fue el de la estabilidad de los átomos. De acuerdo con la teoría clásica un electrón orbitando alrededor de un núcleo cargado positivamente debería emitir energía electromagnética perdiendo así velocidad hasta caer sobre el núcleo. La evidencia empírica era que esto no sucedía, y sería la mecánica cuántica la que resolvería este hecho primero mediante postulados ad hoc formulados por Bohr y más tarde mediante modelos como el modelo atómico de Schrödinger basados en supuestos más generales. A continuación se explica el fracaso del modelo clásico. En mecánica clásica, un átomo de hidrógeno es un tipo de problema de los dos cuerpos en que el protón sería el primer cuerpo que tiene más del 99% de la masa del sistema y el electrón es el segundo cuerpo que es mucho más ligero. Para resolver el problema de los dos cuerpos es conveniente hacer la descripción del sistema, colocando el origen del sistema de referencia en el centro de masa de la partícula de mayor masa, esta descripción es correcta considerando como masa de la otra partícula la masa reducida que viene dada por 999 Siendo watt Ese proceso acabaría con el colapso del átomo sobre el núcleo en un tiempo muy corto dadas las grandes aceleraciones existentes. A partir de los datos de la ecuación anterior el tiempo de colapso sería de 10-8 s, es decir, de acuerdo con la física clásica los átomos de hidrógeno no serían estables y no podrían existir más de una cienmillonésima de segundo. Esa incompatibilidad entre las predicciones del modelo clásico y la realidad observada llevó a buscar un modelo que explicara fenomenológicamente el átomo. El modelo atómico de Bohr era un modelo fenomenológico y provisorio que explicaba satisfactoriamente aunque de manera heurística algunos datos, como el orden de magnitud del radio atómico y los espectros de absorción del átomo, pero no explicaba cómo era posible que el electrón no emitiera radiación perdiendo energía. La búsqueda de un modelo más adecuado llevó a la formulación del modelo atómico de Schrödinger en el cual puede probarse que el valor esperado de la aceleración es nulo, y sobre esa base puede decirse que la energía electromagnética emitida debería ser también nula. Sin embargo, al contrario del modelo de Bohr, la representación cuántica de Schrödinger es difícil de entender en términos intuitivos. Desarrollo histórico Artículo principal: Historia de la mecánica cuántica La teoría cuántica fue desarrollada en su forma básica a lo largo de la primera mitad del siglo XX. El hecho de que la energía se intercambie de forma discreta se puso de relieve por hechos experimentales como los siguientes, inexplicables con las herramientas teóricas anteriores de la mecánica clásica o la electrodinámica: Fig. 1: La función de onda del electrón de un átomo de hidrógeno posee niveles de energía definidos y discretos denotados por un número cuántico n=1, 2, 3,... y valores definidos de momento angular caracterizados por la notación: s, p, d,... Las áreas brillantes en la figura corresponden a densidades elevadas de probabilidad de encontrar el electrón en dicha posición. Espectro de la radiación del cuerpo negro, resuelto por Max Planck con la cuantización de la energía. La energía total del cuerpo negro resultó que tomaba valores discretos más que continuos. Este fenómeno se llamó cuantización, y los intervalos posibles más pequeños entre los valores discretos son llamados quanta (singular: quantum, de la palabra latina para «cantidad», de ahí el nombre de mecánica cuántica). La magnitud de un cuanto es un valor fijo llamado constante de Planck, y que vale: 6,626 ×10-34 J·s. Bajo ciertas condiciones experimentales, los objetos microscópicos como los átomos o los electrones exhiben un comportamiento ondulatorio, como en la interferencia. Bajo otras condiciones, las mismas especies de objetos exhiben un comportamiento corpuscular, de partícula, («partícula» quiere decir un objeto que puede ser localizado en una región concreta del espacio), como en la dispersión de partículas. Este fenómeno se conoce como dualidad onda-partícula. Las propiedades físicas de objetos con historias asociadas pueden ser correlacionadas, en una amplitud prohibida para cualquier teoría clásica, solo pueden ser descritos con precisión si se hace referencia a ambos a la vez. Este fenómeno es llamado entrelazamiento cuántico y la desigualdad de Bell describe su diferencia con la correlación ordinaria. Las medidas de las violaciones de la desigualdad de Bell fueron algunas de las mayores comprobaciones de la mecánica cuántica. Explicación del efecto fotoeléctrico, dada por Albert Einstein, en que volvió a aparecer esa misteriosa necesidad de cuantizar la energía. Efecto Compton. El desarrollo formal de la teoría fue obra de los esfuerzos conjuntos de varios físicos y matemáticos de la época como Schrödinger, Heisenberg, Einstein, Dirac, Bohr y Von Neumann entre otros (la lista es larga). Algunos de los aspectos fundamentales de la teoría están siendo aún estudiados activamente. La mecánica cuántica ha sido también adoptada como la teoría subyacente a muchos campos de la física y la química, incluyendo la física de la materia condensada, la química cuántica y la física de partículas. La región de origen de la mecánica cuántica puede localizarse en la Europa central, en Alemania y Austria, y en el contexto histórico del primer tercio del siglo XX. Suposiciones más importantes Artículo principal: Interpretaciones de la mecánica cuántica Las suposiciones más importantes de esta teoría son las siguientes: Al ser imposible fijar a la vez la posición y el momento de una partícula, se renuncia al concepto de trayectoria, vital en mecánica clásica. En vez de eso, el movimiento de una partícula puede ser explicado por una función matemática que asigna, a cada punto del espacio y a cada instante, la probabilidad de que la partícula descrita se halle en tal posición en ese instante (al menos, en la interpretación de la Mecánica cuántica más usual, la probabilista o interpretación de Copenhague). A partir de esa función, o función de ondas, se extraen teóricamente todas las magnitudes del movimiento necesarias. Existen dos tipos de evolución temporal, si no ocurre ninguna medida el estado del sistema o función de onda evolucionan de acuerdo con la ecuación de Schrödinger, sin embargo, si se realiza una medida sobre el sistema, este sufre un «salto cuántico» hacia un estado compatible con los valores de la medida obtenida (formalmente el nuevo estado será una proyección ortogonal del estado original). Existen diferencias notorias entre los estados ligados y los que no lo están. La energía no se intercambia de forma continua en un estado ligado, sino en forma discreta lo cual implica la existencia de paquetes mínimos de energía llamados cuantos, mientras en los estados no ligados la energía se comporta como un continuo. Descripción de la teoría Interpretación de Copenhague Artículo principal: Interpretación de Copenhague Para describir la teoría de forma general es necesario un tratamiento matemático riguroso, pero aceptando una de las tres interpretaciones de la mecánica cuántica (a partir de ahora la Interpretación de Copenhague), el marco se relaja. La mecánica cuántica describe el estado instantáneo de un sistema (estado cuántico) con una función de onda que codifica la distribución de probabilidad de todas las propiedades medibles, u observables. Algunos observables posibles sobre un sistema dado son la energía, posición, momento y momento angular. La mecánica cuántica no asigna valores definidos a los observables, sino que hace predicciones sobre sus distribuciones de probabilidad. Las propiedades ondulatorias de la materia son explicadas por la interferencia de las funciones de onda. Estas funciones de onda pueden variar con el transcurso del tiempo. Esta evolución es determinista si sobre el sistema no se realiza ninguna medida aunque esta evolución es estocástica y se produce mediante colapso de la función de onda cuando se realiza una medida sobre el sistema (Postulado IV de la MC). Por ejemplo, una partícula moviéndose sin interferencia en el espacio vacío puede ser descrita mediante una función de onda que es un paquete de ondas centrado alrededor de alguna posición media. Según pasa el tiempo, el centro del paquete puede trasladarse, cambiar, de modo que la partícula parece estar localizada más precisamente en otro lugar. La evolución temporal determinista de las funciones de onda es descrita por la ecuación de Schrödinger. Algunas funciones de onda describen estados físicos con distribuciones de probabilidad que son constantes en el tiempo, estos estados se llaman estacionarios, son estados propios del operador hamiltoniano y tienen energía bien definida. Muchos sistemas que eran tratados dinámicamente en mecánica clásica son descritos mediante tales funciones de onda estáticas. Por ejemplo, un electrón en un átomo sin excitar se dibuja clásicamente como una partícula que rodea el núcleo, mientras que en mecánica cuántica es descrito por una nube de probabilidad estática que rodea al núcleo. Cuando se realiza una medición en un observable del sistema, la función de ondas se convierte en una del conjunto de las funciones llamadas funciones propias o estados propios del observable en cuestión. Este proceso es conocido como colapso de la función de onda. Las probabilidades relativas de ese colapso sobre alguno de los estados propios posibles son descritas por la función de onda instantánea justo antes de la reducción. Considerando el ejemplo anterior sobre la partícula en el vacío, si se mide la posición de la misma, se obtendrá un valor impredecible x. En general, es imposible predecir con precisión qué valor de x se obtendrá, aunque es probable que se obtenga uno cercano al centro del paquete de ondas, donde la amplitud de la función de onda es grande. Después de que se ha hecho la medida, la función de onda de la partícula colapsa y se reduce a una que esté muy concentrada en torno a la posición observada x. La ecuación de Schrödinger es determinista en el sentido de que, dada una función de onda a un tiempo inicial dado, la ecuación suministra una predicción concreta de qué función tendremos en cualquier tiempo posterior. Durante una medida, el eigen-estado al cual colapsa la función es probabilista y en este aspecto la mecánica cuántica es no determinista. Así que la naturaleza probabilista de la mecánica cuántica nace del acto de la medida. Esto conduce al problema de definir objetivamente en qué momento se produce la medida y la evolución pasa de lineal y determinista, a no-lineal y estocástica/aleatoria, cuestión que se conoce como problema de la medida y que, además de la interpretación de Copenhague, ha dado lugar a un número elevado de propuestas de resolución, conocidas como interpretaciones de la mecánica cuántica. Formulación matemática Artículos principales: Postulados de la mecánica cuántica y Notación braket. En la formulación matemática rigurosa, desarrollada por Dirac y von Neumann, los estados posibles de un sistema cuántico están representados por vectores unitarios (llamados estados) que pertenecen a un Espacio de Hilbert complejo separable (llamado el espacio de estados). Qué tipo de espacio de Hilbert es necesario en cada caso depende del sistema; por ejemplo, el espacio de estados para los estados de posición y momento es el espacio de funciones de cuadrado integrable Cada magnitud observable queda representada por un operador lineal hermítico definido sobre un dominio denso del espacio de estados. Cada estado propio de un observable corresponde a un eigenvector del operador, y el valor propio o eigenvalor asociado corresponde al valor del observable en aquel estado propio. El espectro de un operador puede ser continuo o discreto. La medida de un observable representado por un operador con espectro discreto solo puede tomar un conjunto numerable de posibles valores, mientras que los operadores con espectro continuo presentan medidas posibles en intervalos reales completos. Durante una medida, la probabilidad de que un sistema colapse a uno de los eigenestados viene dada por el cuadrado del valor absoluto del producto interno entre el estado propio o auto-estado (que podemos conocer teóricamente antes de medir) y el vector estado del sistema antes de la medida. Podemos así encontrar la distribución de probabilidad de un observable en un estado dado computando la descomposición espectral del operador correspondiente. El principio de incertidumbre de Heisenberg se representa por la aseveración de que los operadores correspondientes a ciertos observables no conmutan. Principio de Incertidumbre Una de las consecuencias del formalismo cuántico es el principio de incertidumbre. En su forma más familiar, establece que ninguna medición de una partícula cuántica puede implicar simultáneamente predicciones precisas para la medición de su posición y la medición de su momento. Tanto posición como momento son observables, esto significa que son representados por operadores hermíticos. El operador posición Dado un estado cuántico, la regla de Born nos permite encontrar valores para y de la misma manera para el momento: El principio de incertidumbre establece que En principio, cualquiera de las desviaciones estándar puede hacerse arbitrariamente pequeña, pero no ambas simultáneamente . Esta desigualdad se generaliza a pares arbitrarios de operadores autoadjuntos y proporciona el límite inferior en el producto de las desviaciones estándar: Otra consecuencia de la relación de conmutación canónica es que los operadores posición y momento son la transformada de Fourier del otro, de modo que una descripción de un objeto según su momento es la transformada de Fourier de su descripción según su posición. El hecho de que la dependencia en cantidad de movimiento sea la transformada de Fourier de la dependencia en posición significa que el operador de cantidad de movimiento es equivalente (hasta un factor de Aplicaciones En muchos aspectos, la tecnología moderna opera a una escala en la que los efectos cuánticos son significativos. Las aplicaciones importantes de la teoría cuántica incluyen la química cuántica, la óptica cuántica, la computación cuántica, los imanes superconductores, los diodos emisores de luz, el amplificador óptico y el láser, el transistor y semiconductores como el microprocesador, imágenes médicas y de investigación como la resonancia magnética y el microscopio electrónico. Las explicaciones de muchos fenómenos biológicos y físicos tienen su origen en la naturaleza del enlace químico, sobre todo la macromolécula del ADN. Relatividad y la mecánica cuántica Artículos principales: Teoría cuántica de campos y Segunda cuantización. El mundo moderno de la física se funda notablemente en dos teorías principales, la relatividad general y la mecánica cuántica, aunque ambas teorías usan principios aparentemente incompatibles. Los postulados que definen la teoría de la relatividad de Einstein y la teoría del quántum están apoyados por rigurosa y repetida evidencia empírica. Sin embargo, ambas se resisten a ser incorporadas dentro de un mismo modelo coherente. Desde mediados del siglo XX, aparecieron teorías cuánticas relativistas del campo electromagnético (electrodinámica cuántica) y las fuerzas nucleares (modelo electrodébil, cromodinámica cuántica), pero no se tiene una teoría cuántica relativista del campo gravitatorio que sea plenamente consistente y válida para campos gravitatorios intensos (existen aproximaciones en espacios asintóticamente planos). Todas las teorías cuánticas relativistas consistentes usan los métodos de la teoría cuántica de campos. En su forma ordinaria, la teoría cuántica abandona algunos de los supuestos básicos de la teoría de la relatividad, como por ejemplo el principio de localidad usado en la descripción relativista de la causalidad. El mismo Einstein había considerado absurda la violación del principio de localidad a la que parecía abocar la mecánica cuántica. La postura de Einstein fue postular que la mecánica cuántica si bien era consistente era incompleta. Para justificar su argumento y su rechazo a la falta de localidad y la falta de determinismo, Einstein y varios de sus colaboradores postularon la llamada paradoja de Einstein-Podolsky-Rosen (EPR), la cual demuestra que medir el estado de una partícula puede instantáneamente cambiar el estado de su socio enlazado, aunque las dos partículas pueden estar a una distancia arbitrariamente grande. Modernamente el paradójico resultado de la paradoja EPR se sabe es una consecuencia perfectamente consistente del llamado entrelazamiento cuántico. Es un hecho conocido que si bien la existencia del entrelazamiento cuántico efectivamente viola el principio de localidad, en cambio no viola la causalidad definido en términos de información, puesto que no hay transferencia posible de información. Si bien en su tiempo, parecía que la paradoja EPR suponía una dificultad empírica para la mecánica cuántica, y Einstein consideró que la mecánica cuántica en la interpretación de Copenhague podría ser descartada por experimento, décadas más tarde los experimentos de Alain Aspect (1981) revelaron que efectivamente la evidencia experimental parece apuntar en contra del principio de localidad. Y por tanto, el resultado paradójico que Einstein rechazaba como «sin sentido» parece ser lo que sucede precisamente en el mundo real."
ksampletext_wikipedia_phys_teoriadecuerdas: str = "Teoría de cuerdas. Las teorías de cuerdas son una serie de hipótesis científicas y modelos fundamentales de física teórica que asumen que las partículas subatómicas, aparentemente puntuales, son en realidad estados vibracionales de un objeto extendido más básico llamado cuerda o filamento. De acuerdo con estas teorías, un electrón no sería un punto sin estructura interna y de dimensión cero, sino una cuerda minúscula en forma de lazo vibrando en un espacio-tiempo de más de cuatro dimensiones; de hecho, el planteamiento matemático de esta teoría no funciona a menos que el universo tenga diez dimensiones. Mientras que un punto simplemente se movería por el espacio, una cuerda podría hacer algo más: vibrar de diferentes maneras. Si vibrase de cierto modo, veríamos un electrón; pero si lo hiciese de otro, veríamos un fotón, un cuark o cualquier otra partícula del modelo estándar dependiendo de la forma concreta en que estuviese vibrando. Estas teorías, ampliadas con otras como la de las supercuerdas o la teoría M, pretende alejarse de la concepción del punto-partícula. La siguiente formulación de una teoría de cuerdas se debe a Jöel Scherk y John Henry Schwarz, que en 1974 publicaron un artículo en el que mostraban que una teoría basada en objetos unidimensionales o cuerdas en lugar de partículas puntuales podía describir la fuerza gravitatoria, aunque estas ideas no recibieron en ese momento mucha atención hasta la primera revolución de supercuerdas de 1984. De acuerdo con la formulación de la teoría de cuerdas surgida de esta revolución, las teorías de cuerdas pueden considerarse de hecho un caso general de teoría de Kaluza-Klein cuantizada. Las ideas fundamentales son dos: Los objetos básicos de la teoría no serían partículas puntuales, sino objetos unidimensionales extendidos (en las cinco teorías de supercuerdas convencionales estos objetos eran unidimensionales o cuerdas; actualmente en la teoría-M se admiten también de dimensión superior o p-branas). Esto renormaliza algunos infinitos de los cálculos perturbativos. El espacio-tiempo en el que se mueven las cuerdas y p-branas de la teoría no sería el espacio-tiempo ordinario de cuatro dimensiones, sino un espacio de tipo Kaluza-Klein, en el que a las cuatro dimensiones convencionales se añaden seis dimensiones compactadas en forma de variedad de Calabi-Yau. Por tanto convencionalmente en la teoría de cuerdas existe una dimensión temporal, tres dimensiones espaciales ordinarias y seis dimensiones compactadas e inobservables en la práctica. La inobservabilidad de las dimensiones adicionales está relacionada con el hecho de que estas estarían compactadas, y solo serían relevantes a escalas pequeñas comparables con la longitud de Planck. Igualmente, con la precisión de medida convencional las cuerdas cerradas con una longitud similar a la longitud de Planck se asemejarían a partículas puntuales. Desarrollos posteriores Tras la introducción de la teoría de cuerdas, se consideró la conveniencia de introducir el principio de que la teoría fuera supersimétrica; es decir, que admitiera una simetría abstracta que relacionara fermiones y bosones. Actualmente la mayoría de teóricos de cuerdas trabajan en teorías supersimétricas; de ahí que la teoría de cuerdas actualmente se llame teoría de supercuerdas. Esta última teoría es básicamente una teoría de cuerdas supersimétrica; es decir, que es invariante bajo transformaciones de supersimetría. Actualmente existen cinco teorías de supercuerdas relacionadas con los cinco modos que se conocen de implementar la supersimetría en el modelo de cuerdas. Aunque dicha multiplicidad de teorías desconcertó a los especialistas durante más de una década, el saber convencional actual sugiere que las cinco teorías son casos límites de una teoría única sobre un espacio de 10 dimensiones (las tres del espacio y una temporal serían las 4 que ya conocemos más otras seis adicionales resabiadas o compactadas) y una que las engloba formando membranas de las cuales se podría escapar parte de la gravedad de ellas en forma de gravitones. Esta teoría única, llamada teoría M, de la que solo se conocerían algunos aspectos, fue conjeturada en 1995. Variantes de la teoría La teoría de supercuerdas es algo actual. En sus principios (mediados de los años 1980) aparecieron unas cinco teorías de cuerdas, las cuales después fueron identificadas como límites particulares de una sola teoría: la teoría M. Las cinco versiones de la teoría actualmente existentes, entre las que pueden establecerse varias relaciones de dualidad, son: La Teoría de cuerdas de Tipo I, donde aparecen tanto cuerdas y D-branas abiertas como cerradas, que se mueven sobre un espacio-tiempo de diez dimensiones. Las D-branas tienen una, cinco y nueve dimensiones espaciales. La Teoría de cuerdas de Tipo IIA. Es también una teoría de diez dimensiones, pero que emplea solo cuerdas y D-branas cerradas. Incorpora los gravitinos (partículas teóricas asociadas al gravitón mediante relaciones de supersimetría). Usa D-branas de dimensión 0, 2, 4, 6 y 8. La Teoría de cuerdas de Tipo IIB. Difiere de la teoría de tipo IIA principalmente en el hecho de que esta última es no quiral (conservando la paridad). La Teoría de cuerda heterótica SO(32) (Heterótica-O), basada en el grupo de simetría O(32). La Teoría de cuerda heterótica E8xE8 (Heterótica-E), basada en el grupo de Lie excepcional E8. Fue propuesta en 1987 por Gross, Harvey, Martinec y Rohm. El término teoría de cuerdas se refiere en realidad a las teorías de cuerdas bosónicas de 26 dimensiones y la teoría de supercuerdas de diez dimensiones, esta última descubierta al añadir supersimetría a la teoría de cuerdas bosónica. Hoy en día la teoría de cuerdas se suele referir a la variante supersimétrica, mientras que la antigua se conoce por el nombre completo de teoría de cuerdas bosónicas. En 1995, Edward Witten conjeturó que las cinco diferentes teorías de supercuerdas son casos límite de una desconocida teoría de once dimensiones llamada teoría-M. La conferencia donde Witten mostró algunos de sus resultados inició la llamada segunda revolución de supercuerdas. En esta teoría M intervienen como objetos animados físicos fundamentales no solo cuerdas unidimensionales, sino toda una variedad de objetos no perturbativos, extendidos en varias dimensiones, que se llaman colectivamente p-branas (este nombre es una aféresis de membrana). Controversia sobre la teoría Aunque la teoría de cuerdas, según sus defensores, pudiera llegar a convertirse en una de las teorías físicas más predictivas, capaz de explicar algunas de las propiedades más fundamentales de la naturaleza en términos geométricos, los físicos que han trabajado en ese campo hasta la fecha no han podido hacer predicciones concretas con la precisión necesaria para confrontarlas con datos experimentales, al parecer se necesita de una tecnología más avanzada para visualizar las partículas, lo cual llevara muchos años. Dichos problemas de predicción se deberían, según el autor, a que el modelo no es falsable, y por tanto, no es científico, o bien a que «la teoría de las supercuerdas es tan ambiciosa que solo puede ser del todo correcta o del todo equivocada. El único problema es que sus matemáticas son tan nuevas y tan difíciles que durante varias décadas no sabremos cuáles son», dicho esto en 1990. D. Gross, premio Nobel de física por su trabajo en el modelo estándar, se convirtió en un formidable luchador de la teoría de cuerdas, pero recientemente ha dicho: «No sabemos de qué estamos hablando». Si los teóricos de cuerdas se equivocan, no pueden equivocarse solo un poco. Si las nuevas dimensiones y las simetrías no existen, consideraremos a los teóricos de cuerdas unos de los mayores fracasados de la ciencia (...). Su historia constituirá una leyenda moral de cómo no hacer ciencia, de cómo no permitir que se sobrepasen tanto los límites, hasta el punto de convertir la conjetura teórica en fantasía. Lee Smolin Otras teorías En 1997, el físico teórico argentino Juan Maldacena propuso un sorprendente modelo del universo según el cual la gravedad surge de cuerdas infinitesimales, delgadas y vibrantes y puede ser reinterpretada en términos físicos. Así, este mundo de cuerdas matemáticamente intrincado, que existe en diez dimensiones espaciales, no sería más que un holograma: la acción real se desarrollaría en un cosmos plano, más simple y en el que no hay gravedad. La idea de Maldacena entusiasmó a los físicos, entre otras razones porque resolvía aparentes inconsistencias entre la física cuántica y la teoría de la gravedad de Einstein. Así, el argentino proporcionó a los científicos una piedra Rosetta matemática, una dualidad, que les permitía resolver los problemas de un modelo que parecían no tener respuesta en el otro, y viceversa. Pero a pesar de la validez de sus ideas aún no se había logrado hallar ninguna prueba rigurosa de su teoría. Según un artículo publicado en la revista científica Nature, Yoshifumi Hyakutake, de la Universidad de Ibaraki (Japón), y sus colegas, proporcionaron en dos de sus estudios, sino una prueba real, al menos una muestra convincente de que la conjetura de Maldacena es cierta. En uno de los estudios, Hyakutake calculó la energía interna de un agujero negro, la posición de su horizonte de sucesos (el límite entre el agujero negro y el resto del universo), su entropía y otras propiedades a partir de las predicciones de la teoría de cuerdas y de los efectos asociados a las partículas virtuales, que aparecen continuamente dentro y fuera de la existencia. En el otro, él y sus colaboradores calcularon la energía interna del correspondiente universo de dimensión inferior sin gravedad. Los dos cálculos informáticos coinciden. Parece que es un cálculo correcto, dice Maldacena, al tiempo que subraya que los hallazgos son una forma interesante de demostrar muchas ideas de la gravedad cuántica y la teoría de cuerdas. Numéricamente han confirmado, tal vez por primera vez, algo de lo que estábamos bastante seguros pero era todavía una conjetura: que la termodinámica de ciertos agujeros negros puede ser reproducida desde un universo dimensional inferior, explica Leonard Susskind, físico teórico de la Universidad de Stanford, en California, quien fue uno de los primeros teóricos en explorar la idea de universos holográficos. Falsacionismo y teoría de cuerdas Artículo principal: Criterio de demarcación La teoría de cuerdas o la teoría M podrían no ser falsables, según sus críticos. Diversos autores han declarado su preocupación de que la teoría de cuerdas no sea falsable y como tal, siguiendo las tesis del filósofo de la ciencia Karl Popper, la teoría de cuerdas sería equivalente a una pseudociencia. El filósofo de la ciencia Mario Bunge ha manifestado lo siguiente: La consistencia, la sofisticación y la belleza nunca son suficientes en la investigación científica. La teoría de cuerdas es sospechosa (de pseudociencia). Parece científica porque aborda un problema abierto que es a la vez importante y difícil, el de construir una teoría cuántica de la gravitación. Pero la teoría postula que el espacio físico tiene seis o siete dimensiones, en lugar de tres, simplemente para asegurarse consistencia matemática. Puesto que estas dimensiones extra son inobservables, y puesto que la teoría se ha resistido a la confirmación experimental durante más de tres décadas, parece ciencia ficción, o al menos, ciencia fallida. La física de partículas está inflada con sofisticadas teorías matemáticas que postulan la existencia de entidades extrañas que no interactúan de forma apreciable, o para nada en absoluto, con la materia ordinaria, y como consecuencia, quedan a salvo al ser indetectables. Puesto que estas teorías se encuentran en discrepancia con el conjunto de la Física, y violan el requerimiento de falsacionismo, pueden calificarse de pseudocientíficas, incluso aunque lleven pululando un cuarto de siglo y se sigan publicando en las revistas científicas más prestigiosas. Mario Bunge, 2006. Impacto de la promoción de la teoría en el mundo académico Smolin indica que la teoría de cuerdas se ha convertido en el principal camino de exploración de «las grandes cuestiones de la física» debido a una agresiva promoción, considerando que resulta prácticamente un «suicidio profesional» para cualquier joven físico teórico no ingresar en sus filas. Expone además que «a pesar de la escasa inversión en [...] otros campos de investigación, algunos de ellos han avanzado más que el de la teoría de cuerdas» e identifica los siguientes rasgos en las «comunidades de supercuerdas»: Tremenda autosuficiencia y conciencia de pertenecer a una élite. Comunidades monolíticas con gran uniformidad de opiniones sobre cuestiones abiertas, generalmente impuestas por los que constituyen la jerarquía de la comunidad. Sentido de identificación con el grupo parecido a la pertenencia a una comunidad religiosa o partido político. Sentido de frontera entre el grupo y otros expertos. Gran desinterés por las ideas y personas que no son del grupo. Una confianza excesiva en interpretar positivamente los resultados e incluso aceptarlos exclusivamente porque son creídos por la mayoría. Una falta de percepción del riesgo que conlleva una nueva teoría."
ksampletext_wikipedia_phys_electromagnetismo: str = "Electromagnetismo. El electromagnetismo es la rama de la física que estudia y unifica los fenómenos eléctricos y magnéticos en una sola teoría. El electromagnetismo describe la interacción de partículas cargadas con campos eléctricos y magnéticos. La interacción electromagnética es una de las cuatro fuerzas fundamentales del universo conocido. El electromagnetismo es una rama de la física que estudia los efectos producidos por el magnetismo, lo cual surge a partir de la corriente eléctrica. Por su parte, el magnetismo es la disciplina que examina los fenómenos asociados a los imanes. Su nombre proviene de Magnesia, un distrito en Asia Menor (actual Turquía), donde se descubrieron por primera vez las piedras llamadas magnetitas, que tienen la capacidad de atraer ciertos metales. El electromagnetismo abarca diversos fenómenos del mundo real, como por ejemplo la luz. La luz es un campo electromagnético oscilante que se irradia desde partículas cargadas aceleradas. Aparte de la gravedad, la mayoría de las fuerzas en la experiencia cotidiana son consecuencia del electromagnetismo. Los principios del electromagnetismo encuentran aplicaciones en diversas disciplinas afines, tales como las microondas, antenas, máquinas eléctricas, comunicaciones por satélite, bioelectromagnetismo, plasmas, investigación nuclear, la fibra óptica, la interferencia y la compatibilidad electromagnéticas, la conversión de energía electromecánica, la meteorología por radar, y la observación remota. Los dispositivos electromagnéticos incluyen transformadores, relés, radio/TV, teléfonos, motores eléctricos, líneas de transmisión, guías de onda y láseres. Los fundamentos de la teoría electromagnética fueron presentados por Michael Faraday y formulados por primera vez de modo completo por James Clerk Maxwell en 1865. La formulación consiste en cuatro ecuaciones diferenciales vectoriales que relacionan el campo eléctrico, el campo magnético y sus respectivas fuentes materiales (corriente eléctrica, polarización eléctrica y polarización magnética), conocidas como ecuaciones de Maxwell, lo que ha sido considerada como la «segunda gran unificación de la física», siendo la primera realizada por Isaac Newton. El estudio de los campos electromagnéticos se puede dividir en electrostática ,el estudio de las interacciones entre cargas en reposo, y la electrodinámica ,el estudio de las interacciones entre cargas en movimiento y la radiación,. La teoría clásica del electromagnetismo se basa en la fuerza de Lorentz y en las ecuaciones de Maxwell. Muchas propiedades ópticas y físicas de la materia también son explicados por la teoría electromagnética. El electromagnetismo es una teoría de campos; es decir, las explicaciones y predicciones que provee se basan en magnitudes físicas vectoriales o tensoriales dependientes de la posición en el espacio y del tiempo. El electromagnetismo describe los fenómenos físicos macroscópicos en los cuales intervienen cargas eléctricas en reposo y en movimiento, usando para ello campos eléctricos y magnéticos y sus efectos sobre las sustancias sólidas, líquidas y gaseosas. Por ser una teoría macroscópica, es decir, aplicable a un número muy grande de partículas y a distancias grandes respecto de las dimensiones de estas, el electromagnetismo no describe los fenómenos atómicos y moleculares. La electrodinámica cuántica proporciona la descripción cuántica de esta interacción, que puede ser unificada con la interacción nuclear débil según el modelo electrodébil. Espectro electromagnético. Historia Esta sección es un extracto de Historia del electromagnetismo.[editar] El físico danés Hans Christian Ørsted, realizando el experimento que le permitió descubrir la relación entre la electricidad y el magnetismo en 1820. La historia del electromagnetismo, considerada como el conocimiento y el uso registrado de las fuerzas electromagnéticas, data de hace más de dos mil años. En la antigüedad ya estaban familiarizados con los efectos de la electricidad atmosférica, en particular del rayo ya que las tormentas son comunes en las latitudes más meridionales, ya que también se conocía el fuego de San Telmo. Sin embargo, se comprendía poco la electricidad y no eran capaces de producir estos fenómenos. Durante los siglos XVII y XVIII, William Gilbert, Otto von Guericke, Stephen Gray, Benjamin Franklin, Alessandro Volta entre otros investigaron estos dos fenómenos de manera separada y llegaron a conclusiones coherentes con sus experimentos. A principios del siglo XIX, Hans Christian Ørsted encontró evidencia empírica de que los fenómenos magnéticos y eléctricos estaban relacionados. De ahí es que los trabajos de físicos como André-Marie Ampère, William Sturgeon, Joseph Henry, Georg Simon Ohm, Michael Faraday en ese siglo, son unificados por James Clerk Maxwell en 1861 con un conjunto de ecuaciones que describían ambos fenómenos como uno solo, como un fenómeno electromagnético. Las ahora llamadas ecuaciones de Maxwell demostraban que los campos eléctricos y los campos magnéticos eran manifestaciones de un solo campo electromagnético. Además, describía la naturaleza ondulatoria de la luz, mostrándola como una onda electromagnética. Con una sola teoría consistente que describía estos dos fenómenos antes separados, los físicos pudieron realizar varios experimentos prodigiosos e inventos muy útiles como la bombilla eléctrica por Thomas Alva Edison o el generador de corriente alterna por Nikola Tesla. El éxito predictivo de la teoría de Maxwell y la búsqueda de una interpretación coherente de sus implicaciones, fue lo que llevó a Albert Einstein a formular su teoría de la relatividad que se apoyaba en algunos resultados previos de Hendrik Antoon Lorentz y Henri Poincaré. En la primera mitad del siglo XX, con el advenimiento de la mecánica cuántica, el electromagnetismo tuvo que mejorar su formulación para que fuera coherente con la nueva teoría. Esto se logró en la década de 1940 cuando se completó una teoría cuántica electromagnética conocida como electrodinámica cuántica Historia de la teoría Hans Christian Oersted Originalmente, la electricidad y el magnetismo se consideraban dos fenómenos independientes entre sí. Este punto de vista cambió, sin embargo, con la publicación en 1873 del Tratado de electricidad y magnetismo de James Maxwell , que mostró que la interacción de cargas positivas y negativas está gobernada por una sola fuerza. Hay cuatro efectos principales, resultantes de estas interacciones, que han sido claramente demostrados por experimentos: Las cargas eléctricas son atraídas o repelidas entre sí con una fuerza inversamente proporcional al cuadrado de la distancia entre ellas: las cargas diferentes se atraen, las cargas iguales se repelen. Los polos magnéticos (o estados de polarización en puntos separados) se atraen o repelen entre sí de manera similar y siempre van en pares: cada polo norte no existe por separado del polo sur. La corriente eléctrica en un cable crea un campo magnético circular alrededor del cable, dirigido (en sentido horario o antihorario) según el flujo de corriente. Se induce una corriente en el bucle del cable cuando se acerca o aleja con relación al campo magnético, o cuando el imán se acerca o aleja del bucle del cable; la dirección de la corriente depende de la dirección de estos movimientos. André-Marie Ampere En preparación para la conferencia, la noche del 21 de abril de 1820, Hans Christian Oersted hizo una observación asombrosa. Cuando estaba compilando el material, notó que la aguja de la brújula se desviaba del polo norte magnético cuando se encendía y apagaba la corriente eléctrica de la batería que estaba usando. Esta desviación lo llevó a creer que los campos magnéticos emanan de todos los lados de un cable a través del cual fluye una corriente eléctrica, al igual que la luz y el calor se propagan en el espacio, y esa experiencia indica una conexión directa entre la electricidad y el magnetismo. Michael Faraday En el momento del descubrimiento, Oersted no ofreció una explicación satisfactoria de este fenómeno y no intentó presentar el fenómeno en cálculos matemáticos. Sin embargo, tres meses después, comenzó a realizar investigaciones más intensivas. Poco después, publicó los resultados de su investigación, demostrando que una corriente eléctrica crea un campo magnético cuando fluye a través de cables. En el sistema CGS , la unidad de inducción electromagnética, Oe, recibió su nombre de su contribución al campo del electromagnetismo. James Clerk Maxwell Las conclusiones de Oersted llevaron a un estudio intensivo de electrodinámica por parte de la comunidad científica mundial. Las obras de Dominique François Arago también se remontan a 1820 , quien advirtió que un cable por el que fluye una corriente eléctrica atrae limaduras de hierro . También magnetizó por primera vez alambres de hierro y acero, colocándolos dentro de una bobina de alambres de cobre por donde pasaba la corriente. También logró magnetizar la aguja colocándola en una bobina y descargando la Botella de Leyden a través de la bobina. Independientemente de Arago, Davy descubrió la magnetización del acero y el hierro por la corriente . Las primeras definiciones cuantitativas de la acción de una corriente sobre un imán de la misma forma se remontan a 1820 y pertenecen a científicos franceses Jean-Baptiste Biot y Félix Savart. Los experimentos de Oersted también influyeron en el físico francés André-Marie Ampere , quien presentó la ley electromagnética entre un conductor y una corriente en forma matemática. El descubrimiento de Oersted también representa un paso importante hacia un concepto de campo unificado. Esta unidad, que fue descubierta por Michael Faraday , completada por James Clerk Maxwell , y también refinada por Oliver Heaviside y Heinrich Hertz, es uno de los logros clave del siglo XIX en física matemática . Este descubrimiento tuvo implicaciones de gran alcance, una de las cuales fue comprender la naturaleza de la luz. La luz y otras ondas electromagnéticas toman la forma de fenómenos oscilatorios autopropagantes cuantificados del campo electromagnético llamados fotones. Diferentes frecuencias de vibración conducen a diferentes formas de radiación electromagnética: desde ondas de radio a bajas frecuencias, a luz visible a frecuencias medias, a rayos gamma a altas frecuencias. Oersted no fue la única persona que descubrió la conexión entre la electricidad y el magnetismo. En 1802, Giovanni Domenico Romagnosi , un jurista italiano, desvió una aguja magnética con descargas electrostáticas. Pero, de hecho, la investigación de Romagnosi no utilizó una celda galvánica y no había corriente continua como tal. El informe del descubrimiento se publicó en 1802 en un periódico italiano, pero la comunidad científica apenas lo notó en ese momento. Ramas Electrostática Artículo principal: Electrostática La electrostática es el estudio de los fenómenos asociados a los cuerpos cargados en reposo. Como describe la ley de Coulomb, estos cuerpos ejercen fuerzas entre sí. Su comportamiento se puede analizar en términos de la idea de un campo eléctrico que rodea cualquier cuerpo cargado, de manera que otro cuerpo cargado colocado dentro del campo estará sujeto a una fuerza proporcional a la magnitud de su carga y de la magnitud del campo en su ubicación. El que la fuerza sea atractiva o repulsiva depende de la polaridad de la carga. La electrostática tiene muchas aplicaciones, que van desde el análisis de fenómenos como tormentas eléctricas hasta el estudio del comportamiento de los tubos electrónicos. Un electroscopio usado para medir la carga eléctrica de un objeto. Cuando hablamos de electrostática nos referimos a los fenómenos que ocurren debido a una propiedad intrínseca y discreta de la materia, la carga, cuando es estacionaria o no depende del tiempo. La unidad de carga elemental, es decir, la más pequeña observable, es la carga que tiene el electrón. Se dice que un cuerpo está cargado eléctricamente cuando tiene exceso o falta de electrones en los átomos que lo componen. Por definición el defecto de electrones se la denomina carga positiva y al exceso carga negativa. La relación entre los dos tipos de carga es de atracción cuando son diferentes y de repulsión cuando son iguales. La carga elemental es una unidad muy pequeña para cálculos prácticos, por eso en el Sistema Internacional la unidad de carga eléctrica, el culombio, se define como la cantidad de carga transportada en un segundo por una corriente de un amperio de intensidad de corriente eléctrica. que equivale a la carga de 6,25 x 1018 electrones. El movimiento de electrones por un conductor se denomina corriente eléctrica y la cantidad de carga eléctrica que pasa por unidad de tiempo se define como la intensidad de corriente. Se pueden introducir más conceptos como el de diferencia de potencial o el de resistencia, que nos conducirían ineludiblemente al área de circuitos eléctricos, y todo eso se puede ver con más detalle en el artículo principal. El nombre de la unidad de carga se debe a Coulomb, quien en 1785 llegó a una relación matemática de la fuerza eléctrica entre cargas puntuales, que ahora se la conoce como ley de Coulomb: Entre dos cargas puntuales Las cargas elementales al no encontrarse solas se las debe tratar como una distribución de ellas. Por eso debe implementarse el concepto de campo, definido como una región del espacio donde existe una magnitud escalar o vectorial dependiente o independiente del tiempo. Así el campo eléctrico Campo eléctrico de cargas puntuales. lim Y así finalmente llegamos a la expresión matemática que define el campo eléctrico: Es importante conocer el alcance de este concepto de campo eléctrico: nos brinda la oportunidad de conocer cuál es su intensidad y qué ocurre con una carga en cualquier parte de dicho campo sin importar el conocimiento de qué lo provoca. Una forma de obtener qué cantidad de fuerza eléctrica pasa por cierto punto o superficie del campo eléctrico es usar el concepto de flujo eléctrico. Este flujo eléctrico El matemático y físico, Carl Friedrich Gauss, demostró que la cantidad de flujo eléctrico en un campo es igual al cociente entre la carga encerrada por la superficie en la que se calcula el flujo, (1) Véanse también: Carga eléctrica, Ley de Coulomb, Campo eléctrico, Potencial eléctrico y Ley de Gauss. Magnetostática Artículo principal: Magnetostática Líneas de fuerza de una barra magnética. La magnetósfera de la Tierra, empujada por el viento solar. No fue sino hasta el año de 1820, cuando Hans Christian Ørsted descubrió que el fenómeno magnético estaba ligado al eléctrico, que se obtuvo una teoría científica para el magnetismo. La presencia de una corriente eléctrica, o sea, de un flujo de carga debido a una diferencia de potencial, genera una fuerza magnética que no varía en el tiempo. Si tenemos una carga q a una velocidad Para determinar el valor de ese campo magnético, Jean Baptiste Biot en 1820, dedujo una relación para corrientes estacionarias, ahora conocida como ley de Biot-Savart: Donde (2) Además en la magnetostática existe una ley comparable a la de Gauss en la electrostática, la ley de Ampère. Esta ley nos dice que la circulación en un campo magnético es igual a la densidad de corriente que exista en una superficie cerrada: Cabe indicar que esta ley de Gauss es una generalización de la ley de Biot-Savart. Además que las fórmulas expresadas aquí son para cargas en el vacío, para más información consúltese los artículos principales. Véanse también: Ley de Ampère, Corriente eléctrica, Campo magnético, Ley de Biot-Savart y Momento magnético dipolar. Electrodinámica clásica Artículo principal: Electrodinámica La electrodinámica es el estudio de los fenómenos asociados a los cuerpos cargados en movimiento y a los campos eléctricos y magnéticos variables. Dado que una carga en movimiento produce un campo magnético, la electrodinámica se refiere a efectos tales como el magnetismo, la radiación electromagnética, y la inducción electromagnética, incluyendo las aplicaciones prácticas, tales como el generador eléctrico y el motor eléctrico. Esta área de la electrodinámica, conocida como electrodinámica clásica, fue sistemáticamente explicada por James Clerk Maxwell, y las ecuaciones de Maxwell describen los fenómenos de esta área con gran generalidad. Una novedad desarrollada más reciente es la electrodinámica cuántica, que incorpora las leyes de la teoría cuántica a fin de explicar la interacción de la radiación electromagnética con la materia. Paul Dirac, Heisenberg y Wolfgang Pauli fueron pioneros en la formulación de la electrodinámica cuántica. La electrodinámica es inherentemente relativista y da unas correcciones que se introducen en la descripción de los movimientos de las partículas cargadas cuando sus velocidades se acercan a la velocidad de la luz. Se aplica a los fenómenos involucrados con aceleradores de partículas y con tubos electrónicos funcionando a altas tensiones y corrientes. En las secciones anteriores se han descrito campos eléctricos y magnéticos que no variaban con el tiempo. Pero los físicos a finales del siglo XIX descubrieron que ambos campos estaban ligados y así un campo eléctrico en movimiento, una corriente eléctrica que varíe, genera un campo magnético y un campo magnético de por sí implica la presencia de un campo eléctrico. Entonces, lo primero que debemos definir es la fuerza que tendría una partícula cargada que se mueva en un campo magnético y así llegamos a la unión de las dos fuerzas anteriores, lo que hoy conocemos como la fuerza de Lorentz: (3) Entre 1890 y 1900 Liénard y Wiechert calcularon el campo electromagnético asociado a cargas en movimiento arbitrario, resultado que se conoce hoy como potenciales de Liénard-Wiechert. Por otro lado, para generar una corriente eléctrica en un circuito cerrado debe existir una diferencia de potencial entre dos puntos del circuito, a esta diferencia de potencial se la conoce como fuerza electromotriz o «fem». Esta fuerza electromotriz es proporcional a la rapidez con que el flujo magnético varía en el tiempo, esta ley fue encontrada por Michael Faraday y es la interpretación de la inducción electromagnética, así un campo magnético que varía en el tiempo induce a un campo eléctrico, a una fuerza electromotriz. Matemáticamente se representa como: (4) El físico James Clerk Maxwell de 1861 relacionó las anteriormente citadas ecuaciones para la ley de Gauss ((1)), ley de Gauss para el campo magnético ((2)), ley de Faraday ((4)) e introdujo el concepto de una corriente de desplazamiento como una densidad de corriente efectiva para llegar a la ley de Ampère generalizada (5): (5) Las cuatro ecuaciones, tanto en su forma diferencial como en la integral aquí descritas, son fruto de la reformulación del trabajo de Maxwell realizada por Oliver Heaviside y Heinrich Rudolf Hertz. Pero el verdadero poder de estas ecuaciones, más la fuerza de Lorentz (3), se centra en que juntas son capaces de describir cualquier fenómeno electromagnético, además de las consecuencias físicas que posteriormente se describirán. Esquema de una onda electromagnética. La genialidad del trabajo de Maxwell es que sus ecuaciones describen un campo eléctrico que va ligado inequívocamente a un campo magnético perpendicular a este y a la dirección de su propagación, este campo es ahora llamado campo electromagnético. Dichos campos podían ser derivados de un potencial escalar. La solución de las ecuaciones de Maxwell implicaba la existencia de una onda que se propagaba a la velocidad de la luz, con lo que además de unificar los fenómenos eléctricos y magnéticos la teoría formulada por Maxwell predecía con absoluta certeza los fenómenos ópticos. Así la teoría predecía a una onda que, contraria a las ideas de la época, no necesitaba un medio de propagación; la onda electromagnética se podía propagar en el vacío debido a la generación mutua de los campos magnéticos y eléctricos. Esta onda a pesar de tener una velocidad constante, la velocidad de la luz c, puede tener diferente longitud de onda y consecuentemente dicha onda transporta energía. La radiación electromagnética recibe diferentes nombres al variar su longitud de onda, como rayos gamma, rayos X, espectro visible, etc.; pero en su conjunto recibe el nombre de espectro electromagnético. Espectro electromagnético. Véanse también: Fuerza de Lorentz, Fuerza electromotriz, Ley de Ampère, Ecuaciones de Maxwell y Campo electromagnético. Electrodinámica relativista Artículo principal: Tensor de campo electromagnético Clásicamente, al fijar un sistema de referencia, se puede descomponer los campos eléctricos y magnéticos del campo electromagnético. Pero, en la teoría de la relatividad especial, al tener a un observador con movimiento relativo respecto al sistema de referencia, este medirá efectos eléctricos y magnéticos diferentes de un mismo fenómeno electromagnético. El campo eléctrico y la inducción magnética a pesar de ser elementos vectoriales no se comportan como magnitudes físicas vectoriales, por el contrario la unión de ambos constituye otro ente físico llamado tensor y en este caso el tensor de campo electromagnético. Así, la expresión para el campo electromagnético es: Esta representación se conoce como formulación covariante tetradimensional del electromagnetismo. Las expresiones covariantes para las ecuaciones de Maxwell (7) y la fuerza de Lorentz (6) se reducen a. Dada la forma de las ecuaciones anteriores, si el dominio sobre el que se extiende el campo electromagnético es simplemente conexo el campo electromagnético puede expresarse como la derivada exterior de un cuadrivector llamado potencial vector, relacionado con los potenciales del electromagnetismo clásico de la siguiente manera: Donde: La relación entre el cuadrivector potencial y el tensor de campo electromanético resulta ser: El hecho de que la interacción electromagnética pueda representarse por un (cuadri)vector que define completamente el campo electromagnético es la razón por la que se afirma en el tratamiento moderno que la interacción electromagnética es un campo vectorial. En relatividad general el tratamiento del campo electromagnético en un espacio-tiempo curvo es similar al presentado aquí para el espacio-tiempo de Minkowski, solo que las derivadas parciales respecto a las coordenadas deben substituirse por derivadas covariantes. Electrodinámica cuántica Diagrama de Feynman mostrando la fuerza electromagnética entre dos electrones por medio del intercambio de un fotón virtual. Artículo principal: Electrodinámica cuántica Posteriormente a la revolución cuántica de inicios del siglo XX, los físicos se vieron forzados a buscar una teoría cuántica de la interacción electromagnética. El trabajo de Einstein con el efecto fotoeléctrico y la posterior formulación de la mecánica cuántica sugerían que la interacción electromagnética se producía mediante el intercambio de partículas elementales llamadas fotones. La nueva formulación cuántica lograda en la década de 1940 describe la interacción entre los bosones, o partículas portadoras de la interacción, y las otras partículas portadoras de materia (los fermiones). La electrodinámica cuántica es principalmente una teoría cuántica de campos renormalizada. Su desarrollo fue obra de Sinitiro Tomonaga, Julian Schwinger, Richard Feynman y Freeman Dyson alrededor de los años 1947 a 1949. En la electrodinámica cuántica, la interacción entre partículas viene descrita por un lagrangiano que posee simetría local, concretamente simetría de gauge. Para la electrodinámica cuántica, el campo de gauge donde los fermiones interactúan es el campo electromagnético, descrito en esta teoría como los estados de bosones (fotones, en este caso) portadores de la interacción. Matemáticamente, el lagrangiano para la interacción entre fermiones mediante intercambio de fotones viene dado por: Donde el significado de los términos son: Véanse también: Teoría cuántica de campos, Ecuación de Dirac y Modelo estándar."
ksampletext_wikipedia_phys_teoriadelarelatividad: str = "Teoría de la relatividad. La teoría de la relatividad incluye tanto a la teoría de la relatividad especial como la de la relatividad general, formuladas principalmente por Albert Einstein a principios del siglo XX, que pretendían resolver la incompatibilidad existente entre la mecánica newtoniana y el electromagnetismo. La teoría de la relatividad especial, publicada en 1905, trata de la física del movimiento de los cuerpos en ausencia de fuerzas gravitatorias, en el que se hacían compatibles las ecuaciones de Maxwell del electromagnetismo con una reformulación de las leyes del movimiento. En la teoría de la relatividad especial, Einstein, Lorentz y Minkowski, entre otros, unificaron los conceptos de espacio y tiempo, en un tramado tetradimensional al que se le denominó espacio-tiempo. La relatividad especial fue una teoría revolucionaria para su época, con la que el tiempo absoluto de Newton quedó relegado y conceptos como la invariabilidad en la velocidad de la luz, la dilatación del tiempo, la contracción de la longitud y la equivalencia entre masa y energía fueron introducidos. Además, con las formulaciones de la relatividad especial, las leyes de la Física son invariantes en todos los sistemas de referencia inerciales; como consecuencia matemática, se encuentra como límite superior de velocidad a la de la luz y se elimina la causalidad determinista que tenía la física hasta entonces. Hay que indicar que las leyes del movimiento de Newton son un caso particular de esta teoría donde la masa, al viajar a velocidades muy pequeñas, no experimenta variación alguna en longitud ni se transforma en energía y al tiempo se le puede considerar absoluto. La teoría de la relatividad general, publicada en 1915, es una teoría de la gravedad que reemplaza a la gravedad newtoniana, aunque coincide numéricamente con ella para campos gravitatorios débiles y velocidades «pequeñas». La teoría general se reduce a la teoría especial en presencia de campos gravitatorios. La relatividad general estudia la interacción gravitatoria como una deformación en la geometría del espacio-tiempo. En esta teoría se introducen los conceptos de la curvatura del espacio-tiempo como la causa de la interacción gravitatoria, el principio de equivalencia que dice que para todos los observadores locales inerciales las leyes de la relatividad especial son invariantes y la introducción del movimiento de una partícula por líneas geodésicas. La relatividad general no es la única teoría que describe la atracción gravitatoria, pero es la que más datos relevantes comprobables ha encontrado. Anteriormente, a la interacción gravitatoria se la describía matemáticamente por medio de una distribución de masas, pero en esta teoría no solo la masa percibe esta interacción, sino también la energía, mediante la curvatura del espacio-tiempo y por eso se necesita otro lenguaje matemático para poder describirla, el cálculo tensorial. Muchos fenómenos, como la curvatura de la luz por acción de la gravedad y la desviación en la órbita de Mercurio, son perfectamente predichos por esta formulación. La relatividad general también abrió otro campo de investigación en la física, conocido como cosmología y es ampliamente utilizado en la astrofísica. El 7 de marzo de 2010, la Academia Israelí de Ciencias exhibió públicamente los manuscritos originales de Einstein (redactados en 1905). El documento, que contiene 46 páginas de textos y fórmulas matemáticas escritas a mano, fue donado por Einstein a la Universidad Hebrea de Jerusalén en 1925 con motivo de su inauguración. Conceptos principales Artículo principal: Anexo:Glosario de relatividad El supuesto básico de la teoría de la relatividad es que la localización de los sucesos físicos, tanto en el tiempo como en el espacio, son relativos al estado de movimiento del observador: así, la longitud de un objeto en movimiento o el instante en que algo sucede, a diferencia de lo que sucede en mecánica newtoniana, no son invariantes absolutos, y diferentes observadores en movimiento relativo entre sí diferirán respecto a ellos (las longitudes y los intervalos temporales, en relatividad son relativos y no absolutos). Relatividad especial Artículo principal: Teoría de la relatividad especial La teoría de la relatividad especial, también llamada teoría de la relatividad restringida, fue publicada por Albert Einstein en 1905 y describe la física del movimiento en el marco de un espacio-tiempo plano. Esta teoría describe correctamente el movimiento de los cuerpos incluso a grandes velocidades y sus interacciones electromagnéticas, se usa básicamente para estudiar sistemas de referencia inerciales (no es aplicable para problemas astrofísicos donde el campo gravitatorio desempeña un papel importante). Estos conceptos fueron presentados anteriormente por Poincaré y Lorentz, que son considerados como precursores de la teoría. Si bien la teoría resolvía un buen número de problemas del electromagnetismo y daba una explicación del experimento de Michelson y Morley, no proporciona una descripción relativista adecuada del campo gravitatorio. Tras la publicación del artículo de Einstein, la nueva teoría de la relatividad especial fue aceptada en unos pocos años por prácticamente la totalidad de los físicos y los matemáticos. De hecho, Poincaré o Lorentz habían estado muy cerca de llegar al mismo resultado que Einstein. La forma geométrica definitiva de la teoría se debe a Hermann Minkowski, antiguo profesor de Einstein en la Politécnica de Zúrich; acuñó el término «espacio-tiempo» (Raumzeit) y le dio la forma matemática adecuada.[nota 1] El espacio-tiempo de Minkowski es una variedad tetradimensional en la que se entrelazaban de una manera indisoluble las tres dimensiones espaciales y el tiempo. En este espacio-tiempo de Minkowski, el movimiento de una partícula se representa mediante su línea de universo (Weltlinie), una curva cuyos puntos vienen determinados por cuatro variables distintas: las tres dimensiones espaciales ( Relatividad general Esta sección es un extracto de Relatividad general.[editar] Representación artística de la explosión de la supernova SN 2006gy, situada a 238 millones de años luz. De ser válido el principio de acción a distancia, las perturbaciones de origen gravitatorio de este estallido nos afectarían inmediatamente y más tarde nos llegarían las de origen electromagnético, que se transmiten a la velocidad de la luz. Esquema bidimensional de la curvatura del espacio-tiempo (cuatro dimensiones) generada por una masa esférica. La teoría general de la relatividad o relatividad general es una teoría del campo gravitatorio y de los sistemas de referencia generales, publicada por Albert Einstein en 1915 y 1916. El nombre de la teoría se debe a que generaliza la llamada teoría especial de la relatividad y el principio de relatividad para un observador arbitrario. Los principios fundamentales introducidos en esta generalización son el principio de equivalencia, que describe la aceleración y la gravedad como aspectos distintos de la misma realidad, la noción de la curvatura del espacio-tiempo y el principio de covariancia generalizado. La teoría de la relatividad general propone que la propia geometría del espacio-tiempo se ve afectada por la presencia de materia, de lo cual resulta una teoría relativista del campo gravitatorio. De hecho la teoría de la relatividad general predice que el espacio-tiempo no será plano en presencia de materia y que la curvatura del espacio-tiempo será percibida como un campo gravitatorio. La intuición básica de Einstein fue postular que en un punto concreto no se puede distinguir experimentalmente entre un cuerpo acelerado uniformemente y un campo gravitatorio uniforme. La teoría general de la relatividad permitió también reformular el campo de la cosmología. Einstein expresó el propósito de la teoría de la relatividad general para aplicar plenamente el programa de Ernst Mach de la relativización de todos los efectos de inercia, incluso añadiendo la llamada constante cosmológica a sus ecuaciones de campo para este propósito. Este punto de contacto real de la influencia de Ernst Mach fue claramente identificado en 1918, cuando Einstein distingue lo que él bautizó como el principio de Mach (los efectos inerciales se derivan de la interacción de los cuerpos) del principio de la relatividad general, que se interpreta ahora como el principio de covariancia general. El matemático alemán David Hilbert escribió e hizo públicas las ecuaciones de la covariancia antes que Einstein, ello resultó en no pocas acusaciones de plagio contra Einstein, pero probablemente sea más porque es una teoría (o perspectiva) geométrica. La misma postula que la presencia de masa o energía «curva» el espacio-tiempo, y esta curvatura afecta la trayectoria de los cuerpos móviles e incluso la trayectoria de la luz. Formalismo de la teoría de la relatividad Representación de la línea de universo de una partícula. Como no es posible reproducir un espacio-tiempo de cuatro dimensiones, en la figura se representa solo la proyección sobre 2 dimensiones espaciales y una temporal. Partículas En la teoría de la relatividad una partícula puntual queda representada por un par Campos Cuando se consideran campos o distribuciones continuas de masa, se necesita algún tipo de generalización para la noción de partícula. Un campo físico posee momentum y energía distribuidos en el espacio-tiempo, el concepto de cuadrimomento se generaliza mediante el llamado tensor de energía-impulso que representa la distribución en el espacio-tiempo tanto de energía como de momento lineal. A su vez un campo dependiendo de su naturaleza puede representarse por un escalar, un vector o un tensor. Por ejemplo el campo electromagnético se representa por un tensor de segundo orden totalmente antisimétrico o 2-forma. Si se conoce la variación de un campo o una distribución de materia, en el espacio y en el tiempo entonces existen procedimientos para construir su tensor de energía-impulso. Magnitudes físicas En relatividad, estas magnitudes físicas son representadas por vectores 4-dimensionales o bien por objetos matemáticos llamados tensores, que generalizan los vectores, definidos sobre un espacio de cuatro dimensiones. Matemáticamente estos 4-vectores y 4-tensores son elementos definidos del espacio vectorial tangente al espacio-tiempo (y los tensores se definen y se construyen a partir del fibrado tangente o cotangente de la variedad que representa el espacio-tiempo). Correspondencia entre E3[nota 2] y M4[nota 3] Espacio tridimensional euclídeo Espacio-tiempo de Minkowski Punto Suceso Longitud Intervalo Velocidad Cuadrivelocidad Momentum Cuadrimomentum Igualmente además de cuadrivectores, se definen cuadritensores (tensores ordinarios definidos sobre el fibrado tangente del espacio-tiempo concebido como variedad lorentziana). La curvatura del espacio-tiempo se representa por un 4-tensor (tensor de cuarto orden), mientras que la energía y el momento de un medio continuo o el campo electromagnético se representan mediante 2-tensores (simétrico el tensor de energía-impulso, antisimétrico el de campo electromagnético). Los cuadrivectores son, de hecho, 1-tensores, en esta terminología. En este contexto se dice que una magnitud es un invariante relativista si tiene el mismo valor para todos los observadores, obviamente todos los invariantes relativistas son escalares (0-tensores), frecuentemente formados por la contracción de magnitudes tensoriales. El intervalo relativista El intervalo relativista puede definirse en cualquier espacio-tiempo, sea este plano como en la relatividad especial, o curvo como en relatividad general. Sin embargo, por simplicidad, discutiremos inicialmente el concepto de intervalo para el caso de un espacio-tiempo plano. El tensor métrico del espacio-tiempo plano de Minkowski se designa con la letra El intervalo, la distancia tetradimensional, se representa mediante la expresión Reproducción de un cono de luz, en el que se representan dos dimensiones espaciales y una temporal (eje de ordenadas). El observador se sitúa en el origen, mientras que el futuro y el pasado absolutos vienen representados por las partes inferior y superior del eje temporal. El plano correspondiente a t = 0 se denomina plano de simultaneidad o hipersuperficie de presente (también llamado «diagrama de Minkowski»). Los sucesos situados dentro de los conos están vinculados al observador por intervalos temporales. Los que se sitúan fuera, por intervalos espaciales. Los intervalos pueden ser clasificados en tres categorías: Intervalos espaciales (cuando Los intervalos nulos pueden ser representados en forma de cono de luz, popularizados por el celebérrimo libro de Stephen Hawking, Breve Historia del Tiempo. Sea un observador situado en el origen, el futuro absoluto (los sucesos que serán percibidos por el individuo) se despliega en la parte superior del eje de ordenadas, el pasado absoluto (los sucesos que ya han sido percibidos por el individuo) en la parte inferior, y el presente percibido por el observador en el punto 0. Los sucesos que están fuera del cono de luz no nos afectan, y por lo tanto se dice de ellos que están situados en zonas del espacio-tiempo que no tienen relación de causalidad con la nuestra. Imaginemos, por un momento, que en la galaxia Andrómeda, situada a 2.5 millones de años luz de nosotros, sucedió un cataclismo cósmico hace 100 000 años. Dado que, primero: la luz de Andrómeda tarda 2 millones de años en llegar hasta nosotros y segundo: nada puede viajar a una velocidad superior a la de los fotones, es evidente, que no tenemos manera de enterarnos de lo que sucedió en dicha Galaxia hace tan solo 100 000 años. Se dice, por lo tanto, que el intervalo existente entre dicha hipotética catástrofe cósmica y nosotros, observadores del presente, es un intervalo espacial ( Imagen de la galaxia Andrómeda, tomada por el telescopio Spitzer, tal como era hace 2.5 millones de años (por estar situada a 2.5 millones de años luz). Los sucesos acaecidos 1 000 000 de años atrás se observarán desde la Tierra dentro de un millón y medio de años. Se dice, por tanto, que entre tales eventos y nosotros existe un intervalo espacial. Análisis El único problema con esta hipótesis, es que al entrar en un agujero negro, se anula el espacio-tiempo, y como ya sabemos, algo que contenga algún volumen o masa, debe tener como mínimo un espacio donde ubicarse, el tiempo en ese caso, no tiene mayor importancia, pero el espacio juega un rol muy importante en la ubicación de volúmenes, por lo que esto resulta muy improbable, pero no imposible para la tecnología. Podemos escoger otro episodio histórico todavía más ilustrativo: El de la estrella de Belén, tal y como fue interpretada por Johannes Kepler. Este astrónomo alemán consideraba que dicha estrella se identificaba con una supernova que tuvo lugar el año 5 a. C., cuya luz fue observada por los astrónomos chinos contemporáneos, y que vino precedida en los años anteriores por varias conjunciones planetarias en la constelación de Piscis. Esa supernova probablemente estalló miles de años atrás, pero su luz no llegó a la Tierra sino hasta el año 5 a. C. De ahí que el intervalo existente entre dicho evento y las observaciones de los astrónomos egipcios y megalíticos (que tuvieron lugar varios siglos antes de Cristo) sea un intervalo espacial, pues la radiación de la supernova nunca pudo llegarles. Por el contrario, la explosión de la supernova por un lado, y las observaciones realizadas por los tres magos en Babilonia y por los astrónomos chinos en el año 5 a. C. por el otro, están unidas entre sí por un intervalo temporal, ya que la luz sí pudo alcanzar a dichos observadores. El tiempo propio y el intervalo se relacionan mediante la siguiente equivalencia: Esta invarianza se expresa a través de la llamada geometría hiperbólica: La ecuación del intervalo Cuadrivelocidad, aceleración y cuadrimomentum Artículos principales: Cuadrivelocidad y Cuadrimomento. En el espacio-tiempo de Minkowski, las propiedades cinemáticas de las partículas se representan fundamentalmente por tres magnitudes: La cuadrivelocidad (o tetravelocidad), la cuadriaceleración y el cuadrimomentum (o tetramomentum). La cuadrivelocidad es un cuadrivector tangente a la línea de universo de la partícula, relacionada con la velocidad coordenada de un cuerpo medida por un observador en reposo cualquiera, esta velocidad coordenada se define con la expresión newtoniana La velocidad coordenada de un cuerpo con masa depende caprichosamente del sistema de referencia que escojamos, mientras que la cuadrivelocidad propia es una magnitud que se transforma de acuerdo con el principio de covariancia y tiene un valor siempre constante equivalente al intervalo dividido entre el tiempo propio ( La cuadriaceleración puede ser definida como la derivada temporal de la cuadrivelocidad ( Junto con los principios de invarianza del intervalo y la cuadrivelocidad, juega un papel fundamental la ley de conservación del cuadrimomentum. Es aplicable aquí la definición newtoniana del momentum Como tanto la velocidad de la luz como el cuadrimomentum son magnitudes conservadas, también lo es su producto, al que se le da el nombre de energía conservada Componentes Magnitud del cuadrimomentum Magnitud en cuerpos con masa Magnitud en fotones (masa = 0) Energía Energía en cuerpos con masa (cuerpos en reposo, p=0) Energía en fotones (masa en reposo = 0) La aparición de la Relatividad Especial puso fin a la secular disputa que mantenían en el seno de la mecánica clásica las escuelas de los mecanicistas y los energetistas. Los primeros sostenían, siguiendo a Descartes y Huygens, que la magnitud conservada en todo movimiento venía constituida por el momentum total del sistema, mientras que los energetistas ,que tomaban por base los estudios de Leibniz, consideraban que la magnitud conservada venía conformada por la suma de dos cantidades: La fuerza viva, equivalente a la mitad de la masa multiplicada por la velocidad al cuadrado ( La mecánica newtoniana dio la razón a ambos postulados, afirmando que tanto el momentum como la energía son magnitudes conservadas en todo movimiento sometido a fuerzas conservativas. Sin embargo, la Relatividad Especial dio un paso más allá, por cuanto a partir de los trabajos de Einstein y Minkowski el momentum y la energía dejaron de ser considerados como entidades independientes y se les pasó a considerar como dos aspectos, dos facetas de una única magnitud conservada: el cuadrimomentum. Componentes y magnitud de los diferentes conceptos cinemáticos Concepto Componentes Expresión algebraica Partículas con masa Fotones Intervalo ot =0} Cuadrivelocidad no definida Aceleración (sistemas inerciales) ot =0} (sistemas no inerciales) Aceleración no definida Cuadrimomentum El tensor de energía-impulso (Tab) Artículo principal: Tensor de energía-impulso Tensor de tensión-energía Tres son las ecuaciones fundamentales que en física newtoniana describen el fenómeno de la gravitación universal: la primera, afirma que la fuerza gravitatoria entre dos cuerpos es proporcional al producto de sus masas e inversamente proporcional al cuadrado de su distancia (1); la segunda, que el potencial gravitatorio ( Sin embargo, estas ecuaciones no son compatibles con la Relatividad Especial por dos razones: En primer lugar la masa no es una magnitud absoluta, sino que su medición deriva en resultados diferentes dependiendo de la velocidad relativa del observador. De ahí que la densidad de masa En segundo lugar, si el concepto de espacio es relativo, también lo es la noción de densidad. Es evidente que la contracción del espacio producida por el incremento de la velocidad de un observador, impide la existencia de densidades que permanezcan invariables ante las transformaciones de Lorentz. Por todo ello, resulta necesario prescindir del término O lo que es lo mismo: El componente donde Además, si los componentes del tensor se miden por un observador en reposo relativo respecto al fluido, entonces, el tensor métrico viene constituido simplemente por la métrica de Minkowski: diag diag Puesto que además la tetravelocidad del fluido respecto al observador en reposo es: como consecuencia de ello, los coeficientes del tensor de tensión-energía son los siguientes: Parte de la materia que cae en el disco de acreción de un agujero negro es expulsada a gran velocidad en forma de chorros. En supuestos como este, los efectos gravitomagnéticos pueden llegar a alcanzar cierta importancia. Donde Podemos, a partir del tensor de tensión-energía, calcular cuánta masa contiene un determinado volumen del fluido: Retomando la definición de este tensor expuesta unas líneas más arriba, se puede definir al coeficiente Del mismo modo, es posible deducir matemáticamente a partir del tensor de tensión-energía la definición newtoniana de presión, introduciendo en la mentada ecuación cualquier par de índices que sean diferentes de cero: La hipersuperficie Finalmente, derivamos parcialmente ambos miembros de la ecuación respecto al tiempo, y teniendo en cuenta que la fuerza no es más que la tasa de incremento temporal del momentum obtenemos el resultado siguiente: Que contiene la definición newtoniana de la presión como fuerza ejercida por unidad de superficie. El tensor electromagnético (Fab) Artículo principal: Tensor de campo electromagnético Las ecuaciones deducidas por el físico escocés James Clerk Maxwell demostraron que electricidad y magnetismo no son más que dos manifestaciones de un mismo fenómeno físico: el campo electromagnético. Ahora bien, para describir las propiedades de este campo los físicos de finales del siglo XIX debían utilizar dos vectores diferentes, los correspondientes los campos eléctrico y magnético. Fue la llegada de la relatividad especial la que permitió describir las propiedades del electromagnetismo con un solo objeto geométrico, el vector cuadripotencial, cuyo componente temporal se correspondía con el potencial eléctrico, mientras que sus componentes espaciales eran los mismos que los del potencial magnético. De este modo, el campo eléctrico puede ser entendido como la suma del gradiente del potencial eléctrico más la derivada temporal del potencial magnético: y el campo magnético, como el rotacional del potencial magnético: abla imes A} Las propiedades del campo electromagnético pueden también expresarse utilizando un tensor de segundo orden denominado tensor de Faraday y que se obtiene diferenciando exteriormente al vector cuadripotencial La fuerza de Lorentz puede deducirse a partir de la siguiente expresión."

ksampletext_wikipedia_chem_valenciaquimica: str = "Valencia (química). La valencia es el número de electrones que le faltan o debe ceder un elemento químico para completar su último nivel de energía. Estos electrones son los que pone en juego durante una reacción química o para establecer un enlace químico con otro elemento. Hay elementos con más de una valencia, por ello fue reemplazado este concepto con el de números de oxidación que finalmente representa lo mismo. A través del siglo XX, el concepto de valencia ha evolucionado en una amplia gama de aproximaciones para describir el enlace químico, incluyendo la estructura de Lewis (1916), la teoría del enlace de valencia (1927), la teoría de los orbitales moleculares (1928), la teoría de repulsión de pares electrónicos de la capa de valencia (1958) y todos los métodos avanzados de química cuántica. En química, históricamente se ha considerado la valencia de un elemento como su capacidad de combinarse con otros elementos, para formar compuestos moleculares. Diferentes autores utilizan distintas definiciones de valencia, lo que crea un debate sobre cuál es la correcta. Por ejemplo, algunos autores confunden número de coordinación con valencia o estado de oxidación con valencia, a pesar de que esos tres términos son diferentes entre sí. Historia La etimología de la palabra «valencia» proviene de 1543, significando molde, del latín valentía poder, capacidad, y el significado químico refiriéndose al «poder combinante de un elemento» está registrado desde 1884, del alemán Valenz. En 1890, William Higgins publicó bocetos sobre lo que él llamó combinaciones de partículas últimas, que esbozaban el concepto de enlaces de valencia. Si, por ejemplo, de acuerdo a Higgins, la fuerza entre la partícula última de oxígeno y la partícula última de nitrógeno era 6, luego la fuerza del enlace debería ser dividida acordemente, y de modo similar para las otras combinaciones de partículas últimas: estas son las de la tabla periódica. Combinaciones de partículas últimas de William Higgins (1789). Sin embargo, el origen no exacto de la teoría de las valencias químicas puede ser rastreado a una publicación de Edward Frankland, en la que combinó las viejas teorías de los radicales libres y «teoría de tipos» con conceptos sobre afinidad química para mostrar que ciertos elementos tienen la tendencia a combinarse con otros elementos para formar compuestos conteniendo tres equivalentes del átomo unido, por ejemplo, en los grupos de tres átomos (vg. NO3, NH3, NI3, etc.) o cinco, por ejemplo en los grupos de cinco átomos (vg. N2O5, NH4O, P2O5, etc.) Es en este modo, según Franklin, que sus afinidades están mejor satisfechas. Siguiendo estos ejemplos y postulados, Franklin declaró cuán obvio esto es que: Una tendencia o ley prevalece (aquí), y que, no importa qué puedan ser los caracteres de los átomos que se unen, el poder combinante de los elementos atrayentes, si me puedo permitir el término, se satisface siempre por el mismo número de estos átomos. Descripción La capacidad combinatoria o afinidad de un átomo de un elemento dado viene determinada por el número de átomos de hidrógeno con los que se combina. En el metano, el carbono tiene una valencia de 4; en el amoníaco, el nitrógeno tiene una valencia de 3; en el agua, el oxígeno tiene una valencia de 2; y en el cloruro de hidrógeno, el cloro tiene una valencia de 1. El cloro, al tener una valencia de uno, puede sustituir al hidrógeno. El fósforo tiene una valencia de 5 en el pentacloruro de fósforo, PCl 5. Los diagramas de valencia de un compuesto representan la conectividad de los elementos, con líneas dibujadas entre dos elementos, a veces llamadas enlaces, que representan una valencia saturada para cada elemento. Las dos tablas siguientes muestran algunos ejemplos de diferentes compuestos, sus diagramas de valencia y las valencias para cada elemento del compuesto. Compuesto H Hidrógeno CH Metano C Propano C Propileno C Acetileno Diagrama  Valencias Hidrógeno: 1 Carbono: 4 Hidrógeno: 1 Carbono: 4 Hidrogeno: 1 Carbono: 4 Hidrógeno: 1 Carbono: 4 Hidrógeno: 1 Compuesto NH Amoníaco NaCN Cianuro de sodio PSCl Thiophosphoryl chloride H Ácido sulfhídrico H 2SO Ácido sulfúrico H Dithionic acid Cl Óxido perclórico XeO Tetraóxido de xenón Diagrama   Valencias Nitrógeno: 3 Hidrógeno: 1 Sodio: 1 Carbono: 4 Nitrógeno: 3 Fósforo: 5 Azufre: 2 Cloro: 1 Azufre: 2 Hidrógeno: 1 Azufre: 6 Oxígeno: 2 Hidrógeno: 1 Azufre: 6 Oxígeno: 2 Hidrógeno: 1 Cloro: 7 Oxígeno: 2 Xenón: 8 Oxígeno: 2 Definiciones modernas La valencia es definida por la IUPAC como: El número máximo de átomos univalentes (originalmente átomos de hidrógeno o de cloro) que pueden combinarse con un átomo del elemento considerado, o con un fragmento, o por el que puede sustituirse un átomo de este elemento. Una descripción moderna alternativa es: El número de átomos de hidrógeno que pueden combinarse con un elemento en un hidruro binario o el doble del número de átomos de oxígeno que se combinan con un elemento en su óxido u óxidos. Esta definición difiere de la definición de la IUPAC, ya que se puede decir que un elemento tiene más de una valencia. Una definición moderna muy similar dada en un artículo reciente define la valencia de un átomo particular en una molécula como el número de electrones que un átomo utiliza en la unión, con dos fórmulas equivalentes para calcular la valencia: valencia = número de electrones en la capa de valencia del átomo libre - número de electrones no enlazantes del átomo en la molécula valencia = número de enlaces + carga formal. Sin embargo esta definición de valencia es incorrecta. Por esta definición, el átomo de nitrógeno en el ion de amonio [NH4]+ es pentavalente, y en el ion de amida [NH2]- es monovalente, que obviamente es falso, porque el átomo de nitrógeno en los iones de amonio y amida es trivalente. Por lo tanto, esta definición es engañosa porque puede dar resultados falsos. Desarrollo histórico La etimología de la palabra valencia se remonta a 1425, con el significado de extracto, preparado, del latín valentia fuerza, capacidad, del anterior valor valía, valor, y el significado químico referido al poder combinatorio de un elemento se registra a partir de 1884, del alemán Valenz. William Higgins combinaciones de partículas últimas (1789) El concepto de valencia se desarrolló en la segunda mitad del siglo XIX y ayudó a explicar con éxito la estructura molecular de los compuestos inorgánicos y orgánicos.La búsqueda de las causas subyacentes de la valencia condujo a las teorías modernas del enlace químico, incluyendo el átomo cúbico (1902), estructura de Lewiss (1916), teoría del enlace de valencia (1927), orbitales molecularess (1928), teoría de repulsión de pares de electrones de la corteza de valencia (1958), y todos los métodos avanzados de la química cuántica. En 1789, William Higgins publicó opiniones sobre lo que denominó combinaciones de partículas últimas, que prefiguraron el concepto de enlaces de valencia. Si, por ejemplo, según Higgins, la fuerza entre la partícula última de oxígeno y la partícula última de nitrógeno fuera 6, entonces la intensidad de la fuerza se dividiría en consecuencia, y lo mismo para las demás combinaciones de partículas últimas (véase la ilustración). Sin embargo, el origen exacto de la teoría de las valencias químicas se remonta a un trabajo de Edward Frankland de 1852, en el que combinó la antigua teoría de los radicales con ideas sobre la afinidad química para demostrar que ciertos elementos tienen tendencia a combinarse con otros elementos para formar compuestos que contienen 3, es decir, en los grupos de 3 átomos (por ejemplo, NO 3, NH 3, NI 3, etc.) o 5, es decir, en los grupos de 5 átomos (por ejemplo, NO 5, NH 4O, PO 5, etc.), equivalentes de los elementos unidos. Según él, ésta es la manera en que mejor se satisfacen sus afinidades, y siguiendo estos ejemplos y postulados, declara lo obvio que es que Una tendencia o ley prevalece (aquí), y es que, cualesquiera que sean los caracteres de los átomos que se unen, el poder combinatorio del elemento que atrae, si se me permite el término, se satisface siempre con el mismo número de estos átomos. En 1857 August Kekulé propuso valencias fijas para muchos elementos, como 4 para el carbono, y las utilizó para proponer fórmulas estructurales para muchas moléculas de orgánica, que todavía se aceptan hoy en día. Lothar Meyer en su libro de 1864, Die modernen Theorien der Chemie, que contenía una primera versión de la tabla periódica con 28 elementos, clasificó por primera vez los elementos en seis familias según su valencia. Los trabajos sobre la organización de los elementos por peso atómico, hasta entonces se habían visto obstaculizados por el uso generalizado de peso equivalentes para los elementos, en lugar de pesos atómicos. La mayoría de los químicos del siglo XIX definían la valencia de un elemento como el número de sus enlaces sin distinguir diferentes tipos de valencia o de enlace. Sin embargo, en 1893 Alfred Werner describió metal de transición complejos de coordinaciónes como [Co(NH 6]Cl 3, en los que distinguió valencias principales y subsidiarias (en alemán: Hauptvalenz y Nebenvalenz), correspondientes a los conceptos modernos de estado de oxidación y número de coordinación respectivamente. Para los elementos del grupo principal, en 1904 Richard Abegg consideró valencias positivas y negativas (estados de oxidación máximo y mínimo), y propuso la regla de Abegg según la cual su diferencia es a menudo 8. Electrones y valencia El modelo de Rutherford del átomo nuclear (1911) demostró que el exterior de un átomo está ocupado por electrones, lo que sugiere que los electrones son responsables de la interacción de los átomos y de la formación de enlaces químicos. En 1916, Gilbert N. Lewis explicó la valencia y el enlace químico en términos de una tendencia de los átomos (del grupo principal) a alcanzar una octeto estable de 8 electrones de valencia. Según Lewis, el enlace covalente conduce a octetos por la compartición de electrones, y el enlace iónico conduce a octetos por la transferencia de electrones de un átomo a otro. El término covalencia se atribuye a Irving Langmuir, quien afirmó en 1919 que el número de pares de electrones que un átomo dado comparte con los átomos adyacentes se denomina covalencia de ese átomo. El prefijo co- significa juntos, de modo que un enlace covalente significa que los átomos comparten una valencia. Posteriormente, ahora es más común hablar de enlaces covalentes en lugar de valencia, que ha caído en desuso en trabajos de nivel superior a partir de los avances en la teoría del enlace químico, pero sigue siendo ampliamente utilizado en estudios elementales, donde proporciona una introducción heurística al tema. En la década de 1930, Linus Pauling propuso que también existen enlaces covalentes polares, que son intermedios entre los covalentes y los iónicos, y que el grado de carácter iónico depende de la diferencia de electronegatividad de los dos átomos enlazados. Pauling también consideró las moléculas hipervalentes, en las que los elementos del grupo principal tienen valencias aparentes superiores a la máxima de 4 permitida por la regla del octeto. Por ejemplo, en la molécula de hexafluoruro de azufre (SF 6), Pauling consideró que el azufre forma 6 enlaces verdaderos de dos electrones utilizando orbitales atómicos híbridos sp3d2, que combinan un orbital s, tres orbitales p y dos orbitales d. Sin embargo, más recientemente, cálculos cuántico-mecánicos sobre esta molécula y otras similares han demostrado que el papel de los orbitales d en el enlace es mínimo, y que la molécula SF 6 debería describirse como una molécula con 6 enlaces covalentes polares (en parte iónicos) formados por sólo cuatro orbitales en el azufre (un s y tres p) de acuerdo con la regla del octeto, junto con seis orbitales en los fluorinos. Cálculos similares en moléculas de metales de transición muestran que el papel de los orbitales p es menor, de modo que un orbital s y cinco orbitales d en el metal son suficientes para describir el enlace. Tipos de valencia Valencia positiva máxima: es el número positivo que refleja la máxima capacidad de combinación de un átomo. Este número coincide con el grupo de la tabla periódica de los elementos al cual pertenece. Por ejemplo, el cloro (Cl) pertenece al grupo 7, por lo que su valencia positiva máxima es 7. Valencia negativa solo para el grupo A no para el grupo B: es el número negativo que refleja la capacidad que tiene un átomo de combinarse con otro pero que esté actuando con valencia positiva. Este número negativo se puede determinar contando lo que le falta a la valencia positiva máxima para llegar a 8, pero con signo -. Por ejemplo: a la valencia máxima positiva del átomo de cloro es 7, por lo que le falta un electrón para cumplir el octeto, entonces su valencia negativa será -1. Vista general El concepto fue desarrollado a mediados del siglo XIX, en un intento por racionalizar la fórmula química de compuestos químicos diferentes. En 1919, Irving Langmuir, tomó prestado el término para explicar el modelo del átomo cúbico de Gilbert N. Lewis al enunciar que el número de pares de electrones que cualquier átomo dado comparte con el átomo adyacente es denominado la covalencia del átomo. El prefijo co- significa «junto», así que un enlace covalente significa que los átomos comparten valencia. De ahí, si un átomo, por ejemplo, tiene una valencia +1, significa que perdió un electrón, y otro con una valencia de -1, significa que tiene un electrón adicional. Luego, un enlace entre estos dos átomos resultaría porque se complementarían o compartirían sus tendencias en el balance de la valencia. Subsecuentemente, actualmente es más común hablar de enlace covalente en vez de valencia, que ha caído en desuso del nivel más alto de trabajo, con los avances en la teoría del enlace químico, pero aún es usado ampliamente en estudios elementales donde provee una introducción heurística a la materia. Definición del número de enlaces Se creía originalmente que el número de enlaces formados por un elemento dado era una propiedad química fija y, en efecto, en muchos casos, es una buena aproximación. Por ejemplo, en muchos de sus compuestos, el carbono forma cuatro enlaces, el oxígeno dos y el hidrógeno uno. Sin embargo, pronto se hizo evidente que, para muchos elementos, la valencia podría variar entre compuestos diferentes. Uno de los primeros ejemplos en ser identificado era el fósforo, que algunas veces se comporta como si tuviera una valencia de tres, y otras como si tuviera una valencia de cinco. Un método para resolver este problema consiste en especificar la valencia para cada compuesto individual: aunque elimina mucho de la generalidad del concepto, esto ha dado origen a la idea de número de oxidación (usado en la nomenclatura Stock y a la notación lambda en la nomenclatura IUPAC de química inorgánica). Definición de IUPAC La Unión Internacional de Química Pura y Aplicada (IUPAC) ha hecho algunos intentos de llegar a una definición desambigua de valencia. La versión actual, adoptada en 1994, es la siguiente: La valencia es el máximo número de átomos univalentes (originalmente átomos de hidrógeno o cloro) que pueden combinarse con un átomo del elemento en consideración, o con un fragmento, o para el cual un átomo de este elemento puede ser sustituido. Esta definición reimpone una valencia única para cada elemento a expensas de despreciar, en muchos casos, una gran parte de su química. La mención del hidrógeno y el cloro es por razones históricas, aunque ambos en la práctica forman compuestos principalmente en los que sus átomos forman un enlace simple. Las excepciones en el caso del hidrógeno incluyen el ion bifluoruro, [HF2]−, y los diversos hidruros de boro tales como el diborano: estos son ejemplos de enlace de tres centros. El cloro forma un número de fluoruro,ClF, ClF3 y ClF5,y su valencia, de acuerdo a la definición de la IUPAC, es cinco. El flúor es el elemento para el que el mayor número de átomos se combinan con átomos de otros elementos: es univalente en todos sus compuestos, excepto en el ion [H2F]+. En efecto, la definición IUPAC sólo puede ser resuelta al fijar las valencias del hidrógeno y el flúor como uno, convención que ha sido seguida acá. Valencias de los elementos. Estados de oxidación de los elementos Las valencias de la mayoría de los elementos se basan en el fluoruro más alto conocido."
ksampletext_wikipedia_chem_quimicaorganica: str = "Química orgánica. La química orgánica es la rama de la química que estudia una clase numerosa de moléculas, que, en su mayoría contienen carbono formando enlaces covalentes: carbono-carbono o carbono-hidrógeno y otros heteroátomos, también conocidos como compuestos orgánicos. Debido a la omnipresencia del carbono en los compuestos que esta rama de la química estudia, esta disciplina también es llamada química del carbono. Historia El trabajo de Friedrich Wöhler sobre la síntesis de la urea es considerado por muchos como el inicio de la química orgánica, y en particular de la síntesis orgánica. La química orgánica constituyó o se instituyó como disciplina en los años treinta. El desarrollo de nuevos métodos de análisis de las sustancias de origen animal y vegetal, basados en el empleo de disolventes, como el éter o el alcohol, permitió el aislamiento de un gran número de sustancias orgánicas que recibieron el nombre de principios inmediatos. La aparición de la química orgánica se asocia a menudo al descubrimiento, en 1828, por el químico alemán Friedrich Wöhler, de que la sustancia inorgánica cianato de amonio podía convertirse en urea, una sustancia orgánica que se encuentra en la orina de muchos animales. Antes de este descubrimiento, los químicos creían que para sintetizar sustancias orgánicas, era necesaria la intervención de lo que llamaban la fuerza vital, es decir, los organismos vivos. El experimento de Wöhler rompió la barrera entre sustancias orgánicas e inorgánicas. De esta manera, los químicos modernos consideran compuestos orgánicos a aquellos que contienen carbono e hidrógeno, y otros elementos (que pueden ser uno o más), siendo los más comunes: oxígeno, nitrógeno, azufre y los halógenos. En 1856, sir William Henry Perkin, mientras trataba de estudiar la quinina, accidentalmente fabricó el primer colorante orgánico ahora conocido como malva de Perkin. La diferencia entre la química orgánica y la química biológica,es que en la segunda las moléculas de ADN tienen una historia y, por ende, en su estructura nos hablan de su historia, del pasado en el que se han constituido, mientras que una molécula orgánica, creada hoy, es solo testigo de su presente, sin pasado y sin evolución histórica. Cronología Artículo principal: Cronología de la Química orgánica 1675: Lémery clasifica los productos químicos naturales, según su origen en minerales, vegetales y animales 1784: Antoine Lavoisier demuestra que todos los productos vegetales y animales están formados básicamente por carbono e hidrógeno y, en menor proporción, nitrógeno, oxígeno y azufre 1807: Jöns Jacob Berzelius clasifica los productos químicos en: Orgánicos: los que proceden de organismos vivos. Inorgánicos: los que proceden de la materia inanimada. 1816: Michel Eugène Chevreul prepara distintos jabones a partir de diferentes fuentes de ácidos grasos y diversas bases, produciendo así distintas sales de ácidos grasos (o jabones), que no resultaron ser más que productos orgánicos nuevos derivados de productos naturales (grasas animales y vegetales). 1828: Friedrich Wöhler, a partir de sustancias inorgánicas y con técnicas normales de laboratorio, sintetizó la sustancia urea, la segunda sustancia orgánica obtenida artificialmente, luego del oxalato de amonio. Fórmula desarrollada urea 1856: Sir William Perkin sintetiza el primer colorante orgánico por accidente. 1865: August Kekulé propuso que los átomos de carbono que forman el benceno se unen formando cadenas cerradas o anillos. Primeros compendios La tarea de presentar la química orgánica de manera sistemática y global se realizó mediante una publicación surgida en Alemania, fundada por el químico Friedrich Konrad Beilstein (1838-1906). Su Handbuch der organischen Chemie (Manual de la química orgánica) comenzó a publicarse en Hamburgo en 1880 y consistió en dos volúmenes que recogían información de unos quince mil compuestos orgánicos conocidos. Cuando la Deutsche chemische Gesellschaft (Sociedad Alemana de Química) trató de elaborar la cuarta reedición, en la segunda década del siglo XX, la cifra de compuestos orgánicos se había multiplicado por diez. Treinta y siete volúmenes fueron necesarios para la edición básica, que aparecieron entre 1916 y 1937. Un suplemento de 27 volúmenes se publicó en 1938, recogiendo información aparecida entre 1910 y 1919. En la actualidad, se está editando el Fünftes Ergänzungswerk (quinta serie complementaria), que recoge la documentación publicada entre 1960 y 1979. Para ofrecer con más prontitud sus últimos trabajos, el Beilstein Institut ha creado el servicio Beilstein On line, que funciona desde 1988. Recientemente, se ha comenzado a editar periódicamente un CD-ROM, Beilstein Current Facts in Chemistry, que selecciona la información química procedente de importantes revistas. Actualmente, la citada información está disponible a través de internet. El alma de la química orgánica: el carbono Estructura tetraédrica del metano. La gran cantidad de compuestos orgánicos que existen tiene su explicación en las características del átomo de carbono, que tiene cuatro electrones en su capa de valencia: según la regla del octeto necesita ocho para completarla, por lo que forma cuatro enlaces (valencia = 4) con otros átomos. Esta especial configuración electrónica da lugar a una variedad de posibilidades de hibridación orbital del átomo de carbono (hibridación química). La molécula orgánica más sencilla que existe es el metano. En esta molécula, el carbono presenta hibridación sp3, con los átomos de hidrógeno formando un tetraedro. El carbono forma enlaces covalentes con facilidad para alcanzar una configuración estable, estos enlaces los forma con facilidad con otros carbonos, lo que permite formar frecuentemente cadenas abiertas (lineales o ramificadas) y cerradas (anillos). Clasificación de compuestos orgánicos La clasificación de los compuestos orgánicos puede realizarse de diversas maneras: atendiendo a su origen (natural o sintético), a su estructura (p. ej.: alifático o aromático), a su funcionalidad (p. ej.: alcoholes o cetonas), o a su peso molecular (p. ej.: monómeros o polímeros). Clasificación según su origen La clasificación de los compuestos orgánicos según el origen es de dos tipos: naturales o sintéticos. A menudo, los de origen natural se entiende que son los presentes en los seres vivos, pero no siempre es así, ya que algunas moléculas orgánicas también se sintetizan ex-vivo, es decir en ambientes inertes, como por ejemplo el ácido fórmico en el cometa Halle-Bopp. Natural In-vivo Los compuestos orgánicos presentes en los seres vivos o biosintetizados constituyen una gran familia de compuestos orgánicos. Su estudio tiene interés en medicina, farmacia, perfumería, cocina y muchos otros campos más. Carbohidratos Los carbohidratos están compuestos fundamentalmente de carbono (C), oxígeno (O) e hidrógeno (H). Son a menudo llamados azúcares, pero esta nomenclatura no es del todo correcta. Tienen una gran presencia en el reino vegetal (fructosa, celulosa, almidón, alginatos), pero también en el animal (glucógeno, glucosa). Se suelen clasificar según su grado de polimerización en: Monosacáridos (glucosa, fructosa, ribosa y desoxirribosa) Disacáridos (sacarosa, lactosa, maltosa) Trisacáridos (maltotriosa, rafinosa) Polisacáridos (alginatos, ácido algínico, celulosa, almidón, etc.) Lípidos Los lípidos son un conjunto de moléculas orgánicas, la mayoría biomoléculas, compuestas principalmente por carbono e hidrógeno y en menor medida oxígeno, aunque también pueden contener fósforo, azufre y nitrógeno. Tienen como característica principal el ser hidrófobas (insolubles en agua) y solubles en disolventes orgánicos como la bencina, el benceno y el cloroformo. En el uso coloquial, a los lípidos se les llama incorrectamente grasas, ya que las grasas son solo un tipo de lípidos procedentes de animales. Los lípidos cumplen funciones diversas en los organismos vivientes, entre ellas la de reserva energética (como los triglicéridos), la estructural (como los fosfolípidos de las bicapas) y la reguladora (como las hormonas esteroides). Proteínas fórmula química de un aminoácido. Las proteínas son polipéptidos, es decir están formados por la polimerización de péptidos, y estos por la unión de aminoácidos. Pueden considerarse así poliamidas naturales, ya que el enlace peptídico es análogo al enlace amida. Comprenden una familia muy importante de moléculas en los seres vivos, pero en especial en el reino animal. Por otra parte, son producto de la expresión de genes contenidos en el ADN. Algunos ejemplos de proteínas son el colágeno, las fibroínas, o la seda de araña. Ácidos nucleicos Los ácidos nucleicos son polímeros formados por la repetición de monómeros denominados nucleótidos, unidos mediante enlaces fosfodiéster. Se forman, así, largas cadenas; algunas moléculas de ácidos nucleicos llegan a alcanzar pesos moleculares gigantescos, con millones de nucleótidos encadenados. Están formados por la moléculas de carbono, hidrógeno, oxígeno, nitrógeno y fosfato. Los ácidos nucleicos almacenan la información genética de los organismos vivos y son los responsables de la transmisión hereditaria. Existen dos tipos básicos, el ADN y el ARN. Moléculas pequeñas Estructura de la testosterona. Una hormona, que se puede clasificar como molécula pequeña en el argot-químico-orgánico. Las moléculas pequeñas son compuestos orgánicos de peso molecular moderado (generalmente se consideran pequeñas aquellas con peso molecular menor a 1000 g/mol) y que aparecen en pequeñas cantidades en los seres vivos, pero no por ello su importancia es menor. A ellas pertenecen distintos grupos de hormonas como la testosterona, el estrógeno u otros grupos como los alcaloides. Las moléculas pequeñas tienen gran interés en la industria farmacéutica por su relevancia en el campo de la medicina. Ex-vivo Son compuestos orgánicos que han sido sintetizados sin la intervención de ningún ser vivo, en ambientes extracelulares y extravirales. Procesos geológicos Sello alemán de 1964 conmemorativo de la descripción de la estructura del benceno por Friedrich August Kekulé en 1865. El petróleo es una sustancia clasificada como mineral en la cual se presentan una gran cantidad de compuestos orgánicos. Muchos de ellos, como el benceno, son empleados por el hombre tal cual, pero muchos otros son tratados o derivados para conseguir una gran cantidad de compuestos orgánicos, como por ejemplo los monómeros para la síntesis de materiales poliméricos o plásticos. Procesos atmosféricos El sistema climático está constituido por la atmósfera, la hidrósfera, la biosfera, la geosfera y sus interacciones. Las variaciones en el equilibrio climático pueden generar diversos procesos como el calentamiento global, el efecto invernadero o la disminución de la capa de ozono. Procesos de síntesis planetaria En el año 2000 el ácido fórmico, un compuesto orgánico sencillo, también fue hallado en la cola del cometa Hale-Bopp. Puesto que la síntesis orgánica de estas moléculas es inviable bajo las condiciones espaciales, este hallazgo parece sugerir que a la formación del sistema solar debió anteceder un periodo de calentamiento durante su colapso final. Sintético Desde la síntesis de Wöhler de la urea un altísimo número de compuestos orgánicos han sido sintetizados químicamente para beneficio humano. Estos incluyen fármacos, desodorantes, perfumes, detergentes, jabones, fibras textiles sintéticas, materiales plásticos, polímeros en general, o colorantes orgánicos. Cadenas hidrocarbonadas sencillas Hidrocarburos El compuesto más simple es el metano, un átomo de carbono con cuatro de hidrógeno (valencia = 1), pero también puede darse la unión carbono-carbono, formando cadenas de distintos tipos, ya que pueden darse enlaces simples, dobles o triples. Cuando el resto de enlaces de estas cadenas son con hidrógeno, se habla de hidrocarburos, que pueden ser: Saturados: con enlaces covalentes simples, alcanos. Insaturados: con dobles enlaces covalentes (alquenos) o triples (alquinos). Hidrocarburos cíclicos: Hidrocarburos saturados con cadena cerrada, como el ciclohexano. Aromáticos: estructura cíclica. Radicales y ramificaciones de cadena Estructura de un hidrocarburo ramificado nombrado 5-butil-3,9-dimetil-undecano. Los radicales son fragmentos de cadenas de carbonos que cuelgan de la cadena principal. Su nomenclatura se hace con la raíz correspondiente (en el caso de un carbono met-, dos carbonos et-, tres carbonos prop-, cuatro carbonos but-, cinco carbonos pent-, seis carbonos hex-, y así sucesivamente) y el sufijo -il. Además, se indica con un número, colocado delante, la posición que ocupan. El compuesto más simple que se puede hacer con radicales es el 2-metilpropano. En caso de que haya más de un radical, se nombrarán por orden alfabético de las raíces. Por ejemplo, el 2-etil, 5-metil, 8-butil, 10-docoseno. Clasificación según los grupos funcionales Los compuestos orgánicos también pueden contener otros elementos, también otros grupos de átomos además del carbono e hidrógeno, llamados grupos funcionales. Un ejemplo es el grupo hidroxilo, que forma los alcoholes: un átomo de oxígeno enlazado a uno de hidrógeno (-OH), al que le queda una valencia libre. Asimismo también existen funciones alqueno (dobles enlaces), éteres, ésteres, aldehídos, cetonas, carboxílicos, carbamoilos, azo, nitro o sulfóxido, entre otros. Alquino Alquino Hidroxilo Hidroxilo Éter Éter Amina Amina Aldehído Aldehído Cetona Cetona Carboxilo Carboxilo Éster Éster Amida Amida Azo Azo Nitro Nitro Sulfóxido Sulfóxido Monómero de la celulosa. Oxigenados Son cadenas de carbonos con uno o varios átomos de oxígeno. Pueden ser: Alcoholes: Las propiedades físicas de un alcohol se basan principalmente en su estructura. El alcohol está compuesto por un alcano y agua. Contiene un grupo hidrofóbico (sin afinidad por el agua) del tipo de un alcano, y un grupo hidroxilo que es hidrófilo (con afinidad por el agua), similar al agua. De estas dos unidades estructurales, el grupo –OH da a los alcoholes sus propiedades físicas características, y el alquilo es el que las modifica, dependiendo de su tamaño y forma. El grupo –OH es muy polar y, lo que es más importante, es capaz de establecer puentes de hidrógeno: con sus moléculas compañeras o con otras moléculas neutras. Dependiendo de la cantidad de grupos -OH que forman parte del alcohol, el mismo puede ser clasificado como monohidroxilado (presencia de un hidroxilo) o polihidroxilado (dos o más grupos hidroxilos en la molécula). Aldehídos: Los aldehídos son compuestos orgánicos caracterizados por poseer el grupo funcional -CHO. Se denominan como los alcoholes correspondientes, cambiando la terminación -ol por -al: Es decir, el grupo carbonilo H-C=O está unido a un solo radical orgánico. Cetonas: Una cetona es un compuesto orgánico caracterizado por poseer un grupo funcional carbonilo unido a dos átomos de carbono, a diferencia de un aldehído, en donde el grupo carbonilo se encuentra unido al menos a un átomo de hidrógeno. Cuando el grupo funcional carbonilo es el de mayor relevancia en dicho compuesto orgánico, las cetonas se nombran agregando el sufijo -ona al hidrocarburo del cual provienen (hexano, hexanona; heptano, heptanona; etc). También se puede nombrar posponiendo cetona a los radicales a los cuales está unido (por ejemplo: metilfenil cetona). Cuando el grupo carbonilo no es el grupo prioritario, se utiliza el prefijo oxo- (ejemplo: 2-oxopropanal). El grupo funcional carbonilo consiste en un átomo de carbono unido con un doble enlace covalente a un átomo de oxígeno. El tener dos átomos de carbono unidos al grupo carbonilo, es lo que lo diferencia de los ácidos carboxílicos, aldehídos, ésteres. El doble enlace con el oxígeno, es lo que lo diferencia de los alcoholes y éteres. Las cetonas suelen ser menos reactivas que los aldehídos dado que los grupos alquílicos actúan como dadores de electrones por efecto inductivo. Ácidos carboxílicos: Los ácidos carboxílicos constituyen un grupo de compuestos que se caracterizan porque poseen un grupo funcional llamado grupo carboxilo o grupo carboxi (–COOH); se produce cuando coinciden sobre el mismo carbono un grupo hidroxilo (-OH) y carbonilo (C=O). Se puede representar como COOH o CO2H... Ésteres: Los ésteres presentan el grupo éster (-O-CO-) en su estructura. Algunos ejemplos de sustancias con este grupo incluyen el ácido acetil salicílico, componente de la aspirina, o algunos compuestos aromáticos como el acetato de isoamilo, con característico olor a plátano. Los aceites también son ésteres de ácidos grasos con glicerol. Éteres: Los éteres presentan el grupo éter(-O-) en su estructura. Suelen tener bajo punto de ebullición y son fácilmente descomponibles. Por ambos motivos, los éteres de baja masa molecular suelen ser peligrosos ya que sus vapores pueden ser explosivos. Nitrogenados Aminas: Las aminas son compuestos orgánicos caracterizados por la presencia del grupo amina (-N<). Las aminas pueden ser primarias (R-NH2), secundarias (R-NH-R) o terciarias (R-NR´-R). Las aminas suelen dar compuestos ligeramente amarillentos y con olores que recuerdan a pescado u orina. Amidas: Las amidas son compuestos orgánicos caracterizados por la presencia del grupo amida (-NH-CO-) en su estructura. Las proteínas o polipéptidos son poliamidas naturales formadas por enlaces peptídicos entre distintos aminoácidos. Isocianatos: Los isocianatos tienen el grupo isocianato (-N=C=O). Este grupo es muy electrófilo, reaccionando fácilmente con el agua para descomponerse mediante la transposición de Hofmann dar una amina y anhídrico carbónico, con los hidroxilos para dar uretanos, y con las aminas primarias o secundarias para dar ureas. Cíclicos Son compuestos que contienen un ciclo saturado. Un ejemplo de estos son los norbornanos, que en realidad son compuestos bicíclicos, los terpenos, u hormonas como el estrógeno, progesterona, testosterona u otras biomoléculas como el colesterol. Aromáticos El furano (C4H4O) es un ejemplo de compuesto aromático. Estructura tridimensional del furano mostrando la nube electrónica de electrones π. Los compuestos aromáticos tienen estructuras cíclicas insaturadas. El benceno es el claro ejemplo de un compuesto aromático, entre cuyos derivados están el tolueno, el fenol o el ácido benzoico. En general se define un compuesto aromático aquel que tiene anillos que cumplen la regla de Hückel, es decir que tienen 4n+2 electrones en orbitales π (n=0,1,2,...). A los compuestos orgánicos que tienen otro grupo distinto al carbono en sus cilos (normalmente N, O u S) se denominan compuestos aromáticos heterocíclicos. Así los compuestos aromáticos se suelen dividir en: Derivados del benceno: Policíclicos (antraceno, naftaleno, fenantreno, etc.), fenoles, aminas aromáticas, fulerenos, etc. Compuestos heterocíclicos: Piridina, furano, tiofeno, pirrol, porfirina, etc. Isómeros Isómeros del C6H12. Ya que el carbono puede enlazarse de diferentes maneras, una cadena puede tener diferentes configuraciones de enlace dando lugar a los llamados isómeros, moléculas tienen la misma fórmula química, pero distintas estructuras y propiedades. Existen distintos tipos de isomería: isomería de cadena, isomería de función, tautomería, estereoisomería, y estereoisomería configuracional. El ejemplo mostrado a la izquierda es un caso de isometría de cadena en la que el compuesto con fórmula C6H12 puede ser un ciclo (ciclohexano) o un alqueno lineal, el 1-hexeno. Un ejemplo de isomería de función sería el caso del propanal y la acetona, ambos con fórmula C3H6O. Compuestos orgánicos Artículo principal: Compuesto orgánico Los compuestos orgánicos pueden dividirse de manera muy general en: Compuestos alifáticos Compuestos aromáticos Compuestos heterocíclicos Compuestos organometálicos Polímeros Relación con la biología Una de las principales relaciones entre la química orgánica y la biología es el estudio de la síntesis y estructura de moléculas orgánicas de importancia en los procesos moleculares realizados por los organismos vivos, es decir en el metabolismo. La bioquímica es el campo interdisciplinar científico que estudia los seres vivos, y ya que estos usan compuestos que contienen carbono, la química orgánica es imprescindible para comprender los procesos metabólicos. En términos biológicos la química orgánica es de gran importancia sobre todo en un contexto celular y esto lo podemos ejemplificar con moléculas como los carbohidratos, presentes desde la membrana plasmática así como en la estructura química del ADN, los lípidos quienes son la base principal de la membrana plasmática, las proteínas que ayudan a dar sostén a un organismo o sus funciones como enzimas y el ADN, molécula encargada de resguardar la información genética de los organismos vivos."
ksampletext_wikipedia_chem_molecula: str = "Molécula. En química, una molécula (del nuevo latín molecula, que es un diminutivo de la palabra moles, masa) es un grupo eléctricamente neutro y suficientemente estable de al menos dos átomos en una configuración definida, unidos por enlaces químicos fuertes covalentes. En este estricto sentido, las moléculas se diferencian de los iones poliatómicos. En la química orgánica y la bioquímica, el término molécula se utiliza de manera menos estricta y se aplica también a los compuestos orgánicos (moléculas orgánicas) y en las biomoléculas. Antes, se definía la molécula de forma menos general y precisa, como la más pequeña parte de una sustancia que podía tener existencia independiente y estable conservando aún sus propiedades fisicoquímicas. De acuerdo con esta definición, podían existir moléculas monoatómicas. En la teoría cinética de los gases, el término molécula se aplica a cualquier partícula gaseosa con independencia de su composición. De acuerdo con esta definición, los átomos de un gas noble se considerarían moléculas aunque se componen de átomos no enlazados. Una molécula puede consistir en varios átomos de un único elemento químico, como en el caso del oxígeno diatómico (O2), o de diferentes elementos, como en el caso del agua (H2O). Los átomos y complejos unidos por enlaces no covalentes como los enlaces de hidrógeno o los enlaces iónicos no se suelen considerar como moléculas individuales. Las moléculas como componentes de la materia son comunes en las sustancias orgánicas (y por tanto en la bioquímica). También conforman la mayor parte de los océanos y de la atmósfera. Sin embargo, un gran número de sustancias sólidas familiares, que incluyen la mayor parte de los minerales que componen la corteza, el manto y el núcleo de la Tierra, contienen muchos enlaces químicos, pero no están formados por moléculas. Además, ninguna molécula típica puede ser definida en los cristales iónicos (sales) o en cristales covalentes, aunque estén compuestos por celdas unitarias que se repiten, ya sea en un plano (como en el grafito) o en tres dimensiones (como en el diamante o el cloruro de sodio). Este sistema de repetir una estructura unitaria varias veces también es válida para la mayoría de las fases condensadas de la materia con enlaces metálicos, lo que significa que los metales sólidos tampoco están compuestos por moléculas. En el vidrio (sólidos que presentan un estado vítreo desordenado), los átomos también pueden estar unidos por enlaces químicos sin que se pueda identificar ningún tipo de molécula, pero tampoco existe la regularidad de la repetición de unidades que caracteriza a los cristales. Casi toda la química orgánica y buena parte de la química inorgánica se ocupan de la síntesis y reactividad de moléculas y compuestos moleculares. La química física y, especialmente, la química cuántica también estudian, cuantitativamente, en su caso, las propiedades y reactividad de las moléculas. La bioquímica está íntimamente relacionada con la biología molecular, ya que ambas estudian a los seres vivos a nivel molecular. El estudio de las interacciones específicas entre moléculas, incluyendo el reconocimiento molecular es el campo de estudio de la química supramolecular. Estas fuerzas explican las propiedades físicas como la solubilidad o el punto de ebullición de un compuesto molecular. Las moléculas rara vez se encuentran sin interacción entre ellas, salvo en gases enrarecidos y en los gases nobles. Así, pueden encontrarse en redes cristalinas, como el caso de las moléculas de H2O en el hielo o con interacciones intensas, pero que cambian rápidamente de direccionalidad, como en el agua líquida. En orden creciente de intensidad, las fuerzas intermoleculares más relevantes son: las fuerzas de Van der Waals y los puentes de hidrógeno. La dinámica molecular es un método de simulación por computadora que utiliza estas fuerzas para tratar de explicar las propiedades de las moléculas. No se puede definir una molécula típica para sales ni para cristales covalentes, aunque estos a menudo se componen de células unitarias repetidas que se extienden en un plano, por ejemplo, el grafeno ; o tridimensionalmente, por ejemplo, el diamante, el cuarzo, o el cloruro de sodio. El tema de la estructura celular unitaria repetida también se aplica a la mayoría de los metales que son fases condensadas con enlaces metálicos. Por tanto, los metales sólidos no están hechos de moléculas. En los vidrios, que son sólidos que existen en un estado vítreo desordenado, los átomos se mantienen unidos por enlaces químicos sin presencia de ninguna molécula definible, ni ninguna de la regularidad de la estructura celular unitaria repetida que caracteriza a las sales, cristales covalentes y rieles. Ciencia molecular La ciencia de las moléculas se denomina química molecular o física molecular, dependiendo de si se centra en la química o en la física. La química molecular se ocupa de las leyes que rigen la interacción entre las moléculas que da lugar a la formación y ruptura de enlaces químicos, mientras que la física molecular se ocupa de las leyes que rigen su estructura y propiedades. En la práctica, sin embargo, esta distinción es imprecisa. En las ciencias moleculares, una molécula consiste en un sistema estable (estado ligado) compuesto por dos o más átomos. Los iones poliatómicos pueden considerarse a veces como moléculas cargadas eléctricamente. El término molécula inestable se utiliza para especies muy reactivas, es decir, conjuntos de corta duración (resonancias) de electrones y núcleos, como radicales, iones moleculares, moléculas de Rydberg, estados de transición, complejos de van der Waals, o sistemas de átomos en colisión como en el condensado de Bose-Einstein. Historia y etimología Artículo principal: Historia de la teoría molecular Según la Real Academia Española el vocablo «molécula» deriva del latín moles mole o masa y el sufijo diminutivo -ula masa pequeña. Molécula (1794) - «partícula extremadamente diminuta», del francés molécule (1678), del Nuevo Latín molecula, diminutivo del latín moles masa, barrera. Un significado vago al principio; la moda de la palabra (utilizada hasta finales del siglo XVIII solo en forma latina) se remonta a la filosofía de Descartes. La definición de molécula ha ido evolucionando a medida que ha aumentado el conocimiento de la estructura de las moléculas. Las definiciones anteriores eran menos precisas, y definían las moléculas como las partículas más pequeñas de sustancia químicas puras que aún conservan su composición y sus propiedades químicas. Esta definición a menudo se rompe ya que muchas sustancias en la experiencia ordinaria, como rocas, sales, y metales, se componen de grandes redes cristalinas de átomos de enlace químico o iones, pero no están hechas de moléculas discretas. Definición y sus límites De manera menos general y precisa, se ha definido molécula como la parte más pequeña de una sustancia química que conserva sus propiedades químicas, y a partir de la cual se puede reconstituir la sustancia sin reacciones químicas. De acuerdo con esta definición, que resulta razonablemente útil para aquellas sustancias puras constituidas por moléculas, podrían existir las moléculas monoatómicas de gases nobles, mientras que las redes cristalinas, sales, metales y la mayoría de vidrios quedarían en una situación confusa. Las moléculas lábiles pueden perder su consistencia en tiempos relativamente cortos, pero si el tiempo de vida medio es del orden de unas pocas vibraciones moleculares, estamos ante un estado de transición que no se puede considerar molécula. Actualmente, es posible el uso de láser pulsado para el estudio de la química de estos sistemas. Las entidades que comparten la definición de las moléculas, pero tienen carga eléctrica se denominan iones poliatómicos, iones moleculares o moléculas ion. Las sales compuestas por iones poliatómicos se clasifican habitualmente dentro de los materiales de base molecular o materiales moleculares. Ejemplo de molécula poliatómica: el agua Las moléculas están formadas por partículas. Una molécula viene a ser la porción de materia más pequeña que aún conserva las propiedades de la materia original. Las moléculas se encuentran fuertemente enlazadas con la finalidad de formar materia. Las moléculas están formadas por átomos unidos por medio de enlaces químicos. Una molécula es una unidad de sustancia que puede ser monoatómica o poliatómica. La unidad de todas las sustancias gaseosas es la molécula. Tipos de moléculas Las moléculas se pueden clasificar en: Moléculas discretas: constituidas por un número bien definido de átomos, sean estos del mismo elemento (moléculas homonucleares, como el dinitrógeno o el fullereno) o de elementos distintos (moléculas heteronucleares, como el agua). Molécula de dinitrógeno, el gas que es el componente mayoritario del aire Molécula de dinitrógeno, el gas que es el componente mayoritario del aire Molécula de fullereno, tercera forma estable del carbono tras el diamante y el grafito Molécula de fullereno, tercera forma estable del carbono tras el diamante y el grafito Molécula de agua, «disolvente universal», de importancia fundamental en innumerables procesos bioquímicos e industriales Molécula de agua, «disolvente universal», de importancia fundamental en innumerables procesos bioquímicos e industriales Representación poliédrica del anión de Keggin, un polianión molecular Representación poliédrica del anión de Keggin, un polianión molecular Macromoléculas o polímeros: constituidas por la repetición de una unidad comparativamente simple ,o un conjunto limitado de dichas unidades, y que alcanzan pesos moleculares relativamente altos. Representación de un fragmento de ADN, un polímero de importancia fundamental en la genética Representación de un fragmento de ADN, un polímero de importancia fundamental en la genética Enlace peptídico que une los péptidos para formar proteínas Enlace peptídico que une los péptidos para formar proteínas Representación de un fragmento lineal de polietileno, el plástico más usado Representación de un fragmento lineal de polietileno, el plástico más usado Primera generación de un dendrímero, un tipo especial de polímero que crece de forma fractal Primera generación de un dendrímero, un tipo especial de polímero que crece de forma fractal Enlaces Los átomos que forman las moléculas se mantienen juntos mediante enlaces covalentes o enlaces iónicos. Varios tipos de elementos no metálicos existen solo como moléculas en el medio ambiente. Por ejemplo, el hidrógeno solo existe como molécula de hidrógeno. Una molécula de un compuesto está formada por dos o más elementos. Una molécula homonuclear está formada por dos o más átomos de un solo elemento. Mientras que algunas personas dicen que un cristal metálico puede considerarse una sola molécula gigante unida por enlaces metálicos, otros señalan que los metales actúan de manera muy diferente a las moléculas. Covalente Artículo principal: Enlace covalente Un enlace covalente que forma H2 (derecha) donde dos átomos de hidrógeno comparten los dos electrones. Un enlace covalente es un enlace químico que implica el intercambio de pares de electrones entre átomos. Estos pares de electrones se denominan pares compartidos o pares de enlace, y el equilibrio estable de fuerzas atractivas y repulsivas entre átomos, cuando comparten electrones, se denomina enlace covalente. Iónico Artículo principal: Enlace iónico El sodio y el flúor experimentan una reacción redox para formar fluoruro de sodio. El sodio pierde su electrón externo para adoptar una configuración electrónica estable, y este electrón entra en el átomo de flúor en forma exotérmica. El enlace iónico es un tipo de enlace químico que implica la atracción electrostática entre iones con carga eléctrica opuesta y es la interacción principal que se produce en los compuestos iónicos. Los iones son átomos que han perdido uno o más electrones (denominados cationes) y átomos que han ganado uno o más electrones (denominados aniones). Esta transferencia de electrones se denomina electrovalencia en contraste con la covalencia. En el caso más simple, el catión es un átomo de metal y el anión es un átomo no metálico, pero estos iones pueden ser de naturaleza más complicada, por ejemplo, iones moleculares como NH4+ o SO4 2−. A temperaturas y presiones normales, la unión iónica crea principalmente sólidos (u ocasionalmente líquidos) sin moléculas identificables separadas, pero la vaporización/sublimación de tales materiales produce pequeñas moléculas separadas donde los electrones aún se transfieren lo suficiente como para que los enlaces se consideren iónicos en lugar de covalentes. Descripción La estructura molecular puede ser descrita de diferentes formas. La fórmula molecular es útil para moléculas sencillas, como H2O para el agua o NH3 para el amoniaco. Contiene los símbolos de los elementos presentes en la molécula, así como su proporción indicada por los subíndices. Para moléculas más complejas, como las que se encuentran comúnmente en química orgánica, la fórmula química no es suficiente, y vale la pena usar una fórmula estructural o una fórmula esqueletal, las que indican gráficamente la disposición espacial de los distintos grupos funcionales. Cuando se quieren mostrar variadas propiedades moleculares, o se trata de sistemas muy complejos como proteínas, ADN o polímeros, se utilizan representaciones especiales, como los modelos tridimensionales (físicos o representados por ordenador). En proteínas, por ejemplo, cabe distinguir entre estructura primaria (orden de los aminoácidos), secundaria (primer plegamiento en hélices, hojas, giros…), terciaria (plegamiento de las estructuras tipo hélice/hoja/giro para dar glóbulos) y cuaternaria (organización espacial entre los diferentes glóbulos). Figura 1. Representaciones de la terpenoide, atisano, 3D (centro izquierda) y 2D (derecha). En el modelo 3D de la izquierda, los átomos de carbono están representados por esferas azules; las blancas representan a los átomos de hidrógeno y los cilindros representan los enlaces. El modelo es una representación de la superficies molecular, coloreada por áreas de carga eléctrica positiva (rojo) o negativa (azul). En el modelo 3D del centro, las esferas azul claro representan átomos de carbono, las blancas de hidrógeno y los cilindros entre los átomos son los enlaces simples. Moléculas en la teoría cuántica La mecánica clásica y el electromagnetismo clásico no podían explicar la existencia y estabilidad de las moléculas, ya que de acuerdo con sus ecuaciones una carga eléctrica acelerada emitiría radiación por lo que los electrones necesariamente perderían energía cinética por radiación hasta caer sobre el núcleo atómico. La mecánica cuántica proveyó el primer modelo cualitativamente correcto que además predecía la existencia de átomos estables y proporcionaba explicación cuantitativa muy aproximada para fenómenos empíricos como los espectros de emisión característicos de cada elemento químico. En mecánica cuántica una molécula o un ion poliatómico se describe como un sistema formado por (1) definido sobre el espacio de funciones antisimetrizadas de cuadrado integrable (2) donde el primer término representa la interacción de los electrones entre sí, el segundo la interacción de los electrones con los núcleos atómicos, y el tercero las interacciones de los núcleos entre sí. En una molécula neutra se tendrá obviamente que: Si Aproximación de Born-Oppenheimer Resolver el problema de autovalores y autofunciones para el hamiltoniano cuántico dado por (1) es un problema matemático difícil, por lo que es común simplificarlo de alguna manera. Así dado que los núcleos atómicos son mucho más pesados que los electrones (entre 103 y 105 veces más) puede suponerse que los núcleos atómicos apenas se mueven comparados con los electrones, por lo que se considera que están congelados en posiciones fijas, con lo cual se puede aproximar el hamiltoniano (1) por la aproximación de Born-Oppenheimer dada por: (3) definido sobre el espacio de funciones Teorema de Kato Los operadores Tosio Kato La propiedad de ser autoadjunto implicará que las energías son cantidades reales, y el que sean acotados inferiormente implicará que existe un estado fundamental de mínima energía por debajo del cual los electrones no pueden decaer, y por tanto, las moléculas serán estables, ya que los electrones no pueden perder y perder energía como parecían predecir las ecuaciones del electromagnetismo clásico. Dos resultados matemáticos adicionales nos dicen como son las energías permitidas de los electrones dentro de una molécula: Teorema HVZ para átomos y moléculas BO El espectro esencial inf W. Hunziker, C. Van Winter y G. M. Zhislin Además dentro de la mecánica cuántica puede demostrarse que pueden existir iones positivos (cationes, con carga positiva comparable al núcleo atómico), mientras que no es igual de fácil tener iones negativos (aniones), el siguiente resultado matemático implica tiene que ver con la posibilidad de cationes y aniones."
ksampletext_wikipedia_chem_compuestoquimico: str = "Compuesto químico. Un compuesto químico es una sustancia formada por la combinación química de dos o más elementos de la tabla periódica. Los compuestos son representados por una fórmula química. Por ejemplo, el agua (H2O) está constituida por dos átomos de hidrógeno y uno de oxígeno. Los elementos de un compuesto no se pueden dividir ni separar por procesos físicos (decantación, filtración, destilación), sino solo mediante procesos químicos. Los compuestos están formados por moléculas o iones con enlaces estables que no obedece a una selección humana arbitraria. Por lo tanto, no son mezclas o aleaciones como el bronce o el chocolate. Un elemento químico unido a un elemento químico idéntico no es un compuesto químico, ya que solo está involucrado un elemento, no dos elementos diferentes. Hay cuatro tipos de compuestos, dependiendo de cómo se mantienen unidos los átomos constituyentes: Moléculas unidas por enlaces covalentes. Compuestos iónicos unidos por enlaces iónicos. Compuestos intermetálicos unidos por enlaces metálicos. Ciertos complejos que se mantienen unidos por enlaces covalentes coordinados. Muchos compuestos químicos tienen un identificador numérico único asignado por el Chemical Abstracts Service (CAS): su número CAS. Fórmula Artículo principal: Fórmula molecular En química inorgánica los compuestos se representan mediante fórmulas químicas. Una fórmula química es una forma de expresar información sobre las proporciones de los átomos que constituyen un compuesto químico en particular, utilizando las abreviaturas normalizadas de los elementos químicos y subíndices para indicar el número de átomos involucrados. Por ejemplo, el agua se compone de dos átomos de hidrógeno unidos a uno de oxígeno átomo: la fórmula química es H2O. En el caso de compuestos no estequiométricos, las proporciones pueden ser reproducibles con respecto a su preparación y dar proporciones fijas de sus elementos componentes, pero proporciones que no son integrales [por ejemplo, para el hidruro de paladio, PdH x (0.02 <x <0.58 )]. El orden de los elementos en la fórmula de los compuestos inorgánicos comienza por la izquierda con el elemento menos electronegativo, hasta la derecha con el más electronegativo. Por ejemplo en el NaCl, el cloro que es más electronegativo que el sodio va en la parte derecha. Para los compuestos orgánicos existen otras varias reglas y se utilizan fórmulas esqueletales o semidesarrolladas para su representación. Definiciones Cualquier sustancia que consista en dos o más tipos diferentes de átomos (elementos químicos) en una proporción estequiométrica fija puede denominarse compuesto químico. El concepto se entiende mejor cuando se consideran sustancias químicas puras. De la composición de proporciones fijas de dos o más tipos de átomos se desprende que los compuestos químicos se pueden convertir, mediante una reacción química, en compuestos o sustancias, cada uno con menos átomos. Los compuestos químicos tienen una estructura química única y definida que se mantiene unida en una disposición espacial concebida por enlaces químicos. Los compuestos químicos pueden ser compuestos moleculares, mantenidos juntos por enlaces covalentes, sales mantenidas entre sí por enlaces iónicos, compuestos intermetálicos mantenidos juntos por enlaces metálicos, o el subconjunto de complejos químicos que se mantienen unidos por enlaces covalentes coordinados . Los elementos químicos puros generalmente no se consideran compuestos químicos, ya que no cumplen con el requisito de dos o más átomos, aunque a menudo consisten en moléculas compuestas de múltiples átomos (como en la molécula diatómica H2, o la molécula poliatómica S8, etc.) Muchos compuestos químicos tienen un identificador numérico único asignado por el Chemical Abstracts Service (CAS): su número CAS. Hay nomenclatura variable y a veces inconsistente para diferenciar sustancias, que incluyen ejemplos verdaderamente no estequiométricos de los compuestos químicos, que requieren que las proporciones sean fijas. Muchas sustancias químicas sólidas, por ejemplo muchos minerales de silicato, no tienen fórmulas simples que reflejen el enlace químico de los elementos entre sí en proporciones fijas; aun así, estas sustancias cristalinas a menudo se denominan compuestos no estequiométricos. Se puede argumentar que están relacionados con dichos productos, en lugar de ser compuestos químicos propiamente dichos, en la medida en que la variabilidad en sus composiciones a menudo se debe a la presencia de elementos extraños atrapados dentro de la estructura cristalina de un compuesto químico verdadero, o debido a perturbaciones en su estructura en relación con el compuesto conocido que surge debido a un exceso o déficit de los elementos constituyentes en lugares de su estructura; tales sustancias no estequiométricas forman la mayor parte de la corteza y el manto de la Tierra. Otros compuestos considerados químicamente idénticos pueden tener cantidades variables de isótopos pesados o ligeros de los elementos constituyentes, lo que cambia ligeramente la proporción en masa de los elementos. Clasificación Se pueden clasificar de acuerdo al tipo de enlace químico o a su composición. Atendiendo al tipo de enlace químico, se pueden dividir en: Moléculas Compuestos iónicos Compuestos intermetálicos Complejos Por su composición, se pueden dividir en dos grandes grupos: Compuestos inorgánicos: Óxidos básicos. También llamados óxidos metálicos, que están formados por un metal y oxígeno. Ejemplos: el óxido plúmbico, óxido de litio. Óxidos ácidos. También llamados óxidos no metálicos, formados por un no metal y oxígeno. Ejemplos: óxido hipocloroso, óxido selenioso. Hidruros, que pueden ser tanto metálicos como no metálicos. Están compuestos por un elemento e hidrógeno. Ejemplos: hidruro de aluminio, hidruro de sodio. Hidrácidos, son hidruros no metálicos que, cuando se disuelven en agua, adquieren carácter ácido. Por ejemplo, el ácido yodhídrico. Hidróxidos, compuestos formados por la reacción entre un óxido básico y el agua, que se caracterizan por presentar el grupo hidroxilo (OH). Por ejemplo, el hidróxido de sodio, o sosa cáustica. Oxácidos, compuestos obtenidos por la reacción de un óxido ácido y agua. Sus moléculas están formadas por hidrógeno, un no metal y oxígeno. Por ejemplo, ácido clórico. Sales binarias, compuestos formados por un hidrácido más un hidróxido. Por ejemplo, el cloruro de sodio. Oxisales, formadas por la reacción de un oxácido y un hidróxido, como por ejemplo el hipoclorito de sodio. Compuestos orgánicos: Compuestos alifáticos, son compuestos orgánicos constituidos por carbono e hidrógeno cuyo carácter no es aromático. Compuestos aromáticos, es un compuesto orgánico cíclico conjugado que posee una mayor estabilidad debido a la deslocalización electrónica en enlaces π. Compuestos heterocíclicos, son compuestos orgánicos cíclicos en los que al menos uno de los componentes del ciclo es de un elemento diferente al carbono. Compuestos organometálicos, es un compuesto en el que los átomos de carbono forman enlaces covalentes, es decir, comparten electrones, con un átomo metálico. Polímeros, son macromoléculas formadas por la unión de moléculas más pequeñas llamadas monómeros. Moléculas Artículo principal: Molécula Una molécula es un grupo eléctricamente neutro de dos o más átomos unidos por enlaces químicos. Una molécula puede ser homonuclear, es decir, estar formada por átomos de un mismo elemento químico, como ocurre con dos átomos en la molécula de oxígeno (O2); o puede ser heteronuclear, es decir, un compuesto químico compuesto por más de un elemento, como el agua (dos átomos de hidrógeno y un átomo de oxígeno; H2O). Los átomos y complejos unidos por enlaces no covalentes como los enlaces de hidrógeno no se suelen considerar como moléculas individuales. Compuestos iónicos Artículo principal: Compuesto iónico Un compuesto iónico es un compuesto químico compuesto de anion que se mantienen unidos por fuerzas electrostáticas denominadas enlace iónico. El compuesto es neutro en general, pero consta de iones cargados positivamente llamados cationes y iones cargados negativamente llamados aniones. Estos pueden ser iones simples como el sodio(Na+) y el cloruro (Cl−) en el cloruro de sodio, o especies poliatómicas como el amonio (NH+ 4) y carbonato (CO2− 3) en el carbonato de amonio. Los iones individuales dentro de un compuesto iónico generalmente tienen múltiples vecinos más cercanos, por lo que no se consideran parte de moléculas, sino parte de una red tridimensional continua, generalmente en una estructura cristalina. Los compuestos iónicos que contienen iones básicos hidróxido (OH−) u óxido(O2−) se clasifican como bases. Los compuestos iónicos sin estos iones también se conocen como sales y pueden formarse mediante reacciones ácido-base. Los compuestos iónicos también se pueden producir a partir de sus iones constituyentes por evaporación de su disolvente, precipitación, congelación, una reacción en estado sólido o la reacción de transferencia de electrones de metales reactivos con no metales reactivos, como los gases halógenos. Los compuestos iónicos suelen tener altos puntos de fusión y ebullición, y son duros y quebradizos. Como sólidos, casi siempre son eléctricamente aislantes, pero cuando se funden o disuelven se vuelven altamente conductores, porque se movilizan los iones. Compuestos intermetálicos Un compuesto intermetálico es un tipo de aleación metálica que forma un compuesto de estado sólido ordenado entre dos o más elementos metálicos. Los intermetálicos son generalmente duros y quebradizos, con buenas propiedades mecánicas a altas temperaturas. Se pueden clasificar como compuestos intermetálicos estequiométricos o no estequiométricos. Complejos químicos Artículo principal: Complejo (química) Un complejo de coordinación consiste en un átomo o ion central, que generalmente es metálico y se llama centro de coordinación, y una matriz circundante de moléculas o iones unidos, que a su vez se conocen como ligandos o agentes complejantes. Muchos compuestos que contienen metales, especialmente los de metales de transición, son complejos de coordinación. Un complejo de coordinación cuyo centro es un átomo metálico se denomina complejo metálico o elemento de bloque d. Enlaces y fuerzas Los compuestos se mantienen unidos por medio de diferentes tipos de enlaces y fuerzas. Las diferencias entre los tipos de enlaces de los compuestos dependen del tipo de elemento presente en el compuesto. Las fuerzas de dispersión de London son las fuerzas más débiles entre las fuerzas intermoleculares. Son fuerzas de atracción temporales que se forman cuando los electrones en dos átomos adyacentes se colocan de manera que crean un dipolo temporal. Además, estas fuerzas son responsables de la condensación de sustancias no polares en líquidos y posterior congelación a un estado sólido dependiendo de la temperatura del ambiente. Un enlace covalente, también conocido como enlace molecular, implica el intercambio de electrones entre dos átomos. Principalmente, este tipo de enlace se produce entre elementos que aparecen uno cerca del otro en la tabla periódica de elementos, aunque se observa entre algunos metales y no metales. Esto se debe al mecanismo de este tipo de enlace. Los elementos cercanos en la tabla periódica tienden a tener electronegatividades similares, lo que significa que tienen una afinidad similar por los electrones. Como ninguno de los elementos tiene una afinidad más fuerte para donar o ganar electrones, hace que los elementos compartan electrones de manera que ambos elementos tengan un octeto más estable. El enlace iónico se produce cuando los electrones de valencia se transfieren completamente entre los elementos. Al contrario que el covalente, este enlace químico crea dos iones de carga opuesta. Los metales en enlaces iónicos generalmente pierden sus electrones de valencia, convirtiéndose en cationes, cargados positivamente. El no metal ganará los electrones del metal, haciendo que el no metal sea un anión, es decir, cargado negativamente. Es decir, los enlaces iónicos se producen entre un donador de electrones, generalmente un metal, y un aceptor de electrones, que tiende a ser un no metal. El enlace de hidrógeno se produce cuando un átomo de hidrógeno unido a un átomo electronegativo forma una conexión electrostática con otro átomo electronegativo a través de dipolos o cargas que interactúan. Reacciones Un compuesto se puede convertir en una composición química diferente (productos) mediante la interacción con un segundo compuesto químico (reactivos) a través de una reacción química. En este proceso, los enlaces entre los átomos se rompen en ambos compuestos que interactúan, y luego los enlaces se reforman para obtener nuevas asociaciones entre los mismos átomos. Esquemáticamente, esta reacción podría describirse como AB + CD → AD + CB, donde A, B, C y D son cada uno átomos únicos; y AB, AD, CD y CB son cada uno compuestos ùnicos."

ksampletext_wikipedia_biol_celula: str = "Célula. La célula (del latín cellula, diminutivo de cella, celda) es la unidad morfológica y funcional de todo ser vivo. De hecho, la célula es el elemento de menor tamaño que puede considerarse vivo. De este modo, puede clasificarse a los organismos vivos según el número de células que posean: si solo tienen una, se les denomina unicelulares (como pueden ser los protozoos o las bacterias, organismos microscópicos); si poseen más, se les llama pluricelulares. En estos últimos el número de células es variable: de unos pocos cientos, como en algunos nematodos, a cientos de billones (1014), como en el caso del ser humano. Las células suelen poseer un tamaño de 10 µm y una masa de 1 ng, si bien existen células mucho mayores. Célula animal La teoría celular, propuesta en 1838 para los vegetales y en 1839 para los animales, por Matthias Jakob Schleiden y Theodor Schwann, postula que todos los organismos están compuestos por células, y que todas las células derivan de otras precedentes. De este modo, todas las funciones vitales emanan de la maquinaria celular y de la interacción entre células adyacentes; además, la tenencia de la información genética, base de la herencia, en su ADN permite la transmisión de aquella de generación en generación. La aparición del primer organismo vivo sobre la Tierra suele asociarse al nacimiento de la primera célula. Si bien existen muchas hipótesis que especulan cómo ocurrió, usualmente se describe que el proceso se inició gracias a la transformación de moléculas inorgánicas en orgánicas bajo unas condiciones ambientales adecuadas; tras esto, dichas biomoléculas se asociaron dando lugar a entes complejos capaces de autorreplicarse. Existen posibles evidencias fósiles de estructuras celulares en rocas datadas en torno a 4 o 3,5 miles de millones de años (gigaaños o Ga).[nota 1] Se han encontrado evidencias muy fuertes de formas de vida unicelulares fosilizadas en microestructuras en rocas de la formación Strelley Pool, en Australia Occidental, con una antigüedad de 3,4 Ga.[cita requerida] Se trataría de los fósiles de células más antiguos encontrados hasta la fecha. Evidencias adicionales muestran que su metabolismo sería anaerobio y basado en el sulfuro. Tipos celulares Existen dos grandes tipos celulares: Célula procariota, propia de los procariontes, que comprende las células de arqueas y bacterias. Célula eucariota, propia de los eucariontes, tales como la célula animal, célula vegetal, y las células de hongos y protistas. Historia y teoría celular La historia de la biología celular ha estado ligada al desarrollo tecnológico que pudiera sustentar su estudio. De este modo, el primer acercamiento a su morfología se inicia con la popularización del microscopio rudimentario de lentes compuestas en el siglo XVII, se suplementa con diversas técnicas histológicas para microscopía óptica en los siglos XIX y XX y alcanza un mayor nivel resolutivo mediante los estudios de microscopía electrónica, de fluorescencia y confocal, entre otros, ya en el siglo XX. El desarrollo de herramientas moleculares, basadas en el manejo de ácidos nucleicos y enzimas permitieron un análisis más exhaustivo a lo largo del siglo XX. Descubrimiento Robert Hooke, quien acuñó el término «célula». Las primeras aproximaciones al estudio de la célula surgieron en el siglo XVII; tras el desarrollo a finales del siglo XVI de los primeros microscopios. Estos permitieron realizar numerosas observaciones, que condujeron en apenas doscientos años a un conocimiento morfológico relativamente aceptable. A continuación se enumera una breve cronología de tales descubrimientos: 1665: Robert Hooke publicó los resultados de sus observaciones sobre tejidos vegetales, como el corcho, realizadas con un microscopio de 50 aumentos construido por él mismo. Este investigador fue el primero que, al ver en esos tejidos unidades que se repetían a modo de celdillas de un panal, las bautizó como elementos de repetición, «células» (del latín cellulae, celdillas). Pero Hooke solo pudo observar células muertas por lo que no pudo describir las estructuras de su interior. Década de 1670: Anton van Leeuwenhoek observó diversas células eucariotas (como protozoos y espermatozoides) y procariotas (bacterias). 1745: John Needham describió la presencia de «animálculos» o «infusorios»; se trataba de organismos unicelulares. Dibujo de la estructura del corcho observado por Robert Hooke bajo su microscopio y tal como aparece publicado en Micrographia. Década de 1830: Theodor Schwann estudió la célula animal; junto con Matthias Schleiden postularon que las células son las unidades elementales en la formación de las plantas y animales, y que son la base fundamental del proceso vital. 1831: Robert Brown describió el núcleo celular. 1839: Purkinje observó el citoplasma celular. 1857: Kölliker identificó las mitocondrias. 1858: Rudolf Virchow postuló que todas las células provienen de otras células. 1860: Pasteur realizó multitud de estudios sobre el metabolismo de levaduras y sobre la asepsia. 1880: August Weismann descubrió que las células actuales comparten similitud estructural y molecular con células de tiempos remotos. 1931: Ernst Ruska construyó el primer microscopio electrónico de transmisión en la Universidad de Berlín. Cuatro años más tarde, obtuvo una resolución óptica doble a la del microscopio óptico. 1981: Lynn Margulis publica su hipótesis sobre la endosimbiosis serial, que explica el origen de la célula eucariota. Teoría celular Artículo principal: Teoría celular El concepto de célula como unidad anatómica y funcional de los organismos surgió entre los años 1830 y 1880, aunque fue en el siglo XVII cuando Robert Hooke describió por vez primera la existencia de las mismas, al observar en una preparación vegetal la presencia de una estructura organizada que derivaba de la arquitectura de las paredes celulares vegetales. En 1830 se disponía ya de microscopios con una óptica más avanzada, lo que permitió a investigadores como Theodor Schwann y Matthias Schleiden definir los postulados de la teoría celular, la cual afirma, entre otras cosas: Que la célula es una unidad morfológica de todo ser vivo: es decir, que en los seres vivos todo está formado por células o por sus productos de secreción. Este primer postulado sería completado por Rudolf Virchow con la afirmación Omnis cellula ex cellula, la cual indica que toda célula deriva de una célula precedente (biogénesis). En otras palabras, este postulado constituye la refutación de la teoría de generación espontánea o ex novo, que hipotetizaba la posibilidad de que se generara vida a partir de elementos inanimados. Un tercer postulado de la teoría celular indica que las funciones vitales de los organismos ocurren dentro de las células, o en su entorno inmediato, y son controladas por sustancias que ellas secretan. Cada célula es un sistema abierto, que intercambia materia y energía con su medio. En una célula ocurren todas las funciones vitales, de manera que basta una sola de ellas para que haya un ser vivo (que será un individuo unicelular). Así pues, la célula es la unidad fisiológica de la vida. El cuarto postulado expresa que cada célula contiene toda la información hereditaria necesaria para el control de su propio ciclo y del desarrollo y el funcionamiento de un organismo de su especie, así como para la transmisión de esa información a la siguiente generación celular. Definición Se define a la célula como la unidad morfológica y funcional de todo ser vivo. De hecho, la célula es el elemento de menor tamaño que puede considerarse vivo. Como tal posee una membrana de fosfolípidos con permeabilidad selectiva que mantiene un medio interno altamente ordenado y diferenciado del medio externo en cuanto a su composición, sujeta a control homeostático, la cual consiste en biomoléculas y algunos metales y electrolitos. La estructura se automantiene activamente mediante el metabolismo, asegurándose la coordinación de todos los elementos celulares y su perpetuación por replicación a través de un genoma codificado por ácidos nucleicos. La parte de la biología que se ocupa de ella es la citología. Características Las células, como sistemas termodinámicos complejos, poseen una serie de elementos estructurales y funcionales comunes que posibilitan su supervivencia; no obstante, los distintos tipos celulares presentan modificaciones de estas características comunes que permiten su especialización funcional y, por ello, la ganancia de complejidad. De este modo, las células permanecen altamente organizadas a costa de incrementar la entropía del entorno, uno de los requisitos de la vida. Características estructurales La existencia de polímeros como la celulosa en la pared vegetal permite sustentar la estructura celular empleando un armazón externo. Individualidad: Todas las células están rodeadas de una envoltura (que puede ser una bicapa lipídica desnuda, en células animales; una pared de polisacárido, en hongos y vegetales; una membrana externa y otros elementos que definen una pared compleja, en bacterias Gram negativas; una pared de peptidoglicano, en bacterias Gram positivas; o una pared de variada composición, en arqueas) que las separa y comunica con el exterior, que controla los movimientos celulares y que mantiene el potencial de membrana. Contienen un medio interno acuoso, el citosol, que forma la mayor parte del volumen celular y en el que están inmersos los orgánulos celulares. Poseen material genético en forma de ADN, el material hereditario de los genes, que contiene las instrucciones para el funcionamiento celular, así como ARN, a fin de que el primero se exprese. Tienen enzimas y otras proteínas, que sustentan, junto con otras biomoléculas, un metabolismo activo. Características funcionales Estructura tridimensional de una enzima, un tipo de proteínas implicadas en el metabolismo celular. Las células vivas son un sistema bioquímico complejo. Las características que permiten diferenciar las células de los sistemas químicos no vivos son: Nutrición. Las células toman sustancias del medio, las transforman de una forma a otra, liberan energía y eliminan productos de desecho, mediante el metabolismo. Crecimiento y multiplicación. Las células son capaces de dirigir su propia síntesis. A consecuencia de los procesos nutricionales, una célula crece y se divide, formando dos células, en una célula idéntica a la célula original, mediante la división celular. Diferenciación. Muchas células pueden sufrir cambios de forma o función en un proceso llamado diferenciación celular. Cuando una célula se diferencia, se forman algunas sustancias o estructuras que no estaban previamente formadas y otras que lo estaban dejan de formarse. La diferenciación es a menudo parte del ciclo celular en que las células forman estructuras especializadas relacionadas con la reproducción, la dispersión o la supervivencia. Señalización. Las células responden a estímulos químicos y físicos tanto del medio externo como de su interior y, en el caso de células móviles, hacia determinados estímulos ambientales o en dirección opuesta mediante un proceso que se denomina quimiotaxis. Además, frecuentemente las células pueden interaccionar o comunicar con otras células, generalmente por medio de señales o mensajeros químicos, como hormonas, neurotransmisores, factores de crecimiento... en seres pluricelulares en complicados procesos de comunicación celular y transducción de señales. Evolución. A diferencia de las estructuras inanimadas, los organismos unicelulares y pluricelulares evolucionan. Esto significa que hay cambios hereditarios (que ocurren a baja frecuencia en todas las células de modo regular) que pueden influir en la adaptación global de la célula o del organismo superior de modo positivo o negativo. El resultado de la evolución es la selección de aquellos organismos mejor adaptados a vivir en un medio particular. Las propiedades celulares no tienen por qué ser constantes a lo largo del desarrollo de un organismo: evidentemente, el patrón de expresión de los genes varía en respuesta a estímulos externos, además de factores endógenos. Un aspecto importante a controlar es la pluripotencialidad, característica de algunas células que les permite dirigir su desarrollo hacia un abanico de posibles tipos celulares. En metazoos, la genética subyacente a la determinación del destino de una célula consiste en la expresión de determinados factores de transcripción específicos del linaje celular al cual va a pertenecer, así como a modificaciones epigenéticas. Además, la introducción de otro tipo de factores de transcripción mediante ingeniería genética en células somáticas basta para inducir la mencionada pluripotencialidad, luego este es uno de sus fundamentos moleculares. Tamaño, forma y función Comparativa de tamaño entre neutrófilos, células sanguíneas eucariotas (de mayor tamaño), y bacterias Bacillus anthracis, procariotas (de menor tamaño, con forma de bastón). El tamaño y la forma de las células depende de sus elementos más periféricos (por ejemplo, la pared, si la hubiere) y de su andamiaje interno (es decir, el citoesqueleto). Además, la competencia por el espacio tisular provoca una morfología característica: por ejemplo, las células vegetales, poliédricas in vivo, tienden a ser esféricas in vitro. Incluso pueden existir parámetros químicos sencillos, como los gradientes de concentración de una sal, que determinen la aparición de una forma compleja. En cuanto al tamaño, la mayoría de las células son microscópicas, es decir, no son observables a simple vista. (un milímetro cúbico de sangre puede contener unos cinco millones de células), A pesar de ser muy pequeñas, el tamaño de las células es extremadamente variable. La célula más pequeña observada, en condiciones normales, corresponde a Mycoplasma genitalium, de 0,2 μm, encontrándose cerca del límite teórico de 0,17 μm. Existen bacterias con 1 y 2 μm de longitud. Las células humanas son muy variables: hematíes de 7 micras, hepatocitos con 20 micras, espermatozoides de 53 μm, óvulos de 150 μm e, incluso, algunas neuronas de en torno a un metro de longitud. En las células vegetales los granos de polen pueden llegar a medir de 200 a 300 μm. Respecto a las células de mayor tamaño; por ejemplo, los xenofióforos, son foraminíferos unicelulares que han desarrollado un gran tamaño, los cuales alcanzar tamaños macroscópicos (Syringammina fragilissima alcanza los 20 cm de diámetro). Para la viabilidad de la célula y su correcto funcionamiento siempre se debe tener en cuenta la relación superficie-volumen. Puede aumentar considerablemente el volumen de la célula y no así su superficie de intercambio de membrana, lo que dificultaría el nivel y regulación de los intercambios de sustancias vitales para la célula. Respecto de su forma, las células presentan una gran variabilidad, e, incluso, algunas no la poseen bien definida o permanente. Pueden ser: fusiformes (forma de huso), estrelladas, prismáticas, aplanadas, elípticas, globosas o redondeadas, etc. Algunas tienen una pared rígida y otras no, lo que les permite deformar la membrana y emitir prolongaciones citoplasmáticas (pseudópodos) para desplazarse o conseguir alimento. Hay células libres que no muestran esas estructuras de desplazamiento, pero poseen cilios o flagelos, que son estructuras derivadas de un orgánulo celular (el centrosoma) que dota a estas células de movimiento. De este modo, existen multitud de tipos celulares, relacionados con la función que desempeñan; por ejemplo: Células contráctiles que suelen ser alargadas, como los miocitos esqueléticos. Células con finas prolongaciones, como las neuronas que transmiten el impulso nervioso. Células con microvellosidades o con pliegues, como las del intestino para ampliar la superficie de contacto y de intercambio de sustancias. Células cúbicas, prismáticas o aplanadas como las epiteliales que recubren superficies como las losas de un pavimento. Estudio de las células Los biólogos utilizan diversos instrumentos para lograr el conocimiento de las células. Obtienen información de sus formas, tamaños y componentes, que les sirve para comprender además las funciones que en ellas se realizan. Desde las primeras observaciones de células, hace más de 300 años, hasta la época actual, las técnicas y los aparatos se han ido perfeccionando, originando una rama más de la biología: la microscopía. Dado el pequeño tamaño de la gran mayoría de las células, el uso del microscopio es de enorme valor en la investigación biológica. En la actualidad, los biólogos utilizan dos tipos básicos de microscopio: los ópticos y los electrónicos. Un microscopio óptico utiliza la luz visible para el estudio de muestras. Obteniedo imágenes aumentadas a partir de la desviación de la luz con lentes de cristal. Es utilizado para la observación de tejidos y células desde su invención en el siglo XVII. Los microscopios electrónicos son aquellos que utilizan electrones a alta velocidad para el análisis de muestras. Lo cual ofrece mayores capacidades de aumento que los de tipo óptico. Utilizándose en ramas como la medicina, y el estudio de materiales a nivel atómico. Además de usarse para la observación de células, virus y tejidos a nivel subcelular. Sin embargo, estos presentan limitaciones, debido a una cámara de vacío y a la preparación que requieren las muestras para ser analizadas, no pueden observarse células vivas. Escaneo de microscopio electrónico de barrido muestra el SARS-CoV-2 emergiendo de la superficie de las células cultivadas en el laboratorio. Imagen del virus SARS-CoV-2, tomada con un microscopio electrónico de barrido. La célula procariota Artículo principal: Célula procariota Las células procariotas son pequeñas y menos complejas que las eucariotas. Contienen ribosomas, pero carecen de sistemas de endomembranas (esto es, orgánulos delimitados por membranas biológicas, como puede ser el núcleo celular). Por ello poseen el material genético en el citosol. Sin embargo, existen excepciones: algunas bacterias fotosintéticas poseen sistemas de membranas internos. También en el filo Planctomycetota existen organismos como Pirellula que rodean su material genético mediante una membrana intracitoplasmática y Gemmata obscuriglobus que lo rodea con doble membrana. Esta última posee además otros compartimentos internos de membrana, posiblemente conectados con la membrana externa del nucleoide y con la membrana plasmática, que no está asociada a peptidoglucano. Estudios realizados en 2017, demuestran otra particularidad de Gemmata: presenta estructuras similares al poro nuclear, en la membrana que rodea su cuerpo nuclear. Por lo general podría decirse que los procariotas carecen de citoesqueleto. Sin embargo se ha observado que algunas bacterias, como Bacillus subtilis, poseen proteínas tales como MreB y mbl que actúan de un modo similar a la actina y son importantes en la morfología celular. Fusinita van den Ent, en Nature, va más allá, afirmando que los citoesqueletos de actina y tubulina tienen origen procariótico. De gran diversidad, los procariotas sustentan un metabolismo extraordinariamente complejo, en algunos casos exclusivo de ciertos taxa, como algunos grupos de bacterias, lo que incide en su versatilidad ecológica. Los procariotas se clasifican, según Carl Woese, en arqueas y bacterias. Arqueas Artículo principal: Arquea Estructura bioquímica de la membrana de arqueas (arriba) comparada con la de bacterias y eucariotas (en medio): nótese la presencia de enlaces éter (2) en sustitución de los tipo éster (6) en los fosfolípidos. Las arqueas poseen un diámetro celular comprendido entre 0,1 y 15 μm, aunque las formas filamentosas pueden ser mayores por agregación de células. Presentan multitud de formas distintas: incluso las hay descritas cuadradas y planas. Algunas arqueas tienen flagelos y son móviles. Las arqueas, al igual que las bacterias, no tienen membranas internas que delimiten orgánulos. Como todos los organismos presentan ribosomas, pero a diferencia de los encontrados en las bacterias que son sensibles a ciertos agentes antimicrobianos, los de las arqueas, más cercanos a los eucariotas, no lo son. La membrana celular tiene una estructura similar a la de las demás células, pero su composición química es única, con enlaces tipo éter en sus lípidos. Casi todas las arqueas poseen una pared celular (algunos Thermoplasma son la excepción) de composición característica, por ejemplo, no contienen peptidoglicano (mureína), propio de bacterias. No obstante, pueden clasificarse bajo la tinción de Gram, de vital importancia en la taxonomía de bacterias; sin embargo, en arqueas, poseedoras de una estructura de pared en absoluto común a la bacteriana, dicha tinción es aplicable, pero carece de valor taxonómico. El orden Methanobacteriales tiene una capa de pseudomureína, que provoca que dichas arqueas respondan como positivas a la tinción de Gram. Como en casi todos los procariotas, las células de las arqueas carecen de núcleo, y presentan un solo cromosoma circular. Existen elementos extracromosómicos, tales como plásmidos. Sus genomas son de pequeño tamaño, sobre 2-4 millones de pares de bases. También es característica la presencia de ARN polimerasas de constitución compleja y un gran número de nucleótidos modificados en los ácidos ribonucleicos ribosomales. Por otra parte, su ADN se empaqueta en forma de nucleosomas, como en los eucariotas, gracias a proteínas semejantes a las histonas y algunos genes poseen intrones. Pueden reproducirse por fisión binaria o múltiple, fragmentación o gemación. Bacterias Artículo principal: Bacteria Estructura de la célula procariota. Las bacterias son organismos relativamente sencillos, de dimensiones muy reducidas, de apenas unas micras en la mayoría de los casos. Como otros procariotas, carecen de un núcleo delimitado por una membrana, aunque presentan un nucleoide, una estructura elemental que contiene una gran molécula generalmente circular de ADN. Carecen de núcleo celular y demás orgánulos delimitados por membranas biológicas. En el citoplasma se pueden apreciar plásmidos, pequeñas moléculas circulares de ADN que coexisten con el nucleoide y que contienen genes: son comúnmente usados por las bacterias en la parasexualidad (reproducción sexual bacteriana). El citoplasma también contiene ribosomas y diversos tipos de gránulos. En algunos casos, puede haber estructuras compuestas por membranas, generalmente relacionadas con la fotosíntesis. Poseen una membrana celular compuesta de lípidos, en forma de una bicapa y sobre ella se encuentra una cubierta en la que existe un polisacárido complejo denominado peptidoglicano; dependiendo de su estructura y subsecuente su respuesta a la tinción de Gram, se clasifica a las bacterias en Gram positivas y Gram negativas. El espacio comprendido entre la membrana celular y la pared celular (o la membrana externa, si esta existe) se denomina espacio periplásmico. Algunas bacterias presentan una cápsula. Otras son capaces de generar endosporas (estadios latentes capaces de resistir condiciones extremas) en algún momento de su ciclo vital. Entre las formaciones exteriores propias de la célula bacteriana destacan los flagelos (de estructura completamente distinta a la de los flagelos eucariotas) y los pili (estructuras de adherencia y relacionadas con la parasexualidad). La mayoría de las bacterias disponen de un único cromosoma circular y suelen poseer elementos genéticos adicionales, como distintos tipos de plásmidos. Su reproducción, binaria y muy eficiente en el tiempo, permite la rápida expansión de sus poblaciones, generándose un gran número de células que son virtualmente clones, esto es, idénticas entre sí. La célula eucariota Artículo principal: Célula eucariota Las células eucariotas son el exponente de la complejidad celular actual. Presentan una estructura básica relativamente estable caracterizada por la presencia de distintos tipos de orgánulos intracitoplasmáticos especializados, entre los cuales destaca el núcleo, que alberga el material genético. Especialmente en los organismos pluricelulares, las células pueden alcanzar un alto grado de especialización. Dicha especialización o diferenciación es tal que, en algunos casos, compromete la propia viabilidad del tipo celular en aislamiento. Así, por ejemplo, las neuronas dependen para su supervivencia de las células gliales. Por otro lado, la estructura de la célula varía dependiendo de la situación taxonómica del ser vivo: de este modo, las células vegetales difieren de las animales, así como de las de los hongos. Por ejemplo, las células animales carecen de pared celular, son muy variables, no tiene plastos, puede tener vacuolas, pero no son muy grandes y presentan centríolos (que son agregados de microtúbulos cilíndricos que contribuyen a la formación de los cilios y los flagelos y facilitan la división celular). Las células de los vegetales, por su lado, presentan una pared celular compuesta principalmente de celulosa, disponen de plastos como cloroplastos (orgánulo capaz de realizar la fotosíntesis), cromoplastos (orgánulos que acumulan pigmentos) o leucoplastos (orgánulos que acumulan el almidón fabricado en la fotosíntesis), poseen vacuolas de gran tamaño que acumulan sustancias de reserva o de desecho producidas por la célula y finalmente cuentan también con plasmodesmos, que son conexiones citoplasmáticas que permiten la circulación directa de las sustancias del citoplasma de una célula a otra, con continuidad de sus membranas plasmáticas. Diagrama de una célula animal. (1. Nucléolo, 2. Núcleo, 3. Ribosoma, 4. Vesícula, 5. Retículo endoplasmático rugoso, 6. Aparato de Golgi, 7. Citoesqueleto (microtúbulos), 8. Retículo endoplasmático liso, 9. Mitocondria, 10. Vacuola, 11. Citoplasma, 12. Lisosoma, 13. Centríolos). Diagrama de una célula vegetal Compartimentos Las células son entes dinámicos, con un metabolismo celular interno de gran actividad cuya estructura es un flujo entre rutas anastomosadas. Un fenómeno observado en todos los tipos celulares es la compartimentalización, que consiste en una heterogeneidad que da lugar a entornos más o menos definidos (rodeados o no mediante membranas biológicas) en las cuales existe un microentorno que aglutina a los elementos implicados en una ruta biológica. Esta compartimentalización alcanza su máximo exponente en las células eucariotas, las cuales están formadas por diferentes estructuras y orgánulos que desarrollan funciones específicas, lo que supone un método de especialización espacial y temporal. No obstante, células más sencillas, como los procariotas, ya poseen especializaciones semejantes. Membrana plasmática y superficie celular Artículo principal: Membrana plasmática La composición de la membrana plasmática varía entre células dependiendo de la función o del tejido en la que se encuentre, pero posee elementos comunes. Está compuesta por una doble capa de fosfolípidos, por proteínas unidas no covalentemente a esa bicapa, y por glúcidos unidos covalentemente a lípidos o proteínas. Generalmente, las moléculas más numerosas son las de lípidos; sin embargo, las proteínas, debido a su mayor masa molecular, representan aproximadamente el 50 % de la masa de la membrana. Un modelo que explica el funcionamiento de la membrana plasmática es el modelo del mosaico fluido, de J. S. Singer y Garth Nicolson (1972), que desarrolla un concepto de unidad termodinámica basada en las interacciones hidrófobas entre moléculas y otro tipo de enlaces no covalentes. Esquema de una membrana celular. Se observa la bicapa de fosfolípidos, las proteínas y otras moléculas asociadas que permiten las funciones inherentes a este orgánulo. Dicha estructura de membrana sustenta un complejo mecanismo de transporte, que posibilita un fluido intercambio de masa y energía entre el entorno intracelular y el externo. Además, la posibilidad de transporte e interacción entre moléculas de células aledañas o de una célula con su entorno faculta a estas poder comunicarse químicamente, esto es, permite la señalización celular. Neurotransmisores, hormonas, mediadores químicos locales afectan a células concretas modificando el patrón de expresión génica mediante mecanismos de transducción de señal. Sobre la bicapa lipídica, independientemente de la presencia o no de una pared celular, existe una matriz que puede variar, de poco conspicua, como en los epitelios, a muy extensa, como en el tejido conjuntivo. Dicha matriz, denominada glucocalix (glicocáliz), rica en líquido tisular, glucoproteínas, proteoglicanos y fibras, también interviene en la generación de estructuras y funciones emergentes, derivadas de las interacciones célula-célula. Estructura y expresión génica Artículo principal: Expresión génica El ADN y sus distintos niveles de empaquetamiento. Las células eucariotas poseen su material genético en, generalmente, un solo núcleo celular, delimitado por una envoltura consistente en dos bicapas lipídicas atravesadas por numerosos poros nucleares y en continuidad con el retículo endoplasmático. En su interior, se encuentra el material genético, el ADN, observable, en las células en interfase, como cromatina de distribución heterogénea. A esta cromatina se encuentran asociadas multitud de proteínas, entre las cuales destacan las histonas, así como ARN, otro ácido nucleico. Dicho material genético se encuentra inmerso en una actividad continua de regulación de la expresión génica; las ARN polimerasas transcriben ARN mensajero continuamente, que, exportado al citosol, es traducido a proteína, de acuerdo a las necesidades fisiológicas. Asimismo, dependiendo del momento del ciclo celular, dicho ADN puede entrar en replicación, como paso previo a la mitosis. No obstante, las células eucarióticas poseen material genético extranuclear: concretamente, en mitocondrias y plastos, si los hubiere; estos orgánulos conservan una independencia genética parcial del genoma nuclear. Síntesis y degradación de macromoléculas Dentro del citosol, esto es, la matriz acuosa que alberga a los orgánulos y demás estructuras celulares, se encuentran inmersos multitud de tipos de maquinaria de metabolismo celular: orgánulos, inclusiones, elementos del citoesqueleto, enzimas... De hecho, estas últimas corresponden al 20 % de las enzimas totales de la célula. Estructura de los ribosomas; 1) subunidad mayor, 2) subunidad menor. Imagen de un núcleo, el retículo endoplasmático y el aparato de Golgi; 1, Núcleo. 2, Poro nuclear.3, Retículo endoplasmático rugoso (REr).4, Retículo endoplasmático liso (REl). 5, Ribosoma en el RE rugoso. 6, Proteínas siendo transportadas.7, Vesícula (transporte). 8, Aparato de Golgi. 9, Lado cis del aparato de Golgi.10, Lado trans del aparato de Golgi.11, Cisternas del aparato de Golgi. Ribosoma: Los ribosomas, visibles al microscopio electrónico como partículas esféricas, son complejos supramoleculares encargados de ensamblar proteínas a partir de la información genética que les llega del ADN transcrita en forma de ARN mensajero. Elaborados en el núcleo, desempeñan su función de síntesis de proteínas en el citoplasma. Están formados por ARN ribosómico y por diversos tipos de proteínas. Estructuralmente, tienen dos subunidades. En las células, estos orgánulos aparecen en diferentes estados de disociación. Cuando están completos, pueden estar aislados o formando grupos (polisomas). También pueden aparecer asociados al retículo endoplasmático rugoso o a la envoltura nuclear. Retículo endoplasmático: El retículo endoplasmático es orgánulo vesicular interconectado que forma cisternas, tubos aplanados y sáculos comunicados entre sí. Intervienen en funciones relacionadas con la síntesis proteica, glicosilación de proteínas, metabolismo de lípidos y algunos esteroides, detoxificación, así como el tráfico de vesículas. En células especializadas, como las miofibrillas o células musculares, se diferencia en el retículo sarcoplásmico, orgánulo decisivo para que se produzca la contracción muscular. Aparato de Golgi: El aparato de Golgi es un orgánulo formado por apilamientos de sáculos denominados dictiosomas, si bien, como ente dinámico, estos pueden interpretarse como estructuras puntuales fruto de la coalescencia de vesículas. Recibe las vesículas del retículo endoplasmático rugoso que han de seguir siendo procesadas. Dentro de las funciones que posee el aparato de Golgi se encuentran la glicosilación de proteínas, selección, destinación, glicosilación de lípidos y la síntesis de polisacáridos de la matriz extracelular. Posee tres compartimientos; uno proximal al retículo endoplasmático, denominado «compartimento cis», donde se produce la fosforilación de las manosas de las enzimas que han de dirigirse al lisosoma; el «compartimento intermedio», con abundantes manosidasas y N-acetil-glucosamina transferasas; y el «compartimento o red trans», el más distal, donde se transfieren residuos de galactosa y ácido siálico, y del que emergen las vesículas con los diversos destinos celulares. Lisosoma: Los lisosomas son orgánulos que albergan multitud de enzimas hidrolíticas. De morfología muy variable, no se ha demostrado su existencia en células vegetales. Una característica que agrupa a todos los lisosomas es la posesión de hidrolasas ácidas: proteasas, nucleasas, glucosidasas, lisozima, arilsulfatasas, lipasas, fosfolipasas y fosfatasas. Procede de la fusión de vesículas procedentes del aparato de Golgi, que, a su vez, se fusionan en un tipo de orgánulo denominado endosoma temprano, el cual, al acidificarse y ganar en enzimas hidrolíticos, pasa a convertirse en el lisosoma funcional. Sus funciones abarcan desde la degradación de macromoléculas endógenas o procedentes de la fagocitosis a la intervención en procesos de apoptosis. La vacuola regula el estado de turgencia de la célula vegetal. Vacuola vegetal: Las vacuolas vegetales, numerosas y pequeñas en células meristemáticas y escasas y grandes en células diferenciadas, son orgánulos exclusivos de los representantes del mundo vegetal. Inmersas en el citosol, están delimitadas por el tonoplasto, una membrana lipídica. Sus funciones son: facilitar el intercambio con el medio externo, mantener la turgencia celular, la digestión celular y la acumulación de sustancias de reserva y subproductos del metabolismo. Inclusión citoplasmática: Las inclusiones son acúmulos nunca delimitados por membrana de sustancias de diversa índole, tanto en células vegetales como animales. Típicamente se trata de sustancias de reserva que se conservan como acervo metabólico: almidón, glucógeno, triglicéridos, proteínas..., aunque también existen de pigmentos. Conversión energética El metabolismo celular está basado en la transformación de unas sustancias químicas, denominadas metabolitos, en otras; dichas reacciones químicas transcurren catalizadas mediante enzimas. Si bien buena parte del metabolismo sucede en el citosol, como la glucólisis, existen procesos específicos de orgánulos. Modelo de una mitocondria: 1. Membrana interna; 2. Membrana externa; 3. Cresta mitocondrial; 4. Matriz mitocondrial. Mitocondria: Las mitocondrias son orgánulos de aspecto, número y tamaño variable que intervienen en el ciclo de Krebs, fosforilación oxidativa y en la cadena de transporte de electrones de la respiración. Presentan una doble membrana, externa e interna, que dejan entre ellas un espacio perimitocondrial; la membrana interna, plegada en crestas hacia el interior de la matriz mitocondrial, posee una gran superficie. En su interior posee generalmente una sola molécula de ADN, el genoma mitocondrial, típicamente circular, así como ribosomas más semejantes a los bacterianos que a los eucariotas. Según la teoría endosimbiótica, se asume que la primera protomitocondria era un tipo de proteobacteria. Estructura de un cloroplasto. Cloroplasto: Los cloroplastos son los orgánulos celulares que en los organismos eucariotas fotosintéticos se ocupan de la fotosíntesis. Están limitados por una envoltura formada por dos membranas concéntricas y contienen vesículas, los tilacoides, donde se encuentran organizados los pigmentos y demás moléculas implicadas en la conversión de la energía lumínica en energía química. Además de esta función, los plastidios intervienen en el metabolismo intermedio, produciendo energía y poder reductor, sintetizando bases púricas y pirimidínicas, algunos aminoácidos y todos los ácidos grasos. Además, en su interior es común la acumulación de sustancias de reserva, como el almidón. Se considera que poseen analogía con las cianobacterias. Modelo de la estructura de un peroxisoma. Peroxisoma: Los peroxisomas son orgánulos muy comunes en forma de vesículas que contienen abundantes enzimas de tipo oxidasa y catalasa; de tan abundantes, es común que cristalicen en su interior. Estas enzimas cumplen funciones de detoxificación celular. Otras funciones de los peroxisomas son: las oxidaciones flavínicas generales, el catabolismo de las purinas, la beta-oxidación de los ácidos grasos, el ciclo del glioxilato, el metabolismo del ácido glicólico y la detoxificación en general. Se forman de vesículas procedentes del retículo endoplasmático. Citoesqueleto Artículo principal: Citoesqueleto Las células poseen un andamiaje que permite el mantenimiento de su forma y estructura, pero más aún, este es un sistema dinámico que interactúa con el resto de componentes celulares generando un alto grado de orden interno. Dicho andamiaje está formado por una serie de proteínas que se agrupan dando lugar a estructuras filamentosas que, mediante otras proteínas, interactúan entre ellas dando lugar a una especie de retículo. El mencionado andamiaje recibe el nombre de citoesqueleto, y sus elementos mayoritarios son: los microtúbulos, los microfilamentos y los filamentos intermedios.[nota 2] Microfilamentos: Los microfilamentos o filamentos de actina están formados por una proteína globular, la actina, que puede polimerizar dando lugar a estructuras filiformes. Dicha actina se expresa en todas las células del cuerpo y especialmente en las musculares, ya que está implicada en la contracción muscular, por interacción con la miosina. Además, posee lugares de unión a ATP, lo que dota a sus filamentos de polaridad. Puede encontrarse en forma libre o polimerizarse en microfilamentos, que son esenciales para funciones celulares tan importantes como la movilidad y la contracción de la célula durante la división celular. Citoesqueleto eucariota: microfilamentos en rojo, microtúbulos en verde y núcleo en azul. Microtúbulos: Los microtúbulos son estructuras tubulares de 25 nm de diámetro exterior y unos 12 nm de diámetro interior, con longitudes que varían entre unos pocos nanómetros a micrómetros, que se originan en los centros organizadores de microtúbulos y que se extienden a lo largo de todo el citoplasma. Se hallan en las células eucariotas y están formadas por la polimerización de un dímero de dos proteínas globulares, la alfa y la beta tubulina. Las tubulinas poseen capacidad de unir GTP. Los microtúbulos intervienen en diversos procesos celulares que involucran desplazamiento de vesículas de secreción, movimiento de orgánulos, transporte intracelular de sustancias, así como en la división celular (mitosis y meiosis) y que, junto con los microfilamentos y los filamentos intermedios, forman el citoesqueleto. Además, constituyen la estructura interna de los cilios y los flagelos. Filamentos intermedios: Los filamentos intermedios son componentes del citoesqueleto. Formados por agrupaciones de proteínas fibrosas, su nombre deriva de su diámetro, de 10 nm, menor que el de los microtúbulos, de 24 nm, pero mayor que el de los microfilamentos, de 7 nm. Son ubicuos en las células animales, y no existen en plantas ni hongos. Forman un grupo heterogéneo, clasificado en cinco familias: las queratinas, en células epiteliales; los neurofilamentos, en neuronas; los gliofilamentos, en células gliales; la desmina, en músculo liso y estriado; y la vimentina, en células derivadas del mesénquima. Micrografía al microscopio electrónico de barrido mostrando la superficie de células ciliadas del epitelio de los bronquiolos. Centríolos: Los centríolos son una pareja de estructuras que forman parte del citoesqueleto de células animales. Semejantes a cilindros huecos, están rodeados de un material proteico denso llamado material pericentriolar; todos ellos forman el centrosoma o centro organizador de microtúbulos que permiten la polimerización de microtúbulos de dímeros de tubulina que forman parte del citoesqueleto. Los centríolos se posicionan perpendicularmente entre sí. Sus funciones son participar en la mitosis, durante la cual generan el huso acromático, y en la citocinesis, así como, se postula, intervenir en la nucleación de microtúbulos. Cilios y flagelos: Se trata de especializaciones de la superficie celular con motilidad; con una estructura basada en agrupaciones de microtúbulos, ambos se diferencian en la mayor longitud y menor número de los flagelos, y en la mayor variabilidad de la estructura molecular de estos últimos. Ciclo vital Artículo principal: Ciclo celular Diagrama del ciclo celular: la interfase, en naranja, alberga a las fases G1, S y G2; la fase M, en cambio, únicamente consta de la mitosis y citocinesis, si la hubiere. El ciclo celular es el proceso ordenado y repetitivo en el tiempo mediante el cual una célula madre crece y se divide en dos células hijas. Las células que no se están dividiendo se encuentran en una fase conocida como G0, paralela al ciclo. La regulación del ciclo celular es esencial para el correcto funcionamiento de las células sanas, está claramente estructurado en fases El estado de no división o interfase. La célula realiza sus funciones específicas y, si está destinada a avanzar a la división celular, comienza por realizar la duplicación de su ADN. El estado de división, llamado fase M, situación que comprende la mitosis y citocinesis. En algunas células la citocinesis no se produce, obteniéndose como resultado de la división una masa celular plurinucleada denominada plasmodio.[nota 3] A diferencia de lo que sucede en la mitosis, donde la dotación genética se mantiene, existe una variante de la división celular, propia de las células de la línea germinal, denominada meiosis. En ella, se reduce la dotación genética diploide, común a todas las células somáticas del organismo, a una haploide, esto es, con una sola copia del genoma. De este modo, la fusión, durante la fecundación, de dos gametos haploides procedentes de dos parentales distintos da como resultado un zigoto, un nuevo individuo, diploide, equivalente en dotación genética a sus padres. La interfase consta de tres estadios claramente definidos. Fase G1: es la primera fase del ciclo celular, en la que existe crecimiento celular con síntesis de proteínas y de ARN. Es el período que trascurre entre el fin de una mitosis y el inicio de la síntesis de ADN. En él la célula dobla su tamaño y masa debido a la continua síntesis de todos sus componentes, como resultado de la expresión de los genes que codifican las proteínas responsables de su fenotipo particular. Fase S: es la segunda fase del ciclo, en la que se produce la replicación o síntesis del ADN. Como resultado cada cromosoma se duplica y queda formado por dos cromátidas idénticas. Con la duplicación del ADN, el núcleo contiene el doble de proteínas nucleares y de ADN que al principio. Fase G2: es la segunda fase de crecimiento del ciclo celular en la que continúa la síntesis de proteínas y ARN. Al final de este período se observa al microscopio cambios en la estructura celular, que indican el principio de la división celular. Termina cuando los cromosomas empiezan a condensarse al inicio de la mitosis. La fase M es la fase de la división celular en la cual una célula progenitora se divide en dos células hijas idénticas entre sí y a la madre. Esta fase incluye la mitosis, a su vez dividida en: profase, metafase, anafase, telofase; y la citocinesis, que se inicia ya en la telofase mitótica. La incorrecta regulación del ciclo celular puede conducir a la aparición de células precancerígenas que, si no son inducidas al suicidio mediante apoptosis, puede dar lugar a la aparición de cáncer. Los fallos conducentes a dicha desregulación están relacionados con la genética celular: lo más común son las alteraciones en oncogenes, genes supresores de tumores y genes de reparación del ADN. Origen Artículos principales: Historia de la vida y Anexo:Cronología de la historia evolutiva de la vida. Origen de la primera célula Artículo principal: Abiogénesis La aparición de la vida, y, por ello, de la célula, probablemente se inició gracias a la transformación de moléculas inorgánicas en orgánicas bajo unas condiciones ambientales adecuadas, produciéndose más adelante la interacción de estas biomoléculas generando entes de mayor complejidad. El experimento de Miller y Urey, realizado en 1953, demostró que una mezcla de compuestos orgánicos sencillos puede transformarse en algunos aminoácidos, glúcidos y lípidos (componentes todos ellos de la materia viva) bajo unas condiciones ambientales que simulan las presentes hipotéticamente en la Tierra primigenia (en torno al eón Hádico). Se ha sugerido que el último antepasado común universal vivió hace más de 4200 millones de años. Se postula que dichos componentes orgánicos se agruparon generando estructuras complejas, los coacervados de Oparin, aún acelulares que, en cuanto alcanzaron la capacidad de autoorganizarse y perpetuarse, dieron lugar a un tipo de célula primitiva, el progenote de Carl Woese, antecesor de los tipos celulares actuales. Una vez se diversificó este grupo celular, dando lugar a las variantes procariotas, arqueas y bacterias, pudieron aparecer nuevos tipos de células, más complejos, por endosimbiosis, esto es, captación permanente de unos tipos celulares en otros sin una pérdida total de autonomía de aquellos. De este modo, algunos autores describen un modelo en el cual la primera célula eucariota surgió por introducción de una arquea en el interior de una bacteria, dando lugar esta primera a un primitivo núcleo celular. No obstante, la imposibilidad de que una bacteria pueda efectuar una fagocitosis y, por ello, captar a otro tipo de célula, dio lugar a otra hipótesis, que sugiere que fue una célula denominada cronocito la que fagocitó a una bacteria y a una arquea, dando lugar al primer organismo eucariota. De este modo, y mediante un análisis de secuencias a nivel genómico de organismos modelo eucariotas, se ha conseguido describir a este cronocito original como un organismo con citoesqueleto y membrana plasmática, lo cual sustenta su capacidad fagocítica, y cuyo material genético era el ARN, lo que puede explicar, si la arquea fagocitada lo poseía en el ADN, la separación espacial en los eucariotas actuales entre la transcripción (nuclear), y la traducción (citoplasmática). Una dificultad adicional es el hecho de que no se han encontrado organismos eucariotas primitivamente amitocondriados como exige la hipótesis endosimbionte. Además, el equipo de María Rivera, de la Universidad de California, comparando genomas completos de todos los dominios de la vida ha encontrado evidencias de que los eucariotas contienen dos genomas diferentes, uno más semejante a bacterias y otro a arqueas, apuntando en este último caso semejanzas a los metanógenos, en particular en el caso de las histonas. Esto llevó a Bill Martin y Miklós Müller a plantear la hipótesis de que la célula eucariota surgiera no por endosimbiosis, sino por fusión quimérica y acoplamiento metabólico de un metanógeno y una α-proteobacteria simbiontes a través del hidrógeno (hipótesis del hidrógeno). Esta hipótesis atrae hoy en día posiciones muy encontradas, con detractores como Christian de Duve. Harold Morowitz, un físico de la Universidad Yale, ha calculado que las probabilidades de obtener la bacteria viva más sencilla mediante cambios al azar es de 1 sobre 1 seguido por 100 000 000 000 ceros. «Este número es tan grande ,dijo Robert Shapiro, que para escribirlo en forma convencional necesitaríamos varios centenares de miles de libros en blanco». Presenta la acusación de que los científicos que han abrazado la evolución química de la vida pasan por alto la evidencia aumentante y «han optado por aceptarla como verdad que no puede ser cuestionada, consagrándola así como mitología». Origen de la célula eucariota Artículo principal: Eucariogénesis En la teoría de la simbiogenesis, la fusión entre una arquéa y una bacteria aeróbia creo la célula eucariota, con mitochondrias aeróbicas, hace unos 2500 millones de años. Una segunda fusión, hace 2000 millones de años, añadió los cloroplastos, originando la célula vegetal. Las células eucariotas se formaron hace 2500 millones de años en un proceso llamado eucariogénesis. Se acepta ampliamente que esto implicó una simbiogénesis, en la que una arquea y una bacteria se unieron para crear el primer ancestro común eucariota. Esta célula tenía un nuevo nivel de complejidad y capacidad, con un núcleo y mitocondrias facultativamente aeróbicas. Evolucionó hasta convertirse en una población de organismos unicelulares que incluía al último ancestro común eucariota, acumulando capacidades a lo largo del camino, aunque la secuencia de los pasos involucrados ha sido cuestionada y es posible que no haya comenzado con la simbiogénesis. Presentaba al menos un centriolo y cilio, sexo (meiosis y singamia), peroxisomas y un quiste latente con una pared celular de quitina y/o celulosa. A su vez, el último ancestro común eucariota dio origen al grupo terminal de los eucariotas, que contiene los ancestros de animales, hongos, plantas y una amplia gama de organismos unicelulares. Las células vegetales se formaron hace unos 2000 millones de años con un segundo episodio de simbiogénesis al que se añadieron cloroplastos, derivados de una cianobacteria."
ksampletext_wikipedia_biol_filogenia: str = "Filogenia. La filogenia es la relación de parentesco entre especies o taxones en general. Aunque el término también aparece en lingüística histórica para referirse a la clasificación de las lenguas humanas según su origen común, el término se utiliza principalmente en su sentido biológico. La filogenética es una disciplina de la biología evolutiva que se ocupa de comprender las relaciones históricas entre diferentes grupos de organismos a partir de la distribución en un árbol o cladograma dicotómico de los caracteres derivados (sinapomorfías) de un ancestro común a dos o más taxones que contiene aquellos caracteres plesiomórficos en común. Incluso en el campo del cáncer, la filogenética permite estudiar la evolución clonal (evolución de los clones de la célula cancerosa original, debido a las mutaciones que ocurran) de los tumores y la cronología molecular, viéndose como varían las poblaciones celulares a lo largo de la progresión de la enfermedad, incluso durante el tratamiento de la misma, mediante el empleo de técnicas de secuenciación del genoma completo en muestras de ADN circulante tumoral. Para reconstruir la filogenia de un grupo taxonómico (familia, género, subgénero, etc.) es imprescindible construir matrices basadas en datos morfológicos y/o moleculares (ADN, ARN y proteínas). Las matrices son analizadas con determinados algoritmos que permiten encontrar los árboles filogenéticos más cortos siguiendo el principio de parsimonia, que supone la menor cantidad de cambios bajo el supuesto de que la evolución acontece de la manera más simple, esto es: los árboles que son considerados como la mejor opción filogenética son aquellos más cortos, es decir, más parsimoniosos. Interpretar los árboles obtenidos implica rastrear la historia del grupo bajo un paradigma evolutivo basado en el supuesto de un antecesor común del que van derivando cada uno de los clados, considerando que estos solo se sustentan por homologías. La condición de homología es resultante de la aceptación a priori de la existencia de monofilia. Explicar las relaciones de filogenéticas sobre la base del mapa de caracteres que ofrecen los cladogramas permite construir clasificaciones más naturales, uno de los propósitos centrales de la sistemática, una disciplina cuyos orígenes, en términos académicos, se remontan a los aportes de Linneo. No obstante, muchas clasificaciones han tenido diversos propósitos y responden a metodologías y criterios diferentes. Las primeras han sido artificiales y meramente utilitarias; otras se han basado en criterios que la ciencia ha depuesto en la actualidad, sustituyendo las categorías taxonómicas o los sistemas de clasificación creados bajo esas metodologías por otros que son legitimados por los científicos. Entre las corrientes más relevantes respecto de las clasificaciones biológicas mediadas por la metodología se encuentran en la actualidad dos programas de investigación que en sus inicios se presentaron como antagónicos: el feneticismo y el cladismo y que si bien comparten el propósito de encontrar un sistema que ordene a la diversidad de especies y de categorías taxonómicas se basan en postulados, supuestos y teorías auxiliares diversas y en metodologías diferentes. La sistemática filogenética se ha impuesto con el devenir de los años a causa de que la homología (que no constituye una premisa bajo la lógica feneticista) es consistente con el supuesto de un antecesor común y, por lo tanto, congruente con la evolución y, en consecuencia, con la posibilidad de definir arreglos taxonómicos más naturales. Esta necesidad de conocer la historia evolutiva de los seres vivos inicia con la publicación de El origen de las especies por Charles Darwin en 1859, aunque existen ideas previas que al menos desde Aristóteles han intentado explicar la diversidad de las formas de vida y sus relaciones. No obstante, explicar las relaciones históricas entre especies en función de la evolución es una tarea interminable y provisoria tal como lo es el conocimiento científico, sujeto a marcos teóricos y a coyunturas políticas. Uno de los hitos en relación con la justificación de estas relaciones fueron las contribuciones de Willi Hennig (entomólogo alemán, 1913-1976), Walter Zimmermann (botánico alemán, 1892-1980), Warren H. Wagner, Jr. (botánico estadounidense, 1920-2000) entre otros por la centralidad de sus aportes, tanto desde el punto de vista teórico como metodológico. Técnicas y uso Filogenética molecular Es la técnica de la filogenia que investiga las relaciones de los seres vivos mediante análisis moleculares de la secuencia de ADN, ARN y proteínas. Constituye la herramienta principal de la biología evolutiva moderna para inferir parentescos, especialmente en grupos donde los rasgos morfológicos son escasos o poco informativos, como los microorganismos. En este enfoque, las similitudes en la secuencia de nucleótidos y aminoácidos se interpretan como sinapomorfías en el análisis, es decir, características compartidas derivadas de un ancestro común. No obstante, dichas similitudes pueden variar considerablemente, ya que un organismo puede compartir secuencias idénticas con varios linajes cercanos, lo que puede generar hipótesis filogenéticas alternativas. La filogenética molecular ha permitido identificar numerosos clados evolutivos que no habían sido reconocidos mediante análisis morfológicos y ha corregido múltiples errores derivados del estudio exclusivo de características anatómicas. No obstante, esta aproximación también puede verse afectada por fenómenos como la atracción de ramas largas, en la que linajes con tasas de evolución acelerada se agrupan erróneamente. Por ello, se emplean modelos evolutivos más complejos y métodos estadísticos como la inferencia bayesiana o la máxima verosimilitud para minimizar dichos sesgos y obtener árboles más precisos. En la actualidad, la filogenética molecular se apoya en herramientas bioinformáticas y en bases de datos genómicas de gran escala, lo que permite analizar miles de genes o incluso genomas completos (filogenómica). Estas técnicas han revolucionado la clasificación biológica y la comprensión de la historia evolutiva de los organismos, contribuyendo a redefinir la sistemática moderna y a mejorar la identificación de especies, el estudio de la biodiversidad y la reconstrucción de eventos evolutivos profundos. Filogenética morfológica Es la técnica de la filogenia que investiga las relaciones de los seres vivos mediante análisis morfológicos como anatomía comparada, homología, embriología, alometría y fósiles. Las similitudes morfológicas entre organismos pueden ser un indicativo de parentesco, pero posteriormente se demostró que las similitudes morfológicas pueden evolucionar convergentemente en linajes diferentes. Actualmente, se usan ciertos caracteres morfológicos (sinapomorfías) que pueden emplearse para determinar las relaciones. La filogenética morfológica es empleada por los paleontólogos para determinar las relaciones entre los fósiles y los grupos existentes. También es usada por algunos zoólogos y botánicos evolutivos para determinar ciertos caracteres morfológicos válidos entre sus grupos de estudio (animales y plantas). Antiguamente, se usó para estudiar las relaciones entre los microorganismos, pero su uso quedó obsoleto debido a la ausencia de caracteres morfológicos en estos grupos. Filogenética molecular-estructural Es una técnica filogenética novedosa que investiga las relaciones mediante un tipo de biomolécula específico o similar que porten los organismos, sin tomar en cuenta la secuencia. Es similar a la filogenética morfológica en el hecho de que la sinapomorfía es una biomolécula única o similar que portan dichos organismos sin recurrir a la secuencia. Por ejemplo, las bacterias son un dominio que se caracteriza por tener una pared celular de peptidoglicanos. Las bacterias de Sphingobacteria se caracterizan por tener esfingolípidos. Los dominios de virus Riboviria, Duplodnaviria, Adnaviria y Varidnaviria se determinaron filogenéticamente mediante la presencia de una proteína única o similar estructuralmente. Inferencia de árboles filogenéticos La reconstrucción de árboles filogenéticos, a partir de datos moleculares o morfológicos, requiere de métodos que permitan inferir las relaciones evolutivas entre los taxones de interés. La filogenética computacional es una rama de la bioinformática, que aplica algoritmos y herramientas informáticas para reconstruir y analizar árboles filogenéticos. Entre los enfoques más empleados para estos propósito se encuentran la máxima parsimonia, los métodos basados en distancias, la máxima verosimilitud y la inferencia bayesiana, cada uno basado en distintos supuestos sobre la evolución de los caracteres. Método de máxima parsimonia: es un modelo de reconstrucción filogenética basado en el principio de parsimonia, según el cual se busca el árbol filogenético que implique la menor cantidad posible de cambios evolutivos o transiciones de un estado a otro. Aunque este método no se utiliza con tanta frecuencia en la actualidad, sigue siendo una herramienta importante para la reconstrucción de árboles, especialmente cuando no se cuenta con datos moleculares y se trabaja con caracteres morfológicos. Métodos basados en distancias: estos métodos de reconstrucción estiman las distancias evolutivas entre pares de taxones. Esta distancia se calcula generalmente alineando las secuencias de ADN o de proteínas y evaluando cuánto difieren entre sí. Una vez obtenidas las distancias, se reconstruye el árbol filogenético mediante algoritmos de agrupamiento que unen primero a los taxones más similares. Algunos de los algoritmos más utilizados son Neighbor-Joining y UPGMA. Método de máxima verosimilitud: en este enfoque de reconstrucción filogenética se asume que el árbol representa un modelo de evolución, y se busca encontrar la topología y la longitud de las ramas que maximizan la probabilidad de que los datos observados hayan ocurrido bajo dicho modelo. Para realizar este proceso, el método requiere un modelo de sustitución que describa la probabilidad de que un nucleótido o un aminoácido cambie por otro a lo largo del tiempo. Aunque es computacionalmente más costoso, es uno de los métodos más utilizados actualmente. Método de inferencia bayesiana: este método aplica la inferencia bayesiana para realizar la reconstrucción filogenética. En este método, el modelo de sustitución, la topología del árbol y las longitudes de las ramas se tratan como parámetros del modelo, y se buscan los valores que maximizan la probabilidad posterior, la cual combina la función de verosimilitud de los datos, la probabilidad previa de los parámetros y la probabilidad marginal de los datos. Debido a su alto costo computacional, normalmente se emplea el algoritmo de cadenas de Markov de Monte Carlo (MCMC) para realizar la inferencia de los parametros. Caracteres y estados del carácter El primer paso para reconstruir la filogenia de los organismos es determinar cuanta similitud hay entre sí, ya sea en morfología, anatomía, embriología, biogeografía, moléculas de ADN, ARN o proteínas, ya que en última instancia estos parecidos pueden ser un indicador de su parecido genético, y, por lo tanto, de sus relaciones evolutivas. La evolución es un proceso muy lento, y en la gran mayoría de los casos nadie la ha visto suceder. Lo que se maneja es una serie de hipótesis acerca de cómo ocurrió la diversificación de los organismos, que desembocó en la aparición de las distintas especies variadamente relacionadas entre sí. Esas hipótesis son las que determinan cómo deberían analizarse los organismos para determinar su filogenia. Supongamos una única población ancestral de plantas. Para establecer que los organismos que componen esta población son morfológicamente similares entre sí, determinamos una serie de caracteres: color de pétalo, leñosidad del tallo, presencia o ausencia de tricomas en las hojas, cantidad de estambres, fruto seco o carnoso, y rugosidad de la semilla. Todas las plantas de esta población ancestral comparten los mismos estados del carácter para cada uno de ellos: los pétalos son blancos, el tallo herbáceo, las hojas sin tricomas, los estambres son 5, el fruto es seco, y la semilla lisa. Finalmente, mediante algún mecanismo de aislamiento reproductivo, la población se divide en dos subpoblaciones que no intercambian material genético entre sí. Al cabo de algunas generaciones, se va haciendo evidente que aparecen mutantes en las dos subpoblaciones nuevas. Algunos de ellos son más exitosos reproductivamente que el resto de la población y, por lo tanto, después de unas generaciones más, su genotipo se convierte en el dominante en esa población. Como las mutaciones ocurren al azar en cada subpoblación, y la probabilidad de que ocurra espontáneamente la misma mutación en cada subpoblación es muy baja, las dos subpoblaciones van acumulando diferentes mutaciones exitosas, generando diferentes genotipos, que se pueden ver reflejados en los cambios que ocurren en los estados de los caracteres. Así, por ejemplo, la subpoblación 1 pasó a poseer el tallo leñoso, y la subpoblación 2 pasó a poseer los pétalos rojos (pero conservando el tallo herbáceo ancestral). Como resultado, la última generación de plantas corresponde a dos poblaciones muy similares entre sí, con muchos caracteres compartidos, salvo la leñosidad del tallo y el color de los pétalos. Nosotros, en nuestra corta vida, solo vemos este resultado de la evolución, e hipotetizamos que lo que ocurrió fue el proceso que se indica más arriba. Esta hipótesis se puede reflejar en un árbol filogenético, un diagrama que resume las relaciones de parentesco entre los ancestros y sus descendientes, como el siguiente: Árbol filogenético que muestra cómo, después de un evento de aislamiento reproductivo entre dos poblaciones de la misma especie, apareció una mutación exitosa en cada población, que pasaron a diferenciarse entre ellas mediante la observación de los estados de sus caracteres. En el cladograma, la especie 1 comparte con su ancestro todos los estados de los caracteres salvo el tallo, que es leñoso. La especie 2, a su vez, comparte con su ancestro todos los caracteres salvo el color de los pétalos, que es rojo. Las dos especies comparten entre sí todos los caracteres salvo la leñosidad del tallo y el color de los pétalos. En este ejemplo, se han establecido 2 linajes: secuencias de poblaciones desde el ancestro hasta los descendientes. En los inicios de la sistemática, los caracteres utilizados para comparar a los grupos entre sí eran conspicuos, principalmente morfológicos. A medida que se acumuló más conocimiento se empezó a tomar cada vez más cantidad de caracteres crípticos, como los anatómicos, embriológicos, serológicos, químicos y finalmente caracteres del cariotipo y los derivados del análisis molecular. Los caracteres correspondientes al ancestro de un grupo de organismos que son retenidos por el grupo se dice que son plesiomórficos (ancestrales), mientras que los que fueron adquiridos exclusivamente por ese grupo (en el ejemplo, el tallo leñoso para la especie 1 o los pétalos rojos para la especie 2) se dice que son sinapomórficos o derivados (nuevos). Nótese que solo la presencia de sinapomorfías nos indica que se ha formado un nuevo linaje, nótese también que en árboles filogenéticos más extensos, como el siguiente: Árbol filogenético que muestra un ejemplo de diversificación de una especie ancestral en 5 especies presentes en la actualidad. el mismo carácter puede ser una sinapomorfía o una plesiomorfía, según desde qué porción del árbol se la observe. Por ejemplo, el tallo leñoso es una sinapomorfía de C (y de C+A+B) pero una plesiomorfía para A, ya que comparte ese estado del carácter con B a través de su ancestro común. Otra forma de decirlo es que el tallo leñoso es un carácter derivado desde el punto de vista de la población original, pero es ancestral para A y para B. El aspecto del árbol filogenético (su topología) solo está dado por las conexiones entre sus nodos, y no por el orden en que son diagramados. Así, [[A+B]+C] es el mismo árbol que [C+[A+B]]. La topología tampoco está dada por la posición en que el árbol es dibujado, a veces se los dibuja erectos (con el ancestro abajo y los grupos terminales arriba), a veces se los dibuja recostados (con el ancestro a la izquierda y los grupos terminales a la derecha). Las dos formas de dibujarlos son igualmente válidas. En los árboles filogenéticos como los aquí expuestos, el largo de las ramas tampoco da ninguna información acerca de cuánto diverge ese linaje en términos de sus caracteres ni acerca de en qué momento geológico ocurrió el aislamiento de ese linaje (pero hay árboles que sí dan esa información). Un cladograma es un árbol filogenético que solo muestra las relaciones evolutivas, sin darle un significado a sus ramas. Por el otro lado, hay dos tipos de árboles filogenéticos con significado en la longitud de sus ramas: el cronograma, donde la longitud de las ramas indica el tiempo transcurrido entre un nodo y otro, y la posición en el tiempo de cada nodo con respecto a los otros; y el filograma, donde la longitud de las ramas indica la cantidad de cambio evolutivo desde el ancestro común más cercano. Monofilia, parafilia y polifilia Artículos principales: Monofilético, Parafilético y Polifilético. Grupos filogenéticos: monofilético, parafilético, polifilético. Un grupo formado por un ancestro y todos sus descendientes se denomina monofilético, también llamado clado. Al grupo al que se le ha excluido alguno de sus descendientes se lo llama parafilético. Los grupos formados por los descendientes de más de un ancestro se denominan polifiléticos.[cita requerida] Por ejemplo, se cree que las aves y los reptiles descienden de un único ancestro común, luego este grupo taxonómico (amarillo en el diagrama) es considerado monofilético. Los reptiles actuales como grupo también tienen un ancestro común a todos ellos, pero ese grupo (reptiles modernos) no incluye a todos los descendientes de tal ancestro porque se está dejando a las aves fuera (solo incluye los de color cian en el diagrama); por lo que un grupo así se considera como parafilético. Un grupo que incluyera a los vertebrados de sangre caliente contendría solo a los mamíferos y las aves (rojo/naranja en el diagrama) y sería polifilético, porque entre los miembros de este agrupamiento no está el más reciente ancestro común de ellos. Los animales de sangre caliente son todos descendientes de un ancestro de sangre fría. La condición endotérmica (sangre caliente) ha aparecido dos veces, independientemente, en el ancestro de los mamíferos, por un lado, y en el de las aves (y quizá algunos o todos los dinosaurios), por otro. Algunos autores sostienen que la diferencia entre grupos parafiléticos y polifiléticos es sutil, y prefieren llamar a estos dos tipos de asemblajes como no monofiléticos. Muchos taxones largamente reconocidos de plantas y animales resultaron ser no monofiléticos según los análisis de filogenia hechos en las últimas décadas, por lo que muchos científicos recomendaron abandonar su uso, ejemplos de estos taxones son Prokaryota, Protista, Pisces, Reptilia, Pteridophyta, Dicotyledoneae, y varios otros más. Como su uso está muy extendido por haber sido tradicionalmente reconocidos, y porque muchos científicos consideran a los taxones parafiléticos válidos (discusión que aún no está terminada en el ambiente científico; el ejemplo más claro de un taxón que muchos desean conservar quizás sean Reptilia), a veces se indica el nombre del taxón, con la salvedad de que su nombre se pone entre comillas, para indicar que el taxón no se corresponde con un clado. La diferencia entre un clado y un taxón es que un clado debe ser un grupo natural (monofilético), mientras que un taxón puede ser o no monofilético, pero los taxones no monofiléticos pierden su validez en la clasificación actual de los organismos. El rol de las sinapomorfías en el análisis filogenético Las sinapomorfías que caracterizan a cada grupo monofilético son estados de los caracteres que se originaron en el ancestro común a todos los miembros del grupo, pero que no estaban presentes en los ancestros anteriores a estos, ancestros comunes tanto a los miembros del grupo como a otros grupos más. Hay que tener en cuenta que si bien una sinapomorfía es un estado del carácter que se hipotetiza que está presente en el ancestro del grupo, no necesariamente será encontrada en todos sus descendientes, debido a que la evolución puede modificarla y hasta revertirla a su estado anterior por azar (proceso que se conoce como reversión). Por lo tanto, no está garantizado que una lista de sinapomorfías vaya a encontrarse en todos los miembros de un grupo, y solo mediante un síndrome de caracteres podemos asegurarnos de que cada miembro pertenece a ese clado.[cita requerida] El concepto de sinapomorfía fue formalizado por primera vez por Hennig (1966) y Wagner (1980). Mucho del análisis filogenético actual se basa en la búsqueda de sinapomorfías que permitan establecer grupos monofiléticos. En ese sentido, son revolucionarios los análisis moleculares de ADN que se están realizando desde hace algunos años, que entre otras técnicas determinan la secuencia de bases del mismo trozo de ADN en diferentes taxones, y comparan directamente sus secuencias de bases. En estos análisis, que se realizan con secuencias conservadas de genes concretos (como el ARNr), cada base es un carácter, y los posibles estados del carácter son las 4 posibles bases: adenina, timina, guanina y citosina. Si bien las sinapomorfías encontradas a través de los análisis moleculares de ADN son oscuras y no son útiles para identificar organismos en el campo o para plantear hipótesis acerca de la adaptación de los organismos a su ambiente, poseen ventajas (como la cantidad de caracteres medidos con poca cantidad de recursos, el establecimiento de caracteres menos subjetivos que los basados en fenotipos), que le otorgan a los análisis filogenéticos una precisión sin precedentes, obligando en muchas ocasiones a abandonar hipótesis evolutivas largamente reconocidas. Además, según la hipótesis del reloj molecular, la comparación de secuencias de ADN permite no solo determinar la distancia genética entre dos especies, sino además estimar el tiempo transcurrido desde el último antecesor común.[cita requerida] Sinapomorfías y especies La regla para construir los árboles filogenéticos es el reconocimiento de grupos monofiléticos (clados) a partir de sus sinapomorfías (estados de los caracteres comunes al grupo). Esto es cierto para todos los nodos del árbol salvo el terminal, a nivel de las especies. No se puede establecer monofilia a nivel de las especies debido a que la naturaleza de las relaciones entre los organismos cambia por encima y por debajo del nivel de especie: por encima del nivel de especie, organismos de dos clados diferentes no pueden cruzarse entre sí y dar descendencia fértil, por lo que sus bagajes genéticos se mantienen sin mezclarse. Por debajo del nivel de especie, existe interfertilidad entre los organismos, por lo que el genoma de cada organismo es el resultado del cruce de dos genomas diferentes. Esta diferencia se puede esquematizar como un árbol ramificado para representar a todas las agrupaciones de organismos por encima del nivel de especie, pero en los organismos que pertenecen a la misma especie, las ramas del árbol se entrecruzan entre sí creando una red interconectada de organismos. Como muchas poblaciones del planeta están en diferentes etapas del proceso de especiación, y a veces se reconocen dos poblaciones diferentes como especies diferentes a pesar de ser algo interfértiles, entonces no es fácil determinar si un estado de un carácter es exclusivo de una de las especies o pertenece también en una baja proporción no muestreada a la otra especie, o si pertenecerá en algún momento debido a una hibridación casual, a la otra especie. Avances recientes En los últimos años, el uso de datos genómicos a gran escala ha transformado la filogenia al permitir la reconstrucción de árboles evolutivos con miles de loci ,lo que se conoce como filogenómica, y enfrentarse simultáneamente a nuevos retos como la inferencia de ortología/paralogs, la hibridación, el intercambio genético lateral y la incongruencia entre genes y especies (Zaharias et al., 2022). Además, se está reconociendo que en algunos linajes complejos, como ciertos grupos de angiospermas, el modelo clásico de árbol bifurcado podría no bastar y se requiere considerar redes evolutivas o grafos reticulados para reflejar procesos de introgresión y duplicación genómica (Li et al., 2025)."
ksampletext_wikipedia_biol_botanica: str = "Botánica. La botánica (del griego, hierba) es la rama de la biología que estudia las plantas, en sentido amplio, incluyendo a las algas, hongos y organismos fotosintéticos no necesariamente clasificados como plantas, bajo todos sus aspectos, incluyendo la descripción, clasificación, distribución, identificación, estudio de la reproducción, fisiología, morfología, relaciones recíprocas, relaciones con los otros seres vivos y efectos provocados sobre el medio en el que se encuentran. Los términos para quien se dedica a esta disciplina son dos: botánico /a y botanista. La botánica estudia las plantas en sentido amplio, abarcando las categorías taxonómicas de las plantas sin flores (criptógamas), las plantas sin flores y sin vasos (briofitas), las plantas sin flores y con vasos (pteridofitas), las plantas con flores (espermatofitas), las plantas con flores y sin fruto (gimnospermas) y las plantas con flores y con fruto (angiospermas), dentro de la clasificación clásica de los organismos vegetales. No obstante, en términos históricos, el objeto de estudio de la botánica no se ha restringido estrictamente al Reino Plantae, sino que ha abarcado un grupo de organismos lejanamente emparentados entre sí, esto es, las cianobacterias, los hongos, las algas y las plantas, los que casi no poseen ningún carácter en común salvo la presencia de cloroplastos (a excepción de los hongos y cianobacterias) o el no poseer capacidad de desplazamiento. En el campo de la botánica hay que distinguir entre la botánica pura, cuyo objeto es ampliar el conocimiento de la naturaleza, y la botánica aplicada, cuyas investigaciones están al servicio de la tecnología agraria, forestal y farmacéutica. Su conocimiento afecta a muchos aspectos de nuestra vida y por tanto es una disciplina estudiada por biólogos y ambientólogos, pero también por farmacéuticos, ingenieros agrónomos, ingenieros forestales, entre otros. La botánica cubre una amplia gama de contenidos, que incluyen aspectos específicos propios de los vegetales, así como de las disciplinas biológicas que se ocupan de la composición química (fitoquímica), de la organización celular y tisular (histología vegetal), del metabolismo y el funcionamiento orgánico (fisiología vegetal), del crecimiento y el desarrollo, de la morfología (fitografía), de la reproducción, de la herencia (genética vegetal), de las enfermedades (fitopatología), de las adaptaciones al ambiente (ecología), de la distribución geográfica (fitogeografía o geobotánica), de los fósiles (paleobotánica) y de la evolución. Los organismos que estudia la botánica La idea de que la naturaleza puede ser dividida en tres reinos (mineral, vegetal y animal) fue propuesta por Nicolás Lemery (1675) y popularizada por Carlos Linneo en el siglo XVIII. Carlos Linneo, a finales del siglo XVIII, introdujo el actual sistema de clasificación. Este incluye los conocimientos sobre las diversas especies vegetales dentro de un sistema más amplio, ofreciendo una versión sintética y enriquecedora. No en vano se ha dicho que el sistema de clasificación de Linneo prefigura lo que después serían las teorías evolutivas. A pesar de que con posterioridad fueron determinados como reinos separados para los hongos (en 1783), protozoarios (en 1858) y bacterias (en 1925) la concepción del siglo XVII de que solo existían dos reinos de organismos dominó la biología por tres siglos. El descubrimiento de los protozoarios en 1675, y de las bacterias en 1683, ambos realizados por Leeuwenhoek, finalmente comenzó a minar el sistema de dos reinos. No obstante, un acuerdo general entre los científicos acerca de que el mundo viviente debería ser clasificado en al menos cinco reinos, solo fue logrado luego de los descubrimientos realizados por la microscopía electrónica en la segunda mitad del siglo XX. Tales hallazgos confirmaron que existían diferencias fundamentales entre las bacterias y los eucariotas y, además, revelaron la tremenda diversidad ultraestructural de los protistas. La aceptación generalizada de la necesidad de utilizar varios reinos para incluir a todos los seres vivos también debe mucho a la síntesis sistemática de Herbert Copeland (1956) y a los influyentes trabajos de Roger Y. Stanier (1961-1962) y Robert H. Whittaker (1969).En el sistema de seis reinos, propuesto por Thomas Cavalier-Smith en 1983 y modificado en 1998, las bacterias son tratadas en un único reino (Bacteria) y los eucariotas se dividen en 5 reinos: protozoarios (Protozoa), animales (Animalia), hongos (Fungi), plantas (Plantae) y Chromista (algas cuyos cloroplastos contienen clorofilas a y c, así como otros organismos sin clorofila relacionados con ellas). La nomenclatura de estos tres últimos reinos, clásico objeto de estudio de la botánica, está sujeta a las reglas y recomendaciones del Código Internacional de Nomenclatura Botánica. Divisiones de la botánica Familias botánicas Las plantas pueden estudiarse desde varios puntos de vista, así, pueden diferenciarse distintas líneas de trabajo de acuerdo con los niveles de organización que se estudien: desde las moléculas y las células, pasando por los tejidos y los órganos, hasta los individuos, las poblaciones y las comunidades vegetales. Otras posibilidades se refieren al estudio de las plantas que vivieron en épocas geológicas pasadas o al de las que viven en la actualidad, al examen de los distintos grupos sistemáticos y a la investigación de cómo pueden ser utilizados los vegetales por el ser humano. Una de las metas más importantes para la botánica, es que junto a la biotecnología e ingeniería genética puedan llegar a crear vida. En general, todas esas direcciones de trabajo se basan en el análisis comparativo de los fenómenos particulares y de su variabilidad, para llegar a una generalización y al reconocimiento de las relaciones regulares que unen dichos fenómenos entre sí. Siempre deben asociarse los métodos estático y dinámico: por un lado el reconocimiento y la interpretación de las estructuras y formas y, por el otro, el análisis de los procesos vitales, de funciones y de fenómenos de desarrollo. El objetivo final de ambos métodos debe ser en todo caso la comprensión de las formas y de las funciones en su dependencia recíproca y en su evolución. Los distintos puntos de vista descritos y el empleo de diferentes métodos de trabajo han conducido a que dentro de la botánica se hayan desarrollado numerosas disciplinas. En primer lugar, se puede citar a la Morfología, la cual, en sentido amplio, es la teoría general de la estructura y forma de las plantas, e incluye la Citología y la Histología. La primera se ocupa del estudio de la fina constitución de las células y se asocia, en los aspectos relacionados con las moléculas, con algunas partes de la Biología Molecular. La Histología es el estudio de los tejidos de las plantas. Citología e Histología, conjuntamente, son necesarias para comprender la Anatomía de las plantas, o sea, su constitución interna. Al ocuparse de los procesos de adaptación, la morfología se relaciona con la ecología, disciplina que investiga las relaciones entre la planta y su ambiente. Tales relaciones están basadas en los estudios de la fisiología vegetal, que se ocupa ,de modo general, al estudio del modo en que se realizan las funciones de la planta en los campos del metabolismo, del cambio de forma (que incluye el crecimiento y desarrollo de la planta) y de los movimientos. La reproducción de las plantas y el modo en que se heredan y cambian las características a través de las generaciones es el campo de la Genética. La botánica sistemática trata de averiguar las afinidades que existen entre los diversos tipos de plantas, basándose en los resultados de todas las disciplinas mencionadas previamente, entre las que, al lado de la morfología, son importantes la citología, la anatomía, el estudio de las esporas y del polen (Palinología), el estudio de la generación sexual y del embrión (Embriología), las sustancias producidas y contenidas en las plantas (fitoquímica), la Genética y la Geobotánica. Como parte de la sistemática, se encuentra principalmente la taxonomía, que se ocupa de la descripción, nomenclatura y ordenación de las especies de plantas existentes, las cuales sobrepasan el número de 330 000. A ella se añade el estudio de la historia evolutiva de las plantas (Filogenia), que se apoya especialmente en la Paleobotánica, el estudio de las plantas que vivieron en otras eras geológicas y en la evolución, que ilustra sobre las leyes y las causas que rigen la formación de las estirpes vegetales. Finalmente, dentro de la botánica existen ramas de estudio que se ocupan de modo especial de grupos particulares de organismos, como la Microbiología (que estudia los microorganismos en general, incluyendo muchos de los que se consideran organismos vegetales), la Bacteriología (que se ocupa de las bacterias), la Micología (que estudia los hongos), la Ficología (que estudia las algas), la Liquenología (estudio de los líquenes), la Briología (estudio de los briófitos: los musgos y las hepáticas), la Pteridología (estudio de los helechos).También existen distintas disciplinas aplicadas, que estudian el valor práctico de las plantas para los seres humanos y con ello establecen el enlace con la Agricultura, la Silvicultura y la Farmacia, entre otras. Como ejemplo de estas disciplinas se pueden mencionar el Mejoramiento Genético de Plantas ,o fitomejoramiento, (estudia la variabilidad genética y la selección de plantas), la Fitopatología (se ocupa de las enfermedades de las plantas y de los métodos de control de las mismas), la Farmacognosia (estudia las plantas medicinales y sus principios activos). Historia Esta sección es un extracto de Historia de la botánica.[editar] Busto de Teofrasto, considerado como el padre de la botánica. La historia de la botánica es la exposición y narración de las ideas, investigaciones y obras relacionadas con la descripción, clasificación, funcionamiento, distribución y relaciones de los organismos pertenecientes a los reinos Fungi, Chromista y Plantae a través de los diferentes períodos históricos.[n 1][n 2] Desde la antigüedad, el estudio de los vegetales se ha abordado con dos aproximaciones bastante diferentes: la teórica y la utilitaria. Desde el primer punto de vista, al que se denomina botánica pura, la ciencia de las plantas se erigió por sus propios méritos como una parte integral de la biología. Desde una concepción utilitaria, por otro lado, la denominada botánica aplicada era concebida como una disciplina subsidiaria de la medicina o de la agronomía. En los diferentes períodos de su evolución una u otra aproximación ha predominado, si bien en sus orígenes ,que datan del siglo VIII a. C., la aproximación aplicada fue la preponderante. La botánica, como muchas otras ciencias, alcanzó la primera expresión definida de sus principios y problemas en la Grecia clásica y, posteriormente, continuó su desarrollo durante la época del Imperio romano. Teofrasto, discípulo de Aristóteles y considerado el «padre de la botánica», legó dos obras importantes que se suelen señalar como el origen de esta ciencia: De historia plantarum [Historia de las plantas] y De causis plantarum [Sobre las causas de las plantas]. Los romanos contribuyeron poco a los fundamentos de la botánica, pero hicieron una gran contribución al conocimiento de la botánica aplicada a la agricultura. El enciclopedista romano Plinio el Viejo aborda las plantas en los libros XII a XXVI de sus 37 volúmenes de Naturalis Historia. Se estima que en la época del imperio romano entre 1300 y 1400 plantas se habían registrado en el oeste. Tras la caída del Imperio en el siglo V, todas las conquistas alcanzadas en la antigüedad clásica tuvieron que redescubrirse a partir del siglo XII, por perderse o ignorarse buena parte de ellas durante la alta Edad Media. La tradición conservadora de la Iglesia y la labor de contadas personalidades hicieron avanzar, aunque muy lentamente, el conocimiento de los vegetales durante este período. En los siglos XV y XVI la botánica se desarrolló como una disciplina científica, separada de la herboristería y de la Medicina, si bien continuó contribuyendo a ambas. Diversos factores permitieron el desarrollo y progreso de la botánica durante esos siglos: la invención de la imprenta, la aparición del papel para la elaboración de los herbarios, y el desarrollo de los jardines botánicos, todo ello unido al desarrollo del arte y ciencia de la navegación que permitió la realización de expediciones botánicas. Todos estos factores conjuntamente supusieron un incremento notable en el número de las especies conocidas y permitieron la difusión del conocimiento local o regional a una escala internacional. Impulsada por las obras de Galileo, Kepler, Bacon y Descartes, en el siglo XVII se originó la ciencia moderna. Debido a la creciente necesidad de los naturalistas europeos de intercambiar ideas e información, se comenzaron a fundar las primeras academias científicas. Joachim Jungius fue el primer científico que combinó una mentalidad entrenada en la filosofía con observaciones exactas de las plantas. Tenía la habilidad de definir los términos con exactitud y, por ende, de reducir el uso de términos vagos o arbitrarios en la sistemática. Se lo considera el fundador del lenguaje científico, el que fue desarrollado más tarde por el inglés John Ray y perfeccionado por el sueco Carlos Linneo. A Linneo se le atribuyen varias innovaciones centrales en la taxonomía. En primer lugar, la utilización de la nomenclatura binomial de las especies en conexión con una rigurosa caracterización morfológica de las mismas. En segundo lugar, el uso de una terminología exacta. Basado en el trabajo de Jungius, Linneo definió con precisión varios términos morfológicos que serían utilizados en sus descripciones de cada especie o género, en particular aquellos relacionados con la morfología floral y con la morfología del fruto. No obstante, el mismo Linneo notó las fallas de su sistema y buscó en vano nuevas alternativas. Su concepto de la constancia de cada especie fue un obstáculo obvio para lograr establecer un sistema natural ya que esa concepción de la especie negaba la existencia de las variaciones naturales, las cuales son esenciales para el desarrollo de un sistema natural. Esta contradicción permaneció durante mucho tiempo y no fue resuelta hasta 1859 con la obra de Charles Darwin. Durante los siglos XVII y XVIII también se originaron dos disciplinas científicas que, a partir de ese momento, iban a tener una profunda influencia en el desarrollo de todos los ámbitos de la botánica: la anatomía y la fisiología vegetal. Las ideas esenciales de la teoría de la evolución por selección natural de Darwin influirían notablemente en la concepción de la clasificación de los vegetales. De ese modo, aparecieron las clasificaciones filogenéticas, basadas primordialmente en las relaciones de proximidad evolutiva entre las distintas especies, reconstruyendo la historia de su diversificación desde el origen de la vida en la Tierra hasta la actualidad. El primer sistema admitido como filogenético fue el contenido en el Syllabus der Planzenfamilien (1892) de Adolf Engler y conocido más tarde como sistema de Engler cuyas numerosas adaptaciones posteriores han sido la base de un marco universal de referencia según el cual se han ordenado (y se siguen ordenando) muchos tratados de floras y herbarios de todo el mundo, si bien algunos de sus principios para interpretar el proceso evolutivo en las plantas han sido abandonados por la ciencia moderna. Los siglos XIX y XX han sido particularmente fecundos en las investigaciones botánicas, las que han llevado a la creación de numerosas disciplinas como la ecología, la geobotánica, la citogenética y la biología molecular y, en las últimas décadas, a una concepción de la taxonomía basada en la filogenia y en los análisis moleculares de ADN y a la primera publicación de la secuencia del genoma de una angiosperma: Arabidopsis thaliana. La botánica moderna (desde 1945) Esta sección es un extracto de Botánica moderna.[editar] La botánica moderna es una ciencia que considera una gran cantidad de nuevos conocimientos en la actualidad que han sido generados por el estudio de las plantas modelo y sobre la botánica actual, en concreto, ésta comenzó desde 1945. Arabidopsis thaliana motivó a los biólogos actuales a estudiar a fondo este tipo de plantas, esta mala hierba fue una de las primeras plantas en ver su genoma secuenciado. Otros más importantes comercialmente como alimentos básicos como el arroz, trigo, maíz, cebada, centeno, mijo y la soja están teniendo también sus secuencias del genoma. Algunas de éstas son un reto puesto que tienen en sus secuencias más de dos juegos de cromosomas haploides, una condición conocida como poliploidía, común en el reino vegetal. Un alga verde Chlamydomonas reinhardtii (un célula, sola, verde alga) es otro organismo modelo importante que ha sido extensivamente estudiado y provee importantes conocimientos a la biología celular. Significado de la botánica como ciencia Los distintos grupos de vegetales participan de manera fundamental en los ciclos de la biosfera. Las plantas y algas son los productores primarios, responsables de la captación de energía solar de la que depende la mayoría de la vida terrestre, de la creación de materia orgánica y también, como subproducto, de la generación del oxígeno que inunda la atmósfera y causa que casi todos los organismos saquen ventaja del metabolismo aerobio. Alimentación humana Casi todo lo que comemos proviene de las plantas, ya sea consumiéndolas directamente (frutas, verduras hortalizas), como indirectamente a través del ganado que se alimenta con plantas que componen el forrajeras. Por lo tanto, las plantas son la base de toda la cadena alimentaria, o lo que los ecólogos llaman el primer nivel trófico. El estudio de las plantas y las técnicas de mejoramiento para producir alimentos son claves para ser capaces de alimentar al mundo y proporcionar una seguridad alimentaria para las generaciones futuras. No obstante, como todas las plantas no son beneficiosas para este fin, la botánica también estudia las especies consideradas nocivas para la agricultura. También estudia los patógenos (fitopatología) que afectan al reino vegetal y la interacción de los humanos con este reino (etnobotánica). Procesos biológicos fundamentales Las plantas son susceptibles de ser estudiadas en sus procesos fundamentales (como la división celular y síntesis proteica por ejemplo), pero sin los problemas éticos que supone estudiar animales o seres humanos. Las leyes de la herencia fueron descubiertas de esta manera por Gregor Mendel, que estudió cómo se hereda la morfología del guisante. Las leyes descubiertas por Mendel a partir del estudio de plantas han conocido desarrollos posteriores, y se han aplicado sobre las propias plantas para conseguir nuevas variedades beneficiosas. Otro estudio clásico efectuado en plantas fue el realizado por Bárbara McClintock, quien descubrió los genes saltarines (o transposones) estudiando el maíz. Son ejemplos que muestran cómo la botánica ha tenido una importancia capital para el entendimiento de los procesos biológicos fundamentales. Aplicaciones de las plantas Muchas de nuestras medicinas y drogas, como el cannabis, vienen directamente del reino vegetal. Otros productos medicinales se derivan de sustancias de origen vegetal; así, la aspirina es un derivado del ácido salicílico, que originalmente se obtenía de la corteza de sauce. La investigación sobre productos farmacéuticamente útiles en las plantas es un campo activo de trabajo que rinde buenos resultados. Estimulantes populares como el café (por su contenido en cafeína), el chocolate, el tabaco (por la nicotina), y el té tienen origen vegetal. Muchas bebidas alcohólicas derivan de la fermentación de plantas como la cebada, el maíz y la uva. Las plantas también nos proveen de muchos materiales, como el algodón, la madera, el papel, el lino, el aceite vegetal, algunos tipos de cuerdas y plásticos. La producción de seda no sería posible sin el cultivo de los árboles de morera. La caña de azúcar y otras plantas han sido recientemente usadas como biomasa para producir una energía renovable alternativa al combustible fósil. Entendimiento de cambios ambientales Las plantas también pueden ayudar al entendimiento de los cambios del medio ambiente de muchas formas. Entendimiento de la destrucción de hábitat y de especies en extinción depende de un catálogo completo y exacto de plantas, de la sistemática y taxonomía. Respuesta de las plantas a radiación ultravioleta puede monitorear problemas como los agujeros en la capa de ozono. El análisis de polen depositado por plantas en miles de millones de años atrás puede ayudar a los científicos a reconstruir los climas del pasado y pronosticar el futuro, una parte esencial de investigaciones sobre cambios climáticos. Recopilar y analizar el tiempo del ciclo de vida es importante para la fenología usado para la investigación de cambios climáticos. Líquenes, sensibles a las condiciones atmosféricas, tienen un uso extensivo como indicadores de contaminación. Las plantas pueden servir como sensores, una especie de “señales tempranas de aviso” que den la alerta sobre cambios importantes en el ambiente. Por último, las plantas son sumamente valoradas en el aspecto recreativo para millones de personas que disfrutan de su uso en la jardinería, la horticultura y el arte culinario. Disciplinas Subdisciplinas de la botánica Anatomía vegetal u organografía Botánica aplicada Botánica marina Botánica pura o general Botánica sistemática Dendrología Ecología vegetal Ficología Fisiología vegetal geobotánica Histología vegetal Morfología vegetal Paleobotánica Palinología Sistemática vegetal Disciplinas relacionadas Agricultura Agronomía Bioquímica y fitoquímica Ecología Etnobotánica fitoterapia Fitopatología Fitosociología Genética Horticultura Micología Microbiología Métodos de la botánica Herbario Artículo principal: Herbario Secado de especímenes en un herbario de Burkina Faso. Un herbario (del latín herbarium) es una colección de plantas o partes de plantas, preservadas, casi siempre a través de la desecación, procesadas para su conservación, e identificadas, y acompañadas de información importante, como nombre científico y nombre común, utilidad, características de la planta en vivo y del sitio de muestreo, así como la ubicación del punto donde se colectó. Estas plantas se conservan indefinidamente, y constituyen un banco de información que representa la flora o vegetación de una región determinada en un espacio reducido. Estos especímenes se usan con frecuencia como material de referencia para definir el taxón de una planta; pues contienen los holotipos para estas plantas. El tipo nomenclatural o, simplemente, tipo es un ejemplar de una dada especie sobre el que se ha realizado la descripción de la misma y que, de ese modo, valida la publicación de un nombre científico basado en él. El tipo del nombre de una especie es por lo general el espécimen de herbario (o pliego de herbario) a partir del cual se ha perfilado la descripción que valida el nombre. El tipo del nombre de un género es la especie sobre la cual se basó la descripción original que validaba el nombre. El tipo del nombre de una familia es el género sobre el cual fue basada la descripción original válida. En los nombres de taxones de rango superior al de familia no se aplica el principio de tipificación. Jardín botánico Artículo principal: Jardín botánico Jardín Botánico de Curitiba. Los jardines botánicos (del latín hortus botanicus) son instituciones habilitadas por un organismo público, privado o asociativo (en ocasiones la gestión es mixta) cuyo objetivo es el estudio, la conservación y divulgación de la diversidad vegetal. Se caracterizan por exhibir colecciones científicas de plantas vivas, que se cultivan para conseguir alguno de estos objetivos: su conservación, investigación, divulgación y enseñanza. En los jardines botánicos se exponen plantas originarias de todo el mundo, generalmente con el objetivo de fomentar el interés de los visitantes hacia el mundo vegetal, aunque algunos de estos jardines se dedican, exclusivamente, a determinadas plantas y a especies concretas. Código Internacional de Nomenclatura para algas, hongos y plantas Estos párrafos son un extracto de Código Internacional de Nomenclatura para algas, hongos y plantas.[editar] El Código Internacional de Nomenclatura para algas, hongos y plantas (ICN) es el compendio de reglas que rigen la nomenclatura taxonómica de los organismos tradicionalmente estudiados por la botánica (plantas, algas y hongos) a efectos de determinar, para cada taxón, un único nombre válido internacionalmente. Hasta el año 2011, con la celebración del XVIII Congreso Internacional de Botánica en Melbourne (Australia), se denominaba Código Internacional de Nomenclatura Botánica (en inglés, ICBN, en español CINB)."
ksampletext_wikipedia_biol_bioquimica: str = "Bioquímica. La bioquímica es una rama de la ciencia que estudia la composición química de los seres vivos, especialmente las proteínas, carbohidratos, lípidos y ácidos nucleicos, además de otras pequeñas moléculas presentes en las células y las reacciones químicas que sufren estos compuestos, como en el metabolismo que les permiten obtener energía (catabolismo) y generar biomoléculas propias (anabolismo). La bioquímica se basa en el concepto de que todo ser vivo contiene carbono y en general las moléculas biológicas están compuestas principalmente de carbono, hidrógeno, oxígeno, nitrógeno, fósforo y azufre. Es la rama de la ciencia que estudia la base química de las moléculas que componen algunas células y los tejidos, que catalizan las reacciones químicas del metabolismo celular como la digestión, la fotosíntesis y la inmunidad, entre otras muchas cosas. Podemos entender la bioquímica como una disciplina científica integradora que elabora el estudio de los biomas y biosistemas. Integra de esta forma las leyes químico-físicas y la evolución biológica que afectan a los biosistemas y a sus componentes. Lo hace desde un punto de vista molecular y trata de entender y aplicar su conocimiento a amplios sectores de la medicina (terapia genética y biomedicina), la agroalimentación, la farmacología. Constituye un pilar fundamental de la biotecnología, y se ha consolidado como una disciplina esencial para abordar los grandes problemas y enfermedades actuales y del futuro, tales como el cambio climático, la escasez de recursos agroalimentarios ante el aumento de población mundial, el agotamiento de las reservas de combustibles fósiles, la aparición de nuevas alergias, el aumento del cáncer, las enfermedades genéticas, la obesidad, etc. La bioquímica es una ciencia experimental y por ello recurrirá al uso de numerosas técnicas instrumentales propias y de otros campos, pero la base de su desarrollo parte del hecho de que lo que ocurre en vivo a nivel subcelular se mantiene o se conserva tras el fraccionamiento subcelular, y a partir de ahí, podemos estudiarlo. Historia Siglo XIX y primera mitad del XX La historia de la bioquímica como la conocemos hoy en día es prácticamente moderna; desde el siglo XIX se comenzó a direccionar una buena parte de la biología y la química a la creación de una nueva disciplina integradora: la química fisiológica o la bioquímica. Pero la aplicación de la bioquímica y su conocimiento probablemente comenzó hace 5000 años, con la producción de pan usando levaduras, en un proceso conocido como fermentación. Es difícil abordar la historia de la bioquímica, en cuanto que, es una mezcla compleja de química orgánica y biología, y en ocasiones, se hace complicado discernir entre lo exclusivamente biológico y lo exclusivamente químico orgánico y es evidente que la contribución a esta disciplina ha sido muy extensa. Aunque es cierto que existen datos experimentales que son básicos en la bioquímica. Se suele situar el inicio de la bioquímica en los descubrimientos en 1828 de Friedrich Wöhler que publicó un artículo acerca de la síntesis de urea, probando que los compuestos orgánicos pueden ser creados artificialmente, en contraste con la creencia comúnmente aceptada durante mucho tiempo, de que la generación de estos compuestos era posible solo en el interior de los seres vivos. La diastasa fue la primera enzima descubierta. En 1833 se extrajo de la solución de malta por Anselme Payen y Jean-François Persoz, dos químicos de una fábrica de azúcar francesa. A mediados del siglo XIX, Louis Pasteur demostró los fenómenos de isomería química existente entre las moléculas de ácido tartárico provenientes de los seres vivos y las sintetizadas químicamente en el laboratorio. También estudió el fenómeno de la fermentación y descubrió que intervenían ciertas levaduras, y por tanto no era exclusivamente un fenómeno químico como se había defendido hasta ahora (entre ellos el propio Liebig); así Pasteur escribió: «la fermentación del alcohol es un acto relacionado con la vida y la organización de las células de las levaduras, y no con la muerte y la putrefacción de las células». Además desarrolló un método de esterilización de la leche, el vino y la cerveza (pasteurización) y contribuyó enormemente a refutar la idea de la generación espontánea de los seres vivos. En 1869 se descubre la nucleína y se observa que es una sustancia muy rica en fósforo. Dos años más tarde, Albrecht Kossel concluye que la nucleína es rica en proteínas y contiene las bases púricas adenina y guanina y las pirimidínicas citosina y timina. En 1889 se aíslan los dos componentes mayoritarios de la nucleína: Proteínas (70 %) Sustancias de carácter ácido: ácidos nucleicos (30 %) En 1878 el fisiólogo Wilhelm Kühne acuñó el término enzima para referirse a los componentes biológicos desconocidos que producían la fermentación. La palabra enzima fue usada después para referirse a sustancias inertes tales como la pepsina. En 1897 Eduard Buchner comenzó a estudiar la capacidad de los extractos de levadura para fermentar azúcar a pesar de la ausencia de células vivientes de levadura. En una serie de experimentos en la Universidad Humboldt de Berlín, encontró que el azúcar era fermentado incluso cuando no había elementos vivos en los cultivos de células de levaduras. Llamó a la enzima que causa la fermentación de la sacarosa, “zimasa”. Al demostrar que las enzimas podrían funcionar fuera de una célula viva, el siguiente paso fue demostrar cuál era la naturaleza bioquímica de esos biocatalizadores. El debate fue extenso; muchos, como el bioquímico alemán Richard Willstätter, discrepaban de que la proteína fuera el catalizador enzimático, hasta que en 1926, James B. Sumner demostró que la enzima ureasa era una proteína pura y la cristalizó. La conclusión de que las proteínas puras podían ser enzimas fue definitivamente probada en torno a 1930 por John Howard Northrop y Wendell Meredith Stanley, quienes trabajaron con diversas enzimas digestivas como la pepsina, la tripsina y la quimotripsina. En 1903 Mijaíl Tswett inicia los estudios de cromatografía para separación de pigmentos. En torno a 1915 Gustav Embden y Otto Meyerhof realizan sus estudios sobre la glucólisis. En 1920 se descubre que en las células hay ADN y ARN y que difieren en el azúcar que forma parte de su composición: desoxirribosa o ribosa. El ADN reside en el núcleo. Unos años más tarde, se descubre que en los espermatozoides hay fundamentalmente ADN y proteínas, y posteriormente Feulgen descubre que hay ADN en los cromosomas con su tinción específica para este compuesto. En 1925 Theodor Svedberg demuestra que las proteínas son macromoléculas y desarrolla la técnica de ultracentrifugación analítica. En 1928, Alexander Fleming descubre la penicilina y desarrolla estudios sobre la lisozima. Richard Willstätter (en torno 1910) estudia la clorofila y comprueba la similitud que hay con la hemoglobina. Posteriormente Hans Fischer en torno a 1930, investiga la química de las porfirinas de las que derivan la clorofila o el grupo porfirínico de la hemoglobina. Consiguió sintetizar hemina y bilirrubina. Paralelamente Heinrich Otto Wieland formula teorías sobre las deshidrogenaciones y explica la constitución de muchas otras sustancias de naturaleza compleja, como la pteridina, las hormonas sexuales o los ácidos biliares. En la década de 1940, Melvin Calvin concluye el estudio del ciclo de Calvin en la fotosíntesis y Albert Claude la síntesis del ATP en las mitocondrias. En torno a 1945 Gerty Cori, Carl Cori, y Bernardo Houssay completan sus estudios sobre el ciclo de Cori. En 1953 James Dewey Watson y Francis Crick, gracias a los estudios previos con cristalografía de rayos X de ADN de Rosalind Franklin y Maurice Wilkins, y los estudios de Erwin Chargaff sobre apareamiento de bases nitrogenadas, deducen la estructura de doble hélice del ADN. En 1957, Matthew Meselson y Franklin Stahl demuestran que la replicación del ADN es semiconservativa. Segunda mitad del siglo XX En la segunda mitad del siglo XX, comienza la auténtica revolución de la bioquímica y la biología molecular moderna, especialmente gracias al desarrollo de las técnicas experimentales más básicas como la cromatografía, la centrifugación, la electroforesis, las técnicas radioisotópicas y la microscopía electrónica, y las técnicas más complejas como la cristalografía de rayos X, la resonancia magnética nuclear, la PCR (Kary Mullis), el desarrollo de la inmuno-técnicas. Desde 1950 a 1975 , se conocen en profundidad y detalle aspectos del metabolismo celular inimaginables hasta ahora (fosforilación oxidativa (Peter Dennis Mitchell), ciclo de la urea y ciclo de Krebs (Hans Adolf Krebs), así como otras rutas metabólicas), se produce toda una revolución en el estudio de los genes y su expresión; se descifra el código genético (Francis Crick, Severo Ochoa, Har Gobind Khorana, Robert W. Holley y Marshall Warren Nirenberg), se descubren las enzimas de restricción (finales de 1960, Werner Arber, Daniel Nathans y Hamilton Smith), la ADN ligasa (en 1972, Mertz y Davis) y finalmente en 1973 Stanley Cohen y Herbert Boyer producen el primer ser vivo recombinante, naciendo así la ingeniería genética, convertida en una herramienta poderosísima con la que se supera la frontera entre especies y con la que podemos obtener un beneficio hasta ahora impensable. En 1970, un argentino, Luis Federico Leloir, médico, bioquímico y farmacéutico recibió el Premio Nobel de Química por sus investigaciones sobre los nucleótidos de azúcar, y el rol que cumplen en la fabricación de los hidratos de carbono. En 1984, otro argentino, César Milstein, oriundo de la ciudad de Bahía Blanca, recibe el Premio Nobel de Medicina por sus investigaciones sobre anticuerpos monoclonales, hoy utilizados para tratar muchas enfermedades, incluidos algunos tipos de cáncer. De 1975 hasta principios del siglo XXI, comienza a secuenciarse el ADN (Allan Maxam, Walter Gilbert y Frederick Sanger), comienzan a crearse las primeras industrias biotecnológicas (Genentech), se aumenta la creación de fármacos y vacunas más eficaces, se eleva el interés por las inmunología y las células madres y se descubre la enzima telomerasa (Elizabeth Blackburn y Carol Greider). En 1989 se utiliza la biorremediación a gran escala en el derrame del petrolero Exxon Valdez en Alaska. Se clonan los primeros seres vivos, se secuencia el ADN de decenas de especies y se publica el genoma completo del hombre (Craig Venter, Celera Genomics y Proyecto Genoma Humano), se resuelven decenas de miles de estructuras proteicas y se publican en PDB, así como genes, en GenBank. Comienza el desarrollo de la bioinformática y la computación de sistemas complejos, que se constituyen como herramientas muy poderosas en el estudio de los sistemas biológicos. Se crea el primer cromosoma artificial y se logra la primera bacteria con genoma sintético (2007, 2009, Craig Venter). Se fabrican las nucleasas con dedos de zinc. Se inducen artificialmente células, que inicialmente no eran pluripotenciales, a células madre pluripotenciales (Shinya Yamanaka). Comienzan a darse los primeros pasos. Ramas de la bioquímica Esquema de una célula típica animal con sus orgánulos y estructuras. El pilar fundamental de la investigación bioquímica clásica se centra en las propiedades de las proteínas, muchas de las cuales son enzimas. Sin embargo, existen otras disciplinas que se centran en las propiedades biológicas de carbohidratos (glucobiología) y lípidos (lipobiología). Por razones históricas la bioquímica del metabolismo de la célula ha sido intensamente investigada, en importantes líneas de investigación actuales (como el Proyecto Genoma, cuya función es la de identificar y registrar todo el material genético humano), se dirigen hacia la investigación del ADN, el ARN, la síntesis de proteínas, la dinámica de la membrana celular y los ciclos energéticos. Las ramas de la bioquímica son muy amplias y diversas, y han ido variando con el tiempo y los avances de la biología, la química y la física. Bioquímica estructural: es un área de la bioquímica que pretende comprender la arquitectura química de las macromoléculas biológicas, especialmente de las proteínas y de los ácidos nucleicos (ADN y ARN). Así se intenta conocer las secuencias peptídicas, su estructura y conformación tridimensional, y las interacciones físico-químicas atómicas que posibilitan a dichas estructuras. Uno de sus máximos retos es determinar la estructura de una proteína conociendo solo la secuencia de aminoácidos, que supondría la base esencial para el diseño racional de proteínas (ingeniería de proteínas). Ciencia que estudia la estructura, propiedades físicas, la reactividad y transformación de los compuestos orgánicos. Química Orgánica Química orgánica: es un área de la química que se encarga del estudio de los compuestos orgánicos (es decir, aquellos que tienen enlaces covalentes carbono-carbono o carbono-hidrógeno) que provienen específicamente de seres vivos. Se trata de una ciencia íntimamente relacionada con la bioquímica clásica, ya que en la mayoría de los compuestos biológicos participa el carbono Mientras que la bioquímica clásica ayuda a comprender los procesos biológicos con base en conocimientos de estructura, enlace químico, interacciones moleculares y reactividad de las moléculas orgánicas, la química bioorgánica intenta integrar los conocimientos de síntesis orgánica, mecanismos de reacción, análisis estructural y métodos analíticos con las reacciones metabólicas primarias y secundarias, la biosíntesis, el reconocimiento celular y la diversidad química de los organismos vivos. De allí surge la Química de Productos Naturales (V. Metabolismo secundario). Enzimología: estudia el comportamiento de los catalizadores biológicos o enzimas, como son algunas proteínas y ciertos ARN catalíticos, así como las coenzimas y cofactores como metales y vitaminas. Así se cuestiona los mecanismos de catálisis, los procesos de interacción de las enzimas-sustrato, los estados de transición catalíticos, las actividades enzimáticas, la cinética de la reacción y los mecanismos de regulación y expresión enzimáticas, todo ello desde un punto de vista bioquímico. Estudia y trata de comprender los elementos esenciales del centro activo y de aquellos que no participan, así como los efectos catalíticos que ocurren en la modificación de dichos elementos; en este sentido, utilizan frecuentemente técnicas como la mutagénesis dirigida. Bioquímica metabólica: es un área de la bioquímica que pretende conocer los diferentes tipos de rutas metabólicas a nivel celular, y su contexto orgánico. De esta forma son esenciales conocimientos de enzimología y biología celular. Estudia todas las reacciones bioquímicas celulares que posibilitan la vida, y así como los índices bioquímicos orgánicos saludables, las bases moleculares de las enfermedades metabólicas o los flujos de intermediarios metabólicos a nivel global. De aquí surgen disciplinas académicas como la bioenergética (estudio del flujo de energía en los organismos vivos), la bioquímica nutricional (estudio de los procesos de nutrición asociados a| rutas metabólicas) y la bioquímica clínica (estudio de las alteraciones bioquímicas en estado de enfermedad o traumatismo). La metabolómica es el conjunto de ciencias y técnicas dedicadas al estudio completo del sistema constituido por el conjunto de moléculas que constituyen los intermediarios metabólicos, metabolitos primarios y secundarios, que se pueden encontrar en un sistema biológico. Xenobioquímica: es la disciplina que estudia el comportamiento metabólico de los compuestos cuya estructura química no es propia en el metabolismo regular de un organismo determinado. Pueden ser metabolitos secundarios de otros organismos (por ejemplo las micotoxinas, los venenos de serpientes y los fitoquímicos cuando ingresan al organismo humano) o compuestos poco frecuentes o inexistentes en la naturaleza. La farmacología es una disciplina que estudia a los xenobióticos que benefician al funcionamiento celular en el organismo debido a sus efectos terapéuticos o preventivos (fármacos). La farmacología tiene aplicaciones clínicas cuando las sustancias son utilizadas en el diagnóstico, prevención, tratamiento y alivio de síntomas de una enfermedad así como el desarrollo racional de sustancias menos invasivas y más eficaces contra dianas biomoleculares concretas. Por otro lado, la toxicología es el estudio que identifica, estudia y describe, la dosis, la naturaleza, la incidencia, la severidad, la reversibilidad y, generalmente, los mecanismos de los efectos adversos (efectos tóxicos) que producen los xenobióticos. Actualmente la toxicología también estudia el mecanismo de los componentes endógenos, como los radicales libres de oxígeno y otros intermediarios reactivos, generados por xenobióticos y endobióticos. Inmunología: área de la biología, la cual se interesa por la reacción del organismo frente a otros organismos como las bacterias y virus. Todo esto tomando en cuenta la reacción y funcionamiento del sistema inmune de los seres vivos. Es esencial en esta área el desarrollo de los estudios de producción y comportamiento de los anticuerpos. Endocrinología: es el estudio de las secreciones internas llamadas hormonas, las cuales son sustancias producidas por células especializadas cuyo fin es de afectar la función de otras células. La endocrinología trata la biosíntesis, el almacenamiento y la función de las hormonas, las células y los tejidos que las secretan, así como los mecanismos de señalización hormonal. Existen subdisciplinas como la endocrinología médica, la endocrinología vegetal y la endocrinología animal. Neuroquímica: es el estudio de las moléculas orgánicas que participan en la actividad neuronal. Este término es empleado con frecuencia para referir a los neurotransmisores y otras moléculas como las drogas neuro-activas que influencian la función neuronal. Quimiotaxonomía: es el estudio de la clasificación e identificación de organismos de acuerdo a sus diferencias y similitudes demostrables en su composición química. Los compuestos estudiados pueden ser fosfolípidos, proteínas, péptidos, heterósidos, alcaloides y terpenos. John Griffith Vaughan fue uno de los pioneros de la quimiotaxonomía. Entre los ejemplos de las aplicaciones de la quimiotaxonomía pueden citarse la diferenciación de las familias Asclepiadaceae y Apocynaceae según el criterio de la presencia de látex; la presencia de agarofuranos en la familia Celastraceae; las sesquiterpenlactonas con esqueleto de germacrano que son características de la familia Asteraceae o la presencia de abietanos en las partes aéreas de plantas del género Salvia del viejo Mundo a diferencia de las del Nuevo Mundo que presentan principalmente neo-clerodanos. Ecología química: es el estudio de los compuestos químicos de origen biológico implicados en las interacciones de organismos vivos. Se centra en la producción y respuesta de moléculas señalizadoras (semioquímicos), así como los compuestos que influyen en el crecimiento, supervivencia y reproducción de otros organismos (aleloquímicos). Virología: área de la biología, que se dedica al estudio de los biosistemas más elementales: los virus. Tanto en su clasificación y reconocimiento, como en su funcionamiento y estructura molecular. Pretende reconocer dianas para la actuación de posibles de fármacos y vacunas que eviten su directa o preventivamente su expansión. También se analizan y predicen, en términos evolutivos, la variación y la combinación de los genomas víricos, que podrían hacerlos finalmente, más peligrosos. Finalmente suponen una herramienta con mucha proyección como vectores recombinantes, y han sido ya utilizados en terapia génica. Imagen: Proteína mioglobina Genética molecular e ingeniería genética: es un área de la bioquímica y la biología molecular que estudia los genes, su herencia y su expresión. Molecularmente, se dedica al estudio del ADN y del ARN principalmente, y utiliza herramientas y técnicas potentes en su estudio, tales como la PCR y sus variantes, los secuenciadores masivos, los kits comerciales de extracción de ADN y ARN, procesos de transcripción-traducción in vitro e in vivo, enzimas de restricción, ADN ligasas… Es esencial conocer como el ADN se replica, se transcribe y se traduce a proteínas (Dogma Central de la Biología Molecular), así como los mecanismos de expresión basal e inducible de genes en el genoma. También estudia la inserción de genes, el silenciamiento génico y la expresión diferencial de genes y sus efectos. Superando así las barreras y fronteras entre especies en el sentido que el genoma de una especie podemos insertarlo en otro y generar nuevas especies. Uno de sus máximos objetivos actuales es conocer los mecanismos de regulación y expresión genética, es decir, obtener un código epigenético. Constituye un pilar esencial en todas las disciplinas biocientíficas, especialmente en biotecnología. La biotecnología moderna tiene múltiples aplicaciones y variadas e incluyen, además de la fabricación de medicamentos, alimentos, papel, entre otros, el mejoramiento de animales y plantas de interés agronómico. Biología Molecular: es la disciplina científica que tiene como objetivo el estudio de los procesos que se desarrollan en los seres vivos desde un punto de vista molecular. Así como la bioquímica clásica investiga detalladamente los ciclos metabólicos y la integración y desintegración de las moléculas que componen los seres vivos, la biología molecular pretende fijarse con preferencia en el comportamiento biológico de las macromoléculas (ADN, ARN, enzimas, hormonas, etc.) dentro de la célula y explicar las funciones biológicas del ser vivo por estas propiedades a nivel molecular. Biología celular: (antiguamente citología, de citos=célula y logos=Estudio o Tratado ) es un área de la biología que se dedica al estudio de la morfología y fisiología de las células procariotas y eucariotas. Trata de conocer sus propiedades, estructura, composición bioquímica, funciones, orgánulos que contienen, su interacción con el ambiente y su ciclo vital. Es esencial en esta área conocer los procesos intrínsecos a la vida celular durante el ciclo celular, como la nutrición, la respiración, la síntesis de componentes, los mecanismos de defensa, la división celular y la muerte celular. También se deben conocer los mecanismos de comunicación de células (especialmente en organismos pluricelulares) o las uniones intercelulares. Es un área esencialmente de observación y experimentación en cultivos celulares, que, frecuentemente, tienen como objetivo la identificación y separación de poblaciones celulares y el reconocimiento de orgánulos celulares. Algunas técnicas utilizadas en biología celular tienen que ver con el empleo de técnicas de citoquímica, siembra de cultivos celulares, observación por microscopía óptica y electrónica, inmunocitoquímica, inmunohistoquímica, ELISA o citometría de flujo."

ksampletext_wikipedia_medi_farmacologia: str = "Farmacología. La farmacología (del griego, pharmacon, fármaco y logos, ciencia) es la rama de las ciencias farmacéuticas que estudia la historia, el origen, las propiedades biofisicoquímicas, la presentación, los efectos fisiológicos, los mecanismos de acción, la absorción, la distribución, la biotransformación, la excreción y el uso terapéutico, entre otras actividades biológicas, de las sustancias químicas que interactúan con los organismos vivos. La farmacología estudia como interactúa el fármaco con el organismo, sus acciones, efectos y propiedades. En un sentido más estricto, se considera la farmacología como el estudio de los fármacos, sea que esas tengan efectos beneficiosos o bien tóxicos. La farmacología tiene aplicaciones clínicas cuando las sustancias son utilizadas en el diagnóstico, prevención y tratamiento de una enfermedad o para el alivio de sus síntomas. Historia Artículo principal: Historia de la farmacia Los orígenes de la farmacología clínica se remontan a la Edad Media, con la farmacognosia y El canon de medicina de Avicena, el Comentario de Pedro de España sobre Isaac Dager y el Comentario de Juan de San Amand sobre el Antedotario de Nicolás. La farmacología temprana se centró en el herbalismo y las sustancias naturales, principalmente extractos de plantas. Las medicinas fueron compiladas en libros llamados farmacopea. Las drogas crudas se han usado desde la prehistoria como una preparación de sustancias de fuentes naturales. Sin embargo, el ingrediente activo de las drogas crudas no se purifica y la sustancia se adulteraba con otras sustancias. La medicina tradicional varía entre culturas y puede ser específica de una cultura particular, como en la medicina tradicional china, mongol, tibetana y coreana. Sin embargo, gran parte de esto se ha considerado como pseudociencia. Las sustancias farmacológicas conocidas como enteógenos pueden tener un uso espiritual y religioso y un contexto histórico. En el siglo XVII, el médico inglés Nicholas Culpeper tradujo y usó textos farmacológicos, en los cuales detalló las plantas y las condiciones que podrían tratar. En el siglo XVIII, gran parte de la farmacología clínica fue establecida por el trabajo de William Withering. La farmacología como disciplina científica no avanzó más hasta mediados del siglo XIX, en medio del gran resurgimiento biomédico de ese período. Antes de la segunda mitad del siglo XIX, la notable potencia y especificidad de drogas como la morfina y la quinina, se explicaron vagamente y con referencia a poderes químicos extraordinarios y afinidades con ciertos órganos o tejidos. El primer departamento de farmacología fue creado por Rudolf Buchheim en 1847, en reconocimiento de la necesidad de comprender cómo las drogas terapéuticas y los venenos producen sus efectos. Posteriormente, el primer departamento de farmacología en Inglaterra se creó en 1905 en el University College de Londres. La farmacología se desarrolló en el siglo XIX como una ciencia biomédica que aplicaba los principios de la experimentación científica a los contextos terapéuticos. El avance de las técnicas de investigación impulsó la investigación farmacológica y su comprensión. El desarrollo de la preparación del baño de órganos, donde las muestras de tejido están conectadas a dispositivos de registro (como un miógrafo) y las respuestas fisiológicas se registran después de la aplicación del medicamento, permitió el análisis de los efectos de los medicamentos en los tejidos. El desarrollo del ensayo de unión al ligando en 1945, permitió la cuantificación de la afinidad de unión de los fármacos en objetivos químicos. Los farmacólogos modernos utilizan técnicas de genética, biología molecular, bioquímica y otras herramientas avanzadas para transformar información sobre mecanismos moleculares y objetivos en terapias dirigidas contra enfermedades, defectos o patógenos, y crear métodos para la atención preventiva, el diagnóstico y, en última instancia, la medicina personalizada. Divisiones La disciplina de la farmacología se puede dividir en muchas subdisciplinas, cada una con un enfoque específico. Sistemas del cuerpo La farmacología también puede centrarse en sistemas específicos que comprenden el cuerpo. Las divisiones relacionadas con los sistemas corporales estudian los efectos de las drogas en diferentes sistemas del cuerpo. Estos incluyen neurofarmacología, en el sistema nervioso central y periférico; inmunofarmacología en el sistema inmune. Otras divisiones incluyen farmacología cardiovascular, renal y endocrina. Psicofarmacología, es el estudio de los efectos de las drogas en la psique, la mente y el comportamiento, como los efectos conductuales de las drogas psicoactivas. Incorpora enfoques y técnicas de neurofarmacología, comportamiento animal y neurociencia conductual, y está interesado en los mecanismos de acción conductuales y neurobiológicos de las drogas psicoactivas. El campo relacionado de la neuropsicofarmacología se centra en los efectos de las drogas en la superposición entre el sistema nervioso y la psique. La farmacometabolómica, también conocida como farmacometabonómica, es un campo que se deriva de la metabolómica. Consiste en la medición directa de metabolitos en los fluidos corporales de un individuo, con el fin de predecir o evaluar el metabolismo de los compuestos farmacéuticos, y para comprender mejor el perfil farmacocinético de un medicamento. La farmacometabolómica se puede aplicar para medir los niveles de metabolitos después de la administración de un medicamento, con el fin de controlar los efectos del medicamento en las vías metabólicas. También estudia el efecto de las variaciones del microbioma en la disposición, acción y toxicidad del fármaco, así como la interacción entre las drogas y el microbioma intestinal. La farmacogenómica es la aplicación de tecnologías genómicas para el descubrimiento de fármacos y la caracterización adicional de fármacos relacionados con el genoma completo de un organismo. Para la farmacología con respecto a genes individuales, la farmacogenética estudia cómo la variación genética da lugar a diferentes respuestas a los fármacos. La farmacoepigenética estudia los patrones de marcado epigenético subyacentes que conducen a variaciones en la respuesta de un individuo al tratamiento médico. Práctica clínica y descubrimiento de fármacos Un toxicólogo trabajando en un laboratorio. La farmacología se puede aplicar dentro de las ciencias clínicas. La farmacología clínica es la ciencia básica de la farmacología que se centra en la aplicación de principios y métodos farmacológicos en la clínica médica y en la atención y los resultados del paciente. Un ejemplo de esto es la posología, que es el estudio de cómo se dosifican los medicamentos. La farmacología está estrechamente relacionada con la toxicología. Tanto la farmacología como la toxicología son disciplinas científicas que se centran en comprender las propiedades y acciones de los productos químicos. Sin embargo, la farmacología enfatiza los efectos terapéuticos de los químicos, usualmente drogas o compuestos que podrían convertirse en drogas, mientras que la toxicología es el estudio de los efectos adversos de los químicos y la evaluación de riesgos. El conocimiento farmacológico se utiliza para aconsejar farmacoterapia en medicina y farmacia. Destino de los fármacos en el organismo Cualquier sustancia que interactúa con un organismo viviente puede ser absorbida por este, distribuida por los distintos órganos, sistemas o espacios corporales, modificada por procesos químicos y finalmente expulsada. La farmacología estudia los procesos en la interacción de fármacos con el hombre y animales llamados procesos LADME que, en orden temporal, son los siguientes: liberación absorción distribución metabolismo excreción El estudio de estos procesos es lo que se conoce como farmacocinética. De la interacción de todos estos procesos, la farmacología puede predecir la biodisponibilidad y vida media de eliminación de un fármaco en el organismo dadas una vía de administración, una dosis y un intervalo de administración. Para que el fármaco ejerza su acción sobre este blanco, debe, generalmente, ser transportado a través de la circulación sanguínea. Absorción Para llegar a la circulación sanguínea el fármaco debe traspasar alguna barrera dada por la vía de administración, que puede ser: cutánea, subcutánea, respiratoria, oral, rectal, muscular, vía ótica, vía oftálmica, vía sublingual. O puede ser inoculada directamente a la circulación por la vía intravenosa. La farmacología estudia la concentración plasmática de un fármaco en relación con el tiempo transcurrido para cada vía de administración y para cada concentración posible, así como las distintas formas de uso de estas vías de administración. Distribución Una vez en la corriente sanguínea, el fármaco, por sus características de tamaño y peso molecular, carga eléctrica, pH, solubilidad, capacidad de unión a proteínas se distribuye entre los distintos compartimientos corporales. La farmacología estudia cómo estas características influyen en el aumento y disminución de concentración del fármaco con el paso del tiempo en distintos sistemas, órganos, tejidos y compartimientos corporales, como por ejemplo, en el líquido cefalorraquídeo, o en la placenta, etc. Metabolismo o biotransformación Muchos fármacos son transformados en el organismo debido a la acción de enzimas. Esta transformación puede consistir en la degradación; (oxidación, reducción o hidrólisis), donde el fármaco pierde parte de su estructura, o en la síntesis de nuevas sustancias con el fármaco como parte de la nueva molécula (conjugación). El resultado de la biotransformación puede ser la inactivación completa o parcial de los efectos del fármaco, el aumento o activación de los efectos, o el cambio por nuevos efectos dependientes de las características de la sustancia sintetizada. La farmacología estudia los mecanismos mediante los cuales se producen estas transformaciones, los tejidos en que ocurre, la velocidad de estos procesos y los efectos de las propias drogas y sus metabolitos sobre los mismos procesos enzimáticos. Excreción Finalmente, el fármaco es eliminado del organismo por medio de algún órgano excretor. Principalmente está el hígado y el riñón, pero también son importantes la piel, las glándulas salivales y lagrimales. Cuando un fármaco es suficientemente hidrosoluble, es derivado hacia la circulación sanguínea, por la cual llega a los riñones y es eliminado por los mismos procesos de la formación de la orina: filtración glomerular, secreción tubular y reabsorción tubular. Si el fármaco, por el contrario, es liposoluble o de tamaño demasiado grande para atravesar los capilares renales, es excretada en la bilis, llegando al intestino grueso donde puede sufrir de la recirculación enterohepática, o bien ser eliminado en las heces. La farmacología estudia la forma y velocidad de depuración de los fármacos y sus metabolitos por los distintos órganos excretores, en relación con las concentraciones plasmáticas del fármaco. El efecto de los fármacos, después de su administración, depende de la variabilidad en la absorción, distribución, metabolismo y excreción. Para que el fármaco alcance su sitio de acción, han de considerarse los siguientes factores: Tasa y grado de absorción a partir del sitio de aplicación. Tasa y grado de distribución en los líquidos y tejidos corporales. Tasa de biotransformación a metabolitos activos o inactivos. Tasa de excreción. Acción de los fármacos sobre el organismo Al estudio del conjunto de efectos sensibles y/o medibles que produce un fármaco en el organismo del ser humano o los animales, su duración y el curso temporal de ellos, se denomina farmacodinámica. Para este estudio, la farmacología entiende al sistema, órgano, tejido o célula destinatario del fármaco u objeto de la sustancia en análisis, como poseedor de receptores con los cuales la sustancia interactúa. La interacción entre sustancia y receptor es un importante campo de estudio, que entre otros aspectos, analiza: Cuantificación de la interacción droga/receptor. Regulación de los receptores, ya sea al aumento, disminución o cambio en el nivel de respuesta. Relación entre dosis y respuesta. La farmacodinámica, define y clasifica a los fármacos de acuerdo con su afinidad, potencia, eficacia y efectos relativos. Algunos de los índices importantes de estas definiciones son la DE50 y la DL50, que son las dosis mínimas necesarias para lograr el efecto deseado y la muerte respectivamente, en el 50% de una población determinada. La relación entre estos valores es el índice terapéutico. De acuerdo con el tipo de efecto preponderante de un fármaco, farmacodinámicamente se les clasifica en: Agonistas farmacológicos, si produce o aumenta el efecto. Antagonistas farmacológicos, si disminuye o elimina el efecto. La farmacodinámica estudia también la variabilidad en los efectos de una sustancia dependientes de factores del individuo tales como: edad, raza, gravidez, estados patológicos, etc. También existe un campo especial de estudio de los efectos farmacológicos de sustancias durante la gestación. En el ser humano, los efectos sobre el embrión y el feto de los fármacos es un campo de intenso estudio. Ramas de la farmacología Farmacocinética: el estudio de los procesos físico-químicos que sufre un fármaco cuando se administra o incorpora a un organismo. Estos procesos serían liberación, absorción, distribución, metabolización y eliminación. Farmacodinámica: ciencia que estudia el mecanismo de acción de los fármacos, es decir estudia como los procesos bioquímicos y fisiológicos dentro del organismo se ven afectados por la presencia del fármaco. Biofarmacia: el estudio de la biodisponibilidad de los fármacos. Farmacognosia: estudio de plantas medicinales y drogas que de ellas se derivan. Química farmacéutica: estudia los fármacos desde el punto de vista químico, lo que comprende el descubrimiento, el diseño, la identificación y preparación de compuestos biológicamente activos, la interpretación de su modo de interacción a nivel molecular, la construcción de su relación estructura-actividad y el estudio de su metabolismo. Farmacia galénica o Farmacotecnia: rama encomendada a la formulación de fármacos como medicamentos. Posología: el estudio de la dosificación de los fármacos. Toxicología: el estudio de los efectos nocivos o tóxicos de los fármacos. Farmacología clínica: evalúa la eficacia y la seguridad de la terapéutica por fármacos. Farmacovigilancia: es una disciplina que permite la vigilancia postcomercialización de los medicamentos a fin de detectar, prevenir y notificar reacciones adversas en grupos de pacientes. Cronofarmacología: El estudio de la correcta administración de medicamentos conforme al ciclo circadiano del ser humano, esto con el fin de maximizar la eficacia y disminuir los efectos colaterales. Margen e índice terapéutico Es un hecho práctico de todos conocido que al incrementar la dosis de un determinado fármaco, se incrementa el riesgo de producción de fenómenos tóxicos o adversos. Para evitar tal situación, los farmacólogos experimentales y clínicos hacen una evaluación de la seguridad del fármaco, con el fin de garantizar que con la dosis empleada se logre el efecto farmacológico deseado con reducción de riesgos de intoxicación. La evaluación más simple y sencilla es la conocida como Margen Terapéutico, que es el margen de dosis que oscila entre la dosis mínima y la dosis máxima terapéutica. De lo anterior se deriva que se puede dosificar un medicamento dentro de este margen, no teniendo sentido alguno el administrar una dosis superior a la máxima terapéutica, ya que con ella no obtendríamos un efecto superior, y nos acercamos a aquella dosis que puede ser tóxicas."
ksampletext_wikipedia_medi_epidemiologia: str = "Epidemiología. La epidemiología es una disciplina científica en el área de la salud pública, no solamente la medicina, que estudia la distribución, frecuencia, magnitud y factores determinantes de las enfermedades existentes en poblaciones humanas definidas. Rich la describió en 1979 como la ciencia que estudia la dinámica de salud en las poblaciones; por lo tanto involucra el análisis e interpretación de las personas que también están sanas.. Quien trabaja como profesional con especialidad en epidemiología se llama epidemiólogo/epidemióloga. Principios La epidemiología ,que, en sentido estricto, podría denominarse epidemiología humana, constituye una parte muy importante dentro de la salud pública, ocupa un lugar especial en la intersección entre las ciencias biomédicas y las ciencias sociales, e integra los métodos y principios de estas ciencias para estudiar la salud y controlar las enfermedades en grupos humanos bien definidos. Existe también una epidemiología veterinaria, que estudia los mismos aspectos en los padecimientos que afectan la salud de los animales; y también podría hablarse de una epidemiología zoológica y botánica, íntimamente relacionadas con la ecología. En epidemiología se estudian y describen las enfermedades que se presentan en una determinada población, para lo cual se tienen en cuenta una serie de patrones de enfermedad, que se reducen a tres aspectos: tiempo, lugar y persona: el tiempo que tarda en surgir, la temporada del año en la que surge y los tiempos en los que es más frecuente; el lugar (la ciudad, la población, el país, el tipo de zona) en donde se han presentado los casos, y las personas más propensas a padecerla (niños, ancianos, etc., según el caso). La epidemiología surgió del estudio de las epidemias de enfermedades infecciosas; de ahí su nombre. Ya en el siglo XX los estudios epidemiológicos se extendieron también a las enfermedades no infecciosas. Para el análisis adecuado de la información epidemiológica se requiere cada vez con mayor frecuencia un equipo multidisciplinario que prevea la participación de profesionales de otros ámbitos científicos, entre los cuales la demografía y la estadística son especialmente importantes. Ciencia Para causar una enfermedad, un patógeno debe crecer y reproducirse en el hospedador. Los epidemiólogos siguen por esta razón, la historia natural de los patógenos. En muchos casos, un patógeno individual no puede crecer fuera del hospedador; si el hospedador muere, el patógeno también muere. Asimismo, los patógenos que matan al hospedador antes de trasmitirlos a otro hospedador, terminarán por extinguirse. Por tanto, la mayoría de los patógenos dependientes del hospedador deben adaptarse a coexistir con el hospedador. Un patógeno bien adaptado vive en equilibrio con el hospedador, tomando lo que necesita para su existencia, y causando solo un mínimo de daño. Estos patógenos a veces pueden causar infecciones crónicas (infecciones de larga duración) en el hospedador. Cuando existe equilibrio entre el hospedador y el patógeno, ambos sobreviven. Por otra parte, el hospedador puede resultar dañado cuando su resistencia es baja, por factores como una dieta insuficiente, edad avanzada y otros agentes estresantes. Además, algunas veces emergen nuevos patógenos naturales para los cuales el hospedador individual, y algunas veces la especie entera, no ha desarrollado resistencia. Estos patógenos emergentes a menudo causan infecciones agudas, caracterizadas por un comienzo rápido y llamativo. En estos casos, los patógenos pueden actuar como fuerzas selectivas en la evolución del hospedador, igual que el hospedador, al desarrollar resistencia, puede ser una fuerza selectiva en la evolución de los patógenos. En los casos en los que el patógeno no depende del hospedador para sobrevivir, con frecuencia el patógeno puede causar una enfermedad aguda devastadora. Objetivos La epidemiología es parte importante de la salud pública y contribuye a: Definir los problemas e inconvenientes de salud importantes de una comunidad; Describir la historia natural de una enfermedad; Descubrir los factores que aumentan el riesgo de contraer una enfermedad (su etiología); Predecir las tendencias de una enfermedad; Determinar si la enfermedad o problema de salud es prevenible o controlable; Determinar la estrategia de intervención (prevención o control) más adecuada; Probar la eficacia de las estrategias de intervención; Cuantificar el beneficio conseguido al aplicar las estrategias de intervención sobre la población; Evaluar los programas de intervención; La medicina moderna, especialmente la mal llamada medicina basada en la evidencia (medicina factual o medicina basada en estudios científicos), está basada en los métodos de la epidemiología. Vocabulario Hay una serie de términos que tienen un significado específico para el epidemiólogo. Una enfermedad es una epidemia cuando ocurre en un número inusualmente alto de individuos de una población simultáneamente; una pandemia es una epidemia que se disemina ampliamente, usualmente por todo el mundo. Una enfermedad endémica es la que está constantemente presente en una población, aunque su incidencia suele ser baja. La incidencia de una enfermedad determinada, es el número de nuevos casos de una enfermedad individual en una población de un determinado período de tiempo. La prevalencia de una enfermedad dada, es el número total de casos nuevos y ya existentes informados en una población y durante un determinado período de tiempo. Un brote de una enfermedad ocurre cuando se observa un número de casos, por lo general en un período de tiempo relativamente corto, en un área geográfica que anteriormente solo había presentado casos esporádicos de la enfermedad. Mortalidad y morbilidad La mortalidad es la incidencia de muerte en la población. Las enfermedades infecciosas fueron la principal causa de la muerte en 1900 en los países desarrollados, pero ahora son mucho menos significativas. Hoy día, las enfermedades no infecciosas asociadas al estilo de vida, como las enfermedades cardíacas y el cáncer, son mucho más prevalentes y causan mayor mortalidad que las enfermedades infecciosas. Sin embargo, la situación actual podría cambiar rápidamente, si se llegaran a afectar en forma importante las infraestructuras y los servicios de salud públicas. En países en desarrollo, las enfermedades infecciosas son todavía la principal causa de mortalidad. La morbilidad se refiere a la incidencia de enfermedades en la población, incluyendo tanto enfermedades mortales como no mortales. Las estadísticas de la morbilidad definen la salud pública de una población con mayor precisión que las de mortalidad, porque muchas enfermedades tienen una mortalidad relativamente baja. Progresión de la enfermedad En términos de sintomatología clínica, el curso de una enfermedad infecciosa aguda puede dividirse en etapas: Infección: el microorganismo invade, coloniza y crece en el hospedador. Período de incubación: el período de tiempo entre la infección y la aparición de los síntomas de la enfermedad. Período agudo: la enfermedad está en su punto culminante, con síntomas claros como fiebre y escalofríos. Período de declive: los síntomas de enfermedad están cediendo, la fiebre disminuye, usualmente después de un período de sudoración intensa, y aparece una sensación de bienestar. Período de convalecencia: el enfermo recupera las fuerzas y vuelve a la normalidad. Metodología La epidemiología se basa en el método científico para la obtención de conocimientos, a través de los estudios epidemiológicos. Ante un problema de salud, y los datos disponibles sobre el mismo, se formula una hipótesis, la cual se traduce en una serie de consecuencias contrastables mediante experimentación. Se realiza entonces un proyecto de investigación que comienza con la recolección de datos y su posterior análisis estadístico, que permite obtener medidas de asociación (odds ratio, riesgo relativo, razón de tasas), medidas de efecto (riesgo atribuible) y medidas de impacto (fracción etiológica o riesgo atribuible proporcional), tanto a nivel de los expuestos como a nivel poblacional. De los resultados de esta investigación es posible obtener conocimientos que servirán para realizar recomendaciones de salud pública, pero también para generar nuevas hipótesis de investigación. En la literatura científica reciente se encuentran varios artículos de revisión, revisados por pares, que proveen una valiosa descripción general de las diferentes metodologías de la epidemiología. Etiología de las enfermedades Artículo principal: Etiología Mapa original del Dr. John Snow. Los puntos muestran los casos de muerte por cólera durante la epidemia ocurrida en Londres en 1854. Las cruces representan los pozos de agua de los que bebieron los enfermos. El triángulo epidemiológico causal de las enfermedades está formado por el medio ambiente, los agentes y el huésped. Un cambio en cualquiera de estos tres componentes alterará el equilibrio existente para aumentar o disminuir la frecuencia de la enfermedad, por lo tanto se pueden llamar factores causales o determinantes de la enfermedad. Las bases de la epidemiología moderna fueron sentadas por Girolamo Fracastoro (Verona, 1487-1573) en sus obras De sympathia et antipathia rerum (Sobre la simpatía y la antipatía de las cosas) y De contagione et contagiosis morbis, et eorum curatione (Sobre el contagio y las enfermedades contagiosas y su curación), ambas publicadas en Venecia en 1546, donde Fracastoro expone sucintamente sus ideas sobre el contagio y las enfermedades transmisibles. Se considera al inglés John Graunt (1620-1674) quien publicó en 1662 el libro Natural and Political Observations Made upon the Bills of Mortality ,sobre Londres, uno de los precursores de la epidemiología y de la demografía. Sin embargo, es John Snow (1813-1858), a quien se considera el precursor de la epidemiología contemporánea, ya que formuló la hipótesis de la transmisión del cólera por el agua y lo demostró confeccionando un mapa de Londres, en donde un reciente brote epidémico había matado más de 500 personas en un período de 10 días. Snow marcó en el mapa los hogares de los que habían muerto. La distribución mostraba que todas las muertes habían ocurrido en el área de Golden Square. La diferencia clave entre este distrito y el resto de Londres era el origen del agua potable. La compañía de agua privada que suministraba al vecindario de Golden Square extraía el agua de una sección del Támesis especialmente contaminado. Cuando se cambió el agua y comenzó a extraerse río arriba, de una zona menos contaminada, cedió la epidemia de cólera. Un progreso muy importante en el siglo XX, publicado en 1956 con los resultados del estudio de médicos británicos, fue la demostración de la relación causal entre fumar (tabaquismo) y el cáncer de pulmón. Transición epidemiológica Constituye un proceso de cambio dinámico a largo plazo en la frecuencia, magnitud y distribución de la morbilidad y mortalidad de la población. La transición epidemiológica, que va acompañada por la transición demográfica, presenta cuatro aspectos a destacar: Desplazamiento en la prevalencia de las enfermedades transmisibles por las no trasmisibles. Desplazamiento en la morbilidad y mortalidad de los grupos jóvenes a los grupos de edad avanzada. Desplazamiento de la mortalidad como fuerza predominante por la morbilidad, sus secuelas e invalideces. Polarización epidemiológica. La polarización epidemiológica sucede cuando en distintas zonas de un país o en distintos barrios de una misma ciudad encontramos diferencias en la morbilidad y mortalidad de la población. Ramas relacionadas Epidemiología descriptiva: es la rama de la epidemiología que describe el epidemiológico en tiempo, lugar y persona, cuantificando la frecuencia y distribución del fenómeno mediante medidas de incidencia, prevalencia y mortalidad, con la posterior formulación de hipótesis. Epidemiología analítica: busca, mediante la observación o la experimentación, establecer posibles relaciones causales entre factores a los que se exponen personas y poblaciones y las enfermedades que presentan. Las medidas empleadas en el estudio de esta rama de la epidemiología son los factores de riesgo, cuyo resultado es una probabilidad. Es posible distinguir dos tipos: riesgo absoluto y riesgo relativo. Riesgo absoluto: probabilidad de una enfermedad (baja, moderada, alta); si se considera la probabilidad de la enfermedad durante un periodo de tiempo, de lo que se está hablando es de una incidencia y no de un riesgo absoluto. Riesgo relativo: cuando se comparan dos riesgos absolutos entre sí; se trata de una probabilidad relativa (más alta o más baja que el otro); se ha de tener en cuenta que un riesgo relativo, por muy alto que sea, puede ser irrelevante; por ejemplo, fumar aumenta 100 veces el riesgo de sufrir una enfermedad, el riesgo sin fumar es de 1/100 000 000, por lo que el incremento por fumar es muy pequeño, prácticamente despreciable. Riesgo atribuible: en una población expuesta a un factor de riesgo, es la diferencia entre la incidencia de enfermedad en expuestos y no expuestos al factor de riesgo. La diferencia entre ambos valores proporciona el valor del riesgo de enfermedad en la cohorte expuesta, que se debe exclusivamente a la exposición al factor de riesgo. Epidemiología experimental: busca, mediante el control de las condiciones del grupo a estudiar, sacar conclusiones más complejas que con la mera observación no son deducibles. Se basa en el control de los sujetos a estudiar y en la aleatorización de la distribución de los individuos en dos grupos, un grupo experimental y un grupo control. Se ocupa de realizar estudios en animales de laboratorio y estudios experimentales con poblaciones humanas. Ecoepidemiología: busca, mediante herramientas ecológicas, estudiar integralmente como interaccionan los factores ambientales con las personas y poblaciones en los medios que los rodean y como ello puede influir en la evolución de enfermedades que se producen como consecuencia de dicha interacción."
ksampletext_wikipedia_medi_medicinainterna: str = "Medicina interna. La medicina interna es una especialidad médica que atiende integralmente los problemas de salud en pacientes adultos, ingresados en un centro hospitalario o en consultas ambulatorias. Objetivos Guía al enfermo en su compleja trayectoria por el sistema sanitario hospitalario, dirigiendo y coordinando la actuación frente a su enfermedad y coordinando al resto de especialistas necesarios para obtener un diagnóstico y tratamiento adecuados. Los médicos internistas son los expertos a quienes recurren los médicos de atención primaria y el resto de especialistas para atender a enfermos complejos cuyo diagnóstico es difícil, que se encuentran afectados por varias enfermedades o que presentan síntomas en varios órganos, aparatos o sistemas del organismo. Dentro de la extensa formación de los internistas, existe la posibilidad de que algunos de ellos se subespecialicen en ciertos campos de la medicina, focalizándose únicamente en ellos, como el control de los factores de riesgo cardiovascular, enfermedades infecciosas y muy especialmente el VIH, la insuficiencia cardiaca congestiva, la enfermedad tromboembólica venosa y enfermedades autoinmunes, cuidados paliativos o unidades de pacientes crónicos complejos. Generalmente el médico internista requiere la atención de otros especialistas a la hora de la realización de pruebas diagnósticas, como a Radiología en caso de necesitar un TAC o una RM, al Digestivo para endoscopias, al cirujano para toma de biopsias, etc., de tal manera que de forma coordinada selecciona las pruebas que más convengan para lograr un diagnóstico certero del paciente. Historia Artículo principal: Historia de la Medicina A finales del siglo XIX comenzó a desarrollarse la medicina hospitalaria, muy unida a las clínicas universitarias, y surgió una nueva orientación en la medicina general, más ligada a las ciencias básicas biomédicas y a la experimentación, que recibió el nombre de Medicina Interna. El internista ha sido considerado, desde entonces, el clínico por excelencia. Dentro de este campo quedaron excluidas las enfermedades quirúrgicas, las obstétricas y las pediátricas, que, asimismo, constituyeron otras especialidades. Éstas, junto con la Medicina Interna, han sido consideradas, desde esa época, como especialidades básicas. La denominación de Medicina Interna parece que tuvo su origen en Alemania, en 1880. En ese año, Strumpell escribió el primer tratado de Enfermedades Internas y, 2 años más tarde, en Wiesbaden, se celebró el I Congreso de Medicina Interna. Se quería indicar un campo de la práctica médica en el que los conceptos se basaban en el nuevo conocimiento que emergía en fisiología, bacteriología y patología, así como la exclusión de los métodos quirúrgicos en la terapéutica empleada. Este nuevo campo también llevaba la connotación de una formación académica y un entrenamiento. Además, estos médicos podían hacer de consultantes de otros especialistas. Es decir, la medicina interna sería como la medicina que trata enfermedades desde dentro, desde el interior del cuerpo, generalmente con medicamentos, en contraposición con la cirugía que trata las enfermedades desde fuera, con intervenciones quirúrgicas. A partir de la segunda mitad del siglo XX surgen las especialidades médicas, ramas de la medicina interna. Se puede caer en el error, que perjudica seriamente al paciente, de que los especialistas no se responsabilicen de pacientes que caigan fuera del área de su particular competencia y cada vez ha sido más frecuente que a un mismo enfermo lo estén atendiendo múltiples especialistas, con los más diversos y, a veces, contradictorios enfoques.[cita requerida] Características La medicina interna es la especialidad de la medicina que se encarga de mantener la homeostasis del medio interno. Históricamente es una especialidad exclusivamente hospitalaria, aunque existen tendencias actuales en otras direcciones: consultas en centros periféricos de especialidades, hospitalización domiciliaria con equipos liderados por internistas, e integración en los equipos de Atención Primaria para colaborar como consultores. Un especialista en medicina interna o médico internista no es un médico interno: En España, los médicos internos residentes (MIR) son los médicos que, una vez superada una carrera teórica general en medicina y cirugía de seis años, deben superar el examen MIR y formarse durante 5 años para conseguir una especialidad de médico internista. En México, el médico interno (también conocido como Médico Interno de Pregrado) es aquel que cursa el quinto o sexto año de la carrera de médico cirujano (que dependiendo la universidad tiene una duración de 6 o 7 años) y un médico residente es aquel que, después de haber terminado la carrera de médico cirujano, cursa una especialidad médica (tras haber aprobado el respectivo Examen Nacional de Aspirantes a las Residencias Médicas). En el caso de la Medicina Interna, actualmente tiene una duración de 4 años, realizándose en el último año el servicio social con una duración de 3 a 4 meses, en alguna comunidad rural o ciudad del interior del país. Al término de su especialidad, se le da el diploma correspondiente a la especialidad de Medicina Interna. En el habla popular se le conoce como médico internista."
ksampletext_wikipedia_medi_neurologia: str = "Neurología. La neurología es la rama de la medicina que estudia el sistema nervioso. Específicamente se ocupa de la prevención, diagnóstico, tratamiento y rehabilitación de todas las enfermedades que involucran al sistema nervioso central, sistema nervioso periférico y el sistema nervioso autónomo. Existe gran número de enfermedades neurológicas, las cuales pueden afectar el sistema nervioso central (cerebro y médula espinal), el sistema nervioso periférico, o el sistema nervioso autónomo.[cita requerida] En España, la neurología como especialidad médica nació en el Hospital de la Santa Creu i Sant Pau de Barcelona (entonces Hospital de la Santa Creu) en 1882 de la mano del Dr. Lluís Barraquer i Roviralta. En un primer momento este servicio del Hospital de la Sant Creu se llamó Dispensario de Electroterapia, una consulta dedicada a pacientes con patologías del cerebro y de la médula espinal. Años después, el dispensario se llamó Servicio de Neurología y Electroterapia y finalmente acabó perdiendo la segunda denominación y se quedó como Servicio de Neurología. El Dr. Barraquer introdujo, por primera vez en el país, la posibilidad de intervenir determinadas lesiones cerebrales y de ofrecer a estos pacientes la única posibilidad de curación que existía para ellos. A partir de 1910, se intervinieron una serie de pacientes con epilepsia focal, casi todos de origen traumático, que consistían en excisiones de las áreas corticales afectadas por la lesión de la cicatriz, lo que favoreció el nacimiento de la neurocirugía. Uno de sus máximos exponentes fue el Dr. Manuel Corachán i Llort (hijo del Dr. Manuel Corachán Garcia), que en 1936 ya practicaba regularmente estas intervenciones en Sant Pau y que murió durante la guerra civil española. Diagnóstico del sujeto con enfermedad neurológica Método clínico en la neurología El objetivo del método clínico en la neurología es servir como base para el tratamiento o la prevención de alguna enfermedad neurológica. En la mayoría de los casos el método consiste en cinco etapas, las cuales son: Pasos para diagnosticar una enfermedad neurológica. Principios de neurología por Adams y Víctor. Identificación de síntomas y signos mediante el interrogatorio y la exploración física. Los síntomas y signos físicos que se consideran importantes respecto al problema en cuestión son interpretados en términos fisiológicos y anatómicos: identificación de trastornos de la función y de la estructura anatómica involucrada. Diagnóstico anatómico/topográfico: Localización del proceso patológico (identificación de las partes del Sistema Nervioso afectadas), donde se reconoce un grupo característico de síntomas y signos, los cuales constituyen un síndrome, lo que nos ayuda a identificar el lugar y la naturaleza de la enfermedad. A esto se le conoce como diagnóstico sindrómico. A partir del diagnóstico anatómico y otros datos médicos (modo, rapidez de inicio, evolución, curso de la enfermedad, afección de sistemas orgánicos extraneurológicos, antecedentes personales y familiares y datos de laboratorio) es posible deducir el diagnóstico patológico. Cuando se identifica el mecanismo y la causalidad de la enfermedad se puede determinar el diagnóstico etiológico. Elaboración del diagnóstico funcional. Esta última etapa se refiere a la valoración del grado de incapacidad, donde se determina si este es temporal o permanente. Es de gran importancia para el tratamiento de la enfermedad y para la estimación del potencial de restablecimiento de la función, es decir, el pronóstico. El método precedente para el diagnóstico de las enfermedades neurológicas puede verse resumido en el diagrama colocado en esta sección. Este enfoque sistemático permite identificar de manera confiable la localización y a menudo el diagnóstico preciso de la enfermedad. Cabe recordar que no siempre es necesario plantear de esta forma la solución a un problema clínico, ya que algunas enfermedades neurológicas tienen cuadros clínicos muy característicos. Exploración neurológica Una investigación (1897), obra de Joaquín Sorolla. La pintura muestra el interior del laboratorio del neurólogo Luis Simarro a finales del siglo XIX. Durante un examen neurológico, el neurólogo revisa la historia médica del paciente, con especial atención a sus condiciones recientes. Después le realiza un examen neurológico. Habitualmente, este examen neurológico evalúa el estado mental, las funciones de los nervios craneales, el sistema motor y el sistema sensitivo. Esta información ayuda al neurólogo a determinar si el problema se halla en el sistema nervioso y su localización clínica. La localización de la patología es la clave del proceso por el cual los neurólogos desarrollan sus diferentes diagnósticos. Pueden ser necesarios estudios posteriores para confirmar el diagnóstico, y finalmente una guía y terapia apropiada. La exploración neurológica se inicia con la exploración del paciente en tanto se practica el interrogatorio. La manera en que el paciente cuenta su enfermedad puede manifestar confusión o incoherencia del pensamiento, trastornos de la memoria o del juicio e incluso dificultades para comprender o expresar ideas. El resto de la exploración neurológica debe efectuarse como la última parte de la exploración física general a partir de, como ya se mencionó, la exploración de nervios craneales, cuello y tronco hasta terminar con las pruebas de las funciones motora, refleja y sensitiva de las extremidades superiores e inferiores. Dicha exploración debe modificarse según el estado del paciente. Desde luego muchas partes de la exploración no pueden efectuarse en el paciente comatoso; niños pequeños y lactantes o pacientes con padecimientos psiquiátricos necesitan explorarse de maneras especiales. Procedimientos de exploración y diagnóstico Pruebas de los nervios craneales: la función de los nervios craneales debe investigarse de manera más compleja en los pacientes que presentan síntomas neurológicos que en aquellos que no los experimentan. Si se sospecha una lesión de la fosa anterior debe someterse a prueba el sentido del olfato a través de cada fosa nasal, determinando si el paciente puede distinguir los olores. Los campos visuales se trazan mediante pruebas de confrontación, en algunos casos por investigación de cada ojo por separado buscando cualquier anomalía. La sensibilidad de la cara se somete a prueba con un alfiler y un poco de algodón, debe determinarse la presencia o ausencia de reflejos corneales. Se observan los movimientos faciales cuando el paciente habla y sonríe ya que la debilidad ligera puede ser más evidente en estas circunstancias. Es necesario inspeccionar las cuerdas vocales con instrumentos especiales en caso de sospecha de padecimiento del bulbo raquídeo o del nervio vago sobre todo cuando se presenta ronquera. Pruebas de la función motora: se deben tomar en cuenta las observaciones de la rapidez y fuerza de los movimientos, tamaño, tono y coordinación muscular. Posiciones prona y supina. Es esencial que el paciente exponga por completo las extremidades para inspeccionarlas por atrofia y fasciculaciones así como para observarlas mientras conserva los brazos estirados en las posiciones prona y supina; que el individuo efectué tareas sencillas como alternar el contacto con su nariz y con el dedo del examinador; hacer que realice movimientos alternos rápidos particularmente los que involucran cambios de dirección, aceleración y desaceleración súbita; que el pulgar toque rápidamente la punta de cada uno de los dedos y efectué movimientos de supinación y pronación del antebrazo; además que complete tareas sencillas como abotonarse la ropa, abrir un broche o manipular herramientas comunes. Pruebas de la función refleja: las pruebas de los reflejos bicipital, tricipital, supinador, rotuliano, aquíleo, cutáneo abdominal y plantar permiten obtener una idea de lo adecuada que es la actividad refleja de la medula espinal. Los reflejos tendinosos requieren que los músculos afectados estén relajados; los reflejos hipoactivos o que apenas pueden descartarse suelen facilitarse mediante contracción voluntaria de otros músculos. La presencia de reflejos cutáneos superficiales de los músculos abdominales, cremasterianos y de otros tipos suele constituir una prueba básica de gran utilidad para identificar lesiones corticospinales. Pruebas de la función sensitiva: esta es la parte más complicada de la exploración neurológica, se reserva para la parte final de la exploración y no debe prolongarse durante más de unos pocos minutos si se requiere que los datos sean confiables. Por lo general se buscan diferencias entre ambos lados del cuerpo, el nivel por debajo del cual se pierde la sensación o la existencia de una zona de anestesia relativa o absoluta. Se explica al paciente con brevedad cada prueba; hablar demasiado sobre estas pruebas con un paciente introspectivo meticuloso puede animarlo para que notifique variaciones menores independientemente de la intensidad del estímulo. No es necesario explorar todas las regiones superficiales de la piel, la investigación rápida de cara, cuello, manos, tronco y pies con un alfiler requiere solo unos cuantos segundos. Las regiones con déficit sensitivo pueden someterse a otras pruebas. El descubrimiento de alguna zona con hiperestesia dirige la atención a un trastorno de sensibilidad superficial. Exploración de la estación y la marcha: ninguna exploración está completa sino se observa al paciente en posición erguida. Quizá la anomalía neurológica más destacada o la única sea la anormalidad de la bipedestación y la marcha, como sucede en algunos trastornos cerebelosos o del lóbulo frontal. Además una alteración de la postura y los movimientos de adaptación automáticos puros al caminar proporciona la pista diagnostica más definitiva en la etapa inicial de la enfermedad de Parkinson y de la parálisis supranuclear progresiva. El paciente médico o quirúrgico sin síntomas neurológicos: para las extremidades superiores suele ser suficiente la observación de los brazos desnudos y estirados en busca de atrofia, debilidad (impulso pronador), temblor o movimientos anormales; la verificación de la fuerza, empuñadura y dorsiflexión a nivel de la muñeca; inquirir acerca de los trastornos sensitivos y desencadenar los reflejos supinador, bicipital y tricipital. El desencadenamiento de los reflejos rotuliano, aquíleo y plantar; las pruebas de vibración y sentido de posición en los dedos de las manos y pies, y la valoración de la coordinación haciendo que el paciente toque de forma alternada su nariz y un dedo del examinador, así como que deslice el talón hacia arriba y abajo por el frente de la pierna opuesta. El paciente comatoso: la exploración cuidadosa del paciente en estupor o comatoso ofrece información considerable en cuanto a la función del sistema nervioso. Se deben reconocer las posturas predominantes de las extremidades y el cuerpo; la presencia o ausencia de movimientos espontáneos en un lado; la posición de la cabeza y los ojos, la velocidad, profundidad y ritmo de la respiración. Se valora la reacción que tiene el paciente al oír su nombre, órdenes sencillas o a estímulos nocivos. Por lo regular es posible determinar si el coma está relacionado con irritación meníngea o enfermedad cerebral focal o del tallo cerebral. En las etapas menos profundas del coma la irritación meníngea produce una resistencia a la flexión pasiva del cuello pero no a la extensión, rotación o inclinación de la cabeza. Diagnóstico de laboratorio: la descripción del método clínico y su aplicación evidencia que la exploración clínica rigurosa debe preceder siempre al empleo de los auxiliares de laboratorio, sin embargo en neurología la finalidad de estos es la prevención. Por tanto en la neurología preventiva la metodología de laboratorio puede adquirir prioridad sobre la metodología clínica. La información genética permite al neurólogo identificar a los pacientes en peligro de desarrollar ciertas enfermedades para iniciar de inmediato la búsqueda de marcadores biológicos antes que los síntomas o signos aparezcan. Las pruebas de investigación bioquímica son aplicables para toda una población y permiten identificar en individuos que aún no muestran síntomas, y en algunas de estas enfermedades es posible aplicar un tratamiento antes de que se sufra una lesión en el sistema nervioso. Trabajo clínico Casos en general Los neurólogos son responsables del diagnóstico, tratamiento y manejo de todas las condiciones mencionadas arriba. Cuando la intervención quirúrgica es requerida, el neurólogo puede referirse al paciente como «neuropaciente». En algunos países, algunas responsabilidades legales de un neurólogo pueden incluir efectuar un diagnóstico de muerte cerebral si el paciente fallece. Suelen tratar personas con enfermedades congénitas si la mayor parte de las manifestaciones son neurológicas. Las punciones lumbares también pueden ser realizadas por estos profesionales. Algunos neurólogos desarrollan un interés a subcampos en particular como las enfermedades cerebrovasculares, los trastornos del movimiento, epilepsia, cefaleas, neurología de la conducta y demencias, trastornos del sueño, control de dolor crónico, esclerosis múltiple o enfermedades neuromusculares. Áreas destacadas Hay superposición de otras especialidades, variando de país en país e incluso en un área geográfica local. El traumatismo craneoencefálico (ETC) agudo es más comúnmente tratado por neurocirujanos, mientras que secuelas de traumas craneoencefálicos pueden ser tratados por neurólogos o especialistas en rehabilitación médica. Aunque los casos de accidente cerebrovascular (ACV) han sido tradicionalmente tratados por médicos internistas u hospitalarios, el surgimiento de neurología vascular y neurólogos intervencionistas han creado una demanda para especialistas en ACV. La organización de JHACO centro certificado en accidentes cerebrovasculares ha incrementado el papel de los neurólogos en el tratamiento de accidentes cerebrovasculares en muchos centros de atención primaria, así como en hospitales de tercer nivel. Algunos casos de enfermedades infecciosas del sistema nervioso son tratados por especialistas en enfermedades infecciosas. La mayoría de los casos de dolor de cabeza son diagnosticados y tratados principalmente por médicos generales, al menos los casos menos severos. Del mismo modo, la mayoría de los casos de ciática y otras radiculopatías mecánicas son atendidos por médicos generales, aunque pueden ser enviados a neurólogos o cirujanos (neurocirujanos o cirujanos ortopédicos). Los trastornos del sueño generalmente son tratados en unidades multidisciplinares en las que participan neurólogos, neumólogos y psiquiatras. Una parálisis cerebral es inicialmente atendida por pediatras, pero el tratamiento puede ser transferido a un neurólogo de adultos después de que el paciente alcanza una cierta edad. Los neuropsicólogos clínicos son usualmente consultados para realizar una evaluación funcional del comportamiento y funciones cognitivas superiores, relacionada con la asistencia en diagnósticos diferenciales, la planificación de estrategias de rehabilitación, el registro de fuerzas y debilidades cognitivas, y la medición de cambios en el tiempo (por ejemplo, para identificar anomalías de envejecimiento o llevando el progreso de una demencia). Relaciones a la neurofisiología clínica En algunos países como Estados Unidos y Alemania, los neurólogos se pueden especializar en neurofisiología clínica, en electroencefalografía, o en el estudio de la conducción nerviosa, en Electromiografías y potenciales evocados. En otros países, es una especialidad independiente (por ejemplo en el Reino Unido y Suecia). Superposición con la psiquiatría A pesar de que las enfermedades mentales son consideradas por algunos de ser desórdenes neurológicos afectando el sistema nervioso central, tradicionalmente se las clasifica por separado, y son tratadas por psiquiatras. En el año 2002, en una reseña del American Journal of Psychiatry, el profesor Joseph B. Martin, decano de Harvard Medical School y neurólogo de profesión, escribió que: «la división en dos categorías es arbitraria, a menudo influenciada por creencias más que por observaciones científicas verificables. Y el hecho de que el cerebro y la mente sean uno solo, hace que esta división sea solamente artificial de todas formas». Esta perspectiva ha propiciado un progresivo acercamiento entre ambas especialidades en las últimas dos décadas, que finalmente se materializó en 2004 con el reconocimiento, en Estados Unidos, de la subespecialidad en «Neurología de la conducta y Neuropsiquiatría». Actualmente, los médicos de esta subespecialidad se encargan del estudio, diagnóstico y tratamiento de las alteraciones de la conducta y los trastornos mentales atribuibles a enfermedades neurológicas. Las enfermedades neurológicas a menudo tienen manifestaciones psiquiátricas, como por ejemplo psicosis, depresión, manía y ansiedad. Estos síndromes neuropsiquiátricos son relativamente habituales en pacientes con ictus, enfermedad de Huntington, parkinsonismos, enfermedad de Alzheimer, enfermedad por cuerpos de Lewy, enfermedad de Pick, encefalitis infecciosas, encefalitis autoinmunes, así como en algunos tipos de epilepsia, por nombrar solo algunas. Efectos del envejecimiento sobre el sistema nervioso Vejez, Emily Samson en la bienvenida al nuevo mundo. De todos los cambios vinculados con la edad tienen una enorme importancia los que tiene el sistema nervioso, algunos signos neurológicos del envejecimiento son: los signos neurooftalmológicos, pérdida de la audición perceptiva progresiva, disminución del sentido del olfato y menor extensión del gusto, reducción de la velocidad y magnitud de actividad motora, tiempo de reacción lento, trastornos de coordinación y agilidad, reducción de la fuerza muscular y adelgazamiento de los músculos, cambios de los reflejos tendinosos y finalmente trastornos del sentido de vibración en los dedos de los pies y en tobillos. Pareja de ancianos. Roger Hsu. Neurología cosmética El emergente campo de la neurología cosmética señala el potencial de terapias para mejorar cuestiones como la eficacia laboral, la atención en la escuela, y una mayor felicidad en la vida personal. A pesar de todo, este campo ha dado también lugar a preguntas acerca de la neuroética o la psicofarmacología. Temas relacionados Temas clásicos Neuroanatomía Neuropediatría Neurootología Neuropsicología y Neurología de la conducta Semiología Métodos Diagnósticos Tomografía axial computarizada Angiografía cerebral Imagen por resonancia magnética (IRM) Electromiografía Tomografía por emisión de positrones Punción lumbar Biopsia cerebral Enfermedades del sistema nervioso central Afasia Anomalías del desarrollo del sistema nervioso central Enfermedades carenciales del sistema nervioso Degeneración combinada subaguda de la médula espinal Encefalopatía de Wernicke Enfermedades cerebrovasculares Enfermedades de la médula espinal Siringomielia Hernia discal Mielitis transversa Enfermedades degenerativas del sistema nervioso central Enfermedad de Alzheimer Atrofia multisistémica Parálisis supranuclear progresiva Enfermedad de Parkinson Esclerosis lateral amiotrófica Enfermedad de Huntington Enfermedades del sistema extrapiramidal Enfermedades desmielinizantes del sistema nervioso central Esclerosis múltiple Enfermedad de Devic Esclerosis concéntrica de Baló Encefalomielitis diseminada aguda Enfermedades infecciosas del sistema nervioso central Meningitis Absceso Cerebral Toxoplasmosis cerebral Encefalitis Enfermedades metabólicas del sistema nervioso central Epilepsias Traumatismos craneoencefálicos Tromboembolismo intracraneal Tumor intracraneal Meningioma Pinealoma Ependimoma Astrocitoma Meduloblastoma Oligodendroglioma Enfermedades del sistema nervioso periférico Síndrome de Guillain-Barré Síndrome de Charcot-Marie-Tooth Enfermedades musculares o miopatías Distrofia muscular de Duchenne Distrofia miotónica de Steinert Enfermedades de la unión neuromuscular Miastenia grave Síndrome miasténico de Lambert-Eaton."

ksampletext_wikipedia_geol_tectonicadeplacas: str = "Tectónica de placas. La tectónica de placas o tectónica global es una teoría que explica la forma en que está estructurada la litosfera (porción externa más fría y rígida de la Tierra). La rama de la Geología que se encarga de su estudio es la tectónica. La teoría da una explicación a las placas tectónicas que forman parte de la superficie de la Tierra y a los deslizamientos que se observan entre ellas en su movimiento sobre el manto terrestre fluido, sus direcciones e interacciones. También explica la formación de las cadenas montañosas (orogénesis). Asimismo, da una explicación satisfactoria al hecho de que los terremotos y los volcanes se concentran en regiones concretas del planeta (como el Cinturón de Fuego del Pacífico) o a la ubicación de las grandes fosas oceánicas junto a los arcos insulares y continentes y no en el centro del océano. Las placas tectónicas se desplazan unas respecto de otras con relativa lentitud, a una velocidad nunca perceptible sin instrumentos, pero con tasas diferentes. La mayor velocidad se da en la dorsal del Pacífico Oriental, cerca de la Isla de Pascua, a unos 3400 km de Chile continental, con una velocidad de separación entre placas de más de 15 cm/año y la más lenta se da en la dorsal ártica, con menos de 2,5 cm/año. Dado que se desplazan sobre la superficie finita de la Tierra, las placas interaccionan unas con otras a lo largo de sus límites provocando intensas deformaciones en la corteza y litosfera de la Tierra, lo que ha dado lugar a la formación de grandes cadenas montañosas (por ejemplo las cordilleras de Himalaya, Alpes, Pirineos, Atlas, Urales, Apeninos, Apalaches, Andes, entre muchos otros) y grandes sistemas de fallas asociadas con estas (por ejemplo, el sistema de fallas de Anatolia del Norte). El contacto por fricción entre los bordes de las placas es responsable de la mayor parte de los terremotos. Otros fenómenos asociados son la creación de volcanes (especialmente notorios en el cinturón de fuego del océano Pacífico) y las fosas oceánicas. Las placas tectónicas se componen de dos tipos distintos de litosfera: la corteza continental, más gruesa, y la corteza oceánica, la cual es relativamente delgada. A la parte superior de la litosfera se la conoce como corteza terrestre, nuevamente de dos tipos (continental y oceánica). Esto significa que una placa litosférica puede ser continental, oceánica, o bien de ambos tipos, en cuyo caso se denomina placa mixta. Uno de los principales puntos de la teoría propone que la cantidad de superficie de las placas (tanto continental como oceánica) que desaparecen en el manto a lo largo de los bordes convergentes de subducción está más o menos en equilibrio con la corteza oceánica nueva que se está formando a lo largo de los bordes divergentes (dorsales oceánicas) a través del proceso conocido como expansión del fondo oceánico. También se suele hablar de este proceso como el principio de la cinta transportadora. En este sentido, el total de la superficie en el globo se mantiene constante, siguiendo la analogía de la cinta transportadora, siendo la corteza la cinta que se desplaza gracias a las fuertes corrientes convectivas de la astenosfera, que hacen las veces de las ruedas que transportan esta cinta, hundiéndose la corteza en las zonas de convergencia, y generándose nuevo piso oceánico en las dorsales. La teoría también explica de forma bastante satisfactoria la forma en que las inmensas masas que componen las placas tectónicas se pueden desplazar, algo que quedaba sin explicar cuando Alfred Wegener propuso la teoría de la deriva continental, aunque existen varios modelos que coexisten: Las placas tectónicas se pueden desplazar porque la litosfera tiene una menor densidad que la astenosfera, que es la capa que se encuentra inmediatamente inferior a la corteza. Esto hace que las placas floten en la astenosfera y el magma líquido más caliente vaya hacia arriba y el más frío y denso hacia abajo, generando una corriente que mueve las placas. Las variaciones de densidad laterales resultan en las corrientes de convección del manto, mencionadas anteriormente. Se cree que las placas son impulsadas por una combinación del movimiento que se genera en el fondo oceánico fuera de la dorsal (debido a variaciones en la topografía y densidad de la corteza, que resultan en diferencias en las fuerzas gravitacionales, arrastre, succión vertical, y zonas de subducción). Una explicación diferente o complementaria se apoya en las diferentes fuerzas que se generan con la rotación del globo terrestre y las fuerzas de marea del Sol y de la Luna; sin embargo, la importancia relativa de cada uno de esos factores no está clara y es objeto de debate.[cita requerida] Placas tectónicas en el mundo Actualmente existen las siguientes placas tectónicas en la superficie de la tierra con límites más o menos definidos, que se dividen en 15 placas mayores (o principales) y 43 placas menores (o secundarias). Las 15 placas mayores Las 15 placas tectónicas mayores. Placa africana Placa antártica Placa arábiga Placa australiana Placa del Caribe Placa de Cocos Placa euroasiática Placa filipina Placa India Placa Juan de Fuca Placa de Nazca Placa norteamericana Placa del Pacífico Placa de Scotia Placa sudamericana Las 42 placas menores Mapa detallado que muestra las placas tectónicas con sus vectores de movimiento. Placa de Altiplano Placa de Amuria Placa de Anatolia Placa de los Andes del Norte Placa Apuliana o Adriática Placa del Arrecife de Balmoral Placa del Arrecife de Conway Placa de Birmania Placa de Bismarck del Norte Placa de Bismarck del Sur Placa Cabeza de Pájaro o Doberai Placa de las Carolinas Placa de Chiloé Placa del Explorador Placa de Futuna Placa Galápagos Placa de Gorda Placa Iraní Placa de Juan Fernández Placa de Kermadec Placa de Manus Placa de Maoke Placa del Mar de Banda Placa del Mar Egeo o Helénica Placa del Mar de las Molucas Placa del Mar de Salomón Placa de las Marianas Placa Niuafoou Placa africana Placa de las Nuevas Hébridas Placa de Ojotsk Placa de Okinawa Placa de Panamá Placa de Pascua Placa de Rivera Placa de Sandwich Placa de Shetland Placa somalí Placa de la Sonda Placa de Timor Placa de Tonga Placa Woodlark Placa del Yangtsé Se han identificado tres tipos de bordes: convergentes (dos placas chocan una contra la otra), divergentes (dos placas se separan) y transformantes (dos placas se deslizan una junto a otra). La teoría de la tectónica de placas se divide en dos partes, la de deriva continental, propuesta por Alfred Wegener en la década de 1910, y la de expansión del fondo oceánico, propuesta y aceptada en la década de 1960, que mejoraba y ampliaba a la anterior. Desde su aceptación ha revolucionado las ciencias de la Tierra, con un impacto comparable al que tuvieron las teorías de la gravedad de Isaac Newton y Albert Einstein en la Física o las leyes de Kepler en la Astronomía. Causas del movimiento de las placas Artículo principal: Convección Movimiento por convección. El origen del movimiento de las placas está en unas corrientes de materiales que suceden en el manto, las denominadas corrientes de convección, y sobre todo, en la fuerza de la gravedad. La convección es una de las tres formas de transferencia de calor y se caracteriza porque se produce por intermedio de un fluido (aire, agua) que transporta el calor entre zonas con diferentes temperaturas. La convección se produce únicamente por medio de materiales fluidos. Éstos, al calentarse, aumentan de volumen y, por lo tanto, disminuyen su densidad y ascienden desplazando el fluido que se encuentra en la parte superior y que está a menor temperatura. Lo que se llama convección en sí, es el transporte de calor por medio de las corrientes ascendente y descendente del fluido. Las corrientes de convección se producen por diferencias de temperatura y densidad, de manera que los materiales más calientes pesan menos y ascienden, y los materiales más fríos son más densos, pesados, y descienden. El manto, aunque es sólido, se comporta como un material plástico o dúctil, es decir, se deforma y se estira sin romperse, debido a las altas temperaturas a las que se encuentra, sobre todo el manto inferior. En las zonas profundas el manto hace contacto con el núcleo, el calor es muy intenso, por eso grandes masas de roca se funden parcialmente y al ser más ligeras ascienden lentamente por el manto, produciendo unas corrientes ascendentes de materiales calientes, las plumas o penachos térmicos. Algunos de ellos alcanzan la litosfera, la atraviesan y contribuyen a la fragmentación de los continentes. En las fosas oceánicas, grandes fragmentos de litósfera oceánica fría se hunden en el manto, originando por tanto unas corrientes descendentes, que llegan hasta la base del manto. Las corrientes ascendentes y descendentes del manto podrían explicar el movimiento de las placas, al actuar como una especie de rodillo que las moviera. Antecedentes históricos Deriva continental Artículo principal: Deriva continental A finales del siglo XIX y principios del XX, los geólogos asumían que las principales características de la Tierra eran fijas y que la mayoría de las características geológicas, como el desarrollo de cuencas y cadenas montañosas, podían explicarse por el movimiento vertical de la corteza, descrito en lo que se denomina teoría geosinclinal. Generalmente, esto se colocó en el contexto de un planeta Tierra en contracción debido a la pérdida de calor en el transcurso de un tiempo geológico relativamente corto. Ya en 1596 se observó que las costas opuestas del Océano Atlántico (aunque es más preciso hablar de los bordes de las plataformas continentales) tienen formas similares y parecen haber encajado en algún momento pasado. Desde entonces se propusieron muchas teorías para explicar esta aparente complementariedad, pero el supuesto de una Tierra sólida hizo que estas diversas propuestas fueran difíciles de aceptar. El descubrimiento de la radiactividad y sus propiedades de calentamiento asociadas en 1895 impulsó un nuevo examen de la edad aparente de la Tierra. Esto se había estimado previamente por su tasa de enfriamiento bajo el supuesto de que la superficie de la Tierra irradiaba como un cuerpo negro. Esos cálculos habían implicado que, incluso si comenzara con un calor rojo, la Tierra habría caído a su temperatura actual en unas pocas decenas de millones de años. Armados con el conocimiento de una nueva fuente de calor, los científicos se dieron cuenta de que la Tierra sería mucho más antigua y que su núcleo todavía estaba lo suficientemente caliente como para ser líquido. Alfred Wegener en el verano de 1912-13 en Groenlandia. En 1915, después de haber publicado un primer artículo en 1912, Alfred Wegener presentó argumentos serios a favor de la idea de la deriva continental en la primera edición de El origen de los continentes y océanos. En ese libro (reeditado en cuatro ediciones sucesivas hasta la última en 1936), señaló cómo la costa este de América del Sur y la costa oeste de África parecían enacajar (de lo que ya se habían percatado anteriormente Benjamin Franklin entre otros).. Wegener no fue el primero en notar esto (Abraham Ortelius, Antonio Snider-Pellegrini, Eduard Suess, Roberto Mantovani y Frank Bursley Taylor lo precedieron, solo por mencionar algunos), pero fue el primero en reunir importantes evidencias fósiles, paleo-topográficas y climatológicas para apoyar esta simple observación (y fue apoyado en esto por investigadores como Alex du Toit). También tuvo en cuenta el parecido de la fauna fósil de los continentes septentrionales y ciertas formaciones geológicas. Wegener conjeturó que el conjunto de los continentes actuales estuvieron unidos en el pasado remoto de la Tierra, formando un supercontinente, denominado Pangea. Además, dado que los estratos rocosos de los márgenes de continentes separados son muy similares, sugiere que estas rocas se formaron de la misma manera, lo que implica que estaban unidas en un principio. Por ejemplo, partes de Escocia e Irlanda contienen rocas muy similares a las que se encuentran en Terranova y Nuevo Brunswick. Además, las Montañas Caledonianas de Europa y partes de los montes Apalaches de América del Norte son muy similares en estructura y litología. Sin embargo, sus ideas no fueron tomadas en serio por muchos geólogos, quienes señalaron que no existía un mecanismo aparente para la deriva continental. En su tesis original, Wegener propuso que los continentes se desplazaban sobre el manto de la Tierra de la misma forma en que uno desplaza una alfombra sobre el piso de una habitación. Sin embargo, esto no es posible, debido a la enorme fuerza de fricción implicada, lo que motivó el rechazo de la explicación de Wegener, y la puesta en suspenso, como hipótesis interesante pero no probada, de la idea del desplazamiento continental hasta la aparición de la Tectónica de placas. Más concretamente, no vieron cómo la roca continental podría atravesar la roca mucho más densa que forma la corteza oceánica. Wegener no pudo explicar la fuerza que impulsó la deriva continental, y su reivindicación no llegó hasta después de su muerte en 1930. Continentes flotantes, paleomagnetismo y zonas sísmicas Como se observó temprano que aunque existía granito en los continentes, el fondo marino parecía estar compuesto de basalto más denso, el concepto predominante durante la primera mitad del siglo XX fue que había dos tipos de corteza, denominada sial (corteza de tipo continental). y sima (corteza de tipo oceánico). Además, se suponía que había una capa estática de estratos debajo de los continentes. Por lo tanto, parecía evidente que una capa de basalto (sial) subyace a las rocas continentales. Sin embargo, basándose en anomalías en la desviación de la plomada de los Andes en Perú, Pierre Bouguer había deducido que las montañas menos densas deben tener una proyección hacia abajo en la capa inferior más densa. El concepto de que las montañas tenían raíces fue confirmado por George B. Airy cien años después, durante un estudio de la gravitación del Himalaya, y los estudios sísmicos detectaron variaciones de densidad correspondientes. Por lo tanto, a mediados de la década de 1950 seguía sin resolverse la cuestión de si las raíces de las montañas estaban apretadas en el basalto circundante o flotaban sobre él como un iceberg. Epicentros de terremotos, 1963–1998. La mayoría de los terremotos tienen lugar en estrechos cinturones que coinciden con los límites entre placas. Durante el siglo XX las mejoras y el mayor uso de instrumentos sísmicos como los sismógrafos permitieron a los científicos comprender que los terremotos tienden a concentrarse en áreas específicas, sobre todo a lo largo de las fosas oceánicas y las dorsales. A finales de la década de 1920 los sismólogos estaban comenzando a identificar varias zonas prominentes de terremotos paralelas a las fosas que normalmente se inclinaban entre 40 y 60° desde la horizontal y se extendían varios cientos de kilómetros hacia el interior de la Tierra. Estas zonas se conocieron más tarde como zonas de Wadati-Benioff, o simplemente zonas de Benioff, en honor a los sismólogos que las reconocieron por primera vez, Kiyoo Wadati de Japón y Hugo Benioff de Estados Unidos. El estudio de la sismicidad global avanzó enormemente en la década de 1960 con el establecimiento de la Red Mundial de Sismógrafos Estandarizados (WWSSN) para monitorizar el cumplimiento del tratado de 1963 que prohibía las pruebas aéreas de armas nucleares. Los datos muy mejorados de los instrumentos de WWSSN permitieron a los sismólogos mapear con precisión las zonas de concentración de terremotos en todo el mundo. Mientras tanto, se desarrollaron debates en torno al fenómeno de la deriva polar. Desde los primeros debates sobre la deriva continental, los científicos habían discutido y utilizado evidencias de que la deriva polar había ocurrido porque los continentes parecían haberse movido a través de diferentes zonas climáticas durante el pasado. Además, los datos paleomagnéticos habían demostrado que el polo magnético también se había desplazado con el tiempo. Razonando de manera opuesta, los continentes podrían haberse movido y girado, mientras que el polo permanecía relativamente fijo. La primera vez que se utilizó la evidencia de la desviación polar magnética para respaldar los movimientos de los continentes fue en un artículo de Keith Runcorn en 1956, y artículos sucesivos de él y sus estudiantes Ted Irving (quien en realidad fue el primero en estar convencido del hecho de que el paleomagnetismo apoyaba la deriva continental) y Ken Creer. A esto siguió inmediatamente un simposio en Tasmania en marzo de 1956. En este simposio, la evidencia se utilizó en la teoría de una expansión de la corteza global. En esta hipótesis, el desplazamiento de los continentes puede explicarse simplemente por un gran aumento en el tamaño de la Tierra desde su formación. Sin embargo, esto fue insatisfactorio porque sus partidarios no pudieron ofrecer un mecanismo convincente para producir una expansión significativa de la Tierra. Ciertamente, no hay evidencia de que la Luna se haya expandido en los últimos 3 000 millones de años; otros trabajos pronto mostrarían que la evidencia estaba igualmente a favor de la deriva continental en un globo con un radio estable. Durante los años treinta hasta finales de los cincuenta, los trabajos de Vening-Meinesz, Holmes, Umbgrove y muchos otros delinearon conceptos que eran cercanos o casi idénticos a la teoría de la tectónica de placas moderna. En particular, el geólogo inglés Arthur Holmes propuso en 1920 que las uniones de placas podrían encontrarse debajo del mar, y en 1928 que las corrientes de convección dentro del manto podrían ser la fuerza impulsora. A menudo, estas contribuciones se olvidan porque: En ese momento no se aceptaba la deriva continental. Algunas de estas ideas se discutieron en el contexto de ideas fijistas abandonadas de un globo deformante sin deriva continental o una Tierra en expansión. Fueron publicadas durante un episodio de extrema inestabilidad política y económica que obstaculizó la comunicación científica. Muchas fueron publicadas por científicos europeos y al principio no se mencionaron o se les dio poco crédito en los artículos sobre la extensión del fondo marino publicados por los investigadores estadounidenses en la década de 1960. Expansión de la dorsal mediooceánica y convección Artículo principal: Expansión del fondo oceánico Sumergible Alvin, que participó en el proyecto FAMOUS de exploración de la dorsal mesoatlántica. El primer mapa de los fondos oceánicos se consigue elaborar en 1956 gracias a los avances en las tecnologías del sónar. Se investigó el Océano Atlántico y se descubrió que: Había una cordillera submarina, a la que llamaron dorsal. Las rocas cercanas a los continentes eran más antiguas que las del centro. Los epicentros de los terremotos tenían lugar en la dorsal. Existían más de 6000 km de dorsales. Por estas razones en 1960 Harry Hess y en 1961 Robert Dietz sugirieron que el suelo oceánico se expande. En 1963 esta hipótesis se comprobó cuando Vine y Matthews identificaron las líneas de magnetismo de distinta polaridad, es decir, que el campo magnético terrestre se invierte. En 1974, dentro del proyecto internacional FAMOUS, un equipo de científicos de la Institución Oceanográfica de Woods Hole (EE. UU.) y del French Centre Oceanologique de Bretagne (Brest, Francia) utilizó buques de investigación en superficie, así como diverso instrumental avanzado que incluía magnetómetros, sonar y sismógrafos, además de dos sumergibles: el Alvin (EE. UU.) y el Archimède (Francia). Las investigaciones confirmaron la existencia de una elevación en el Océano Atlántico central y descubrieron que el fondo del lecho marino, debajo de la capa de sedimentos, consistía en basalto, no en granito, que es el componente principal de los continentes. También encontraron actividad volcánica y sísmica y que la corteza oceánica era mucho más delgada que la corteza continental. Todos estos nuevos hallazgos plantearon preguntas importantes e intrigantes. Las fuentes hidrotermales encontradas en las dorsales son consecuencia de una intensa actividad volcánica. Los nuevos datos recopilados sobre las cuencas oceánicas también mostraron características particulares en cuanto a la batimetría. Uno de los principales resultados de estos conjuntos de datos fue que en todo el mundo se detectó un sistema de dorsales oceánicas. Una conclusión importante fue que a lo largo de este sistema se estaba creando un nuevo fondo oceánico, lo que llevó al concepto de la Gran Grieta Global. Esto se describió en el artículo crucial de Bruce Heezen (1960) basado en su trabajo con Marie Tharp, que desencadenaría una verdadera revolución en el pensamiento. Una consecuencia profunda de la expansión del lecho marino es que se crea y se sigue creando una nueva corteza a lo largo de las dorsales oceánicas. Por lo tanto, Heezen defendió la supuesta hipótesis de la Tierra en expansión de S. Warren Carey (ver arriba). Entonces, todavía quedaba la pregunta: ¿cómo se puede agregar continuamente nueva corteza a lo largo de las dorsales oceánicas sin aumentar el tamaño de la Tierra? En realidad, esta cuestión ya había sido resuelta por numerosos científicos durante los años cuarenta y cincuenta, como Arthur Holmes, Vening-Meinesz, Coates y muchos otros: la corteza en exceso desaparece a lo largo de las llamadas fosas oceánicas, donde se produce el proceso conocido como subducción. Por lo tanto, cuando varios científicos a principios de la década de 1960 comenzaron a razonar sobre los datos que tenían a su disposición sobre el fondo del océano, las piezas de la teoría encajaron rápidamente. La pregunta intrigó particularmente a Harry Hammond Hess, un geólogo de la Universidad de Princeton y contraalmirante de la Reserva Naval, y a Robert S. Dietz, un científico de la U.S. National Geodetic Survey, quien acuñó por primera vez el término expansión del fondo oceánico. Dietz y Hess (el primero publicó la misma idea un año antes en Nature, pero la prioridad pertenece a Hess, que ya había distribuido un manuscrito inédito de su artículo de 1962 en 1960) se encontraban entre el pequeño puñado que realmente entendió las amplias implicaciones de la expansión del fondo marino y cómo eventualmente estaría de acuerdo con las ideas, en ese momento poco convencionales y no aceptadas, de la deriva continental y los modelos elegantes y movilistas propuestos por investigadores anteriores como Holmes. En el mismo año, Robert R. Coats del U.S. Geological Survey describió las principales características de la subducción del arco insular en las Islas Aleutianas. Su artículo, aunque poco conocido (e incluso ridiculizado) en ese momento, desde entonces ha sido llamado seminal y profético. En realidad, muestra que el trabajo de científicos europeos sobre arcos de islas y cinturones montañosos realizado y publicado durante la década de 1930 hasta la década de 1950 fue aplicado y apreciado también en los Estados Unidos. Lava almohadillada como la producida por la actividad volcánica en las dorsales, apenas cubierta por una fina capa de sedimentos, lo que indica su reciente formación. Si la corteza terrestre se estaba expandiendo a lo largo de las dorsales oceánicas, razonaron Hess y Dietz como Holmes y otros antes que ellos, debe estar encogiéndose en otros lugares. Hess siguió a Heezen, sugiriendo que la nueva corteza oceánica se separa continuamente de las dorsales en un movimiento similar a una cinta transportadora. Y, utilizando los conceptos movilistas desarrollados anteriormente, concluyó correctamente que muchos millones de años después, la corteza oceánica finalmente desciende a lo largo de los márgenes continentales donde se forman fosas oceánicas (cañones estrechos y muy profundos), por ejemplo a lo largo del borde de la cuenca del Océano Pacífico. El paso importante que dio Hess fue que las corrientes de convección serían la fuerza impulsora en este proceso, llegando a las mismas conclusiones que Holmes había obtenido décadas antes con la única diferencia de que el adelgazamiento de la corteza oceánica se realizó utilizando el mecanismo de Heezen de propagación a lo largo de las dorsales. Por lo tanto, Hess concluyó que el Océano Atlántico se estaba expandiendo mientras que el Océano Pacífico se estaba reduciendo. A medida que la vieja corteza oceánica se consume en las fosas (al igual que Holmes y otros, pensó que esto se hacía mediante el engrosamiento de la litosfera continental, no, como se entiende ahora, por el enterramiento a una escala mayor de la propia corteza oceánica en el manto), nuevo magma se eleva y erupciona a lo largo de las dorsales que se extienden para formar una nueva corteza. En efecto, las cuencas oceánicas se están reciclando perpetuamente, con la creación de una nueva corteza y la destrucción de la antigua litosfera oceánica que ocurren simultáneamente. Por lo tanto, los nuevos conceptos movilistas explicaron claramente por qué la Tierra no se agranda con la expansión del fondo del mar, por qué hay tan poca acumulación de sedimentos en el fondo del océano y por qué las rocas oceánicas son mucho más jóvenes que las rocas continentales. Inversiones magnéticas y bandeado magnético A partir de la década de 1950, científicos como Victor Vacquier, utilizando instrumentos magnéticos (magnetómetros) adaptados de dispositivos aéreos desarrollados durante la Segunda Guerra Mundial para detectar submarinos, comenzaron a reconocer extrañas variaciones magnéticas en el fondo del océano. Este hallazgo, aunque inesperado, no fue del todo sorprendente porque se sabía que el basalto, la roca volcánica rica en hierro que forma el fondo del océano, contiene un mineral fuertemente magnético (magnetita) y puede distorsionar localmente las lecturas de la brújula. Esta distorsión fue reconocida por los marineros islandeses ya a finales del siglo XVIII. Más importante aún, debido a que la presencia de magnetita le da al basalto propiedades magnéticas mensurables, estas variaciones magnéticas recién descubiertas proporcionaron otro medio para estudiar el fondo del océano profundo. Cuando la roca recién formada se enfriaba, tales materiales magnéticos registraron el campo magnético terrestre en ese momento. Bandeado magnético del fondo marino. La dorsal es el eje de simetría de un patrón de bandas con polaridad alterna normal (color) e invertida (blanco) A medida que se cartografió cada vez más el fondo marino durante la década de 1950, las variaciones magnéticas resultaron no ser ocurrencias aleatorias o aisladas, sino que revelaron patrones reconocibles. Cuando estos patrones magnéticos se mapearon en una amplia región, el fondo del océano mostró un patrón similar a una cebra: una franja con polaridad normal y la franja adyacente con polaridad invertida. El patrón general, definido por estas bandas alternas de roca polarizada normal e inversamente, se conoció como bandas magnéticas y fue publicado por Ron G. Mason y sus colaboradores en 1961, quienes no encontraron, sin embargo, una explicación para estos datos en términos de expansión del fondo marino, como Vine, Matthews y Morley unos años más tarde. El descubrimiento de las bandas magnéticas requería una explicación. A principios de la década de 1960, científicos como Heezen, Hess y Dietz habían comenzado a teorizar que las dorsales oceánicas marcan zonas estructuralmente débiles donde el suelo oceánico se estaba partiendo en dos a lo largo de la cresta de la dorsal. El nuevo magma de las profundidades de la Tierra se eleva fácilmente a través de estas zonas débiles y finalmente erupciona a lo largo de la cresta de las dorsales para crear una nueva corteza oceánica. Este proceso, que en un principio se denominó hipótesis de la cinta transportadora y más tarde expansión del fondo oceánico, opera durante muchos millones de años y continúa formando un nuevo fondo oceánico en todo el sistema de cordilleras oceánicas de 64.000 km de longitud. Solo cuatro años después de que se publicaran los mapas con el patrón de cebra de bandas magnéticas, el vínculo entre la expansión del fondo oceánico y estos patrones fue establecido, correcta e independientemente, por Lawrence Morley, Fred Vine y Drummond Matthews, en 1963, conocida actualmente como la hipótesis de Vine-Matthews-Morley. Esta hipótesis vinculó estos patrones con reversiones geomagnéticas y fue apoyada por varias líneas de evidencia: Edades de los basaltos del fondo oceánico. En rojo las rocas más jóvenes y en morado las más altiguas las franjas son simétricas alrededor de las crestas de las dorsales oceánicas; en o cerca de la cresta de la dorsal, las rocas son muy jóvenes y envejecen progresivamente lejos de la cresta de la dorsal; las rocas más jóvenes en la cresta de la dorsal siempre tienen la polaridad actual (normal); franjas de roca paralelas a la cresta de la dorsal alternan en polaridad magnética (normal-invertida-normal, etc.), lo que sugiere que se formaron durante diferentes épocas que documentan los episodios normales y de inversión (ya conocidos de estudios independientes) del campo magnético de la Tierra. En las dorsales no existen apenas sedimentos sino rocas volcánicas solidificadas, mientras que la cubierta sedimentaria va aumentando su grosor a ambos lados de la dorsal. Al explicar tanto las bandas magnéticas similares a las de una cebra como la construcción del sistema de cordilleras oceánicas, la hipótesis de expansión del fondo oceánico ganó rápidamente adeptos y representó otro avance importante en el desarrollo de la teoría de la tectónica de placas. Además, la corteza oceánica ahora llegó a ser apreciada como una grabación en cinta natural de la historia de las inversiones del campo geomagnético del de la Tierra. En la actualidad, se dedican extensos estudios a la calibración de los patrones de inversión normal en la corteza oceánica, por un lado, y escalas de tiempo conocidas derivadas de la datación de capas de basalto en secuencias sedimentarias (magnetoestratigrafía), por el otro, para llegar a estimaciones de las tasas de propagación pasadas y reconstrucciones de placas. La revolución de la tectónica de placas Después de todas estas consideraciones, la tectónica de placas (o, como se llamó inicialmente nueva tectónica global) fue rápidamente aceptada en el mundo científico, y siguieron numerosos artículos que definieron los conceptos implicados: Ciclo de Wilson. En 1965, Tuzo Wilson, quien había sido un promotor de la hipótesis de la extensión del fondo marino y la deriva continental desde el principio, agregó el concepto de fallas transformantes al modelo, completando las clases de tipos de fallas necesarias para hacer que la movilidad de las placas funcionara a nivel global. En 1965 se celebró en la Royal Society de Londres un simposio sobre deriva continental que debe considerarse como el inicio oficial de la aceptación de la tectónica de placas por parte de la comunidad científica, y cuyos resúmenes se publican como Blackett, Bullard & Runcorn (1965). En este simposio, Edward Bullard y sus colaboradores mostraron con un cálculo de computadora cómo los continentes a ambos lados del Atlántico encajarían mejor para cerrar el océano, lo que se conoció como el famoso ajuste de Bullard. En 1966 Wilson publicó el artículo que se refería a reconstrucciones de placas tectónicas previas, introduciendo el concepto de lo que ahora se conoce como el ciclo de Wilson. En 1967, en la reunión de la Unión Americana de Geofísica, W. Jason Morgan propuso que la superficie de la Tierra consta de 12 placas rígidas que se mueven entre sí. Jason Morgan propuso también la existencia de plumas del manto para explicar los puntos calientes. Dos meses después Xavier Le Pichon publicó un modelo completo basado en seis placas principales con sus movimientos relativos, lo que marcó la aceptación final por parte de la comunidad científica de la tectónica de placas. En el mismo año McKenzie y Parker presentaron de forma independiente un modelo similar al de Morgan usando traslaciones y rotaciones en una esfera para definir los movimientos de las placas. La revolución de la tectónica de placas fue el cambio científico y cultural que se desarrolló a partir de la aceptación de la teoría de la tectónica de placas y supuso un cambio de paradigma y una revolución científica que transformó la geología. Límites de placas Son los bordes de una placa y es ahí donde se presenta la mayor actividad tectónica (sismos, formación de montañas, actividad volcánica), ya que es donde se produce la interacción entre placas. Hay tres clases de límite: Divergentes: son límites en los que las placas se separan unas de otras y, por lo tanto, emerge magma desde regiones más profundas (por ejemplo, la dorsal mesoatlántica formada por la separación de las placas de Eurasia y Norteamérica y las de África y Sudamérica). Convergentes: son límites en los que una placa choca contra otra, formando una zona de subducción (la placa oceánica se hunde bajo la placa continental) o un cinturón orogénico (si las placas chocan y se comprimen). Son también conocidos como bordes activos. Transformantes: son límites donde los bordes de las placas se deslizan una con respecto a la otra a lo largo de una falla de transformación. En determinadas circunstancias se forman zonas de límite o borde, donde se unen tres o más placas formando una combinación de los tres tipos de límites. Límite divergente o constructivo: las dorsales Dorsal oceánica. Artículo principal: Borde divergente Son las zonas de la litosfera en que se forma nueva corteza oceánica y en las cuales se separan las placas. En los límites divergentes, las placas se alejan y el vacío que resulta de esta separación es rellenado por material de la corteza, que surge del magma de las capas inferiores. Se cree que el surgimiento de bordes divergentes en las uniones de tres placas está relacionado con la formación de puntos calientes. En estos casos se junta material de la astenosfera cerca de la superficie y la energía cinética es suficiente para hacer pedazos la litosfera. El punto caliente que originó la dorsal mesoatlántica se encuentra actualmente debajo de Islandia, y el material nuevo ensancha la isla algunos centímetros cada siglo. Un ejemplo típico de este tipo de límite son las dorsales oceánicas, como la dorsal mesoatlántica entre otras, y en el continente las grietas, como el Gran Valle del Rift. Límite convergente o destructivo La placa oceánica se hunde por debajo de la placa continental. Artículo principal: Borde convergente Las características de los bordes convergentes dependen del tipo de litosfera de las placas que chocan. Con frecuencia las placas no se deslizan en forma continua; sino que se acumula tensión en ambas placas hasta llegar a un nivel de energía acumulada que sobrepasa el necesario para producir el deslizamiento brusco de la placa marina. La energía potencial acumulada es liberada como presión o movimiento; debido a la titánica cantidad de energía almacenada, estos movimientos ocasionan terremotos, de mayor o menor intensidad. Los puntos de mayor actividad sísmica suelen asociarse con este tipo de límites de placas. Cuando una placa oceánica (más densa) choca contra una continental (menos densa) la placa oceánica es empujada debajo, formando una zona de subducción. En la superficie, la modificación topográfica consiste en una fosa oceánica en el agua y un grupo de montañas en tierra. En los Andes centrales, la interacción de la placa de Nazca con la placa sudamericana ha dado lugar a la formación del Oroclinal de Bolivia, una curvatura que refleja la intensa actividad tectónica en esta región. Cuando dos placas continentales colisionan (colisión continental), se forman extensas cordilleras formando un borde de obducción. La cadena del Himalaya es el resultado de la colisión entre la placa Indoaustraliana y la placa Euroasiática. Cuando dos placas oceánicas chocan, el resultado es un arco de islas (por ejemplo, Japón). Límite transformante, conservativo o neutro Falla de San Andrés. Artículo principal: Borde transformante El movimiento de las placas a lo largo de las fallas de transformación puede causar considerables cambios en la superficie, lo que es particularmente significativo cuando esto sucede en las proximidades de un asentamiento humano. Debido a la fricción, las placas no se deslizan en forma continua, sino que se acumula tensión en ambas placas hasta llegar a un nivel de energía acumulada que sobrepasa el necesario para producir el movimiento. La energía potencial acumulada es liberada como presión o movimiento en la falla. Debido a la gran cantidad de energía almacenada, estos movimientos ocasionan terremotos de mayor o menor intensidad. Un ejemplo de este tipo de límite es la falla de San Andrés, ubicada en el oeste de Norteamérica, que es parte del sistema de fallas producto del roce entre la placa Norteamericana y la del Pacífico. Medición de la velocidad de las placas tectónicas La medición actual de la velocidad de las placas tectónicas se realiza mediante medidas precisas de GPS. La velocidad antigua de las placas se obtiene mediante la restitución de cortes geológicos (en corteza continental) o mediante la medida de la posición de las inversiones del campo magnético terrestre registradas en el fondo oceánico."
ksampletext_wikipedia_geol_mineral: str = "Mineral. Un mineral es una sustancia natural, de composición química definida. Normalmente es sólido e inorgánico, y tiene una cierta estructura cristalina. Es diferente de una roca, que puede ser un agregado de minerales o no minerales y que no tiene una composición química específica. La definición exacta de mineral es objeto de debate, especialmente con respecto a la exigencia de ser abiogénico, y en menor medida, a si debe tener una estructura atómica ordenada. El estudio de los minerales se llama mineralogía. Hay más de 5300 especies minerales conocidas, de ellas más de 5090 aprobadas por la Asociación Internacional de Mineralogía (IMA por sus siglas en inglés). Continuamente se descubren y describen nuevos minerales, entre 50 y 80 al año. La diversidad y abundancia de especies minerales es controlada por la química de la Tierra. El silicio y el oxígeno constituyen aproximadamente el 75 % de la corteza terrestre, lo que se traduce directamente en el predominio de los minerales de silicato, que componen más del 90% de la corteza terrestre. Los minerales se distinguen por diversas propiedades químicas y físicas. Diferencias en la composición química y en la estructura cristalina distinguen varias especies, y estas propiedades, a su vez, están influidas por el entorno geológico de la formación del mineral. Cambios en la temperatura, la presión o en la composición del núcleo de una masa de roca causan cambios en sus minerales. Los minerales pueden ser descritos por varias propiedades físicas que se relacionan con su estructura química y composición. Las características más comunes que los identifican son la estructura cristalina y el hábito, la dureza, el lustre, la diafanidad, el color, el rayado, la tenacidad, la exfoliación, la fractura, la partición y la densidad relativa. Otras pruebas más específicas para la caracterización de ciertos minerales son el magnetismo, el sabor o el olor, la radioactividad y la reacción a los ácidos fuertes. Los minerales se clasifican por sus componentes químicos clave siendo los dos sistemas dominantes la clasificación de Dana y la clasificación de Strunz. La clase de silicatos se subdivide en seis subclases según el grado de polimerización en su estructura química. Todos los silicatos tienen una unidad básica en forma de tetraedro de sílice [SiO 4]4− , es decir, un catión de silicio unido a cuatro aniones de oxígeno. Estos tetraedros pueden ser polimerizados para dar las subclases: neosilicatos (no polimerizados, y por lo tanto, solo tetraedros), sorosilicatos (dos tetraedros enlazados entre sí), ciclosilicatos (anillos de tetraedros), inosilicatos (cadenas de tetraedros), filosilicatos (láminas de tetraedros), y tectosilicatos (redes en tres dimensiones de tetraedros). Otros grupos minerales importantes son los elementos nativos, sulfuros, óxidos, haluros, carbonatos, sulfatos y fosfatos. Definición Definición básica La definición general de un mineral comprende los siguientes criterios: ser de origen natural; ser estable a temperatura ambiente; estar representado por una fórmula química; ser generalmente abiogénico (no resultado de la actividad de los organismos vivos); y tener disposición atómica ordenada. Las tres primeras características generales son menos debatidas que las dos últimas.: 2–4  El primer criterio significa que un mineral se tiene que formar por un proceso natural, lo que excluye compuestos antropogénicos. La estabilidad a temperatura ambiente, en el sentido más simple, es sinónimo de que el mineral sea sólido. Más específicamente, un compuesto tiene que ser estable o metaestable a 25 °C. Son ejemplos clásicos de excepciones a esta regla el mercurio nativo, que cristaliza a -39 °C, y el hielo de agua, que es sólido solo por debajo de 0 °C; puesto que estos dos minerales se habían descrito con anterioridad a 1959, fueron adoptados por la Asociación Internacional de Mineralogía (IMA). Los avances modernos suponen un amplio estudio de los cristales líquidos, que también concierne ampliamente a la mineralogía. Los minerales son compuestos químicos, y, como tales, pueden ser descritos por una fórmula fija o una variable. Muchos grupos de minerales y especies están compuestos por una solución sólida; las sustancias puras generalmente no se encuentran debido a la contaminación o sustitución química. Por ejemplo, el grupo del olivino se describe por la fórmula variable (Mg, Fe) 2SiO 4, que es una solución sólida de dos especies de miembro extremo, la forsterita rica en magnesio y la fayalita rica en hierro, que se describen mediante una fórmula química fija. Otras especies minerales podrían tener composiciones variables, tales como el sulfuro de mackinawita, (Fe, Ni) 8, que es principalmente un sulfuro ferroso, pero que tiene una impureza de níquel muy significativa que se refleja en su fórmula.: 2–4  El requisito de que una especie mineral para ser válida ha de ser abiogénica también se ha descrito como similar a que sea inorgánica; sin embargo, este criterio es impreciso y a los compuestos orgánicos se les ha asignado una rama de clasificación separada. Por último, la exigencia de tener una disposición atómica ordenada es generalmente sinónimo de cristalinidad; sin embargo, los cristales también son periódicos, por lo que se utiliza en su lugar el criterio más amplio.: 2–4  Una disposición atómica ordenada da lugar a una variedad de propiedades físicas macroscópicas, como la forma cristalina, la dureza y la exfoliación.: 13–14  Ha habido varias propuestas recientes para modificar la definición para considerar las sustancias biogénicas o amorfas como minerales. La definición formal de un mineral aprobada por la IMA en 1995 es: Un mineral es un elemento o compuesto químico que es normalmente cristalino y que se ha formado como resultado de procesos geológicos. IMA (1995) Además, las sustancias biogénicas fueron excluidas explícitamente: Las sustancias biogénicas son compuestos químicos producidos totalmente por procesos biológicos sin un componente geológico (por ejemplo, cálculos urinarios, cristales de oxalato en tejidos vegetales, conchas de moluscos marinos, etc.) y no son considerados como minerales. Sin embargo, si hubo procesos geológicos implicados en la génesis del compuesto, entonces el producto puede ser aceptado como un mineral. IMA (1995) Avances recientes Los sistemas de clasificación de minerales y sus definiciones están evolucionando para recoger los últimos avances de la ciencia mineral. Los cambios más recientes han sido la adición de una clase orgánica, tanto en el nuevo Dana y en los esquemas de la clasificación de Strunz. La clase orgánica incluye un grupo muy raro de minerales con hidrocarburos. La Comisión sobre nuevos minerales y nombres de minerales de la IMA aprobó en 2009 un esquema jerárquico para la denominación y clasificación de los grupos minerales y de los nombres de los grupos y estableció siete comisiones y cuatro grupos de trabajo para revisar y clasificar los minerales en una lista oficial de sus nombres publicados. De acuerdo con estas nuevas reglas, las especies minerales pueden ser agrupadas de diferentes maneras, sobre la base de la química, la estructura cristalina, la aparición, la asociación, la historia genética o los recursos, por ejemplo, dependiendo de la finalidad para que sirva la clasificación. mineral species can be grouped in a number of different ways, on the basis of chemistry, crystal structure, occurrence, association, genetic history, or resource, for example, depending on the purpose to be served by the classification. IMA La exclusión de Nickel (1995) de las sustancias biogénicas no fue universalmente respetada. Por ejemplo, Heinz A. Lowenstam (1981) declaró que «los organismos son capaces de formar una gran variedad de minerales, algunos de los cuales no se pueden formar inorgánicamente en la biosfera.» La distinción es una cuestión de clasificación y tiene menos que ver con los constituyentes de los minerales mismos. Skinner (2005) considera todos los sólidos como minerales potenciales e incluye los biominerales en el reino mineral, que son aquellos creados por las actividades metabólicas de los organismos. Skinner amplió la definición previa de un mineral para clasificar como mineral cualquier «elemento o compuesto, amorfo o cristalino, formado a través de los procesos biogeoquímicos». Los recientes avances en la genéticas de alta resolución y espectroscopía de absorción de rayos X están proporcionando revelaciones sobre las relaciones biogeoquímicas entre microorganismos y minerales que pueden hacer obsoleta la exclusión biogénica de Nickel (1995) y una necesidad la inclusión biogénica de Skinner (2005). Por ejemplo, el IMA encargó al Grupo de trabajo de Mineralogía ambiental y Geoquímica tratar de los minerales en la hidrosfera, atmósfera y biosfera. El alcance del grupo incluye microorganismos formadores de minerales, que existen en casi todas las rocas, en el suelo y en la superficie de las partículas que atraviesan el globo hasta una profundidad de al menos 1600 metros por debajo del fondo del mar y 70 kilómetros en la estratosfera (posiblemente se introduzcan en la mesosfera). Los ciclos biogeoquímicos han contribuido a la formación de minerales durante miles de millones de años. Los microorganismos pueden precipitar los metales de la disolución, contribuyendo a la formación de yacimientos de mineral. También pueden catalizar la disolución de los minerales. Antes de la lista de la Asociación Internacional de Mineralogía, más de 60 biominerales ya habían sido descubiertos, nombrados y publicados. Estos minerales (un subconjunto tabulado en Lowenstam (1981)) se consideran propiamente minerales de acuerdo con la definición de Skinner (2005). Estos biominerales no figuran en la lista oficial de nombres de minerales de la IMA, aunque muchos de estos biominerales representativos se distribuyen entre las 78 clases minerales que figuran en la clasificación de Dana. Otra clase rara de minerales (principalmente de origen biológico) incluye los cristales líquidos minerales que tienen propiedades tanto de líquidos y cristales. Hasta la fecha se han identificado más de 80.000 compuestos cristalinos líquidos. La definición de mineral de Skinner (2005) toma en cuenta esta cuestión afirmando que un mineral puede ser cristalino o amorfo, incluyendo en este último grupo los cristales líquidos. Aunque los biominerales y los cristales líquidos no son la forma más común de minerales, ayudan a definir los límites de lo que constituye propiamente un mineral. La definición formal de Nickel (1995) menciona explícitamente la cristalinidad como una clave para la definición de una sustancia como un mineral. Un artículo de 2011 define la icosahedrita, una aleación de hierro-cobre-aluminio, como mineral; llamada así por su singular simetría icosaédrica natural, es un cuasi cristal. A diferencia de un verdadero cristal, los cuasicristales están ordenados pero no de forma periódica. Rocas, menas y gemas Un esquisto es una roca metamórfica que se caracteriza por la abundancia de placas minerales. En este ejemplo, la roca tiene prominentes porfiroblastos de silimanita (de hasta 3 cm). Los minerales no son equivalentes a las rocas. Una roca puede ser un agregado de uno o más minerales, o no tener ningún mineral.: 15–16  Rocas como la caliza o la cuarcita se componen principalmente de un mineral ,calcita o aragonito en el caso de la caliza, y cuarzo, en la última,.: 719–721, 747–748  Otras rocas pueden ser definidas por la abundancia relativa de los minerales clave (esenciales); un granito está definido por las proporciones de cuarzo, feldespato alcalino y plagioclasa.: 694–696  Los otros minerales de la roca se denominan accesorios, y no afectan en gran medida la composición global de la roca. Las rocas también pueden estar compuestas enteramente de material no mineral; el carbón es una roca sedimentaria compuesta principalmente de carbono derivado de manera orgánica.: 15–16, 728–730  En las rocas, algunas especies y grupos minerales son mucho más abundantes que otros; estos se denominan minerales formativos. Los principales ejemplos son el cuarzo, feldespatos, las micas, los anfíboles, los piroxenos, los olivinos, y la calcita; excepto la última, todos son minerales silicatos.: 15  En general, alrededor de unos 150 minerales se consideran particularmente importantes, ya sea en términos de su abundancia o valor estético en términos de coleccionismo.: 14  Los minerales y rocas comercialmente valiosos se conocen como minerales industriales y rocas industriales. Por ejemplo, la moscovita, una mica blanca, puede ser utilizada para ventanas (a veces conocida como isinglass), como material de relleno o como aislante.: 531–532  Las menas son minerales que tienen una alta concentración de un determinado elemento, normalmente un metal. Ejemplos de ello son el cinabrio (HgS), un mineral de mercurio, esfalerita (ZnS), un mineral de zinc, o la casiterita (SnO 2), un mineral de estaño. Las gemas son minerales con un alto valor ornamental, y se distinguen de las no gemas por su belleza, durabilidad, y por lo general, rareza. Hay alrededor de 20 especies minerales que se califican como minerales gema, que constituyen alrededor de las 35 piedras preciosas más comunes. Los minerales gema están a menudo presentes en diversas variedades, y así un mineral puede dar cuenta de varias piedras preciosas diferentes; por ejemplo, rubí y el zafiro son ambas corindón, Al 3.: 14–15  Nomenclatura y clasificación Clasificación histórica de los minerales Los minerales se solían clasificar en la antigüedad con criterios de su aspecto físico; Teofrasto, en el s. III a. C., creó la primera lista sistemática cualitativa conocida; Plinio el Viejo (s. I d. C.), en su Historia Natural, realizó una sistemática mineral, trabajo que, en la Edad Media, sirvió de base a Avicena; Linneo (1707-1778) intentó idear una nomenclatura fundándose en los conceptos de género y especie, pero no tuvo éxito y dejó de usarse en el siglo XIX; con el posterior desarrollo de la química, el químico sueco Axel Fredrik Cronstedt (1722-1765) elaboró la primera clasificación de minerales en función de su composición; el geólogo estadounidense James Dwight Dana, en 1837, propuso una clasificación considerando la estructura y composición química. La clasificación más actual se funda en la composición química y la estructura cristalina de los minerales. Las clasificaciones más empleadas son las de Strunz y Kostov. Clasificación moderna Los minerales se clasifican según la variedad, especie, serie y grupo, en orden creciente de generalidad. El nivel básico de definición es el de las especies minerales, que se distinguen de otras especies por sus propiedades químicas y físicas específicas y únicas. Por ejemplo, el cuarzo se define por su fórmula química, SiO 2, y por una estructura cristalina específica que lo distingue de otros minerales con la misma fórmula química (denominados polimorfos). Cuando existe un rango de composición entre dos especies minerales, se define una serie mineral. Por ejemplo, la serie de la biotita está representada por cantidades variables de la endmembers flogopita, siderofilita, annita, y eastonita. Por contraste, un grupo mineral es una agrupación de especies minerales con algunas propiedades químicas comunes que comparten una estructura cristalina. El grupo piroxeno tiene una fórmula común de XY(Si, Al) 6, en donde X e Y son ambos cationes, siendo X generalmente mayor que Y (radio iónico); los piroxenos son silicatos de cadena sencilla que cristalizan en cualquiera de los sistemas cristalinos monoclínico o ortorrómbico. Finalmente, una variedad mineral es un tipo específico de especies minerales que difieren por alguna característica física, como el color o el hábito del cristal. Un ejemplo es la amatista, que es una variedad púrpura del cuarzo.: 20–22  Para ordenar minerales dos son las clasificaciones más comunes, la de Dana y la de Strunz, ambas basadas en la composición, en especial respecto a los grupos químicos importantes, y en la estructura. James Dwight Dana, un geólogo principal de su tiempo, publicó por primera vez su System of Mineralogy [Sistema de Mineralogía] en 1837; en 1997 se editó su octava edición. La clasificación de Dana asigna un número de cuatro partes a una especie mineral. Su número de clase se basa en los grupos de composición importantes; el número de tipo da la relación de cationes/aniones en el mineral; y los dos últimos números corresponden al grupo de minerales por similitud estructural dentro de un tipo o clase determinada. La clasificación de Strunz ,utilizada con menor frecuencia y llamada así por el mineralogista alemán Karl Hugo Strunz, se basa en el sistema de Dana, pero combina tanto criterios químicos como estructurales, estos últimos con respecto a la distribución de los enlaces químicos.: 558–559  En enero de 2016, la IMA había aprobado 5.090 especies minerales. Se han nombrado en general en honor de una persona (45 %) ,ver: Anexo:Minerales nombrados según personas,, seguidos por la ubicación del lugar, mina o yacimiento del descubrimiento (23 %); otras etimologías comunes son los nombres basados en la composición química (14 %) y en las propiedades físicas (8 %).: 20–22, 556  El sufijo común -ita usado en los nombres de las especies minerales desciende del antiguo sufijo griego - ί τ η ς (-ites), que significa relacionado con o que pertenece a. Química mineral Hübnerita, el miembro final rico en manganeso de la serie de la wolframita, con cuarzo menor en el fondo La abundancia y diversidad de minerales es controlada directamente por su composición química, que a su vez, depende de la abundancia de los elementos en la Tierra. La mayoría de los minerales observados derivan de la corteza terrestre. Ocho elementos representan la mayor parte de los componentes clave de los minerales, debido a su abundancia en la corteza terrestre. Estos ocho elementos suponen más del 98 % de la corteza en peso, y son, en orden decreciente: oxígeno, silicio, aluminio, hierro, magnesio, calcio, sodio y potasio. El oxígeno y el silicio son, con mucho, los dos más importantes ,el oxígeno compone, en peso, el 46,6 % de la corteza terrestre, y el silicio un 27,7 %.: 4–7  Los minerales que se forman son controlados directamente por la química mayor del cuerpo matriz. Por ejemplo, un magma rico en hierro y magnesio formará minerales máficos, como el olivino y los piroxenos; por el contrario, un magma más rico en sílice cristalizará para formar minerales que incorporen más SiO 2, como los feldespatos y cuarzos. La caliza, la calcita o la aragonita (todas CaCO 3) se forman porque la roca es rica en calcio y carbonato. Un corolario es que no se encontrará un mineral en una roca cuya química mayor no se parezca a la química mayor del mineral dado, con la excepción de algunas trazas de minerales. Por ejemplo, la cianita, Al 2SiO 5, se forma a partir del metamorfismo de lutitas ricas en aluminio; no sería probable que ocurriera en rocas pobres en aluminio, como la cuarcita. La composición química puede variar entre las especies terminales de una serie de solución sólida. Por ejemplo, los feldespatos plagioclasa comprenden una serie continua que va desde el miembro extremo de la albita, rica en sodio (NaAlSi 8), hasta la anortita, rica en calcio (CaAl 2Si 8), con cuatro variedades intermedias reconocidas entre ellas (recogidas en orden de riqueza del sodio al calcio): oligoclasa, andesina, labradorita y bytownita.: 586  Otros ejemplos de serie son la serie del olivino, desde la forsterita, rica en magnesio, a la fayalita, rica en hierro, y la serie del wolframita, desde la hübnerita, rica en manganeso, hasta la ferberita, rica en hierro. La sustitución química y la coordinación de poliedros explican esta característica común de los minerales. En la naturaleza, los minerales no son sustancias puras, y se contaminan por otros elementos que están presentes en el sistema químico dado. Como resultado, es posible que un elemento sea sustituido por otro.: 141  La sustitución química se producirá entre iones de un tamaño y carga similares; por ejemplo, K+ no sustituirá a Si4+ debido a las incompatibilidades químicas y estructurales causadas por la gran diferencia en tamaño y carga. Un ejemplo común de sustitución química es el del Si4+ > por Al3+ , que están próximos en carga, tamaño y abundancia en la corteza terrestre. En el ejemplo de la plagioclasa, hay tres casos de sustitución. Los feldespatos son todos armazones de sílice, que tienen una relación de silicio-oxígeno de 2:1, y el espacio para otros elementos se da por la sustitución del ion Si4+ por el ion Al3+ para dar una unidad de base de [AlSi 8]− ; sin la sustitución, la fórmula puede ser cargada-equilibrada como SiO 2, dando cuarzo.: 14  La importancia de esta propiedad estructural se explica además por los poliedros de coordinación. La segunda sustitución se produce entre el ion Na+ y el ion Ca2+ ; sin embargo, la diferencia en la carga tiene que contabilizarse haciendo una segunda sustitución del ion Si4+ por el ion Al3+ .: 585  La coordinación de poliedros es una representación geométrica de cómo un catión está rodeado por un anión. En mineralogía, debido a su abundancia en la corteza terrestre, los poliedros de coordinación se consideran generalmente en términos del oxígeno. La unidad base de los minerales de silicato es el tetraedro de sílice ,un ion [SiO 4]4− rodeado de cuatro O2− ,. Una forma alternativa de describir la coordinación del silicato es mediante un número: en el caso del tetraedro de sílice, se dice que tiene un número de coordinación de 4. Diversos cationes tienen un rango específico de posibles números de coordinación; para el silicio, es casi siempre 4, excepto para minerales de muy altas presiones en los que los compuestos se comprimen de tal manera que el silicio está seis veces (octaédrico) coordinado con el oxígeno. Los cationes mayores tienen un número de coordinación más grande debido al aumento en el tamaño relativo en comparación con el oxígeno (la última subcapa orbital de los átomos más pesados es diferente también). Los cambios en los números de coordinación conduce a diferencias físicas y mineralógicas; por ejemplo, a alta presión, tal como en el manto, muchos minerales, especialmente algunos silicatos como el olivino y los granates cambiarán a una estructura de perovskita, en el que el silicio está en coordinación octaédrica. Otro ejemplo son los aluminosilicatos cianita, andalucita y silimanita (polimorfos, ya que comparten la fórmula Al 2SiO 5), que se diferencian por el número de coordinación del Al3+ ; estos minerales transitan de uno al otro como una respuesta a los cambios en la presión y en la temperatura.: 4–7  En el caso de materiales de silicato, la sustitución del ion Si4+ por Al3+ permite una variedad de minerales, debido a la necesidad de equilibrar las cargas.: 12–17  Cuando los minerales reaccionan, los productos a veces asumirán la forma del reactivo; el producto mineral se denomina por ser un pseudomorfo de (o después) del reactivo. Aquí se ilustra un pseudomorfo de la caolinita después de la ortoclasa. Aquí, el pseudomorfo conserva la macla Carlsbad común en la ortoclasa. Los cambios de temperatura, de presión y de composición alteran la mineralogía de una roca simple: los cambios en la composición pueden ser causados por procesos como la erosión o metasomatismo (alteración hidrotérmica); los cambios en la temperatura y en la presión se producen cuando la roca madre se somete a movimientos tectónicos o magmáticos en diferentes regímenes físicos; y los cambios en las condiciones termodinámicas favorecen que algunas asociaciones de minerales reaccionen entre sí para producir nuevos minerales. Como tal, es posible que dos rocas tengan una química de roca base idéntica, o muy similar, sin tener una mineralogía similar. Este proceso de alteración mineralógica está relacionado con el ciclo de las rocas. Un ejemplo de una serie de reacciones minerales se ilustra como sigue.: 549  El feldespato ortoclasa (KAlSi 8) es un mineral que se encuentra comúnmente en el granito, una roca ígnea plutónica. Cuando se expone a la intemperie, reacciona para formar caolinita (Al 2Si 5(OH) 4, un mineral sedimentario, y ácido silícico): 2KAlSi 8 + 5H 2O + 2H+ → Al 2Si 5(OH) 4 + 4H 2SiO 3 + 2K+ Bajo condiciones metamórficas de bajo grado, la caolinita reacciona con el cuarzo para formar pirofilita (Al 2Si 10(OH) 2): 2Si 5(OH) 4 + SiO 2 → Al 2Si 10(OH) A medida que aumenta el grado metamórfico, la pirofilita reacciona para formar cianita y cuarzo: 2Si 10(OH) 2 → Al 2SiO 5 + 3SiO 2 + H Alternativamente, un mineral puede cambiar su estructura cristalina como consecuencia de cambios de temperatura y de presión sin reaccionar. Por ejemplo, el cuarzo se convertirá en una variedad de sus polimorfos de SiO 2, como la tridimita y la cristobalita a altas temperaturas, y en coesita a altas presiones.: 579  Propiedades físicas de los minerales La caracterización de los minerales puede variar de ser muy simple a muy difícil. Un mineral puede ser identificado por varias propiedades físicas, siendo algunas de ellas suficientes para una plena identificación sin ambigüedades. En otros casos, los minerales solo se pueden clasificar mediante análisis más complejos, ópticos, químicos o de difracción de rayos X; estos métodos, sin embargo, pueden ser costosos y consumen mucho tiempo. Las propiedades físicas que se estudian para la clasificación son la estructura cristalina y el hábito, la dureza y el lustre, la diafanidad, el color, el rayado, la exfoliación y la fractura, y la densidad relativa. Otras pruebas menos generales son la fluorescencia y fosforescencia, el magnetismo, la radioactividad, la tenacidad (respuesta a los cambios mecánicos inducidos de forma), la piezoelectricidad y la reactividad para diluir ácidos.: 22–23  Estructura cristalina y hábito Acicular natrolita Artículos principales: Sistema cristalino, Hábito cristalino y Macla. El topacio tiene una forma característica de cristal alargado ortorrómbico. La estructura cristalina resulta de la disposición espacial geométrica ordenada de los átomos en la estructura interna de un mineral. Esta estructura cristalina se basa en una disposición atómica o iónica interna regular, que se expresa a menudo en la forma geométrica que el cristal toma. Incluso cuando los granos minerales son demasiado pequeños para ser vistos o son de forma irregular, la estructura cristalina subyacente siempre es periódica y se puede determinar por difracción de rayos X.: 2–4  Los minerales por lo general son descritos por su contenido de simetría. Los cristales están cristalográficamente restringidos a 32 grupos de puntos, que se diferencian por su simetría. Estos grupos se clasifican a su vez en categorías más amplias, siendo las de mayor alcance seis familias de cristales.: 69–80  (a veces una de las familias, la hexagonal, también se divide en dos sistemas cristalinos: el trigonal, que tiene un eje tres veces simétrico, y el hexagonal, que tiene un eje seis veces simétrico). Estas familias pueden ser descritas por las longitudes relativas de los tres ejes cristalográficos, y los ángulos que forman entre ellos; estas relaciones corresponden a las operaciones de simetría que definen los grupos de puntos más estrechos. Se resumen a continuación; a, b, y c representan los ejes, y α, β, y γ representan el ángulo opuesto al eje cristalográfico respectivo (por ejemplo, α es el ángulo opuesto al eje a, es decir el ángulo entre los ejes b y c.):: 69–80  Sistema cristalino Ejes Ángulos entre ejes Ejemplo comunes La química y la estructura cristalina, en conjunto, definen un mineral. Con una restricción a grupos de 32 puntos, los minerales de diferente química pueden tener una estructura cristalina idéntica. Por ejemplo, la halita (NaCl), la galena (PbS) y la periclasa (MgO) pertenecen todas al grupo de puntos hexaoctahedral (familia isométrica), ya que tienen una estequiometría similar entre sus diferentes elementos constitutivos. En contraste, los polimorfos son agrupaciones de minerales que comparten una fórmula química, pero que tienen una estructura diferente. Por ejemplo, la pirita y la marcasita, ambos sulfuros de hierro, tienen la fórmula FeS 2; sin embargo, el primero es isométrico mientras que el último es ortorrómbico. Este polimorfismo se extiende a otros sulfuros de fórmula genérica AX 2; estos dos grupos son conocidos colectivamente como los grupos de la pirita y marcasita.: 654–655  El polimorfismo se puede extender más allá del contenido de la pura simetría. Los aluminosilicatos son un grupo de tres minerales ,cianita, andalucita y silimanita, que comparten la fórmula química Al 2SiO 5. La cianita es triclínica, mientras que la andalucita y la silimanita son ambas ortorrómbicas y pertenecen al grupo de puntos bipiramidal. Estas diferencias surgen en correspondencia a cómo el aluminio se coordina dentro de la estructura cristalina. En todos los minerales, un ion de aluminio está siempre seis veces coordinado con el oxígeno; el silicio, por regla general está en coordinación de cuatro veces en todos los minerales; una excepción es un caso como la stishovita (SiO 2, un polimorfo de cuarzo de ultra-alta presión con estructura de rutilo).: 581  En la cianita, el segundo aluminio está en coordinación seis veces; su fórmula química se puede expresar como Al Al SiO 5, para reflejar su estructura cristalina. La andalucita tiene el segundo aluminio en coordinación cinco veces (Al Al SiO 5) y la silimanita lo tiene en coordinación de cuatro veces ((Al Al SiO 5).: 631–632  Las diferencias en la estructura cristalina y la química influyen mucho en otras propiedades físicas del mineral. Los alótropos del carbono, el diamante y el grafito, tienen propiedades muy distintas; el diamante es la sustancia natural más dura, tiene un lustre adamantino, y pertenece a la familia isométrica, mientras que el grafito es muy blando, tiene un lustre grasiento, y cristaliza en la familia hexagonal. Esta diferencia se explica por diferencias en el enlace. En el diamante, los átomos de carbono están en orbitales híbridos sp3, lo que significa que forman un marco o armazón en el que cada carbono está unido covalentemente a cuatro vecinos de una manera tetraédrica. Por otro lado, el grafito forma láminas de átomos de carbono en orbitales híbridos sp2, en los que cada átomo de carbono está unido covalentemente a sólo otros tres. Estas hojas se mantienen unidas por fuerzas mucho más débiles que las fuerzas de van der Waals, y esta discrepancia se traduce en grandes diferencias macroscópicas.: 166  Maclas de contacto en la espinela La macla es la interpenetración entre dos o más cristales de una única especie mineral. La geometría de la macla está controlada por la simetría del mineral y, como resultado, hay varios tipos: de contacto, reticuladas, geniculadas, de penetración, cíclicas y polisintéticas. Las maclas de contacto, o maclas simples, constan de dos cristales unidos en un plano; este tipo de maclas es común en la espinela; las maclas reticuladas, comunes en forma de rutilo, son cristales entrelazados que se asemejan a un reticulado. Las maclas geniculadas tienen una mezcla en el medio que es causada por el comienzo del maclado. Las maclas de penetración constan de dos cristales individuales que han crecido uno dentro de otro; ejemplos de este hermanamiento son las maclas en forma de cruz de la estaurolita y las maclas de Carlsbad en la ortoclasa. Las maclas cíclicas son causadas por el maclado repetido en torno a un eje de rotación. Se produce alrededor de tres, cuatro, cinco, seis, o ocho ejes de plegado. Las maclas polisintéticas son similares a las maclas cíclicas por la presencia de maclados repetitivos aunque, en lugar de producirse alrededor de un eje de rotación, lo hacen siguiendo planos paralelos, por lo general en una escala microscópica.: 41–43 : 39  El hábito cristalino se refiere a la forma general de cristal. Se utilizan varios vocablos para describir esta propiedad: acicular, que describe cristales en forma de aguja como en la natrolita; acuchillado; arborescente o dendrítica (patrón de árbol, común en el cobre nativo); equante, que es típico del granate; prismático (alargado en una dirección); y tabular, que se diferencia de acuchillado en que el primero es plano mientras que este último tiene un alargamiento definido. En relación con la forma cristalina, la calidad de las caras del cristal es diagnóstico de algunos minerales, especialmente con un microscopio petrográfico. Los cristales euhedrales tienen una forma externa definida, mientras que los cristales anhedrales no la tienen; las formas intermedias se denominan subhedrales.: 32–39 : 38  Dureza Artículo principal: Escalas de dureza El diamante es el material natural más duro (dureza de Mohs de 10). La dureza de un mineral define cuánto puede resistir el rayado. Esta propiedad física depende de la composición química y de la estructura cristalina, y por ello no es necesariamente constante en todas las caras; la debilidad cristalográfica hace que algunas direcciones sean más blandas que otras.: 28–29  Un ejemplo de esta propiedad se muestra en la cianita, que tiene una dureza de Mohs de 5½ en la dirección paralela a [001], pero de 7 paralela a . La escala más común de medición es la escala de dureza de Mohs ordinaria. Definida por diez indicadores, un mineral con un índice más alto raya los minerales que están por debajo de él en la escala. La escala va desde el talco, un silicato estratificado, hasta el diamante, un polimorfo de carbono que es el material natural más duro.: 28–29  Escala de Mohs de dureza Lustre y diafanidad Artículo principal: Lustre La pirita tiene un brillo metálico La esfalerita tiene un brillo submetálico El lustre o brillo indica cómo se refleja la luz que incide sobre la superficie del mineral, una propiedad que no depende del color y sí de su naturaleza química: es más intenso en sustancias que tienen enlaces metálicos y menor en las de enlaces iónicos o covalentes. El tipo y la intensidad del brillo dependen del índice de refracción y de la relación entre la luz absorbida y la reflejada. Hay numerosos vocablos cualitativos para su descripción, que se agrupan en tres: brillo metálico, cuando reflejan casi toda la luz visible que reciben. Son opacos y con índices de refracción mayores de 3. Suelen ser metales nativos (cuando no están oxidados) y muchos sulfuros (pirita) y óxidos de metales de transición (hematites). brillo submetálico, cuando reflejan una pequeña parte de la luz visible que reciben. Son opacos y su índice de refracción es ligeramente inferior a 3. Suelen ser elementos semimetálicos (grafito), sulfuros y óxidos. brillo no metálico, cuando transmiten la luz en cierto grado. Esta condición es ambigua y se emplean varios vocablos para estimar los matices: vítreo, con índice de refracción 1.33-2.00. Son minerales transparentes, en general compuestos por aniones oxigenados (oxoaniones), como carbonatos, sulfatos, fosfatos, silicatos, nitratos, etc. También varios halogenuros y óxidos (cuarzo hialino o cristal de roca); adamantino, con índice de refracción 2.00-2.50. Es el brillo típico del diamante y de algunas otras variedades aunque a veces para estas se usa el término «subadamantino»; nacarado o perlado, un brillo irisado típico de minerales fácilmente exfoliables, como las micas, el yeso y la apofilita; craso o graso, motivado por la presencia de pequeñas rugosidades en la superficie, a veces microscópicas. Lo tienen algunas blendas, la nefelina y el cuarzo en masa o lechoso; resinoso o céreo, de minerales como el azufre y ciertas blendas y granates; sedoso, característico de minerales fibrosos, como el yeso fibroso, la crisotila y la ulexita; mate, cuando no presentan ningún reflejo, como la creta (calcita) o las arcillas. En este caso también se dice que el mineral no tiene brillo.: 26–28  Diferentes brillos de minerales Vítreo: cuarzo Vítreo: cuarzo Adamantino: diamantes tallados Adamantino: diamantes tallados Nacarado: moscovita Nacarado: moscovita Craso: ópalo musgoso Craso: ópalo musgoso Resinoso: ámbar Resinoso: ámbar Sedoso: selenita, variedad del yeso Sedoso: selenita, variedad del yeso Ceroso: jade Ceroso: jade Mate: caolinita Mate: caolinita La diafanidad de un mineral describe la capacidad de la luz de pasar a través de él. Los minerales transparentes no disminuyen la intensidad de la luz que pasa a través de ellos. Un ejemplo de estos minerales es la moscovita (mica de potasio); algunas variedades son lo suficientemente claras como para haber sido utilizadas como vidrios en las ventanas. Los minerales translúcidos permiten pasar algo de luz, pero menos que los que son transparentes. La jadeíta y nefrita (formas minerales del jade) son ejemplos de minerales con esta propiedad. Los minerales que no dejan pasar la luz se denominan opacos.: 25  La diafanidad de un mineral depende del espesor de la muestra. Cuando un mineral es suficientemente delgado (por ejemplo, en una lámina delgada para petrografía) puede llegar a ser transparente, incluso si esa propiedad no se ve en la muestra de mano. Por el contrario, algunos minerales, como la hematita o la pirita son opacos incluso en láminas delgadas.: 25  Color y raya Artículo principal: Método de la raya El color, en general, no es una característica que permita caracterizar minerales. Se muestra una uvarovita verde (izquierda) y una grosularia rojo-rosada (derecha), ambos granates. Las propiedades que servirían para el diagnóstico serían los cristales rombododecaédricos, el lustre resinoso, y la dureza, de alrededor de 7. Elbaita dicróica Esmeralda El color es la propiedad más obvia de un mineral, pero a menudo no sirve para caracterizarlo.: 23  Es causada por la radiación electromagnética que interactúa con los electrones (excepto en el caso de incandescencia, que no se aplica a los minerales).: 131–144  Por su contribución en el color, se definen tres grandes clases de minerales: minerales idiocromáticos (o autocoloreados), que deben su color a los constituyentes principales y que son diagnosticables.: 24  Son minerales siempre del mismo color, como la malaquita (verde), la azurita (azul) y muchos minerales metálicos. Sus colores suelen variar ligeramente debido a la presencia de pequeñas cantidades de otros metales: el oro, por ejemplo, es menos amarillo cuando se mezcla con un poco de plata, y más rosado cuando es mezclado junto con cobre. minerales alocromáticos (o coloreados por otros), que deben su coloración a pequeñas cantidades en la composición consideradas como impurezas, a las que se llama cromóforos, usualmente metales (hierro, cromo, cobre, vanadio o manganeso). Son capaces de adoptar más de una coloración, como el berilo o las dos variedades del corindón, el rubí y el zafiro.: 24  Algunos minerales alocromáticos que pueden tener prácticamente cada color imaginable, e incluso pueden tener muchos colores en un solo cristal. minerales pseudocromáticos (o de color falso), cuya coloración proviene de la estructura física del cristal y la interferencia con las ondas de luz. Son ejemplos la labradorita, la bornita y el ópalo, que está formado por capas microscópicas de esferas de sílice. Al pasar a su través la luz se separa en los colores que la componen, más o menos como ocurre cuando se refleja en una capa de aceite sobre el agua. Algunos metales, como el hierro, pueden ser tanto alocromático como idiocromático: en el primer caso es considerado como una impureza, mientras que en el segundo forma parte intrínseca del mineral coloreado. El color de algunos minerales puede cambiar, ya sea de manera natural o con un poco de ayuda. Los bajos niveles de radiación, que se dan a menudo en la naturaleza, pueden contribuir a oscurecer algunos minerales incoloros. Los mismos berilos de color amarillo verdoso se tratan artificialmente ahora con calor para darles una coloración más azulada. Además del simple color del cuerpo, los minerales pueden tener otras propiedades ópticas distintivas que pueden implican variabilidad del color: juego de colores, como en el ópalo, significa que la muestra refleja diferentes colores cuando se ilumina, a causa de que la luz se refleja desde las ordenadas esferas de sílice microscópicas de su estructura física. pleocroísmo, facultad de absorber las radiaciones luminosas de distinta manera en función de la dirección de vibración: un mismo cristal puede aparecer con coloraciones diferentes dependiendo de la orientación en que haya caído en la preparación microscópica iridiscencia, una variedad del juego de colores por la que la luz se dispersa en un recubrimiento sobre la superficie del cristal, planos de exfoliación o capas desactivas que tienen gradaciones químicas menores.: 24–26  chatoyancia («ojo de gato») es el efecto de bandas onduladas de color que se observan cuando se rota la muestra; asterismo, una variedad de la chatonyancia, un fenómeno sobre un área que hace aparecer una estrella sobre la superficie reflectante de un corte de cabujón. Se da en algunos rubíes, zafiros y otras gemas (granate-estrella, diópsido-estrella, espinela-estrella, etc.) y particularmente en el corundum de calidad gema.: 24–26  empañamiento Propiedades ópticas Asterismo en un zafiro-estrella azul Asterismo en un zafiro-estrella azul Ojo de tigre Ojo de tigre Iridiscencia en la labradorita Iridiscencia en la labradorita Barras de tungsteno con cristales evaporados, parcialmente oxidados con un colorido empañado Barras de tungsteno con cristales evaporados, parcialmente oxidados con un colorido empañado pleocroísmo en la cordierita, fuertemente dicroica pleocroísmo en la cordierita, fuertemente dicroica Placas de raya con pirita (izqda.) y rodocrosita (dcha.) La raya de un mineral se refiere al color de un mineral en forma de polvo, que puede o no ser idéntico al color de su cuerpo.: 24  La forma más común de evaluar esta propiedad se hace con una placa de raya, que está hecha de porcelana y es de color blanco o negro. La raya de un mineral es independiente de los elementos traza o de cualquier alteración de la superficie a causa de la intemperie.: 24  Un ejemplo común de esta propiedad se ilustra con la hematita, que es de color negro, plata o rojo en la muestra, pero que tiene una raya de color rojo cereza. a marrón rojizo.: 24  La raya es más a menudo distintiva de los minerales metálicos, en contraste con los minerales no metálicos, cuyo color de cuerpo está creada por elementos alocromáticos. La prueba de la raya se ve limitada por la dureza del mineral, ya que los minerales de dureza superior a siete rayan ellos la placa.: 24  Exfoliación, partición, fractura y tenacidad Artículos principales: Exfoliación, Fractura y Tenacidad. Perfecta exfoliación basal en la biotita (negra), y buena exfoliación en la matrix (ortoclasa rosa) Por definición, los minerales tienen una disposición atómica característica y cualquier debilidad de esa estructura cristalina es la causa de la existencia de los planos de debilidad. La rotura del mineral a lo largo de esos planos se denomina exfoliación. La calidad de la exfoliación puede ser descrita en función de cómo de limpia y fácilmente se rompa el mineral; los vocablos con los que se describen comúnmente esa calidad, en orden decreciente, son «perfecto», «bueno», «distinto» y «pobre». En particular en los minerales transparentes, o en una sección delgada, la exfoliación se puede ver como una serie de líneas paralelas que señalan las superficies planas cuando se ven de lado. La exfoliación no es una propiedad universal de los minerales; por ejemplo, el cuarzo, compuesto por tetraedros de sílice muy interconectados, no tiene ninguna debilidad cristalográfica que le permitiría exfoliarse. Por el contrario, las micas, que tienen una exfoliación basal perfecta, consisten en láminas de tetraedros de sílice que se mantienen juntas muy débilmente.: 39–40 : 29–30  Como la exfoliación es función de la cristalografía, hay gran variedad de tipos de exfoliación produciéndose en uno, dos, tres, cuatro o seis direcciones. La exfoliación basal en una única dirección es una característica distintiva de las micas. La exfoliación en dos direcciones, denominada prismática, se produce en anfíboles y piroxenos. Los minerales como la galena o la halita tienen exfoliación cúbica (o isométrica) en tres direcciones, a 90°; cuando hay tres direcciones de exfoliación, pero no a 90°, como en la calcita o en la rodocrosita, se denomina exfoliación romboédrica. La exfoliación octaédrica (cuatro direcciones) está presente en la fluorita y en el diamante, y la esfalerita tiene seis direcciones de exfoliación del dodecaedro.: 39–40 : 30–31  Los minerales con muchas exfoliaciones pueden no romper igual de bien en todas las direcciones; por ejemplo, la calcita tiene buena exfoliación en tres direcciones, pero el yeso solo tiene una exfoliación perfecta en una dirección, y pobre en las otras dos. Los ángulos entre los planos de exfoliación varían entre los minerales. Por ejemplo, dado que los anfíboles son silicatos de cadena doble y los piroxenos son silicatos de cadena única, el ángulo entre sus planos de exfoliación es diferente: los piroxenos exfolian en dos direcciones a aproximadamente 90°, mientras que los anfíboles lo hacen claramente en dos direcciones separadas aproximadamente a 120° y 60°. Los ángulos de exfoliación se pueden medir con un goniómetro de contacto, que es similar a un transportador.: 39–40 : 30–31  La partición, a veces llamada «exfoliación falsa», es similar en apariencia a la exfoliación pero se produce por defectos estructurales en el mineral en lugar de por una debilidad sistemática. La partición varía de cristal a cristal de un mismo mineral, mientras que todos los cristales de un mineral determinado exfoliaran si la estructura atómica permite tal propiedad. En general, la partición es causada por una cierta tensión aplicada a un cristal. Las fuentes de las tensiones incluyen la deformación (por ejemplo, un aumento de la presión), exsolución o maclado. Los minerales que a menudo muestran partición son los piroxenos, la hematita, la magnetita y el corindón.: 39–40 : 30–31  Cuando un mineral se rompe en una dirección que no corresponde a un plano de exfoliación, se habla de fractura. Hay varios tipos: concoidea, cuando se forman superficies redondeadas cóncavas o convexas, de relieve suave. Se produce solo en minerales muy homogéneo, siendo el ejemplo clásico la fractura del cuarzo; lisa, cuando aparecen superficies planas, suaves y sin asperezas; desigual o irregular, cuando surgen superficies rugosas e irregulares. Se da en el cobre nativo: 31–33 ; fibrosa o astillosa, cuando se rompe como una madera, formando astillas; ganchuda, cuando la superficie de rotura aparece dentada; terrosa, cuando se desmorona como un terrón. La tenacidad está relacionada tanto con la exfoliación y la fractura. Mientras que la fractura y la exfoliación describen las superficies que se crean cuando el mineral se rompe, la tenacidad describe la resistencia que ofrece el mineral a tal ruptura. Los minerales pueden ser:: 30–31  frágiles, cuando rompen con facilidad con poco esfuerzo; maleables, cuando se laminan mediante golpes; sectiles, cuando se secciona con una cuchilla formando virutas; dúctiles, cuando se puede estirar convirtiéndose en un hilo; flexibles, cuando al ser doblados no recuperan la forma al cesar el esfuerzo; elásticos, cuando al ser doblados recuperan la forma al cesar el esfuerzo. Densidad relativa La galena (PbS) es un mineral de alta densidad relativa. La densidad relativa (a veces llamada gravedad específica) describe numéricamente la densidad de un mineral. Las dimensiones de la densidad son unidades de masa divididas por unidades de volumen: kg/m³ o en g/cm³. La densidad relativa mide la cantidad de agua desplazada por una muestra mineral. Se define como el cociente de la masa de la muestra y la diferencia entre el peso de la muestra en el aire y su correspondiente peso en agua; la densidad relativa es una relación adimensional, sin unidades. Para la mayoría de los minerales, esta propiedad no sirve para caracterizarlos. Los minerales que forman las rocas ,normalmente silicatos y ocasionalmente carbonatos, tienen una densidad relativa de 2.5–3.5.<: 43–44  Una alta densidad relativa si permite diagnosticar algunos minerales. La variación química (y por consiguiente, en la clase mineral) se correlaciona con un cambio en la densidad relativa. Entre los minerales más comunes, los óxidos y sulfuros tienden a tener una alta densidad relativa, ya que incluyen elementos con mayor masa atómica. Una generalización es que los minerales metálicos o con brillo diamantino tienden a tener densidades relativas más altas que las que tienen los minerales no-metálicos o de brillo mate. Por ejemplo, la hematita, Fe 3, tiene una densidad relativa de 5.26 mientras que la galena, PbS, tiene una gravedad específica de 7.2–7.6, que es el resultado de su alto contenido en hierro y en plomo, respectivamente. La densidad relativa es muy alta en los metales nativos; la kamacita, una aleación de hierro-níquel común en los meteoritos de hierro, tiene una densidad relativa de 7.9, y el oro tiene una densidad relativa observada entre 15 y 19.3.: 43–44  Otras propiedades Carnotita (amarillo) es un mineral radioactivo Se pueden utilizar otras propiedades para identificar minerales, aunque son menos generales y solo aplicables a ciertos minerales. La inmersión en ácido diluido (a menudo en HCl al 10 %) ayuda a distinguir los carbonatos de otras clases de minerales. El ácido reacciona con el grupo del carbonato ([CO3] 2-), lo que causa que el área afectada sufra efervescencia, con desprendimiento de gas dióxido de carbono. Esta prueba se puede ampliar para poner a prueba el mineral en su forma original de cristal o en polvo. Un ejemplo de esta prueba se realiza para distinguir la calcita de la dolomita, especialmente dentro de las rocas (caliza y dolomía, respectivamente). La efervescencia de la calcita es inmediata en ácido, mientras que para que lo haga la dolomita el ácido debe aplicarse a muestras en polvo o sobre una superficie rayada en una roca.: 44–45  Los minerales de zeolita no sufren efervescencia en ácido; en vez de eso, se vuelven esmerilados después de 5-10 minutos, y si se dejan en ácido durante un día, se disuelven o se convierten en un gel de sílice. El magnetismo es una propiedad muy notable de ciertos minerales. Entre los minerales comunes, la magnetita muestra esta propiedad con fuerza, y también está presente, aunque no con tanta intensidad, en la pirrotita y la ilmenita.: 44–45  Algunos minerales también pueden identificarse mediante la prueba del sabor u olor. La halita, NaCl, es la sal de mesa; su homólogo de potasio, la silvita, tiene un sabor amargo pronunciado. Los sulfuros tienen un olor característico, sobre todo cuando las muestras están fracturadas, reaccionando o en polvo.: 44–45  La radiactividad es una propiedad poco frecuente, aunque algunos minerales pueden integrar elementos radiactivos. Pueden ser constituyentes que los definen, como el uranio en la uraninita, la autunita y la carnotita, o como impurezas traza. En este último caso, la desintegración de los elementos radiactivos daña el cristal mineral; el resultado, denominado «halo radiactivo» o «halo pleocroico», es observable mediante diversas técnicas, en especial en las láminas finas de petrografía.: 44–45  Clases de minerales Dado que la composición de la corteza terrestre está dominada por el silicio y el oxígeno, los elementos con silicatos son, con mucho, la clase de minerales más importante en términos de formación de rocas y diversidad: la mayoría de las rocas se componen en más de un 95% de minerales de silicato, y más del 90% de la corteza terrestre está compuesta por estos minerales.: 104  Además de los componentes principales, silicio y oxígeno, son comunes en los minerales de silicato otros elementos comunes en la corteza terrestre, como el aluminio, el magnesio, el hierro, el calcio, el sodio y el potasio.: 5  Los silicatos más importantes que forman rocas son los feldespatos, los cuarzos, los olivinos, los piroxenos, los anfíboles y las micas. A su vez, los minerales no-silicatos se subdividen en varias clases por su química dominante: elementos nativos, sulfuros, haluros, óxidos e hidróxidos, carbonatos y nitratos, boratos, sulfatos, fosfatos y compuestos orgánicos. La mayoría de las especies minerales no silicatos son extremadamente raras (constituyen en total un 8% de la corteza terrestre), aunque algunas son relativamente comunes, como la calcita, pirita, magnetita y hematita. Hay dos estilos estructurales principales observados en los no-silicatos: el empaquetamiento compacto y los tetraedros enlazados como aparecen en los silicatos. Las estructuras compactas son una manera de empaquetar densamente átomos y reducir al mínimo el espacio intersticial. El empaquetado compacto hexagonal consiste en apilar capas en las que cada capa es la misma (ababab), mientras que el empaquetado cúbico consiste en grupos de apilamiento de tres capas (abcabcabc). Análogos a los tetraedros de sílice enlazados son los tetraedros que forman los iones SO 4 (sulfato), PO 4 (fosfato), AsO 4 (arseniato), y VO 4 (vanadato). Los minerales no-silicatos tienen una gran importancia económica, ya que concentran más elementos que los minerales de silicato: 641–643  y se explotan especialmente como menas.: 641, 681  Silicatos Artículo principal: Minerales silicatos Esquema del tetraedro [SiO4]4− base de los silicatos Los silicatos son sales que combinan la sílice SiO 2 con otros óxidos metálicos. La base de la unidad de un mineral de silicato es el tetraedro [SiO4]4− : en la mayoría de casos, el silicio se encuentra coordinado cuatro veces, o en coordinación tetraédrica, con el oxígeno; en situaciones de muy altas presiones, el silicio estará coordinado seis veces, o en coordinación octaédrica, como en la estructura de perovskita o en el cuarzo polimorfo stishovita (SiO2). (En el último caso, el mineral ya no tiene una estructura de silicato, si no de rutilo (TiO 2) y su grupo asociado, que son óxidos simples.) Estos tetraedros de sílice son luego polimerizados en algún grado para crear otras estructuras, como cadenas unidimensionales, láminas bidimensionales o armazones tridimensionales. El mineral de un silicato básico sin polimerización de tetraedros requiere de otros elementos que equilibren la base cargada 4-. En las otras estructuras de silicato son varias las combinaciones de elementos que equilibran esa carga negativa. Es común que el Si4+ sea sustituido por Al3+ debido a la similitud en radio iónico y en carga; en otros casos, los tetraedros de [AlO 4]5− forman las mismas estructuras que lo hacían los tetraedros no sustituidos, pero los requisitos del equilibrio de cargas son diferentes.: 104–120  El grado de polimerización puede ser descrito tanto por la estructura formada como por el número de vértices tetraédricos (u oxígenos de coordinación) compartidos (por el aluminio y el silicio en sitios tetraédricos):: 105  los ortosilicatos (o nesosilicatos) no tienen ninguna vinculación de poliedros, así que los tetraedros no comparten vértices; los disilicatos (o sorosilicatos) tienen dos tetraedros que comparten un átomo de oxígeno; los inosilicatos son silicatos en cadena: los de cadena simple tienen dos vértices compartidos y los de cadena doble dos o tres; los filosilicatos forman una estructura de lámina que requiere tres oxígenos compartidos (en el caso de silicatos de cadena doble, algunos tetraedros deben compartir dos vértices en lugar de tres como harían si resultase una estructura de lámina); los silicatos en armazón o tectosilicatos, tienen tetraedros que comparten los cuatro vértices; los silicatos de anillo, o ciclosilicatos, solo necesitan tetraedros que compartan dos vértices para formar la estructura cíclica.: 104–117  Se describen a continuación en orden decreciente de polimerización, las subclases de silicato. Enlaces de tetraedros Ortosilicato: tetraedros simples Ortosilicato: tetraedros simples Sorosilicatos: dobles tetraedros Sorosilicatos: dobles tetraedros Inosilicatos: cadenas de tetraedros Inosilicatos: cadenas de tetraedros Inosilicatos: cadenas dobles de tetraedros Inosilicatos: cadenas dobles de tetraedros Ciclosilicatos: Anillos de tetraedros Ciclosilicatos: Anillos de tetraedros Tectosilicatos Artículo principal: Tectosilicato El cuarzo es el principal mineral de la serie de los tectosilicatos (cristal de roca de la mina La Gardette, Francia). Esquema de la estructura interna tridimensional de un cuarzo (cuarzo-β). Las esferas rojas representan iones de oxígeno y las esferas grises iones de silicio. Los tectosilicatos son muy abundantes, constituyendo aproximadamente el 64 % de los minerales de la corteza terrestre.También conocidos como silicatos de estructura en armazón, tienen el grado de polimerización más alto y tienden a ser químicamente estables como resultado de la fuerza de los enlaces covalentes.: 502  Son ejemplos el cuarzo, los feldespatos, los feldespatoides, y las zeolitas. Tienen una estructura basada en un entramado tridimensional de tetraedros (ZO 4) con los cuatro vértices ocupados por el ion O2- compartidos, lo que implica relaciones Z:O=1:2. La Z es silicio (Si) (la fórmula resultante es SiO 2, sílice), pero parte del Si4+ puede ser reemplazado por Al3+ (en raras ocasiones por Fe3+ , Ti3+ y B3+ ). Al suceder esto, las cargas negativas resultantes se compensan con la entrada de cationes grandes, como el K+ , el Na+ o el Ca2+ (y con menos frecuencia Ba2+ , Sr2+ y Cs+ ). También pueden tener aniones complementarios F−, Cl−, S2−, CO32−, SO42−. El cuarzo (SiO 2) es la especie mineral más abundante, formando el 12 % de la corteza terrestre. Se caracteriza por su alta resistividad química y física. Tiene varios polimorfos, incluyendo la tridimita y la cristobalita a altas temperaturas, la coesita a alta presión y la stishovita a ultra-alta presión. Este último mineral solo puede formarse en la Tierra por impacto de meteoritos, y su estructura está tan compuesta que había cambiado de una estructura de silicato a la de rutilo (TiO 2). El polimorfo de sílice que es más estable en la superficie de la Tierra es el α-cuarzo. Su homólogo, el cuarzo-β, está presente solo a altas temperaturas y presiones (a 1 bar, cambia a cuarzo-α por debajo de 573 °C). Estos dos polimorfos difieren en un retorcimiento de los enlaces; este cambio en la estructura da al cuarzo-β mayor simetría que al cuarzo-α, y por lo tanto también se les llama cuarzo alto (β) y cuarzo bajo (α).: 104 : 578–583  Los feldespatos son el grupo más abundante en la corteza terrestre, en torno al 50 %. En los feldespatos, los Al3+ sustitutos de los Si4+ crean un desequilibrio de carga que debe ser explicado por la adición de cationes. La estructura de base se convierte ya en [AlSi 8], ya en [Al 2Si 8]2− . Hay 22 especies minerales de feldespatos, subdivididas en dos grandes subgrupos ,alcalino y plagioclasa, y dos grupos menos comunes ,celsiana y banalsita,. Los feldespatos alcalinos son los más comunes en una serie que va desde la entre ortoclasa, rica en potasio, a la albita, rica en sodio; en el caso de las plagioclasas, la serie más común varía desde la albita a la anortita, rica en calcio. El maclado de cristales es común en los feldespatos, especialmente con maclas polisintéticas en las plagioclasas y maclas de Carlsbad en los feldespatos alcalinos. Si el último subgrupo se enfría lentamente a partir de una masa fundida, se forma laminillas de exsolución porque los dos componentes ,ortoclasa y albita, son inestables en solución sólida. La exsolution puede darse desde una escala microscópica hasta ser fácilmente observable en la muestra de mano; se forma una textura pertitica cuando un feldespato rico en Na exsolve en un huésped rico en K. La textura opuesta (antipertitica), cuando un feldespato rico en K exsolve en un huésped rico en Na, es muy rara.: 583–588  Los feldespatoides son estructuralmente similares a los feldespatos, pero se diferencian en que se forman en condiciones de carencia de silicio lo que permite una mayor sustitución por Al3+ . Como resultado, los feldespatoides no se pueden asociar con cuarzo. Un ejemplo común de un feldespatoide es la nefelina ((Na, K)AlSiO 4); comparada con los feldespatos alcalinos, la nefelina tiene una relación Al 3: SiO 2 de 1: 2, en lugar de 1:6 en el feldespato.: 588  Las zeolitas a menudo tienen hábitos de cristal distintivos, produciendo agujas, placas o bloques masivos. Se forman en presencia de agua a bajas temperaturas y presiones, y tienen canales y huecos en su estructura. Las zeolitas tienen varias aplicaciones industriales, especialmente en el tratamiento de aguas residuales.: 589–593  Ejemplos de tectosilicatos Albita Albita Anortita Anortita Ortoclasa Ortoclasa Nefelina Nefelina Zeolita Zeolita Filosilicatos Artículo principal: Filosilicato Moscovita, una especie mineral del grupo de las micas, dentro de la subclase de los filosilicatos Modelo poliédrico de la lámina de tetraedros de sílice Los filosilicatos son un grupo de minerales muy extendidos en la corteza terrestre, integrantes de muchos tipos de rocas, ígneas, metamórficas y sedimentarias. Las arcillas están formadas fundamentalmente por filosilicatos. La característica principal de los filosilicatos es su disposición en capas, que ocasiona hábitos típicos fácilmente reconocibles (minerales hojosos o escamosos). Además suelen ser minerales blandos y poco densos. Los filosilicatos consisten en apilamientos de láminas de tetraedros polimerizados. Las láminas, desde el punto de vista estructural, son de dos tipos: tetraédricas y octaédricas. Los tetraédricas están enlazados a tres sitios de oxígeno, lo que da una relación característica de silicio:oxígeno de 2:5. Ejemplos importantes son la mica, el grupo de las cloritas y los grupos de caolinita-serpentina. Las láminas están débilmente enlazadas por fuerzas de van der Waals o enlaces de hidrógeno, lo que provoca una debilidad cristalográfica, que a su vez conduce a una prominente exfoliación basal entre los filosilicatos.: 525  Además de los tetraedros, los filosilicatos tienen una hoja de octaedros (elementos de coordinación seis con oxígeno) que equilibran los tetraedros de base, que tienen una carga negativa (por ejemplo, [Si 10]4− ) Estas hojas de tetraedros (T) y octaedros (O) se apilan en una gran variedad de combinaciones para crear los distintos grupos de los filosilicatos. En una capa octaédrica, hay tres sitios octaédricos en una estructura única; sin embargo, no todos los sitios pueden estar ocupados. En ese caso, el mineral se denomina dioctahédrico, mientras que en otro caso se denomina trioctaédrico.: 110  El grupo de la caolinita-serpentina consiste en pilas de T-O (minerales de arcilla 1:1); su dureza varía de 2 a 4, cuando las láminas están retenidas por enlaces de hidrógeno. Los minerales de arcilla 2:1 (pirofilita-talco) consisten en pilas T-O-T, pero son más blandos (dureza 1-2), ya que están se mantienen unidos por fuerzas de van der Waals. Estos dos grupos de minerales están divididos en subgrupos según la ocupación octaédrica; específicamente, la caolinita y la pirofilita son dioctaédricos mientras que la serpentina y el talco son trioctaédricos.: 110–113  Las micas son también filosilicatos T-O-T apilados, pero difieren de los otro miembros de las subclases apiladas T-O-T y T-O en que incorporan aluminio en las láminas tetraédricas (los minerales de arcilla tienen Al3+ en los sitios octaédricos). Ejemplos comunes de micas son la moscovita y las series de la biotita. El grupo de la clorita se relaciona con el grupo de la mica, pero con una capa similar a la brucita (Mg(OH) 2) entre la de las pilas T-O-T.<: 602–605  A causa de su estructura química, los filosilicatos típicamente tienen capas flexibles, elásticas, transparentes que son aislantes eléctricos y se pueden dividir en escamas muy finas. Las micas se puede utilizar en la electrónica como aislantes, en la construcción, como relleno óptico, o incluso en cosméticos. La crisotila, una especie de serpentina, es la especie mineral más común en el amianto industrial, ya que es menos peligrosa en términos de la salud que los asbestos anfíboles.: 593–595  Ejemplos de filosilicatos Fuchsita, una mica Fuchsita, una mica Biotita Biotita Crisotilo Crisotilo Brucita Brucita Serpentina Serpentina Inosilicatos Artículo principal: Inosilicato Disposición cristalina de los inosilicatos Tremolita asbestiforme, parte del grupo de los anfiboles en la subclase de los inosilicatos Aegirina, un clinopiroxeno hierro-sodio. Es parte de la subclase inosilicatos. Los inosilicatos son metasilicatos que consisten en tetraedros unidos repetidamente en cadenas. Estas cadenas pueden ser simples ,cuando un tetraedro está unido a otros dos para formar una cadena continua, o dobles, cuando dos cadenas sencillas se combinan entre ellas. Los silicatos de cadena individuales tienen una relación de silicio:oxígeno de 1:3 (por ejemplo, [Si 6]4− ), mientras que las variedades de doble cadena tiene una proporción de 4:11, por ejemplo [Si 22]12− . Los inosilicatos tienen dos importantes grupos de minerales que forman rocas; los piroxenos, generalmente silicatos de cadena simple, y los anfiboles, de cadena doble.: 537  Hay cadenas de orden superior (por ejemplo, cadenas de tres, cuatro o cinco miembros) pero son raras. El grupo de los piroxenos consta de 21 especies minerales.: 112  Los piroxenos tienen una fórmula de estructura general (XYSi 6), siendo X un sitio octaédrico e Y otro que puede variar en número de coordinación de seis a ocho. La mayoría de las variedades de los piroxenos consisten en permutaciones de Ca2+ , Fe2+ y Mg2+ que equilibran la carga negativa de la cadena principal. Los piroxenos son comunes en la corteza terrestre (aproximadamente el 10 %) y son un componente clave de las rocas ígneas máficas.: 612–613  Los anfiboles tienen una gran variabilidad química, por ello descritos a veces como un «cesto mineralógico» o un «tiburón mineralógico nadando en un mar de elementos». La columna vertebral de los anfíboles es la [Si 22]12− ; está equilibrada por cationes en tres posiciones posibles, aunque la tercera posición no siempre se utiliza y un elemento puede ocupar las restantes. Los anfíboles están generalmente hidratados, es decir, que tienen un grupo hidroxilo ([OH]− ), aunque puede ser reemplazado por un fluoruro, un cloruro, o un ion de óxido.: 606–612  Debido a su química variable, hay más de 80 especies de anfíboles, aunque las variaciones más comunes, como en los piroxenos, implican mezclas de Ca2+ , Fe2+ y Mg2+ .: 112  Varias especies minerales de los anfíboles pueden tener un hábito cristalino asbestiforme. Estos minerales de asbesto forman fibras largas, delgadas, flexibles y fuertes, que son aislantes eléctricos, químicamente inertes y resistentes al calor; como tal, tienen varias aplicaciones, especialmente en materiales de construcción. Sin embargo, los asbestos son conocidos carcinógenos, y causan varias enfermedades más, como la asbestosis; los asbestos anfíboles (antofilita, tremolita, actinolita, grunerita y riebeckita) se consideran más peligrosos que el asbesto serpentina crisotilo.: 611–612  Ejemplos de inosilicatoss Diopsida, un piroxeno Diopsida, un piroxeno Piroxeno Piroxeno Antofilita (anfibol) Antofilita (anfibol) Tremolita (anfibol) Tremolita (anfibol) Crocidolita, variedad de riebeckita (anfíbol) Crocidolita, variedad de riebeckita (anfíbol) Ciclosilicatos Artículo principal: Ciclosilicato Estructura en anillo de la dioptasa La clase de los ciclosilicatos corresponde a la clase 9.C de la clasificación de Strunz y tiene 16 familias. Está integrada por tres o más tetraedros de [SiO4]4− unidos por sus vértices, formando un anillo cerrado, simple o doble, el cual puede tener enlaces iónicos con metales como por ejemplo sodio, calcio, hierro, aluminio, potasio, magnesio, etc. Algunos ejemplos de ciclosilicatos son la turmalina, cordierita, rubelita, benitoita, dioptasa, etc. Los ciclosilicatos, o silicatos de anillo, tienen una relación de silicio a oxígeno de 1:3. Los anillos de seis miembros son los más comunes, con una estructura de base de [Si 28]12− ; ejemplos del grupo son la turmalina y el berilo. Hay otras estructuras de anillo, habiendo sido descritas las de 3, 4, 8, 9 y 12.: 113–115  Los ciclosilicatos tienden a ser fuertes, con cristales alargados y estriados.: 558  Los anillos pueden ser simples o ramificados, aislados unos de otros o agrupados en dos. Estos anillos están generalmente apilados en la estructura y determinar canales que puede estar vacíos u ocupados por iones o moléculas. Los ciclosilicates se clasifican según el tipo de anillos, y en particular por el número de tetraedros en el anillo. Las turmalinas tienen una química muy compleja que puede ser descrita por una fórmula general XY 6(BO 3) 3T 18V 3W. El T 18 es la estructura básica del anillo, donde T es generalmente Si4+ , pero pueden ser sustituidos por Al3+ o B3+ . Las turmalinas pueden dividirse en subgrupos por el sitio que ocupe el X, y de ahí se subdividen por la química del sitio W. Los sitios Y y Z pueden acomodar una variedad de cationes, especialmente diversos metales de transición; esta variabilidad en el contenido del metal de transición estructural da al grupo de la turmalina mayor variabilidad en color. Otro ciclosilicato es el berilo, Al 2Be 3Si 18, cuyas variedades incluyen piedras preciosas como la esmeralda (verde) y la aguamarina (azulado). La cordierita es estructuralmente similar al berilo, y es un mineral metamórfico común.: 617–621  Ejemplos de ciclosilicatoss Elbaita, una turmalina con una distintiva banda coloreada Elbaita, una turmalina con una distintiva banda coloreada Benitoita Benitoita Cordierita Cordierita Dioptasa Dioptasa Berilo Berilo Sorosilicatos Artículo principal: Sorosilicato La epidota a menudo tiene un color verde pistacho distintivo. La clase de los sorosilicatos corresponde a la clase 9.B de la clasificación de Strunz y tiene 10 familias, de dos tipos, el de las epidotas y el de las idocrasas. Los sorosilicatos, también denominados disilicatos, tienen un enlace tetraedro-tetraedro en un oxígeno, lo que resulta en una relación de 2:7 de silicio al oxígeno. El elemento estructural común resultante es el grupo [Si 7]6− . Los disilicatos más comunes son, con mucho, los miembros del grupo de la epidota. Las epidotas se encuentran en diversos entornos geológicos, que van desde las cordilleras oceánicas a los granitos y hasta las metapelitas. Las epidotas se construyen alrededor de la estructura [(Si 4)(Si 7)]10− ; por ejemplo, las especies minerales de epidota tiene calcio, aluminio y hierro férrico para equilibrar las cargas: Ca 2Al 2(Fe3+ ,Al)(SiO 4)(Si 7)O(OH). La presencia de hierro como Fe3+ y Fe2+ ayuda a entender la fugacidad de oxígeno, que a su vez es un factor significativo en petrogénesis.: 612–627  Otros ejemplos de sorosilicatos son la lawsonita, un mineral metamórfico que forma las facies blueschist (ajuste de zona de subducción con baja temperatura y alta presión), la vesuvianita, que ocupa una cantidad significativa de calcio en su estructura química.: 612–627 : 565–573  Ortosilicatos Artículo principal: Ortosilicato Andradita negra, un miembro terminal del grupo de granates Modelo estructural del zirconio La clase de los ortosilicatos corresponde a la clase 9.A de la clasificación de Strunz y tiene 10 familias con cerca de 120 especies. Los ortosilicatos consisten en tetraedros aislados que tienen las cargas equilibrada por otros cationes.: 116–117  También denominados nesosilicatos, este tipo de silicatos tiene una relación silicio:oxígeno de 1:4 (por ejemplo, SiO 4). Los ortosilicatos típicos tienden a formar bloques de cristales equantes, y son bastante pesados.: 573  Varios minerales que forman rocas son parte de esta subclase, como los aluminosilicatos, el grupo del olivino o el grupo del granate. Los aluminosilicatos ,cianita, andalucita, y silimanita, todos Al 2SiO 5, están estructuralmente compuestos por un tetraedro [SiO 4]− y un Al3+ en coordinación octaédrica. El restante Al3+ puede estar en coordinación de seis (cianita), cinco (andalucita) o cuatro (silimanita); qué mineral se forma en un entorno dado depende de las condiciones de presión y temperatura. En la estructura del olivino, la serie principal de olivino (Mg, Fe) 2SiO 4 consisten en forsterita, rica en magnesio, y fayalita, rica en hierro. Tanto el hierro como el magnesio están en coordinación octaédrica con el oxígeno. Existen otras especies minerales que tienen esta estructura, como la tefroita, Mn 2SiO 4.: 574–575  El grupo del granate tiene una fórmula general de X 2(SiO 3, donde X es un gran catión ocho veces coordinado, e Y es un catión menor seis veces coordinado. Hay seis miembros terminales ideales de granate, divididos en dos grupos. Los granates piralspita tienen Al3+ en la posición Y: piropo (Mg 3Al 2(SiO 3), almandino (Fe 3Al 2(SiO 3), y espesartina (Mn 3Al 2(SiO 3). Los granates ugrandita tienen Ca2+ en la posición X: uvarovita (Ca 3Cr 2(SiO 3), grossular (Ca 3Al 2(SiO 3) y andradita (Ca 3Fe 2(SiO 3). Si bien hay dos subgrupos de granate, existen soluciones sólidas entre los seis miembros finales.: 116–117  Otros ortosilicatos son el zircón, la estaurolita y el topacio. El zirconio (ZrSiO 4) es útil en geocronología ya que el Zr4+ puede ser sustituido por U6+ ; además, debido a su estructura muy resistente, es difícil resetearlo como un cronómetro. La estaurolita es un común mineral índice de grado intermedio metamórfico. Tiene una estructura cristalina particularmente complicada que solo fue descrita plenamente en 1986. El topacio (Al 2SiO 4(F, OH) 2, que se encuentra a menudo en pegmatitas graníticas asociadas con turmalina, piedra preciosa es un mineral común.: 627–634  Ejemplos de ortosilicatos Andalucita, un aluminosilicato Andalucita, un aluminosilicato Almandina, del grupo del granate Almandina, del grupo del granate Humita Humita Ludwigita, del grupo del olivino Ludwigita, del grupo del olivino Zirconio Zirconio Minerales no silicatos Elementos nativos Artículo principal: Elementos nativos Oro nativo. Raro espécimen de cristales gruesos que crecen fuera de un tallo central (3.7 x 1.1 x 0.4 cm, de Venezuela). Los elementos nativos son aquellos minerales integrados por elementos que no están unidos químicamente a otros elementos. Este grupo incluye minerales metales nativos, semi-metales y no metales, y varias aleaciones sólidas y soluciones. Los metales se mantienen unidos por enlaces metálicos, lo que les confiere propiedades físicas distintivas, como su lustre metálico brillante, ductilidad y maleabilidad, y conductividad eléctrica. Los elementos nativos se subdividen en grupos por su estructura o atributos químicos. El grupo del oro, con una estructura cercana al empaquetamiento cúbico, incluye metales como el oro, la plata y el cobre. El grupo del platino es similar en estructura al grupo de oro. El grupo del hierro-níquel se caracteriza por tener varias especies de aleaciones de hierro-níquel. Dos ejemplos son la kamacita y la taenita, que se encuentran en meteoritos de hierro; estas especies difieren en la cantidad de Ni en la aleación; la kamacita tiene menos de 5–7 % de níquel y es una variedad de hierro nativo, mientras que el contenido de níquel de la taenita es del 7–37 %. Los minerales del grupo del arsénico se componen de semi-metales, que tienen solamente algunos metálicos; por ejemplo, carecen de la maleabilidad de los metales. El carbono nativo aparece en dos alótropos, el grafito y el diamante; el último se forma a muy alta presiones en el manto, lo que le confiere una estructura mucho más fuerte que el grafito.: 644–648  Sulfuros Artículo principal: Minerales sulfuros Cinabrio rojo (HgS), una mena del mercurio, sobre dolomita La clase de los minerales sulfuros y sulfosales ,denominación engañosa pues los sulfuros solo son una parte del grupo, corresponde a la clase 2 de la clasificación de Strunz y en ella se incluyen: minerales sulfuros ,con el ion S2− ,-, los seleniuros, teluriuros, arseniuros, antimoniuros, bismutiuros, sulfoarseniuros y sulfosales. Los sulfuros se clasifican por la relación del metal o del semimetal con el azufre, M:S igual a 2:1, o 1:1.: 649  A pesar de que los sulfuros son mucho menos abundantes que los silicatos, su química y sus estructuras son muy variadas, lo que explica porque el número de minerales de sulfuro es muy alto en relación con su abundancia. Se agrupan entre los sulfuros los minerales compuestos de uno o más metales o semimetales con un azufre, que tienen una fórmula de tipo general de M p, donde M es un metal (Ag, Cu, Pb, Zn, Fe, Ni, Hg, As, Sb, Mo, Hg, Tl, V). Los arseniuros, los antimoniuros, los telurios... se clasifican entre los «sulfuros» sensu lato debido a su similitud estructural con los sulfuros. Los sulfuros minerales se caracterizan por la unión covalente, la opacidad y el brillo metálico; se estudian con el microscopio de reflexión. Los sulfuros tienden a ser blandos y frágiles, con un alto peso específico y la mayoría son semiconductores. Muchos sulfuros en polvo, como la pirita, tienen un olor sulfuroso cuando son pulverizados. Los sulfuros son susceptibles a la intemperie, y muchos se disuelven fácilmente en agua; estos minerales disueltos se pueden después volver a redepositar, lo que crea yacimientos de menas secundarias.: 357  Muchos minerales de sulfuro son importantes económicamente como minerales metálicos; son ejemplos la esfalerita (ZnS), una mena de zinc; la galena (PbS), una mena de plomo; el cinabrio (HgS), una mena de mercurio; y la molibdenita (MoS 2, una mena de molibdeno.: 651–654  La pirita (FeS 2) es el sulfuro que aparece más y se puede encontrar en la mayoría de entornos geológicos. No es, sin embargo, una mena de hierro, pero puede ser oxidada para producir ácido sulfúrico.: 654  Relacionados con los sulfuros están las raras sulfosales, en las que un elemento metálico está unido al azufre y a un semimetal, como antimonio, arsénico o bismuto. Al igual que los sulfuros, las sulfosales son típicamente minerales blandos, pesados y frágiles.: 383  Ejemplos de sulfuros Pirita, disulfuro de hierro Pirita, disulfuro de hierro Esfalerita, mena de zinc Esfalerita, mena de zinc Molibdenita, mena de molibdeno Molibdenita, mena de molibdeno Estannita, mena de estaño Estannita, mena de estaño Reálgar, sulfuro de arsénico Reálgar, sulfuro de arsénico Óxidos Artículo principal: Minerales óxidos La clase de los minerales óxidos e hidróxidos corresponde a la clase 4 de la clasificación de Strunz y en ella se incluyen: óxidos, hidróxidos, vanadatos, arsenitos, antimonitos, bismutitos, sulfitos, selenitos, teluritos y yodatos. Los minerales óxidos se dividen en tres categorías: óxidos simples, hidróxidos y óxidos múltiples. Los óxidos simples se caracterizan por O2− como anión principal y enlace principalmente iónico. Se pueden subdividir además por la relación del oxígeno a los cationes. El grupo de la periclasa consta de minerales con una relación 1:1. Óxidos con una relación 2:1 incluyen la cuprita (Cu 2O) y el hielo de agua. minerales del grupo del corindón tienen una proporción de 2:3, e incluye minerales como el corindón (Al 3) y la hematita (Fe 3). Los minerales del grupo del rutilo tienen una proporción de 1:2; la especie del mismo nombre, rutilo (TiO 2) es el principal mena del titanio; Otros ejemplos incluyen la casiterita (SnO 2, mena de estaño), y pirolusita (MnO 2, mena de manganeso).: 400–403 : 657–660  En hidróxidos, el anión dominante es el ion hidroxilo, OH− . Las bauxitas son la mena principal del aluminio, y son una mezcla heterogénea de minerales de hidróxido de diáspora, gibbsita, y bohmita; se forman en áreas con una alta tasa de meteorización química (principalmente condiciones tropicales).: 663–664  Por último, varios óxidos son compuestos de dos metales con oxígeno. Un grupo importante dentro de esta clase son las espinelas, con una fórmula general de X2+ Y3+ 4. Ejemplos de especies incluyen la propia espinela (MgAl 4), la cromita (FeCr 4) y la magnetita (Fe 4). Esta última es fácilmente distinguible por su fuerte magnetismo, que se produce ya que tiene hierro en dos estados de oxidación (Fe2+ Fe3+ 4), lo que hace que sea un óxido múltiple en lugar de un óxido simple.: 660–663  Ejemplos de minerales óxidos Anatasa, dióxido de titanio (TiO 2) Anatasa, dióxido de titanio (TiO Cuprita, óxido de cobre Cuprita, óxido de cobre Casiterita, óxido de estaño Casiterita, óxido de estaño Gibbsita, hidróxido de aluminio Gibbsita, hidróxido de aluminio Magnetita, óxido de hierro Magnetita, óxido de hierro Haluros Cristales de halita cúbica rosa (NaCl; clase haluro) en una matriz de nahcolita (NaHCO 3; un carbonato, y la forma mineral del bicarbonato sódico, que se utilizan como bicarbonato de sodio). baking soda). Artículo principal: Minerales haluros La clase de los minerales haluros corresponde a la clase 3 de la clasificación de Strunz y en ella se incluyen: haluros o halogenuros simples o complejos, con H2O o sin ella, así como derivados oxihaluros, hidroxihaluros y haluros con doble enlace. Los minerales haluros son compuestos en los que un halógeno (flúor, cloro, yodo y bromo) es el anión principal. Estos minerales tienden a ser blandos, débiles, quebradizos y solubles en agua. Los ejemplos más comunes de haluros son la halita (NaCl, sal de mesa), la silvita (KCl) y la fluorita (CaF 2). La halita y la silvita se forman comúnmente como evaporitas, y pueden ser minerales dominantes en las rocas sedimentarias químicas. La criolita, Na 3AlF 6, es un mineral clave en la extracción de aluminio a partir de la bauxita; Sin embargo, dado que la única ocurrencia significativa está en Ivittuut, Groenlandia, en una pegmatita granítica, ya agotada, la criolita sintética se puede hacer a partir de la fluorita.: 425–430  Carbonatos Artículo principal: Minerales carbonatos Cristales de calcita de la mina Sweetwater, condado de Reynolds, Misuri (6,2 × 6 × 3,3 cm) La clase de los minerales carbonatos y nitratos corresponde a la clase 5 de la clasificación de Strunz y en ella se incluyen carbonatos, carbonatos de uranilo y nitratos. Los minerales carbonatos son aquellos en los que el grupo aniónico principal es un carbonato, [CO 3]2− . Los carbonatos tienden a ser frágiles, muchos tienen exfoliación romboédrica, y todos reaccionan con ácido.: 431  Debido a la última característica, los geólogos de campo a menudo llevan ácido clorhídrico diluido para distinguir los carbonatos de los no-carbonatos. La reacción del ácido con los carbonatos, que se encuentra más comúnmente como los polimorfos calcita y aragonita (CaCO 3), se refiere a la disolución y precipitación del mineral, que es un elemento clave en la formación de las cuevas de caliza ,con elementos como estalactitas y estalagmitas, y los accidentes geográficos kársticos. Los carbonatos se forman con mayor frecuencia en forma de sedimentos biogénicos o químicos en ambientes marinos. El grupo carbonato es estructuralmente un triángulo, donde un catión central de C4+ está rodeado por tres aniones O2− ; diferentes grupos de minerales se forman a partir de diferentes disposiciones de estos triángulos.: 667  El mineral de carbonato más común es la calcita, que es el componente principal de la sedimentaria caliza y del mármol metamórfico. La calcita, CaCO 3, puede tener una impureza de alto contenido en magnesio; en condiciones de alto magnesio, se formará en su lugar su polimorfo, la aragonita; la geoquímica marina se puede describir, en este sentido, como un mar de aragonito o mar de calcita, dependiendo de qué mineral se forme preferentemente. La dolomita es un carbonato doble, de fórmula CaMg(CO 2. La dolomitization secundaria de la caliza es común, en la que la calcita o la aragonita se convierten en dolomita; esta reacción aumenta el espacio de los poros (el volumen de la celda unidad de la dolomita es el 88 % del de la calcita), lo que puede crear un yacimiento de petróleo y gas. Estas dos especies minerales son miembros de los grupos de minerales del mismo nombre: el grupo de la calcita incluye carbonatos con fórmula general XCO 3 y el de la dolomita la de XY(CO 2.: 668–669  Ejemplos de minerales carbonatos Rodocrosita Rodocrosita Smithsonita Smithsonita Dolomita con calcita y calcopirita Dolomita con calcita y calcopirita Azurita y malaquita Azurita y malaquita Hanksita, uno de los pocos minerales considerado un carbonato y un sulfato Hanksita, uno de los pocos minerales considerado un carbonato y un sulfato Sulfatos Artículo principal: Minerales sulfatos Rosa del desierto de yeso La clase de los minerales sulfatos corresponde a la clase 7 de la clasificación de Strunz y en ella se incluyen: sulfatos, selenatos, teluratos, cromatos, molibdatos y wolframatos. Los minerales sulfatos tienen todos el anión sulfato, [SO 4]2− . Tienden a ser de transparentes a translúcidos, blandos, y muchos son frágiles.: 453  Los minerales de sulfato se forman comúnmente como evaporitas, donde se precipitan de la evaporación de las aguas salinas; alternativamente, los sulfatos también se pueden encontrar en los sistemas de vetas hidrotermales asociados con sulfuros,: 456–457  o como productos de oxidación de sulfuros.: 674  Los sulfatos se pueden subdividir en minerales anhidros e hidratados. El sulfato hidratado más común, con mucho, es el yeso, CaSO 4⋅2H 2O. Se forma como un evaporita, y se asocia con otros evaporitas como la calcita y la halita; si incorpora granos de arena cuando cristaliza, el yeso puede formar rosas del desierto. El yeso tiene muy baja conductividad térmica y mantiene una temperatura baja cuando se calienta a medida que pierde el calor por deshidratación; como tal, el yeso se utiliza como aislante en materiales de construcción. El equivalente anhidro del yeso es la anhidrita; se puede formar directamente de agua de mar en condiciones muy áridas. El grupo de la barita tiene la fórmula general XSO 4, donde X es un catión grande 12-enlazado. Son ejemplos la barita (BaSO 4), la celestina (SrSO 4), y la anglesita (PbSO 4); la anhidrita no es parte del grupo de la barita, ya que el más pequeño Ca2+ sólo tiene enlace ocho veces.: 672–673  Ejemplos de minerales sulfatos Barita con cerusita Barita con cerusita Fenicocroíta, un cromato Fenicocroíta, un cromato Lindgrenita, molibdato de cobre Lindgrenita, molibdato de cobre Anhidrita Anhidrita Xocomecatlita, un tellurato Xocomecatlita, un tellurato Fosfatos Artículo principal: Minerales fosfatos La clase de los minerales fosfatos corresponde a la clase 8 de la clasificación de Strunz y en ella se incluyen fosfatos, arseniatos y vanadatos. Son 51 familias agrupadas en 7 divisiones, un grupo grande y diverso, que sin embargo, tiene solo unas pocas especies relativamente comunes. Los minerales fosfatos se caracterizan por el anión fosfato coordinado tetraédricamente [PO 4]3− , aunque la estructura se puede generalizar siendo el fósforo sustituido por antimonio ([SbO 4]3− ), arsénico ([AsO 4]3− ), o vanadio ([VO 4]3− ). Los aniones de cloro (Cl− ), flúor (F− ) e hidróxido (OH− ) también encajan en la estructura cristalina. El fosfato más común es el grupo de la apatita, un nombre genérico que designa fosfatos hexagonales de composición bastante variable, Ca 5(PO 3(OH, Cl,F). Las especies más comunes del grupo son la fluorapatita (Ca 5(PO 3F), la clorapatita (Ca 5(PO 3Cl) y la hidroxiapatita (Ca 5(PO 3(OH)). Los minerales de este grupo son los principales constituyentes cristalinos de los dientes y de los huesos de los vertebrados. Otro grupo relativamente abundante es el grupo de la monacita, que tiene una estructura general de ATO 4, donde T es el fósforo o arsénico, y A es, a menudo, un elemento de las tierras raras. La monacita es importante en dos sentidos: en primer lugar, como sumidero de tierras raras, puede concentrar la cantidad suficiente de estos elementos para convertirse en una mena; en segundo lugar, los elementos del grupo de la monacita pueden incorporar cantidades relativamente grandes de uranio y torio, que pueden ser utilizadas para datar una roca basándose en la desintegración del U y Th en plomo.: 675–680  Ejemplos de minerales fosfatos Apatita Apatita Vivianita, un fosfato hidratado de hierro Vivianita, un fosfato hidratado de hierro Piromorfita, un cloro-fosfato anhidro de plomo Piromorfita, un cloro-fosfato anhidro de plomo Turquesa, fosfato hidratado de cobre y aluminio Turquesa, fosfato hidratado de cobre y aluminio Lazulita, un fosfato de hierro, aluminio y magnesio Lazulita, un fosfato de hierro, aluminio y magnesio Minerales orgánicos Artículo principal: Minerales compuestos orgánicos La clase de los minerales compuestos orgánicos corresponde a la clase 10 de la clasificación de Strunz y en ella se incluyen sales y ácidos orgánicos que aparezcan en minas y los hidrocarburos. Son 7 familias agrupadas en 3 divisiones, un grupo escaso. Estos raros compuestos contienen carbono orgánico, pero se pueden formar también mediante un proceso geológico. Por ejemplo, la whewellita, CaC 4⋅H 2O es un oxalato que se puede depositar en las venas de menas hidrotermales. Mientras el oxalato de calcio hidratado se puede encontrar en las vetas de carbón y en otros depósitos sedimentarios que comprenden materia orgánica, la ocurrencia hidrotérmica no se considera que está relacionada con la actividad biológica.: 681  Importancia y utilidad Artículo principal: Mineral industrial Minerales diversos Los minerales tienen gran importancia por sus múltiples aplicaciones en los diversos campos de la actividad humana. La industria moderna depende directa o indirectamente de los minerales. Algunos minerales se utilizan prácticamente tal como se extraen; por ejemplo el azufre, el talco, la sal de mesa, etc. Otros, en cambio, deben ser sometidos a diversos procesos para obtener el producto deseado, como el hierro, cobre, aluminio, estaño, etc. Los minerales constituyen la fuente de obtención de los diferentes metales, base tecnológica de la sociedad actual. Así, de distintos tipos de cuarzo y silicatos, se produce el vidrio. Los nitratos y fosfatos son utilizados como abono para la agricultura. Ciertos materiales, como el yeso, son utilizados profusamente en la construcción. Los minerales que entran en la categoría de piedras preciosas o semipreciosas, como los diamantes, topacios, rubíes, se destinan a la confección de joyas. Astrobiología Se ha sugerido que los biominerales podrían ser indicadores importantes de vida extraterrestre y que por lo tanto podrían desempeñar un papel importante en la búsqueda de vida pasada o presente en el planeta Marte. Por otra parte, se cree que los componentes orgánicos (biofirmas), que a menudo se asocian con los biominerales, juegan un papel crucial tanto en reacciones pre-bióticas como bióticas. El 24 de enero de 2014 la NASA informó que los estudios actuales de los astromóviles Curiosity y Opportunity en Marte estarán ahora destinados a la búsqueda de evidencia de vida antigua, incluyendo una biosfera basada en microorganismos autótrofos, quimiótrofos y/o quimiolitoautotróficos, así como en agua antigua, incluyendo ambientes fluvo-lacustres (llanuras relacionadas con antiguos ríos o lagos) que pueden haber sido habitables. La búsqueda de evidencia de habitabilidad, tafonomía (relacionada con los fósiles), y el carbono orgánico en el planeta Marte son ahora un objetivo primordial de la NASA."
ksampletext_wikipedia_geol_tierra: str = "Tierra. La Tierra (del latín terra) es un planeta del sistema solar que gira alrededor de su estrella ,el Sol, en la tercera órbita más interna. Es el más denso y el quinto mayor de los ocho planetas del sistema solar. También es el más grande de los cuatro planetas terrestres o rocosos (planetas interiores). La Tierra se formó hace aproximadamente 4550 millones de años y la vida surgió unos mil millones de años después. Es el hogar de millones de especies, incluidos los seres humanos y actualmente el único cuerpo astronómico donde se conoce la existencia de vida. La atmósfera y otras condiciones abióticas han sido alteradas significativamente por la biosfera del planeta, favoreciendo la proliferación de organismos aerobios, así como la formación de una capa de ozono que junto con el campo magnético terrestre bloquean la radiación solar dañina, permitiendo así la vida en la Tierra. Las propiedades físicas de la Tierra, la historia geológica y su órbita han permitido que la vida siga existiendo. Se estima que el planeta seguirá siendo capaz de sustentar vida durante otros 500 millones de años, ya que según las previsiones actuales, pasado ese tiempo la creciente luminosidad del Sol terminará causando la extinción de la biosfera. La superficie terrestre o corteza está dividida en varias placas tectónicas que se deslizan sobre el magma durante periodos de varios millones de años. La superficie está cubierta por continentes e islas; estos poseen varios lagos, ríos y otras fuentes de agua, que junto con los océanos de agua salada que representan cerca del 71 % de la superficie constituyen la hidrósfera. No se conoce ningún otro planeta con este equilibrio de agua líquida,[nota 6] que es indispensable para cualquier tipo de vida conocida. Los polos de la Tierra están cubiertos en su mayoría de hielo sólido (indlandsis de la Antártida) o de banquisas (casquete polar ártico). El interior del planeta es geológicamente activo, con una gruesa capa de manto relativamente sólido, un núcleo externo líquido que genera un campo magnético, y un sólido núcleo interior compuesto por aproximadamente un 88 % de hierro. La Tierra interactúa gravitatoriamente con otros objetos en el espacio, especialmente el Sol y la Luna. En la actualidad, la Tierra completa una órbita alrededor del Sol cada vez que realiza 366.26 giros sobre su eje, lo cual es equivalente a 365.26 días solares o un año sideral.[nota 7] El eje de rotación de la Tierra se encuentra inclinado 23.4° con respecto a la perpendicular a su plano orbital, lo que produce las variaciones estacionales en la superficie del planeta con un período de un año tropical (365.24 días solares). La Tierra posee un único satélite natural, la Luna, que comenzó a orbitar la Tierra hace 4530 millones de años; esta produce las mareas, estabiliza la inclinación del eje terrestre y reduce gradualmente la velocidad de rotación del planeta. Hace aproximadamente 3800 a 4100 millones de años, durante el llamado bombardeo intenso tardío, numerosos asteroides impactaron en la Tierra, causando significativos cambios en la mayor parte de su superficie. Tanto los minerales del planeta como los productos de la biosfera aportan recursos que se utilizan para sostener a la población humana mundial. Sus habitantes están agrupados en unos 200 estados soberanos independientes, que interactúan a través de la diplomacia, los viajes, el comercio y la acción militar. Las culturas humanas han desarrollado muchas ideas sobre el planeta, incluida la personificación de una deidad, la creencia en una Tierra plana o en la Tierra como centro del universo, y una perspectiva moderna del mundo como un entorno integrado que requiere administración. Eponimia y etimología El nombre del planeta Tierra se diferencia del de otros planetas del sistema solar porque no proviene de la mitología grecorromana de manos de autores griegos o romanos. El término latino «terra» significa literalmente suelo o tierra firme y de ahí deriva la palabra en español. «La Tierra» también se usa como sinónimo intercambiable por «mundo», «globo» y «planeta». En la Antigüedad la palabra tierra se usaba indistintamente para referirse al suelo, a la tierra como uno de los cuatro elementos, así como al mundo habitado, sin distinción clara entre ambos. Durante la Edad Media y hasta el Renacimiento, hay textos en latín que usan terra para referirse al mundo habitable y al orbe terrestre. En el tratado De sphaera mundi (~1230) de Johannes de Sacrobosco se refiere a “orbis” al ámbito de la tierra (o mundo terrestre) como esfera. Textos como Cosmographia de Bernardo Silvestre o los geógrafos medievales usaban “orbis terrarum” (círculo de las tierras) para referirse al mundo. En De revolutionibus orbium coelestium (Copérnico, 1543), en latín, aparecen frases como “terra quoque sphaerica sit” («que la Tierra también sea esférica»); Copérnico presentó el Sol como centro y situó la Tierra como uno de los planetas. En los trabajos de Kepler, en obras como Epitome Astronomiae Copernicanae, también aparece “Terra” en contextos genéricos (“in Terra” o “Terra et Luna”). Pero fue Valentín Naboth (o Valentinus Nabodus), un astrónomo y matemático del siglo XVI, en su obra Primae de coelo et terra institutiones (1573), quien asoció la Tierra con la diosa romana Terra o Tellus. Se trata de una costumbre renacentista de armonizar conocimiento científico con la mitología clásica: «La Tierra, llamada en latín Terra o Tellus, es la madre fértil que sostiene todas las criaturas; por eso la designamos con el nombre de la antigua diosa que los romanos veneraban como la dadora de vida y la cuidadora del suelo». Cronología Artículos principales: Historia de la Tierra y Edad de la Tierra. Los científicos han podido reconstruir información detallada sobre el pasado de la Tierra. Según estos estudios el material más antiguo del sistema solar se formó hace 4567.2 ± 0.6 millones de años, y en torno a unos 4550 millones de años atrás (con una incertidumbre del 1 %) se habían formado ya la Tierra y los otros planetas del sistema solar a partir de la nebulosa solar, una masa en forma de disco compuesta del polvo y gas remanente de la formación del Sol. Este proceso de formación de la Tierra a través de la acreción tuvo lugar mayoritariamente en un plazo de 10-20 millones de años. La capa exterior del planeta, inicialmente fundida, se enfrió hasta formar una corteza sólida cuando el agua comenzó a acumularse en la atmósfera. La Luna se formó poco antes, hace unos 4530 millones de años. Representación gráfica de la teoría del gran impacto. El actual modelo consensuado sobre la formación de la Luna es la teoría del gran impacto, que postula que la Luna se creó cuando un objeto del tamaño de Marte, con cerca del 10 % de la masa de la Tierra, impactó tangencialmente contra esta. En este modelo, parte de la masa de este cuerpo podría haberse fusionado con la Tierra, mientras otra parte habría sido expulsada al espacio, proporcionando suficiente material en órbita como para desencadenar nuevamente un proceso de aglutinamiento por fuerzas gravitatorias, y formando así la Luna. La desgasificación de la corteza y la actividad volcánica produjeron la atmósfera primordial de la Tierra. La condensación de vapor de agua, junto con el hielo y el agua líquida aportada por los asteroides y por protoplanetas, cometas y objetos transneptunianos, produjeron los océanos. El recién formado Sol solo tenía el 70 % de su luminosidad actual: sin embargo, existen evidencias que muestran que los primitivos océanos se mantuvieron en estado líquido; una contradicción denominada la «paradoja del joven Sol débil», ya que aparentemente el agua no debería ser capaz de permanecer en ese estado líquido, sino en el sólido, debido a la poca energía solar recibida. Sin embargo, una combinación de gases de efecto invernadero y mayores niveles de actividad solar contribuyeron a elevar la temperatura de la superficie terrestre, impidiendo así que los océanos se congelaran. Hace 3500 millones de años se formó el campo magnético de la Tierra, lo que ayudó a evitar que la atmósfera fuese arrastrada por el viento solar. Se han propuesto dos modelos para el crecimiento de los continentes: el modelo de crecimiento constante, y el modelo de crecimiento rápido en una fase temprana de la historia de la Tierra. Las investigaciones actuales sugieren que la segunda opción es más probable, con un rápido crecimiento inicial de la corteza continental, seguido de un largo período de estabilidad.[nota 8] En escalas de tiempo de cientos de millones de años de duración, la superficie terrestre ha estado en constante remodelación, formando y fragmentando continentes. Estos continentes se han desplazado por la superficie, combinándose en ocasiones para formar un supercontinente. Hace aproximadamente 750 millones de años (Ma), uno de los primeros supercontinentes conocidos, Rodinia, comenzó a resquebrajarse. Los continentes más tarde se recombinaron nuevamente para formar Pannotia, entre 600 a 540 Ma, y finalmente Pangea, que se fragmentó hace 180 Ma hasta llegar a la configuración continental actual. Evolución de la vida Historia de la vida ver • discusión • editar -4500 ,–-4000 ,–-3500 ,–-3000 ,–-2500 ,–-2000 ,–-1500 ,–-1000 ,–-500 ,–0 , Agua Vida unicelular Fotosíntesis Eucariotas Vida multicelular Vida terrestre Dinosaurios  Mamíferos Flores Tierra primitiva (−4540) Primeras aguas Vida temprana Meteoritos LHB Primeras evidencias de oxígeno Oxígeno atmosférico Gran Oxidación Primeras evidencias de reproducción sexual Biota ediacárica Explosión cámbrica Primeros humanos PongolanoHuronianoCriogénicoAndinoKarooCuaternario Escala vertical: millones de años. Etiquetas color naranja: eras de hielo conocidas. Artículo principal: Historia de la vida La Tierra proporciona el único ejemplo conocido de un entorno que ha dado lugar a la evolución de la vida. Se presume que procesos químicos altamente energéticos produjeron una molécula autorreplicante hace alrededor de 4000 millones de años, y hace entre 3500 y 3800 millones de años existió el último antepasado común universal. El desarrollo de la fotosíntesis permitió que los seres vivos recogiesen de forma directa la energía del Sol; el oxígeno resultante acumulado en la atmósfera formó una capa de ozono (una forma de oxígeno molecular [O3]) en la atmósfera superior. La incorporación de células más pequeñas dentro de las más grandes dio como resultado el desarrollo de las células complejas llamadas eucariotas. Los verdaderos organismos multicelulares se formaron cuando las células dentro de colonias se hicieron cada vez más especializadas. La vida colonizó la superficie de la Tierra en parte gracias a la absorción de la radiación ultravioleta por parte de la capa de ozono. En la década de 1960 surgió una hipótesis que afirmaba que durante el período Neoproterozoico, desde 750 hasta los 580 Ma, se produjo una intensa glaciación en la que gran parte del planeta fue cubierto por una capa de hielo. Esta hipótesis ha sido denominada la «Glaciación global», y es de particular interés, ya que este suceso precedió a la llamada explosión del Cámbrico, en la que las formas de vida multicelulares comenzaron a proliferar. Tras la explosión del Cámbrico, hace unos 535 Ma se han producido cinco extinciones en masa. De ellas, el evento más reciente ocurrió hace 65 Ma, cuando el impacto de un asteroide provocó la extinción de los dinosaurios no aviarios, así como de otros grandes reptiles, sobreviviendo algunos pequeños animales como los mamíferos, que por aquel entonces eran similares a las actuales musarañas. Durante los últimos 65 millones de años los mamíferos se diversificaron, hasta que hace varios millones de años, un animal africano con aspecto de simio conocido como el Orrorin tugenensis adquirió la capacidad de mantenerse en pie. Esto le permitió utilizar herramientas y favoreció su capacidad de comunicación, proporcionando la nutrición y la estimulación necesarias para desarrollar un cerebro más grande, y permitiendo así la evolución de la especie humana. El desarrollo de la agricultura y de la civilización permitió a los humanos alterar la Tierra en un corto espacio de tiempo como no lo había hecho ninguna otra especie, afectando tanto a la naturaleza como a la diversidad y cantidad de formas de vida. El presente patrón de edades de hielo comenzó hace alrededor de 40 Ma y luego se intensificó durante el Pleistoceno, hace alrededor de 3 Ma. Desde entonces las regiones en latitudes altas han sido objeto de repetidos ciclos de glaciación y deshielo, en ciclos de 40 000-100 000 años. La última glaciación continental terminó hace 10 000 años. Véase también: Anexo:Cronología de la historia evolutiva de la vida Futuro Artículo principal: Futuro de la Tierra Ciclo de la vida solar. El futuro del planeta está estrechamente ligado al del Sol. Como resultado de la acumulación constante de helio en el núcleo del Sol, la luminosidad total de la estrella irá poco a poco en aumento. La luminosidad del Sol crecerá en un 10 % en los próximos 1.1 Ga (1100 millones de años) y en un 40 % en los próximos 3.5 Ga. Los modelos climáticos indican que el aumento de la radiación podría tener consecuencias nefastas en la Tierra, incluyendo la pérdida de los océanos del planeta. Se espera que la Tierra sea habitable por alrededor de otros 500 millones de años a partir de este momento, aunque este período podría extenderse hasta 2300 millones de años si se elimina el nitrógeno de la atmósfera. El aumento de temperatura en la superficie terrestre acelerará el ciclo del CO2 inorgánico, lo que reducirá su concentración hasta niveles letalmente bajos para las plantas (10 ppm para la fotosíntesis C4) dentro de aproximadamente 500 a 900 millones de años. La falta de vegetación resultará en la pérdida de oxígeno en la atmósfera, lo que provocará la extinción de la vida animal a lo largo de varios millones de años más. Después de otros mil millones de años, todas las aguas superficiales habrán desaparecido y la temperatura media global alcanzará los 70 °C. Incluso si el Sol fuese eterno y estable, el continuo enfriamiento interior de la Tierra se traduciría en una gran pérdida de CO2 debido a la reducción de la actividad volcánica, y el 35 % del agua de los océanos podría descender hasta el manto debido a la disminución del vapor de ventilación en las dorsales oceánicas. El Sol, siguiendo su evolución natural, se convertirá en una gigante roja en unos 5 Ga. Los modelos predicen que el Sol se expandirá hasta unas 250 veces su tamaño actual, alcanzando un radio cercano a 1 UA (unos 150 millones de kilómetros). El destino que sufrirá la Tierra entonces no está claro. Siendo una gigante roja, el Sol perderá aproximadamente el 30 % de su masa, por lo que sin los efectos de las mareas, la Tierra se moverá a una órbita de 1.7 UA (unos 250 millones de kilómetros) del Sol cuando la estrella alcance su radio máximo. Por lo tanto se espera que el planeta escape inicialmente de ser envuelto por la tenue atmósfera exterior expandida del Sol. Aun así, cualquier forma de vida restante sería destruida por el aumento de la luminosidad del Sol (alcanzando un máximo de cerca de 5000 veces su nivel actual). Sin embargo, una simulación realizada en 2008 indica que la órbita de la Tierra decaerá debido a los efectos de marea y arrastre, ocasionando que el planeta penetre en la atmósfera estelar y se vaporice. Véase también: Extinción humana Composición y estructura Artículo principal: Ciencias de la Tierra La Tierra es un planeta terrestre, lo que significa que es un cuerpo rocoso y no un gigante gaseoso como Júpiter. Es el más grande de los cuatro planetas terrestres del sistema solar en tamaño y masa, y también es el que tiene la mayor densidad, la mayor gravedad superficial, el campo magnético más fuerte y la rotación más rápida de los cuatro. También es el único planeta terrestre con placas tectónicas activas. El movimiento de estas placas produce que la superficie terrestre esté en constante cambio, siendo responsables de la formación de montañas, de la sismicidad y del vulcanismo. El ciclo de estas placas también juega un papel preponderante en la regulación de la temperatura terrestre, contribuyendo al reciclaje de gases con efecto invernadero como el dióxido de carbono, por medio de la renovación permanente de los fondos oceánicos. Forma Comparación de tamaño de los planetas interiores (de izquierda a derecha): Mercurio, Venus, Tierra y Marte. Artículo principal: Historia de la geodesia La forma de la Tierra es muy parecida a la de un geoide o esferoide oblato, una esfera achatada por los polos, resultando en un abultamiento alrededor del ecuador. Este abultamiento está causado por la rotación de la Tierra, y ocasiona que el diámetro en el ecuador sea 43 km más largo que el diámetro de un polo a otro. Hace aproximadamente 22 000 años la Tierra tenía una forma más esférica, la mayor parte del hemisferio norte se encontraba cubierto por hielo, y a medida que el hielo se derretía causaba una menor presión en la superficie terrestre en la que se sostenía, causando esto un tipo de «rebote». Este fenómeno siguió ocurriendo hasta mediados de los años noventa, cuando los científicos se percataron de que este proceso se había invertido, es decir, el abultamiento aumentaba. Las observaciones del satélite GRACE muestran que, al menos desde 2002, la pérdida de hielo de Groenlandia y de la Antártida ha sido la principal responsable de esta tendencia. Volcán Chimborazo, el punto terrestre más alejado del centro de la Tierra. La topografía local se desvía de este esferoide idealizado, aunque las diferencias a escala global son muy pequeñas: la Tierra tiene una desviación de aproximadamente una parte entre 584, o el 0.17 %, desde el esferoide de referencia, que es menor que la tolerancia del 0.22 % permitida en las bolas de billar. Las mayores desviaciones locales en la superficie rocosa de la Tierra son el monte Everest (8 848 m sobre el nivel local del mar) y el abismo Challenger, al sur de la fosa de las Marianas (10 911 m bajo el nivel local del mar). Debido a la protuberancia ecuatorial, el punto terrestre más alejado del centro de la Tierra es el volcán Chimborazo en Ecuador. La idea de que la forma de la Tierra se aproxima a la de un elipsoide data del siglo XVIII por Pierre Louis Maupertuis. Las primeras ideas antiguas sobre la forma de la Tierra sostenían que la Tierra era plana. Así, por ejemplo, en la antigua Mesopotamia, donde el mundo era visto como un disco rodeado por el océano, más allá del cual se levantaban los pilares de un cielo esférico. También lo es de la cosmología bíblica, tal como aparece en libro de Isaías. Más adelante surgió el concepto de la Tierra esférica como materia de especulación filosófica hasta el siglo III a. C., cuando la astronomía helenística estableció como un hecho, gracias sobre todo a la medición empírica de Eratóstenes. El paradigma helenístico fue gradualmente adoptado en el Viejo Mundo durante la Antigüedad y la Edad Media. Una demostración práctica de la esfericidad de la Tierra fue llevada a cabo por Fernando de Magallanes y Juan Sebastián Elcano en su expedición de circunnavegación del mundo. Para un artículo más detallado sobre las pruebas que demuestran la esfericidad de la Tierra, véase Evidencias empíricas de la forma esférica de la Tierra. Tamaño La circunferencia en el ecuador es de 40 091 km. El diámetro en el ecuador es de 12 756 km y en los polos de 12 730 km. El diámetro medio de referencia para el esferoide es de unos 12 742 km, que es aproximadamente 40 000 km/π, ya que el metro se definió originalmente como la diezmillonésima parte de la distancia desde el ecuador hasta el Polo Norte por París, Francia. Estimaciones del tamaño de la Tierra aparecieron desde los tiempos de Aristóteles. La primera medición fue hecha por Eratóstenes, el 240 a. C. En esa época se aceptaba que la Tierra era esférica. Eratóstenes calculó el tamaño de la Tierra midiendo el ángulo con que alumbraba el Sol en el solsticio, tanto en Alejandría como en Siena, distante 750 km. El tamaño que obtuvo fue de un diámetro de 12 000 km y una circunferencia de 40 000 km, es decir, con un error de solo el 6 % respecto a los datos actuales. Posteriormente Posidonio de Apamea repitió las mediciones en el año 100 a. C., obteniendo el dato de 29 000 km para la circunferencia, considerablemente más impreciso respecto a los datos actuales. Este último valor fue el que aceptó Ptolomeo, por lo que prevaleció ese valor en los siglos siguientes. Por la Edad Media el astrónomo islámico Al-Biruni utilizó un nuevo método para computar la circunferencia terráquea, obteniendo un valor cercano a los valores modernos. En contraste con sus predecesores, Al-Biruni desarrolló un nuevo método utilizando cálculos trigonométricos basado en el ángulo formado entre un plano y la cima de una montaña, con lo que obtuvo mejores mediciones de la circunferencia terrestre e hizo posible el realizar esta medición desde un solo lugar, por una sola persona. Desde la cima, divisó el ángulo con el horizonte, lo cual, junto con la altura de la montaña (que había calculado previamente), le permitió calcular la curvatura de la Tierra. También hizo uso del álgebra para formular ecuaciones trigonométricas y utilizó el astrolabio para medir ángulos. Composición química de la corteza Compuesto Fórmula Composición Continental Oceánica sílice SiO2 60.2 % 48.6 % alúmina Al2O3 15.2 % 16.5 % cal CaO 5.5 % 12.3 % magnesio MgO 3.1 % 6.8 % óxido de hierro (II) FeO 3.8 % 6.2 % óxido de sodio Na2O 3.0 % 2.6 % óxido de potasio K2O 2.8 % 0.4 % óxido de hierro (III) Fe2O3 2.5 % 2.3 % agua H2O 1.4 % 1.1 % dióxido de carbono CO2 1.2 % 1.4 % óxido de titanio TiO2 0.7 % 1.4 % óxido de fósforo P2O5 0.2 % 0.3 % Total 99.6 % 99.9 % Composición química La masa de la Tierra es aproximadamente de 5.98 × 1024 kg. Se compone principalmente de hierro (32.1 %), oxígeno (30.1 %), silicio (15.1 %), magnesio (13.9 %), azufre (2.9 %), níquel (1.8 %), calcio (1.5 %) y aluminio (1.4 %), con el 1.2 % restante formado por pequeñas cantidades de otros elementos. Debido a la segregación de masa, se cree que la zona del núcleo está compuesta principalmente de hierro (88.8 %), con pequeñas cantidades de níquel (5.8 %), azufre (4.5 %), y menos del 1 % formado por trazas de otros elementos. El geoquímico F. W. Clarke (1847-1931), llamado «el padre de la geoquímica por haber determinado la composición de la corteza de la Tierra», calculó que un poco más del 47 % de la corteza terrestre se compone de oxígeno. Los componentes de las rocas más comunes de la corteza de la Tierra son casi todos los óxidos. Cloro, azufre y flúor son las únicas excepciones significativas, y su presencia total en cualquier roca es generalmente mucho menor del 1 %. Los principales óxidos son sílice, alúmina, óxido de hierro, de calcio, de magnesio, potasio a y sodio. La sílice actúa principalmente como un ácido, formando silicatos, y los minerales más comunes de las rocas ígneas son de esta naturaleza. A partir de un cálculo sobre la base de 1672 análisis de todo tipo de rocas, Clarke dedujo que un 99.22 % de las rocas están compuestas por 11 óxidos (véase el cuadro a la derecha). Todos los demás compuestos aparecen solamente en cantidades muy pequeñas. Véase también: Abundancia de los elementos en la Tierra Estructura interna Artículo principal: Estructura de la Tierra El interior de la Tierra, al igual que el de los otros planetas terrestres, está dividido en capas según su composición química o sus propiedades físicas (reológicas), pero, a diferencia de los otros planetas terrestres, tiene un núcleo interno y externo distintos. Su capa externa es una corteza de silicato sólido, químicamente diferenciado, bajo la cual se encuentra un manto sólido de alta viscosidad. La corteza está separada del manto por la discontinuidad de Mohorovičić, variando el espesor de la misma desde un promedio de 6 km en los océanos a entre 30 y 50 km en los continentes. La corteza y la parte superior fría y rígida del manto superior se conocen comúnmente como la litosfera, y es de la litosfera de lo que están compuestas las placas tectónicas. Debajo de la litosfera se encuentra la astenosfera, una capa de relativamente baja viscosidad sobre la que flota la litosfera. Dentro del manto, entre los 410 y 660 km bajo la superficie, se producen importantes cambios en la estructura cristalina. Estos cambios generan una zona de transición que separa la parte superior e inferior del manto. Bajo el manto se encuentra un núcleo externo líquido de viscosidad extremadamente baja, descansando sobre un núcleo interno sólido. El núcleo interno puede girar con una velocidad angular ligeramente superior que el resto del planeta, avanzando de 0.1 a 0.5° por año. Capas geológicas de la Tierra Corte de la Tierra desde el núcleo hasta la exosfera (no está a escala). Profundidad km Componentes de las capas Densidad g/cm³ 0-60 Litosfera[nota 9] , 0-35 Corteza[nota 10] 2.2-2.9 35-60 Manto superior 3.4-4.4 35-2890 Manto 3.4-5.6 100-700 Astenosfera , 2890-5100 Núcleo externo 9.9-12.2 5100-6378 Núcleo interno 12.8-13.1 Calor El calor interno de la Tierra proviene de una combinación del calor residual de la acreción planetaria (20 %) y el calor producido por la desintegración radiactiva (80 %). Los isótopos con mayor producción de calor en la Tierra son el potasio-40, el uranio-238, el uranio-235 y el torio-232. En el centro del planeta, la temperatura puede llegar hasta los 7000 K y la presión puede alcanzar los 360 GPa. Debido a que gran parte del calor es proporcionado por la desintegración radiactiva, los científicos creen que en la historia temprana de la Tierra, antes de que los isótopos de reducida vida media se agotaran, la producción de calor de la Tierra fue mucho mayor. Esta producción de calor extra, que hace aproximadamente 3000 millones de años era el doble que la producción actual, pudo haber incrementado los gradientes de temperatura dentro de la Tierra, incrementando la convección del manto y la tectónica de placas, permitiendo la producción de rocas ígneas como las komatitas que no se forman en la actualidad. Isótopos actuales de mayor producción de calor Isótopo Calor emitido Vatios/kg isótopo Vida media años Concentración media del manto kg isótopo/kg manto Calor emitido W/kg manto 238U 9.46 × 10−5 4.47 × 109 30.8 × 10−9 2.91 × 10−12 235U 5.69 × 10−4 7.04 × 108 0.22 × 10−9 1.25 × 10−13 232Th 2.64 × 10−5 1.40 × 1010 124 × 10−9 3.27 × 10−12 40K 2.92 × 10−5 1.25 × 109 36.9 × 10−9 1.08 × 10−12 El promedio de pérdida de calor de la Tierra es de 87 mW m−2, que supone una pérdida global de 4.42 × 1013 W. Una parte de la energía térmica del núcleo es transportada hacia la corteza por plumas del manto, una forma de convección que consiste en afloramientos de roca a altas temperaturas. Estas plumas pueden producir puntos calientes y coladas de basalto. La mayor parte del calor que pierde la Tierra se filtra entre las placas tectónicas, en las surgencias del manto asociadas a las dorsales oceánicas. Casi todas las pérdidas restantes se producen por conducción a través de la litosfera, principalmente en los océanos, ya que allí la corteza es mucho más delgada que en los continentes. Placas tectónicas Artículo principal: Tectónica de placas Placas tectónicas Muestra de la extensión y los límites de las placas tectónicas, con superposición de contornos en los continentes que se apoyan Nombre de la placa Área 106 km² color #FB9B7A Placa Africana[nota 8] 78.0 color #8A9BBE Placa Antártica 60.9   Placa Indoaustraliana 47.2 color #7FA172 Placa Euroasiática 67.8 color #AC8D7F Placa Norteamericana 75.9 color #AD82B0 Placa Sudamericana 43.6 color #FEE6AA Placa Pacífica 103.3 La mecánicamente rígida capa externa de la Tierra, la litosfera, está fragmentada en piezas llamadas placas tectónicas. Estas placas son elementos rígidos que se mueven en relación uno con otro siguiendo uno de estos tres patrones: bordes convergentes, en los que dos placas se aproximan; bordes divergentes, en los que dos placas se separan, y bordes transformantes, en los que dos placas se deslizan lateralmente entre sí. A lo largo de estos bordes de placa se producen los terremotos, la actividad volcánica, la formación de montañas y la formación de fosas oceánicas. Las placas tectónicas se deslizan sobre la parte superior de la astenosfera, la sólida pero menos viscosa sección superior del manto, que puede fluir y moverse junto con las placas, y cuyo movimiento está fuertemente asociado a los patrones de convección dentro del manto terrestre. A medida que las placas tectónicas migran a través del planeta, el fondo oceánico se subduce bajo los bordes de las placas en los límites convergentes. Al mismo tiempo, el afloramiento de material del manto en los límites divergentes crea las dorsales oceánicas. La combinación de estos procesos recicla continuamente la corteza oceánica nuevamente en el manto. Debido a este proceso de reciclaje, la mayor parte del suelo marino tiene menos de 100 millones de años de edad. La corteza oceánica más antigua se encuentra en el Pacífico Occidental, y tiene una edad estimada de unos 200 millones de años. En comparación, la corteza continental más antigua registrada tiene 4030 millones de años de edad. Las siete placas más grandes son la Pacífica, Norteamericana, Euroasiática, Africana Antártica, Indoaustraliana y Sudamericana. Otras placas notables son la placa Índica, la placa arábiga, la placa del Caribe, la placa de Nazca en la costa occidental de América del Sur y la placa Escocesa en el sur del océano Atlántico. La placa de Australia se fusionó con la placa de la India hace entre 50 y 55 millones de años. Las placas con movimiento más rápido son las placas oceánicas, con la placa de Cocos avanzando a una velocidad de 75 mm/año y la placa del Pacífico moviéndose 52-69 mm/año. En el otro extremo, la placa con movimiento más lento es la placa eurasiática, que avanza a una velocidad típica de aproximadamente 21 mm/año. Superficie Histograma de elevación de la corteza terrestre. Artículos principales: Superficie terrestre, Accidente geográfico y Anexo:Puntos extremos del mundo. El relieve de la Tierra varía enormemente de un lugar a otro. Cerca del 70.8 % de la superficie está cubierta por agua, con gran parte de la plataforma continental por debajo del nivel del mar. La superficie sumergida tiene características montañosas, incluyendo un sistema de dorsales oceánicas, así como volcanes submarinos, fosas oceánicas, cañones submarinos, mesetas y llanuras abisales. El restante 29.2 % no cubierto por el agua se compone de montañas, desiertos, llanuras, mesetas y otras geomorfologías. La superficie del planeta se moldea a lo largo de períodos de tiempo geológicos, debido a la erosión tectónica. Las características de esta superficie formada o deformada mediante la tectónica de placas están sujetas a una constante erosión a causa de las precipitaciones, los ciclos térmicos y los efectos químicos. La glaciación, la erosión costera, la acumulación de los arrecifes de coral y los grandes impactos de meteoritos también actúan para remodelar el paisaje. Altimetría y batimetría actual. Datos del Modelo Digital de Terreno del National Geophysical Data Center de Estados Unidos. La corteza continental se compone de material de menor densidad, como las rocas ígneas, el granito y la andesita. Menos común es el basalto, una densa roca volcánica que es el componente principal de los fondos oceánicos. Las rocas sedimentarias se forman por la acumulación de sedimentos compactados. Casi el 75 % de la superficie continental está cubierta por rocas sedimentarias, a pesar de que estas solo forman un 5 % de la corteza. El tercer material rocoso más abundante en la Tierra son las rocas metamórficas, creadas a partir de la transformación de tipos de roca ya existentes mediante altas presiones, altas temperaturas, o ambas. Los minerales de silicato más abundantes en la superficie de la Tierra incluyen el cuarzo, los feldespatos, el anfíbol, la mica, el piroxeno y el olivino. Los minerales de carbonato más comunes son la calcita (que se encuentra en piedra caliza) y la dolomita. La pedosfera es la capa más externa de la Tierra. Está compuesta de tierra y está sujeta a los procesos de formación del suelo. Existe en el encuentro entre la litosfera, la atmósfera, la hidrosfera y la biosfera. Actualmente el 13.31 % del total de la superficie terrestre es tierra cultivable, y solo el 4.71 % soporta cultivos permanentes. Cerca del 40 % de la superficie emergida se utiliza actualmente como tierras de cultivo y pastizales, estimándose un total de 1.3 × 107 km² para tierras de cultivo y 3.4 × 107 km² para tierras de pastoreo. La elevación de la superficie terrestre varía entre el punto más bajo de −418 m en el mar Muerto a una altitud máxima, estimada en 2005, de 8848 m en la cima del monte Everest. La altura media de la tierra sobre el nivel del mar es de 840 m. Imágenes satelitales de la Tierra Planisferio terrestre (composición de fotos satelitales). El satélite ambiental Envisat de la ESA desarrolló un retrato detallado de la superficie de la Tierra. A través del proyecto GLOBCOVER se desarrolló la creación de un mapa global de la cobertura terrestre con una resolución tres veces superior a la de cualquier otro mapa por satélite hasta aquel momento. Utilizó reflectores radar con antenas de ancho sintéticas, capturando con sus sensores la radiación reflejada. La NASA completó un nuevo mapa tridimensional, que es la topografía más precisa del planeta, elaborada durante cuatro años con los datos transmitidos por el transbordador espacial Endeavour. Los datos analizados corresponden al 80 % de la masa terrestre. Cubre los territorios de Australia y Nueva Zelanda con detalles sin precedentes. También incluye más de mil islas de la Polinesia y la Melanesia en el Pacífico sur, así como islas del Índico y el Atlántico. Muchas de esas islas apenas se levantan unos metros sobre el nivel del mar y son muy vulnerables a los efectos de las marejadas y tormentas, por lo que su conocimiento ayudará a evitar catástrofes; los datos proporcionados por la misión del Endeavour tendrán una amplia variedad de usos, como la exploración virtual del planeta. Véase también: Cartografía Hidrosfera Los océanos poseen el mayor volumen de agua en la Tierra. Artículo principal: Hidrosfera La abundancia de agua en la superficie de la Tierra es una característica única que distingue al «Planeta Azul» de otros en el sistema solar. La hidrosfera de la Tierra está compuesta fundamentalmente por océanos, pero técnicamente incluye todas las superficies de agua en el mundo, incluidos los mares interiores, lagos, ríos y aguas subterráneas hasta una profundidad de 2000 m. El lugar más profundo bajo el agua es el abismo Challenger de la fosa de las Marianas, en el océano Pacífico, con una profundidad de −10 911.4 m.[nota 11] La masa de los océanos es de aproximadamente 1.35 × 1018 toneladas métricas, o aproximadamente 1/4400 de la masa total de la Tierra. Los océanos cubren un área de 361.84 × 106 km² con una profundidad media de 3682.2 m, lo que resulta en un volumen estimado de 1.3324 × 109 km³. Si se nivelase toda la superficie terrestre, el agua cubriría la superficie del planeta hasta una altura de más de 2.7 km. El área total de la Tierra es de 5.1 × 108 km². Para la primera aproximación, la profundidad media sería la relación entre los dos, o de 2.7 km. Aproximadamente el 97.5 % del agua es salada, mientras que el restante 2.5 % es agua dulce. La mayor parte del agua dulce, aproximadamente el 68.7 %, se encuentra actualmente en estado de hielo. La salinidad media de los océanos es de unos 35 gramos de sal por kilogramo de agua (35 ‰). La mayor parte de esta sal fue liberada por la actividad volcánica, o extraída de las rocas ígneas ya enfriadas. Los océanos son también un reservorio de gases atmosféricos disueltos, siendo estos esenciales para la supervivencia de muchas formas de vida acuática. El agua de los océanos tiene una influencia importante sobre el clima del planeta, actuando como un foco calórico de gran tamaño. Los cambios en la distribución de la temperatura oceánica pueden causar alteraciones climáticas, tales como la Oscilación del Sur, El Niño. Atmósfera Artículo principal: Atmósfera terrestre La presión atmosférica media al nivel del mar se sitúa en torno a los 101.325 kPa, con una escala de altura de aproximadamente 8.5 km. Está compuesta principalmente de un 78 % de nitrógeno y un 21 % de oxígeno, con trazas de vapor de agua, dióxido de carbono y otras moléculas gaseosas. La altura de la troposfera varía con la latitud, entre 8 km en los polos y 17 km en el ecuador, con algunas variaciones debido a la climatología y los factores estacionales. La biosfera de la Tierra ha alterado significativamente la atmósfera. La fotosíntesis oxigénica evolucionó hace 2700 millones de años, formando principalmente la atmósfera actual de nitrógeno-oxígeno. Este cambio permitió la proliferación de los organismos aeróbicos, así como la formación de la capa de ozono que bloquea la radiación ultravioleta proveniente del Sol, permitiendo la vida fuera del agua. Otras funciones importantes de la atmósfera para la vida en la Tierra incluyen el transporte de vapor de agua, proporcionar gases útiles, quemar los meteoritos pequeños antes de que alcancen la superficie, y moderar la temperatura. Este último fenómeno se conoce como el efecto invernadero: trazas de moléculas presentes en la atmósfera capturan la energía térmica emitida desde el suelo, aumentando así la temperatura media. El dióxido de carbono, el vapor de agua, el metano y el ozono son los principales gases de efecto invernadero de la atmósfera de la Tierra. Sin este efecto de retención del calor, la temperatura superficial media sería de −18 °C y la vida probablemente no existiría. Clima y tiempo atmosférico Artículos principales: Clima y Tiempo atmosférico. Imagen satelital de la nubosidad de la Tierra usando el espectroradiómetro de imágenes de media resolución de la NASA. La atmósfera terrestre no tiene unos límites definidos, haciéndose poco a poco más delgada hasta desvanecerse en el espacio exterior. Tres cuartas partes de la masa atmosférica están contenidas dentro de los primeros 11 km de la superficie del planeta. Esta capa inferior se llama troposfera. La energía del Sol calienta esta capa y la superficie bajo esta, causando la expansión del aire. El aire caliente se eleva debido a su menor densidad, siendo sustituido por aire de mayor densidad, es decir, aire más frío. Esto da como resultado la circulación atmosférica que genera el tiempo y el clima a través de la redistribución de la energía térmica. Las líneas principales de circulación atmosférica las constituyen los vientos alisios en la región ecuatorial por debajo de los 30° de latitud, y los vientos del oeste en latitudes medias entre los 30° y 60°. Las corrientes oceánicas también son factores importantes para determinar el clima, especialmente la circulación termohalina que distribuye la energía térmica de los océanos ecuatoriales a las regiones polares. El vapor de agua generado a través de la evaporación superficial es transportado según los patrones de circulación de la atmósfera. Cuando las condiciones atmosféricas permiten la elevación del aire caliente y húmedo, el agua se condensa y se deposita en la superficie en forma de precipitaciones. La mayor parte del agua es transportada a altitudes más bajas mediante los sistemas fluviales y por lo general regresa a los océanos o es depositada en los lagos. Este ciclo del agua es un mecanismo vital para sustentar la vida en la tierra y es un factor primario de la erosión que modela la superficie terrestre a lo largo de períodos geológicos. Los patrones de precipitación varían enormemente, desde varios metros de agua por año a menos de un milímetro. La circulación atmosférica, las características topológicas y las diferencias de temperatura determinan las precipitaciones medias de cada región. La cantidad de energía solar que llega a la Tierra disminuye al aumentar la latitud. En las latitudes más altas la luz solar incide en la superficie en un ángulo menor, teniendo que atravesar gruesas columnas de atmósfera. Como resultado, la temperatura media anual del aire a nivel del mar se reduce en aproximadamente 0.4 °C por cada grado de latitud alejándose del ecuador. La Tierra puede ser subdividida en franjas latitudinales más o menos homogéneas con un clima específico. Desde el ecuador hasta las regiones polares, se encuentran la zona intertropical (o ecuatorial), el clima subtropical, el clima templado y los climas polares. El clima también puede ser clasificado en función de la temperatura y las precipitaciones, en regiones climáticas caracterizadas por masas de aire bastante uniformes. La metodología de clasificación más usada es la clasificación climática de Köppen (modificada por el estudiante de Wladimir Peter Köppen, Rudolph Geiger), que cuenta con cinco grandes grupos (zonas tropicales húmedas, zonas áridas, zonas húmedas con latitud media, clima continental y frío polar), que se dividen en subtipos más específicos. Atmósfera superior Imagen de la NASA en la que se observa la Luna parcialmente oscurecida y deformada por la refracción atmosférica. Artículo principal: Espacio exterior Por encima de la troposfera, la atmósfera suele dividir en estratosfera, mesosfera y termosfera. Cada capa tiene un gradiente adiabático diferente, que define la tasa de cambio de la temperatura con respecto a la altura. Más allá de éstas se encuentra la exosfera, que se atenúa hasta penetrar en la magnetosfera, donde los campos magnéticos de la Tierra interactúan con el viento solar. Dentro de la estratosfera se encuentra la capa de ozono; un componente que protege parcialmente la superficie terrestre de la luz ultravioleta, siendo un elemento importante para la vida en la Tierra. La línea de Kármán, definida en los 100 km sobre la superficie de la Tierra, es una definición práctica usada para establecer el límite entre la atmósfera y el espacio. La energía térmica hace que algunas de las moléculas en el borde exterior de la atmósfera de la Tierra incrementen su velocidad hasta el punto de poder escapar de la gravedad del planeta. Esto da lugar a una pérdida lenta pero constante de la atmósfera hacia el espacio. Debido a que el hidrógeno no fijado tiene un bajo peso molecular puede alcanzar la velocidad de escape más fácilmente, escapando así al espacio exterior a un ritmo mayor que otros gases. La pérdida de hidrógeno hacia el espacio contribuye a la transformación de la Tierra desde su inicial estado reductor a su actual estado oxidante. La fotosíntesis proporcionó una fuente de oxígeno libre, pero se cree que la pérdida de agentes reductores como el hidrógeno fue una condición previa necesaria para la acumulación generalizada de oxígeno en la atmósfera. Por tanto, la capacidad del hidrógeno para escapar de la atmósfera de la Tierra puede haber influido en la naturaleza de la vida desarrollada en el planeta. En la atmósfera actual, rica en oxígeno, la mayor parte del hidrógeno se convierte en agua antes de tener la oportunidad de escapar. En cambio, la mayor parte de la pérdida de hidrógeno actual proviene de la destrucción del metano en la atmósfera superior. Campo magnético Diagrama que muestra las líneas del campo magnético de la magnetosfera de la Tierra. Las líneas son arrastradas de vuelta en el sentido contrario a las solares bajo la influencia del viento solar. Esquema de la magnetosfera de la Tierra. Los flujos de viento solar, de izquierda a derecha Artículo principal: Campo magnético terrestre El campo magnético de la Tierra tiene una forma similar a un dipolo magnético, con los polos actualmente localizados cerca de los polos geográficos del planeta. En el ecuador del campo magnético (ecuador magnético), la fuerza del campo magnético en la superficie es 3.05 × 10−5T, con un momento magnético dipolar global de 7.91 × 1015 T m³. Según la teoría del dínamo, el campo se genera en el núcleo externo fundido, región donde el calor crea movimientos de convección en materiales conductores, generando corrientes eléctricas. Estas corrientes inducen a su vez el campo magnético de la Tierra. Los movimientos de convección en el núcleo son caóticos; los polos magnéticos se mueven y periódicamente cambian de orientación. Esto da lugar a reversiones geomagnéticas a intervalos de tiempo irregulares, unas pocas veces cada millón de años. La inversión más reciente tuvo lugar hace aproximadamente 700 000 años. El campo magnético forma la magnetosfera, que desvía las partículas de viento solar. En dirección al Sol, el arco de choque entre el viento solar y la magnetosfera se encuentra a unas 13 veces el radio de la Tierra. La colisión entre el campo magnético y el viento solar forma los cinturones de radiación de Van Allen; un par de regiones concéntricas, con forma tórica, formadas por partículas cargadas muy energéticas. Cuando el plasma entra en la atmósfera de la Tierra por los polos magnéticos se crean las auroras polares. Rotación y órbita Rotación Inclinación del eje de la Tierra (u oblicuidad) y su relación con el eje de rotación y el plano orbital. Artículo principal: Rotación de la Tierra El período de rotación de la Tierra con respecto al Sol, es decir, un día solar, es de alrededor de 86 400 segundos de tiempo solar (86 400.0025 segundos SIU). El día solar de la Tierra es ahora un poco más largo de lo que era durante el siglo XIX debido a la aceleración de marea, los días duran entre 0 y 2 ms SIU más. La rotación de la Tierra fotografiada por DSCOVR EPIC el 29 de mayo de 2016, unas semanas antes del solsticio. El período de rotación de la Tierra en relación con las estrellas fijas, llamado día estelar por el Servicio Internacional de Rotación de la Tierra y Sistemas de Referencia (IERS por sus siglas en inglés), es de 86 164.098903691 segundos del tiempo solar medio (UT1), o de 23h 56m 4.098903691s.[nota 12] El período de rotación de la Tierra en relación con el equinoccio vernal, mal llamado el día sidéreo, es de 86 164.09053083288 segundos del tiempo solar medio (UT1) (23h 56m 4.09053083288s). Por tanto, el día sidéreo es más corto que el día estelar en torno a 8.4 ms. La longitud del día solar medio en segundos SIU está disponible en el IERS para los períodos 1623-2005 y 1962-2005. Aparte de los meteoros en la atmósfera y de los satélites en órbita baja, el movimiento aparente de los cuerpos celestes vistos desde la Tierra se realiza hacia al oeste, a una velocidad de 15°/h = 15′/min. Para las masas cercanas al ecuador celeste, esto es equivalente a un diámetro aparente del Sol o de la Luna cada dos minutos (desde la superficie del planeta, los tamaños aparentes del Sol y de la Luna son aproximadamente iguales). Órbita Artículo principal: Traslación de la Tierra Galaxia espiral barrada Ilustración de la galaxia Vía Láctea, mostrando la posición del Sol La Tierra orbita alrededor del Sol a una distancia media de unos 150 millones de kilómetros, completando una órbita cada 365.2564 días solares, o un año sideral. Desde la Tierra, esto genera un movimiento aparente del Sol hacia el este, desplazándose con respecto a las estrellas a un ritmo de alrededor de 1°/día, o un diámetro del Sol o de la Luna cada 12 horas. Debido a este movimiento, en promedio la Tierra tarda 24 horas (un día solar) en completar una rotación sobre su eje hasta que el sol regresa al meridiano. La velocidad orbital de la Tierra es de aproximadamente 29.8 km/s (107 000 km/h), que es lo suficientemente rápida como para recorrer el diámetro del planeta (12 742 km) en siete minutos, o la distancia entre la Tierra y la Luna (384 000 km) en cuatro horas. La Luna gira con la Tierra en torno a un baricentro común, debido a que este se encuentra dentro de la Tierra, a 4541 km de su centro, el sistema Tierra-Luna no es un planeta doble, la Luna completa un giro cada 27.32 días con respecto a las estrellas de fondo. Cuando se combina con la revolución común del sistema Tierra-Luna alrededor del Sol, el período del mes sinódico, desde una luna nueva a la siguiente, es de 29.53 días. Visto desde el polo norte celeste, el movimiento de la Tierra, la Luna y sus rotaciones axiales son todas contrarias a la dirección de las manecillas del reloj (sentido antihorario). Visto desde un punto de vista situado sobre los polos norte del Sol y la Tierra, la Tierra parecería girar en sentido antihorario alrededor del Sol. Los planos orbitales y axiales no están alineados: El eje de la Tierra está inclinado unos 23.4 grados con respecto a la perpendicular al plano Tierra-Sol, y el plano entre la Tierra y la Luna está inclinado unos 5 grados con respecto al plano Tierra-Sol. Sin esta inclinación, habría un eclipse cada dos semanas, alternando entre los eclipses lunares y eclipses solares. La esfera de Hill, o la esfera de influencia gravitatoria, de la Tierra tiene aproximadamente 1.5 Gm (o 1 500 000 kilómetros) de radio.[nota 13] Esta es la distancia máxima en la que la influencia gravitatoria de la Tierra es más fuerte que la de los más distantes Sol y resto de planetas. Los objetos deben orbitar la Tierra dentro de este radio, o terminarán atrapados por la perturbación gravitatoria del Sol. Desde el año de 1772, se estableció que cuerpos pequeños pueden orbitar de manera estable la misma órbita que un planeta, si esta permanece cerca de un punto triangular de Lagrange (también conocido como «punto troyano») los cuales están situados 60° delante y 60° detrás del planeta en su órbita. La Tierra es el cuarto planeta con un asteroide troyano (2010 TK7) después de Júpiter, Marte y Neptuno de acuerdo a la fecha de su descubrimiento[nota 14] Este fue difícil de localizar debido al posicionamiento geométrico de la observación, este fue descubierto en 2010 gracias al telescopio WISE (Wide-Field Infrared Survey Explorer) de la NASA, pero fue en abril de 2011 con el telescopio «Canadá-Francia-Hawái» cuando se confirmó su naturaleza troyana, y se estima que su órbita permanezca estable dentro de los próximos 10 000 años. La Tierra, junto con el sistema solar, está situada en la galaxia Vía Láctea, orbitando a alrededor de 28 000 años luz del centro de la galaxia. En la actualidad se encuentra unos 20 años luz por encima del plano ecuatorial de la galaxia, en el brazo espiral de Orión. Estaciones e inclinación axial Artículo principal: Oblicuidad de la eclíptica Las estaciones se producen en la Tierra debido a la inclinación de su eje de rotación respecto al plano definido por su órbita (de la eclíptica). En la ilustración es invierno en el hemisferio norte y verano en el hemisferio sur. (La distancia y el tamaño entre los cuerpos no está a escala). Debido a la inclinación del eje de la Tierra, la cantidad de luz solar que llega a un punto cualquiera en la superficie varía a lo largo del año. Esto ocasiona los cambios estacionales en el clima, siendo verano en el hemisferio norte ocurre cuando el Polo Norte está apuntando hacia el Sol, e invierno cuando apunta en dirección opuesta. Durante el verano, el día tiene una duración más larga y la luz solar incide más perpendicularmente en la superficie. Durante el invierno, el clima se vuelve más frío y los días más cortos. En la zona del círculo polar ártico se da el caso extremo de no recibir luz solar durante una parte del año; fenómeno conocido como la noche polar. En el hemisferio sur se da la misma situación pero de manera inversa, con la orientación del Polo Sur opuesta a la dirección del Polo Norte. Espacio oscuro con la Tierra creciente a menor Luna izquierda, media luna en la parte superior derecha, el 30 % del diámetro aparente de la Tierra, cinco veces el diámetro aparente distancia entre la Tierra en la parte izquierda baja, la Luna creciente en la esquina superior derecha, el diámetro aparente de la Tierra es del 30 %; cinco veces el diámetro aparente entre la Tierra desde el espacio; la luz solar proveniente del lado derecho. La Tierra y la Luna vistas desde Marte, imagen del Mars Reconnaissance Orbiter. Desde el espacio, la Tierra puede verse en fases similares a las fases lunares. Por convenio astronómico, las cuatro estaciones están determinadas por solsticios (puntos de la órbita en los que el eje de rotación terrestre alcanza la máxima inclinación hacia el Sol ,solsticio de verano, o hacia el lado opuesto ,solsticio de invierno,) y por equinoccios, cuando la inclinación del eje terrestre es perpendicular a la dirección del Sol. En el hemisferio norte, el solsticio de invierno se produce alrededor del 21 de diciembre, el solsticio de verano el 21 de junio, el equinoccio de primavera el 20 de marzo y el equinoccio de otoño el 23 de septiembre. En el hemisferio sur la situación se invierte, con el verano y los solsticios de invierno en fechas contrarias a la del hemisferio norte. De igual manera sucede con el equinoccio de primavera y de otoño. El ángulo de inclinación de la Tierra es relativamente estable durante largos períodos de tiempo. Sin embargo, la inclinación se somete a nutaciones; un ligero movimiento irregular, con un período de 18.6 años. La orientación (en lugar del ángulo) del eje de la Tierra también cambia con el tiempo, precesando un círculo completo en cada ciclo de 25 800 años. Esta precesión es la razón de la diferencia entre el año sidéreo y el año tropical. Ambos movimientos son causados por la atracción variante del Sol y la Luna sobre el abultamiento ecuatorial de la Tierra. Desde la perspectiva de la Tierra, los polos también migran unos pocos metros sobre la superficie. Este movimiento polar tiene varios componentes cíclicos, que en conjunto reciben el nombre de movimientos cuasiperiódicos. Además del componente anual de este movimiento, existe otro movimiento con ciclos de 14 meses llamado el bamboleo de Chandler. La velocidad de rotación de la Tierra también varía en un fenómeno conocido como variación de duración del día. En tiempos modernos, el perihelio de la Tierra se produce alrededor del 3 de enero y el afelio alrededor del 4 de julio. Sin embargo, estas fechas cambian con el tiempo debido a la precesión orbital y otros factores, que siguen patrones cíclicos conocidos como ciclos de Milankovitch. La variación de la distancia entre la Tierra y el Sol resulta en un aumento de alrededor del 6.9 %[nota 15] de la energía solar que llega a la Tierra en el perihelio en relación con el afelio. Puesto que el hemisferio sur está inclinado hacia el Sol en el momento en que la Tierra alcanza la máxima aproximación al Sol, a lo largo del año el hemisferio sur recibe algo más de energía del Sol que el hemisferio norte. Sin embargo, este efecto es mucho menos importante que el cambio total de energía debido a la inclinación del eje, y la mayor parte de este exceso de energía es absorbido por la superficie oceánica, que se extiende en mayor proporción en el hemisferio sur. Satélite natural y otros elementos orbitales Características Diámetro 3474.8 km Masa 7.349 × 1022 kg Semieje mayor 384 400 km Periodo orbital 27 d 7 h 43.7 m Luna Artículos principales: Luna y Sistema Tierra-Luna. La Luna es el satélite natural de la Tierra. Es un cuerpo del tipo terrestre relativamente grande: con un diámetro de alrededor de la cuarta parte del de la Tierra, es el segundo satélite más grande del sistema solar en relación con el tamaño de su planeta, después del satélite Caronte de su planeta enano Plutón. Los satélites naturales que orbitan los demás planetas se denominan «lunas» en referencia a la Luna de la Tierra. Detalles del sistema Tierra-Luna. Además del radio de cada objeto, de la distancia entre ellos, y de la inclinación del eje de cada uno, se muestra la distancia del baricentro del sistema Tierra-Luna al centro de la Tierra (4641 km). Imágenes Archivado el 1 de noviembre de 2011 en Wayback Machine. e información de la NASA. El eje de la Luna se localiza por la tercera ley de Cassini. La atracción gravitatoria entre la Tierra y la Luna causa las mareas en la Tierra. El mismo efecto en la Luna ha dado lugar a su acoplamiento de marea, lo que significa que su período de rotación es idéntico a su periodo de traslación alrededor de la Tierra. Como resultado, la luna siempre presenta la misma cara hacia nuestro planeta. A medida que la Luna orbita la Tierra, diferentes partes de su cara son iluminadas por el Sol, dando lugar a las fases lunares. La parte oscura de la cara está separada de la parte iluminada del terminador solar. Debido a la interacción de las mareas, la Luna se aleja de la Tierra a una velocidad de aproximadamente 38 mm al año. Acumuladas durante millones de años, estas pequeñas modificaciones, así como el alargamiento del día terrestre en alrededor de 23 µs, han producido cambios significativos. Durante el período devónico, por ejemplo, (hace aproximadamente 410 millones de años) un año tenía 400 días, cada uno con una duración de 21.8 horas. Secuencia de imágenes que muestran la rotación de la Tierra y la traslación de la Luna vistas desde la sonda espacial Galileo. La Luna pudo haber afectado dramáticamente el desarrollo de la vida, moderando el clima del planeta. Evidencias paleontológicas y simulaciones computarizadas muestran que la inclinación del eje terrestre está estabilizada por las interacciones de marea con la Luna. Algunos teóricos creen que sin esta estabilización frente al momento ejercido por el Sol y los planetas sobre la protuberancia ecuatorial de la Tierra, el eje de rotación podría ser caóticamente inestable, mostrando cambios caóticos durante millones de años, como parece ser el caso de Marte. Vista desde la Tierra, la Luna está justo a una distancia que la hace que el tamaño aparente de su disco sea casi idéntico al del Sol. El diámetro angular (o ángulo sólido) de estos dos cuerpos coincide porque aunque el diámetro del Sol es unas 400 veces más grande que el de la Luna, también está 400 veces más distante. Esto permite que en la Tierra se produzcan los eclipses solares totales y anulares. La teoría más ampliamente aceptada sobre el origen de la Luna, la teoría del gran impacto, afirma que esta se formó por la colisión de un protoplaneta del tamaño de Marte, llamado Tea, con la Tierra primitiva. Esta hipótesis explica (entre otras cosas) la relativa escasez de hierro y elementos volátiles en la Luna, y el hecho de que su composición sea casi idéntica a la de la corteza terrestre. Representación a escala del tamaño y distancia relativa entre la Tierra y la Luna. Representación a escala del tamaño y distancia relativa entre la Tierra y la Luna. Otros elementos orbitales A fecha de 2016, el planeta Tierra tiene nueve cuasisatélites naturales o asteroides coorbitales conocidos: el (3753) Cruithne, el 2002 AA29, 2003 YN107, 2004 GU9, 2006 FV35, 2010 SO16 2013 LX28, 2014 OL339 y 2016 HO3. El 15 de febrero de 2020 se descubrió que 2020 CD3 es un satélite natural temporal terrestre. A fecha de septiembre de 2021, existen 4550 satélites operativos creados por el hombre orbitando la Tierra. Localización de la Tierra Artículo principal: Anexo:Localización de la Tierra en el universo Diagrama de nuestra ubicación dentro del universo observable. (Click aquí para ver en pantalla completa.) Habitabilidad Artículo principal: Habitabilidad planetaria Un planeta que pueda sostener vida se denomina habitable, incluso aunque en él no se originara vida. La Tierra proporciona las (actualmente entendidas como) condiciones necesarias, tales como el agua líquida, un ambiente que permite el ensamblaje de moléculas orgánicas complejas, y la energía suficiente para mantener un metabolismo. Hay otras características que se cree que también contribuyen a la capacidad del planeta para originar y mantener la vida: la distancia entre la Tierra y el Sol, así como su excentricidad orbital, la velocidad de rotación, la inclinación axial, la historia geológica, la permanencia de la atmósfera, y la protección ofrecida por el campo magnético. Biosfera Artículo principal: Biosfera Se denomina «biosfera» al conjunto de los diferentes tipos de vida del planeta junto con su entorno físico, modificado por la presencia de los primeros. Generalmente se entiende que la biosfera empezó a evolucionar hace 3500 millones de años. La Tierra es el único lugar donde se sabe que existe vida. La biosfera se divide en una serie de biomas, habitados por plantas y animales esencialmente similares. En tierra, los biomas se separan principalmente por las diferencias en latitud, la altitud sobre el nivel del mar y la humedad. Los biomas terrestres situados en los círculos ártico o antártico, en gran altura o en zonas extremadamente áridas son relativamente estériles de vida vegetal y animal; la diversidad de especies alcanza su máximo en tierras bajas y húmedas, en latitudes ecuatoriales. Recursos naturales y uso de la tierra Artículo principal: Recurso natural La Tierra proporciona recursos que son explotados por los seres humanos con diversos fines. Algunos de estos son recursos no renovables, tales como los combustibles fósiles, que son difícilmente renovables a corto plazo. De la corteza terrestre se obtienen grandes depósitos de combustibles fósiles, consistentes en carbón, petróleo, gas natural y clatratos de metano. Estos depósitos son utilizados por los seres humanos para la producción de energía, y también como materia prima para la producción de sustancias químicas. Los cuerpos minerales también se han formado en la corteza terrestre a través de distintos procesos de mineralogénesis, como consecuencia de la erosión y de los procesos implicados en la tectónica de placas. Estos cuerpos albergan fuentes concentradas de varios metales y otros elementos útiles. La biosfera de la Tierra produce muchos productos biológicos útiles para los seres humanos, incluyendo (entre muchos otros) alimentos, madera, fármacos, oxígeno, y el reciclaje de muchos residuos orgánicos. El ecosistema terrestre depende de la capa superior del suelo y del agua dulce, y el ecosistema oceánico depende del aporte de nutrientes disueltos desde tierra firme. Los seres humanos también habitan la tierra usando materiales de construcción para construir refugios. Para 1993, el aprovechamiento de la tierra por los humanos era de aproximadamente: Uso de la tierra Tierra cultivable Cultivos permanentes Pastos permanentes Bosques y tierras arboladas Áreas urbanas Otros Porcentaje 13.13 % 4.71 % 26 % 32 % 1.5 % 30 % La cantidad de tierras de regadío en 1993 se estimaban en 2 481 250 km². Medio ambiente y riesgos Grandes áreas de la superficie de la Tierra están sujetas a condiciones climáticas extremas, tales como ciclones tropicales, huracanes, o tifones que dominan la vida en esas zonas. Muchos lugares están sujetos a terremotos, deslizamientos, tsunamis, erupciones volcánicas, tornados, dolinas, ventiscas, inundaciones, sequías y otros desastres naturales. Muchas áreas concretas están sujetas a la contaminación causada por el hombre del aire y del agua, a la lluvia ácida, a sustancias tóxicas, a la pérdida de vegetación (sobrepastoreo, deforestación, desertificación), a la pérdida de vida salvaje, la extinción de especies, la degradación del suelo y su agotamiento, a la erosión y a la introducción de especies invasoras. Según las Naciones Unidas, existe un consenso científico que vincula las actividades humanas con el calentamiento global, debido a las emisiones industriales de dióxido de carbono y el calor residual antropogénico. Se prevé que esto produzca cambios tales como el derretimiento de los glaciares y superficies heladas, temperaturas más extremas, cambios significativos en el clima y un aumento global del nivel del mar. Geografía humana Artículo principal: Geografía humana La cartografía ,el estudio y práctica de la elaboración de mapas,, y subsidiariamente la geografía, han sido históricamente las disciplinas dedicadas a describir la Tierra. La topografía o determinación de lugares y distancias, y en menor medida la navegación, o determinación de la posición y de la dirección, se han desarrollado junto con la cartografía y la geografía, suministrando y cuantificando la información necesaria. La Tierra tiene aproximadamente 8200 millones de habitantes (según datos a julio de 2024). Las proyecciones indicaban que la población humana mundial llegaría a 7000 millones a principios de 2012, pero esta cifra fue superada a mediados de octubre de 2011 y se espera llegar a 10 300 millones en 2080. Se piensa que la mayor parte de este crecimiento tendrá lugar en los países en vías de desarrollo. La región del África subsahariana tiene la tasa de natalidad más alta del mundo. La densidad de población varía mucho en las distintas partes del mundo, pero la mayoría de la población vive en Asia. Está previsto que para el año 2020 el 60 % de la población mundial se concentre en áreas urbanas, frente al 40 % en áreas rurales. Se estima que solamente una octava parte de la superficie de la Tierra es apta para su ocupación por los seres humanos; tres cuartas partes está cubierta por océanos, y la mitad de la superficie terrestre es: desierto (14 %), alta montaña (27 %), u otros terrenos menos adecuados. El asentamiento permanente más septentrional del mundo es Alert, en la Isla de Ellesmere en Nunavut, Canadá. (82°28′N). El más meridional es la Base Amundsen-Scott, en la Antártida, casi exactamente en el Polo Sur. (90°S) La Tierra de noche. Imagen compuesta a partir de los datos de iluminación del DMSP/OLS, representando una imagen simulada del mundo de noche. Esta imagen no es fotográfica y muchas características son más brillantes de lo que le parecería a un observador directo. Las naciones soberanas independientes reclaman la totalidad de la superficie de tierra del planeta, a excepción de algunas partes de la Antártida y la zona no reclamada de Bir Tawil entre Egipto y Sudán. En el año 2011 existen 204 Estados soberanos, incluidos los 192 Estados miembros de las Naciones Unidas. Hay también 59 territorios dependientes, y una serie de áreas autónomas, territorios en disputa y otras entidades. Históricamente, la Tierra nunca ha tenido un gobierno soberano con autoridad sobre el mundo entero, a pesar de que una serie de estados-nación han intentado dominar el mundo, sin éxito. Las Naciones Unidas es una organización mundial intergubernamental que se creó con el objetivo de intervenir en las disputas entre las naciones, a fin de evitar los conflictos armados. Sin embargo, no es un gobierno mundial. La ONU sirve principalmente como un foro para la diplomacia y el derecho internacional. Cuando el consenso de sus miembros lo permite, proporciona un mecanismo para la intervención armada. Duración: 48 segundos.0:48 La Tierra de noche. El vídeo de la EEI comienza justo al sureste de Alaska. La primera ciudad que pasa por encima de la Estación Espacial Internacional (vista unos 10 segundos en el vídeo) es la de San Francisco y sus alrededores. Si se mira con mucho cuidado, se puede ver que en el puente Golden Gate se encuentra: una franja más pequeña de luces justo antes de la cercana ciudad de San Francisco, nubes a la derecha de la imagen. También se pueden ver tormentas eléctricas muy evidentes en la costa del océano Pacífico, con nubes. A medida que el video avanza, la EEI pasa por encima de América Central (las luces verdes se pueden ver aquí), con la península de Yucatán a la izquierda. El paseo termina en la Estación Espacial Internacional es la ciudad capital de Bolivia, La Paz. El primer humano en orbitar la Tierra fue Yuri Gagarin el 12 de abril de 1961. Hasta 2004, alrededor de 400 personas visitaron el espacio exterior y alcanzado la órbita de la Tierra. De estos, doce han caminado sobre la Luna. En circunstancias normales, los únicos seres humanos en el espacio son los de la Estación Espacial Internacional (EEI). La tripulación de la estación, compuesta en la actualidad por seis personas, suele ser reemplazada cada seis meses. Los seres humanos que más se han alejado de la Tierra se distanciaron 400 171 kilómetros, alcanzados en la década de 1970 durante la misión Apolo 13. Véase también: Mundo Perspectiva cultural La primera fotografía hecha por astronautas del «amanecer de la Tierra», tomada desde el Apolo 8. La palabra «Tierra» proviene del latín terra (en minúsculas) y que, en mayúsculas, se asoció a dos diosas arquetipos de la «madre tierra», Gea para los griegos y Tellus para los romanos. Especialmente, en la Edad Contemporánea, se le ha dado el nombre poético de Gaia. El símbolo astronómico estándar de la Tierra consiste en una cruz circunscrita por un círculo. A diferencia de lo sucedido con el resto de los planetas del sistema solar, la humanidad no comenzó a ver la Tierra como un objeto en movimiento, en órbita alrededor del Sol, hasta alcanzado el siglo XVI. La Tierra a menudo se ha personificado como una deidad, en particular, una diosa. En muchas culturas la diosa madre también es retratada como una diosa de la fertilidad. En muchas religiones los mitos sobre la creación recuerdan una historia en la que la Tierra es creada por una deidad o deidades sobrenaturales. Varios grupos religiosos, a menudo asociados a las ramas fundamentalistas del protestantismo o el islam, afirman que sus interpretaciones sobre estos mitos de creación, relatados en sus respectivos textos sagrados son la verdad literal, y que deberían ser consideradas junto con los argumentos científicos convencionales de la formación de la Tierra y el desarrollo y origen de la vida, o incluso reemplazarlos. Tales afirmaciones son rechazadas por la comunidad científica y otros grupos religiosos. Un ejemplo destacado es la controversia entre el creacionismo y la teoría de la evolución. En el pasado hubo varias creencias en una Tierra plana, pero esta creencia fue desplazada por el concepto de una Tierra esférica, debido a la gran evidencia de esta como su circunnavegación. La perspectiva humana acerca de la Tierra ha cambiado tras el comienzo de los vuelos espaciales, y actualmente la biosfera se interpreta desde una perspectiva global integrada. Esto se refleja en el creciente movimiento ecologista, que se preocupa por los efectos que causa la humanidad sobre el planeta."
ksampletext_wikipedia_geol_volcan: str = "Volcán. Un volcán (del portugués, y este del latín Vulcano, dios romano del fuego) es una estructura geológica en la tierra o en el mar, generalmente una montaña, por la que emerge el magma que se divide en lava y gases provenientes del interior de la Tierra. El ascenso del magma ocurre en episodios de actividad violenta denominados erupciones, que pueden variar en intensidad, duración y frecuencia, desde suaves corrientes de lava hasta explosiones extremadamente destructivas. En ocasiones, los volcanes adquieren una forma cónica por la acumulación de material de erupciones anteriores. En la cumbre se encuentra su cráter o caldera. Por lo general los volcanes se forman en los límites de las placas tectónicas, aunque existen los llamados puntos calientes, donde no hay contacto entre placas, como es el caso de las islas Hawái. Aproximadamente el 75% de los volcanes activos del mundo están ubicados en el llamado cinturón de fuego del Pacífico. Los volcanes pueden tener muchas formas y despedir distintos materiales. Algunas de las formas más comunes son el estratovolcán, el cono de escoria, la caldera volcánica y el volcán en escudo. También existen numerosos volcanes submarinos ubicados a lo largo de las dorsales mediooceánicas. Algunos volcanes alcanzan una altitud superior a los 6000 metros sobre el nivel del mar. El volcán más alto del mundo es el Nevado Ojos del Salado, en Argentina y Chile, siendo además la segunda cumbre más alta de los hemisferios sur y occidental (solo superado por el cerro argentino Aconcagua). Los volcanes no solo existen en la Tierra, sino también en otros planetas y satélites. Algunos están formados por materiales considerados fríos y se denominan criovolcanes. En ellos, el hielo actúa como roca, mientras que el agua fría líquida interna actúa como magma; esto ocurre en la luna de Júpiter llamada Europa. Relación entre vulcanismo y las placas tectónicas Límites de placa divergentes En las crestas oceánicas medias, dos placas tectónicas divergen entre sí a medida que se forma una nueva corteza oceánica por el enfriamiento y la solidificación de la roca fundida caliente. Debido a que la corteza es muy delgada en estas crestas debido al tirón de las placas tectónicas, la liberación de presión conduce a la expansión adiabática (sin transferencia de calor o materia) y al derretimiento parcial del manto, causando vulcanismo y creando una nueva corteza oceánica. La mayoría de los límites de placas divergentes se encuentran en el fondo de los océanos; por lo tanto, la mayor parte de la actividad volcánica en la Tierra es submarina, formando un nuevo fondo marino. Los fumadores negros (también conocidos como respiraderos de aguas profundas) son evidencia de este tipo de actividad volcánica. Donde la cresta oceánica media está sobre el nivel del mar, se forman islas volcánicas; por ejemplo, Islandia. Placas convergentes Las zonas de subducción son lugares donde chocan dos placas, generalmente una placa oceánica y una placa continental. En este caso, la placa oceánica se subduce, o se sumerge, debajo de la placa continental, formando una trinchera oceánica profunda en alta mar. En un proceso llamado fusión de flujo, el agua liberada de la placa subductora reduce la temperatura de fusión de la cuña del manto suprayacente, creando así magma. Este magma tiende a ser extremadamente viscoso debido a su alto contenido de sílice, por lo que a menudo no alcanza la superficie sino que se enfría y solidifica en profundidad. Cuando llega a la superficie, sin embargo, se forma un volcán. Ejemplos típicos son el Etna y los volcanes en el Anillo de Fuego del Pacífico. Puntos calientes Los puntos calientes son áreas volcánicas formadas por plumas de manto, que son columnas de material caliente que se elevan desde el límite núcleo-manto en un espacio fijo que causa la fusión de grandes volúmenes. En algunos casos, debido a que las placas tectónicas se mueven a través de ellas, cada volcán se vuelve inactivo y se forma uno nuevo a medida que la placa avanza sobre el penacho térmico, como en el caso del archipiélago de Hawái; también lo ha hecho la llanura del río Snake, con la caldera de Yellowstone como parte de la placa norteamericana sobre el punto caliente. Otros ejemplos de vulcanismo asociado a punto caliente son las islas Canarias, esta vez con un desplazamiento mínimo de la placa africana, o Islandia, que además coincide con un límite divergente de placas. Tipos de volcanes según su actividad Los volcanes, teniendo en cuenta la frecuencia de sus erupciones, se pueden clasificar en tres tipos: activos, inactivos (durmientes) o extintos. Volcanes activos Los volcanes activos son aquellos que pueden entrar en actividad eruptiva en cualquier momento, es decir, que permanecen en estado de latencia. Esto ocurre con la mayoría de los volcanes, pues ocasionalmente entran en actividad, permaneciendo en reposo la mayor parte del tiempo. El período de actividad eruptiva puede durar desde una hora hasta varios años, como fue el caso del volcán de Pacaya y del Irazú. Hasta el momento, no se ha descubierto ningún método seguro para predecir las erupciones. Volcanes durmientes o inactivos Los volcanes durmientes o inactivos son aquellos que mantienen ciertos signos de actividad, como la presencia de aguas termales, y han entrado en actividad esporádicamente. Dentro de esta categoría suelen incluirse las fumarolas y los volcanes con largos períodos de inactividad entre una erupción y otra. Un volcán se considera durmiente si desde hace siglos no ha tenido una erupción. Volcanes extintos Artículo principal: Volcán extinto Los volcanes extintos son aquellos cuya última erupción fue registrada hace más de 25 000 años. Sin embargo, no se descarta la posibilidad de que puedan despertar y liberar una erupción más fuerte que la de un volcán que está activo, causando grandes desastres. También se les llama extintos cuando han sido alejados de su fuente de magma, perdiendo poco a poco su actividad, esto sucede únicamente en volcanes de punto caliente, a diferencia de los volcanes de zonas de subducción. Tipos de erupciones volcánicas Artículo principal: Erupción volcánica Erupción en el 2011 del volcán Tungurahua, Ecuador. La temperatura, composición, viscosidad y elementos disueltos en el magma son los factores que determinan el tipo de erupción y la cantidad de productos volátiles que la acompañan. Hawaiana Artículo principal: Erupción hawaiana Volcán hawaiano en Kilauea. En este tipo de erupción, la lava generalmente es bastante fluida y no ocurren desprendimientos gaseosos explosivos. Estas lavas se desbordan cuando rebasan el cráter y se deslizan con facilidad por la ladera del volcán, formando verdaderas corrientes que recorren grandes distancias. Por esta razón, los volcanes de tipo hawaiano son de pendiente suave. Algunos residuos de lava, al ser arrastrados por el viento, forman hilos cristalinos que los nativos hawaianos llaman cabellos de la diosa Pele, la diosa del fuego. El volcán hawaiano más famoso es el Kilauea. Estromboliana o mixta Artículo principal: Erupción estromboliana Erupción del Estrómboli en verano de 2015 (animado). Erupción del Estrómboli (Italia) en 1980. Este tipo de erupción recibe el nombre del Estrómboli, volcán de las islas Eolias (mar Tirreno), al norte de Sicilia. Se origina cuando hay alternancia de los materiales en erupción, formándose un cono estratificado en capas de lavas fluidas y materiales sólidos. La lava es fluida, va desprendiendo gases abundantes y violentos con proyecciones de escorias, bombas y lapilli. Debido a que los gases pueden desprenderse con facilidad, no se producen pulverizaciones o cenizas. Cuando la lava rebosa por los bordes del cráter, desciende por las laderas y barrancos, pero no alcanza grandes extensiones como en las erupciones de tipo hawaiano. Vulcaniana Vulcano. Del nombre del volcán Vulcano en las islas Eolias. Esta erupción se caracteriza porque en ella se desprenden grandes cantidades de gases, la lava liberada es poco fluida y se consolida con rapidez. En este tipo de erupción, las explosiones son muy fuertes y pulverizan la lava, produciendo mucha ceniza, la cual es lanzada al aire acompañada de otros materiales fragmentarios. Cuando el magma sale al exterior en forma de lava, se solidifica rápidamente, pero los gases que se desprenden rompen y resquebrajan su superficie, volviéndola áspera y muy irregular y formando lava de tipo Aa. Los conos de estos volcanes son de pendiente muy inclinada. Pliniana o vesubiana Artículo principal: Erupción pliniana Nombrada así en honor a Plinio el Joven, difiere de la erupción volcánica en que en ésta la presión de los gases es muy fuerte y produce explosiones muy violentas, que en los casos extremos (plinianos) puede dar lugar a unas coladas piroclásticas o nubes ardientes que bruscamente se precipitan por las laderas del volcán alcanzando gran rapidez y sepultando en sólo unos minutos una gran extensión de terreno. Estos fenómenos críticos pueden sepultar y abrasar de golpe ciudades enteras, como ocurrió con Pompeya y Herculano por la actividad del volcán Vesubio. Al final de la deposición de esta colada piroclástica ardiente se transforma en la denominada roca ignimbrita; además se genera precipitaciones de cenizas, las cuales también pueden llegar a sepultar grandes extensiones como última capa fría. Se caracteriza por alternar erupciones de piroclasto con erupciones de coladas de lava, dando lugar a una superposición en estratos, lo que hace que este tipo de volcanes alcance grandes dimensiones; que también se denominan «Estratovolcanes». Ejemplo de ellos son el Teide, el Popocatépetl y el Fujiyama. Freatomagmática o surtseyana Artículos principales: Erupción surtseyana y Erupción freatomagmática. Los volcanes de tipo freatomagmático se encuentran en aguas someras, presentan un lago en el interior de su cráter y en ocasiones forman atolones. Sus erupciones son extraordinariamente violentas, ya que a la energía propia del volcán se le suma la expansión del vapor de agua súbitamente calentado. Normalmente no presentan emisiones de lava ni extrusiones de rocas. Algunas de las mayores erupciones freáticas son las del Krakatoa, el Kīlauea y la Isla de Surtsey. Peleana De los volcanes de las Antillas es célebre la Montaña Pelada, ubicada en la isla Martinica, que en la erupción de 1902 destruyó la capital, Saint-Pierre. La lava en esta erupción es extremadamente viscosa y se consolida con gran rapidez, llegando a tapar por completo el cráter formando un pitón o aguja. La enorme presión de los gases sin salida provoca una enorme explosión que levanta el pitón, o bien destroza la parte superior de la ladera. Así ocurrió el 8 de mayo de 1902, cuando las paredes del volcán cedieron a tan enorme empuje que se abrió un conducto por el que salieron con extraordinaria fuerza los gases acumulados a elevada temperatura y que, mezclados con cenizas, formaron una nube ardiente que ocasionó 28 000 víctimas. [cita requerida] Erupciones submarinas Artículo principal: Erupción submarina En el fondo oceánico se producen erupciones volcánicas cuyas lavas pueden formar islas volcánicas si llegan a la superficie. Las erupciones suelen ser de corta duración en la mayoría de los casos, debido al equilibrio isostático de las lavas al enfriarse cuando entran en contacto con el agua y también por la erosión marina. Algunas islas como las Cícladas en Grecia o las islas Canarias en España tienen este origen. Avalanchas de origen volcánico Artículo principal: Lahar Armero después de la tragedia (Colombia). Hay volcanes que generan un número de víctimas elevado, debido a que sus grandes cráteres están durante el periodo de reposo convertidos en lagos o cubiertos de nieve. Al recobrar su actividad, el agua mezclada con cenizas y otros restos, es lanzada formando torrentes y avalanchas o coladas de barro (que se denominan «lahares») que tienen una enorme capacidad destructiva. Un ejemplo de esto fue la erupción del Nevado de Ruiz en Colombia, el 13 de noviembre de 1985. El Nevado del Ruiz es un volcán explosivo en el que la cumbre del cráter (5321 m s.n.m.) estaba recubierta por un casquete de hielo; al ascender la lava se recalentaron las capas de hielo y se formaron unas coladas de barro que invadieron el valle del río Lagunilla, sepultando la ciudad de Armero, dejando 24 000 muertos y decenas de miles de heridos.[cita requerida] Erupciones fisurales Se originan en una larga dislocación de la corteza terrestre, que puede ser desde apenas unos metros hasta varios kilómetros. La lava que fluye a lo largo de la rotura es fluida y recorre grandes extensiones formando amplias mesetas (traps), con uno o más kilómetros de espesor y miles de km². Un ejemplo de vulcanismo fisural es la meseta del Decán en la India. Véase también: Índice de explosividad volcánica Volcán en escudo Artículo principal: Volcán en escudo Columnas de basalto de la «Calzada del Gigante» en Irlanda del Norte. Cuando la lava expulsada por el volcán es fluida, de tipo hawaiano, el volcán adquiere una forma de una estructura amplia y abovedada, que por su apariencia se los denomina en escudo. Los volcanes de escudo se asemejan a la superficie superior de un escudo que reposara en el suelo con el lado convexo hacia arriba. Un volcán en escudo está formado principalmente por lavas basálticas (ricas en hierro) y poco material piroclástico. El mayor volcán de la Tierra es el Mauna Loa, un volcán en escudo en las islas Hawái. El Mauna Loa nace en las profundidades del mar, a unos 5000 metros y se eleva sobre el nivel del mar por unos 4170 metros. Los volcanes en escudo como el Mauna Loa se forman a lo largo de millones de años gracias a ciclos de erupciones de lava que se van superponiendo unas con otras. El volcán de escudo más activo es el Kīlauea, localizado en la Isla de Hawái, al lado de Mauna Loa. En el período histórico el Kilauea ha entrado unas cincuenta veces en erupción y es, por lo tanto, el volcán de este tipo más estudiado. El resultado de erupciones constantes durante millones de años ha dado lugar a la creación de las montañas más grandes de la Tierra (si se tiene en cuenta la altura contando desde la base en el lecho marino). Por ejemplo, el Mauna Loa, desde su base submarina hasta su cúspide, cuenta con una altura de 9.5 km, más alto que el monte Everest. Los geólogos creen que las primeras etapas de formación de los volcanes en escudo consisten en erupciones frecuentes de delgadas coladas de basalto muy líquidas. Además de estas erupciones también se producen erupciones laterales. Normalmente con el cese de cada fase eruptiva se produce el hundimiento del área de la cima. En las últimas fases, las erupciones son más esporádicas y la erupción piroclástica se hace más frecuente. A medida que esto sucede, las coladas de lava tienden a ser más viscosas, lo que provoca que sean más cortas y potentes. Así, va aumentando la pendiente de la ladera del área de la cima. Los volcanes en escudo son muy comunes y también se han identificado en el sistema solar. El más grande conocido hasta la fecha es el monte Olimpo, sobre la superficie de Marte, encontrándose también varios de estos volcanes sobre la superficie de Venus, aunque de apariencia más achatada. Flujo piroclástico Artículo principal: Flujo piroclástico Flujo piroclástico expulsado por el volcán Mayón en Filipinas. Cuando las erupciones de un volcán llegan acompañadas de gases calientes y cenizas se produce lo que se conoce como flujo piroclástico o «nube ardiente». También conocida como avalancha incandescente, el flujo piroclástico se desplaza pendiente abajo a velocidades cercanas a los 200 km/h. La sección basal de estas nubes contienen gases calientes y partículas que flotan en ellos. De esta forma, las nubes transportan fragmentos de rocas que –gracias al rebote de los gases calientes en expansión– se depositan a lo largo de más de 100 km desde su punto de origen. En 1902 una nube ardiente de un pequeño volcán llamado monte Pelée en la isla caribeña de Martinica destruyó la ciudad portuaria de San Pedro. La destrucción fue tan devastadora que murió casi toda la población (unos 28 000 habitantes). A diferencia de Pompeya, que quedó enterrada en un manto de cenizas en un plazo de tres días y las casas quedaron intactas (salvo los techos por el peso de las cenizas), la ciudad de San Pedro fue destruida solo en minutos y la energía liberada fue tal que los árboles fueron arrancados de raíz, las paredes de las casas desaparecieron y las monturas de los cañones se desintegraron. La erupción del monte Pelée muestra cuan distintos pueden ser dos volcanes del mismo tipo. Lahar Artículo principal: Lahar Los conos compuestos también producen coladas de barro llamadas lahar, una palabra de origen indonesio. Estos flujos se producen cuando las cenizas y derrubios volcánicos se saturan de agua y descienden pendiente abajo, normalmente siguiendo los cauces de los ríos. Algunos de los lahares se producen cuando la saturación es provocada por la lluvia, mientras que en otros casos cuando grandes volúmenes de hielo y nieve se funden por una erupción volcánica. En Islandia, el último caso se denomina jökulhlaup y es un fenómeno devastador. Destrucciones importantes de lahares se dieron en 1980 con la erupción del monte Santa Helena, en Estados Unidos, que a pesar de los destrozos producidos, no produjo muchas víctimas debido a que la región está poco poblada. Otro fue en 1985 con la erupción del Nevado del Ruiz, en Colombia, la cual generó un lahar que acabó con la vida de 25 000 personas. Formas volcánicas relacionadas Calderas Artículo principal: Caldera volcánica Caldera Aniakchak, en Alaska. La mayoría de los volcanes presentan en su cima un cráter de paredes empinadas, por el interior. Cuando el cráter supera 1 km de diámetro se denomina caldera volcánica. Las calderas son estructuras de forma circular y la mayoría se forma cuando la estructura volcánica se hunde sobre la cámara magmática parcialmente vacía que se sitúa por debajo. Si bien la mayoría de las calderas se crea por el hundimiento producido después de una erupción explosiva, esto no es así en todos los casos. En el caso de los enormes volcanes en escudo de Hawái, las calderas se crearon por la continua subsidencia a medida que el magma se drenaba desde la cámara magmática durante las erupciones laterales. También las calderas de las islas Galápagos se han ido hundiendo por derrames laterales. Las calderas de gran tamaño se forman cuando un cuerpo lavático granítico (félsico) se ubica cerca de la superficie curvando de esta manera las rocas superiores. Posteriormente, una fractura en el techo permite al magma rico en gases y muy viscoso ascender hasta la superficie, donde expulsa de manera explosiva, enormes volúmenes de material piroclástico, fundamentalmente cenizas y fragmentos de pumita. Estos materiales se denominan coladas piroclásticas y pueden alcanzar velocidades de 100 km/h. Cuando estos materiales se detienen, los fragmentos calientes se fusionan para formar una toba soldada que se asemeja a una colada de lava solidificada. Finalmente, el techo se derrumba dando lugar a una caldera. Este procedimiento puede repetirse varias veces en el mismo lugar. Se conocen al menos 138 calderas que superan los 5 km de diámetro. Muchas de estas calderas son difíciles de ubicar, por lo que han sido identificadas con imágenes de satélites. Entre las más importantes se encuentra La Garita con unos 32 km de diámetro y una longitud de 80 que está ubicada en las montañas de San Juan al sur del estado de Colorado. Erupciones fisurales y llanuras de lava Artículo principal: Fisura volcánica Cono piroclástico en el volcán fisural Laki en Islandia. A pesar de que las erupciones volcánicas están relacionadas con estructuras en forma de cono, la mayor parte del material volcánico es extruido por fracturas en la corteza denominadas fisuras. Estas fisuras permiten la salida de lavas de baja viscosidad que recubren grandes áreas. La meseta del Columbia en el noroeste de Estados Unidos se formó de esta manera. Las erupciones fisurales expulsaron lava basáltica muy líquida. Las coladas siguientes cubrieron el relieve y formaron una llanura de lava (plateau) que en algunos lugares tiene casi 1.5 km de grosor. La fluidez se evidencia en la superficie recorrida por la lava: unos 150 km desde su origen. A estas coladas se las denomina basalto de inundación. Este tipo de coladas sucede principalmente en el suelo oceánico y no puede verse. A lo largo de las dorsales oceánicas, donde la expansión del suelo oceánico es activa, las erupciones fisurales generan nuevo suelo oceánico. Islandia está ubicada encima de la dorsal centroatlántica y ha experimentado numerosas erupciones fisurales. Las erupciones fisurales más grandes de Islandia ocurrieron en 1783 y se denominaron erupciones de Laki. Laki es una fisura o volcán fisural de 25 km de largo que generó más de veinte chimeneas separadas que expulsaron corrientes de lava basáltica muy fluida. El volumen total de lava expulsada por las erupciones de Laki fue superior a los 12 km³. Los gases arruinaron las praderas y mataron al ganado islandés. La hambruna subsiguiente mató cerca de diez mil personas. La caldera está situada muy por debajo de la boca del volcán. Domo de lava Artículo principal: Domo de lava Domos de lava en el cráter del monte Santa Helena (Estados Unidos). La lava rica en sílice es viscosa y por lo tanto, apenas fluye; cuando es extruida fuera de la chimenea puede producir una masa bulbosa de lava solidificada que se denomina domo de lava. Debido a su viscosidad, la mayoría está compuesto por riolitas y otros por obsidianas. La mayoría de los domos volcánicos se desarrollan a partir de una erupción explosiva de un magma rico en gases. Aunque la mayoría de los domos volcánicos están asociados a conos compuestos, algunos se forman de manera independiente. Tal es el caso de la línea de domos riolíticos y de obsidiana en los en California. Chimeneas y pitones volcánicos Artículos principales: Chimenea volcánica y Cuello volcánico. Volcán Teide (Tenerife, España). Los volcanes se alimentan del magma a través de conductos denominados chimeneas. Estas tuberías pueden extenderse hasta unos 200 km de profundidad. En este caso, las estructuras proveen de muestras del manto que han experimentado muy pocas alteraciones durante su ascenso. Las chimeneas volcánicas mejor conocidas son las sudafricanas que están cargadas de diamantes. Las rocas que rellenan estas chimeneas se originaron a profundidades de 150 km, donde la presión es lo bastante elevada como para generar diamantes y otros minerales de alta presión. Debido a que los volcanes están siendo rebajados constantemente por la erosión y la meteorización, los conos de cenizas son desgastados con el tiempo, pero no sucede lo mismo con otros volcanes. Conforme la erosión progresa, la roca que ocupa la chimenea y que es más resistente, puede permanecer de pie sobre el terreno circundante mucho después de que haya desaparecido el cono que la contiene. A estas estructuras de las denomina pitón volcánico. Shiprock, en Nuevo México, es un claro ejemplo de este tipo de estructuras. Cuevas volcánicas Artículo principal: Cueva volcánica Una cueva volcánica es cualquier cavidad formada en rocas volcánicas, aunque el uso común de este término se reserva a cuevas primarias o singenéticas creadas por procesos volcánicos de modo que tanto la oquedad como la roca encajante se forman a la vez. Material volcánico El Puu Ōō, cono volcánico de Hawái. Artículo principal: Roca volcánica El material que se forma por la actividad de un volcán son las rocas volcánicas, efusivas o extrusivas, principalmente basaltos y andesitas. Según su textura pueden ser coladas, piroclastos (lapilli, pumita), obsidiana, etc. Volcanes extraterrestres Monte Olimpo, el volcán más grande del sistema solar situado en el planeta Marte. La Tierra no es el único planeta del sistema solar que tiene actividad volcánica. Venus tiene un intenso vulcanismo con unos cientos de miles de volcanes. Marte tiene la cumbre más alta del sistema solar: el monte Olimpo, un volcán dado por apagado con una base de unos 600 km y más de 27 km de altura. No obstante, este planeta parece tener cierta actividad volcánica apreciable. Nuestra Luna está cubierta de inmensos campos de basalto y tiene presencia de domos lunares de origen volcánico similares a un volcán en escudo como por ejemplo el Mons Rümker, lo que sugiere que tuvo una corta pero considerable actividad volcánica que hoy muy probablemente está extinta. Debido a las bajas temperaturas del espacio, algunos volcanes de nuestro sistema solar están formados de hielo que actúa como roca, mientras su agua líquida interna actúa como la magma; esto ocurre -por ejemplo- en la fría luna de Júpiter llamada Europa. Estos reciben el nombre de criovolcán, de los cuales hay también en Encélado. La Voyager 2 descubrió en agosto de 1989, sobre Tritón, rastros de criovulcanismo y géiseres. La búsqueda de vida extraterrestre se ha interesado en buscar rastros de vida en sistemas criovolcánicos donde hay agua líquida y por ende, una fuente de radiación en calor considerable; estos son elementos esenciales para la vida. Existen volcanes un poco más similares a los terrestres, sobre otros satélites de Júpiter como en el caso de Ío. La sonda Voyager 1 permitió fotografiar en marzo de 1979 una erupción en Ío. Los astrofísicos estudian los datos de esta información, que extiende el campo de estudio de la vulcanología. El conocimiento del fenómeno tal como se produce sobre la Tierra pasa en adelante por su estudio en el espacio. La temperatura y composición química de los volcanes del sistema solar varían considerablemente entre los planetas y los satélites. Además, el tipo de materiales que arrojan en sus erupciones es muy diferente de los arrojados en la Tierra.[cita requerida] Peligros Esta sección es un extracto de Peligros volcánicos.[editar] Un diagrama esquemático muestra algunas de las muchas formas en que los volcanes pueden causar problemas a los que están cerca. Un peligro volcánico es la probabilidad de que ocurra una erupción volcánica o un suceso geofísico relacionado, en una determinada área geográfica y dentro de un período de tiempo específico. El riesgo asociado depende de la proximidad y vulnerabilidad de un bien, recurso natural o una población, cerca de donde podría ocurrir un suceso volcánico. Protección civil Véase también: Protección Civil España En España, la Norma Básica de Protección Civil, aprobada por Real Decreto 407/1992, de 24 de abril, dispone en su apartado 6 que el riesgo volcánico será objeto de planes especiales en los ámbitos territoriales que lo requieran. Estos planes especiales habrán de ser elaborados de acuerdo con una Directriz Básica previamente aprobada por el Gobierno. La Directriz Básica de Planificación de Protección Civil ante el Riesgo Volcánico fue aprobada por Acuerdo del Consejo de Ministros del 19 de enero de 1996 y publicada por Resolución de la Secretaría de Estado de Interior el 21 de febrero de 1996. En ella, se consideran tres niveles de planificación: estatal, autonómico y de ámbito local. Por Acuerdo del Consejo de Ministros del 25 de enero de 2013, se aprueba el Plan Estatal de Protección Civil ante el Riesgo Volcánico. Creencias tradicionales sobre los volcanes Vulcano forjando los rayos de Júpiter (1636), de Pedro Pablo Rubens. Muchos cuentos antiguos atribuyen las erupciones volcánicas a causas sobrenaturales, tales como la acción de dioses o semidioses. Los antiguos griegos aun pensaban que el poder caprichoso de los volcanes sólo podía ser explicado como un acto divino, mientras que el astrónomo de los siglos XVI-XVII Johannes Kepler creía que eran los conductos lagrimales de la Tierra. Previamente, el jesuita Atanasio Kircher, luego de haber sido testigo de erupciones del Etna y el Estrómboli y haber visitado el cráter del monte Vesubio, publicó su propuesta de que el planeta Tierra tenía un fuego central conectado a numerosos otros causados por la combustión de azufre, betún y carbón. Varias explicaciones fueron propuestas para explicar el comportamiento de los volcanes antes de que el entendimiento moderno de la estructura de la tierra se desarrollara. La acción volcánica solía atribuirse a reacciones químicas y a la delgada capa de piedra fundida cerca de la superficie. Volcanes activos en América del Sur Argentina Artículo principal: Anexo:Volcanes de Argentina Numerosos volcanes se distribuyen a lo largo del territorio de la República Argentina. Algunos volcanes se encuentran definitivamente extintos y otros activos, aunque la proporción va a depender de la definición de activo y extinguido; aquí se consideran activos los que han tenido erupciones probables o verificadas en los últimos 10 000 años. Los volcanes de Argentina son variados tanto en forma como en emplazamiento tectónico. La mayoría de los volcanes argentinos pertenecen al Cinturón volcánico de los Andes, aunque hay grandes y voluminosos volcanes de retroarco. Dada la naturaleza del vulcanismo, es imposible establecer un número exacto de volcanes. Cabe destacar que Argentina junto con Chile acogen al volcán más alto del mundo: Nevado Ojos del Salado. Bolivia Artículo principal: Anexo:Volcanes de Bolivia Bolivia acoge numerosos volcanes activos y extinguidos a través de su territorio. Los volcanes activos se encuentran en el oeste de Bolivia. Nevado Sajama (del aimara: chak xaña oeste) es un estratovolcán en Bolivia, ubicado en el parque nacional Sajama al oeste del país en el departamento de Oruro. No se sabe con certeza la fecha de su última erupción. Sin embargo, se le considera un volcán extinto. El volcán Ollagüe es un volcán activo situado en la frontera de Bolivia y Chile, en la región de Antofagasta en Chile y el Departamento de Potosí en Bolivia, en la cordillera de los Andes, con una altura de 5870 metros. Acotango es un estratovolcán ubicado en la frontera de Bolivia y Chile, entre el departamento de Oruro y la región de Arica y Parinacota. Su zona de influencia directa está protegida por el parque nacional Lauca, por el lado chileno, y el parque nacional Sajama, por el lado boliviano. Chile El volcán Villarrica es el más activo de Sudamérica, ha presentado alta actividad desde el s. VII d. C.. Artículo principal: Anexo:Volcanes de Chile Los volcanes en Chile son supervisados por el Servicio Nacional de Geología y Minería de Chile (SERNAGEOMIN). Entre las tareas de este organismo están, desde 1974, la publicación de la revista científica Andean Geology ,que se llamaba Revista Geológica de Chile hasta 2009,, y visualizar el Sistema de Información de Geología de Exploración (SIGEX) ,que reúne información sistematizada de los proyectos de exploración en Chile y los antecedentes técnicos y administrativos, entre otros,. La información fue obtenida de sitios web y otras fuentes públicas. De este modo, SERNAGEOMIN contribuye a consolidar el conocimiento geológico-minero del país (Art. 21 del Código de Minería de 1988). Según la Red Nacional de Vigilancia Volcánica del Servicio Nacional de Geología y Minería de Chile, el país posee 90 volcanes considerados como «activos», de entre los cuales destacan: Nevados de Chaitén, Villarrica, Planchón Peteroa y el complejo volcánico Laguna del Maule, este último compartido con Argentina. Colombia Artículo principal: Anexo:Volcanes de Colombia El Nevado del Ruiz: De acuerdo con el Servicio Geológico Colombiano este volcán presenta actividad sísmica regular, así como emisiones de ceniza. Su altura es de 5364 m y se encuentra en la zona cafetera del país. En noviembre de 1985 tuvo una erupción donde fallecieron más de 25 000 habitantes de la población de Armero. El volcán Galeras: Se ubica en el departamento de Nariño y está considerado como el volcán más activo de Colombia. En 1993 unos turistas y un grupo de científicos que se encontraban dentro de su cráter murieron, luego de una erupción. Durante los últimos años ha mantenido una actividad constante, con explosiones pequeñas y expulsión de ceniza y humo ocasional. Ecuador Artículo principal: Anexo:Volcanes de Ecuador Los volcanes activos del Ecuador continental pertenecen a la Zona Volcánica Norte (ZVN) de los Andes, la cual es parte del Cinturón Volcánico de los Andes. La Escuela Politécnica Nacional, también conocida como EPN, es una universidad pública, ubicada en Quito, Ecuador. El Instituto Geofísico dirige en los países volcanes en las montañas de los Andes de Ecuador y en las Islas Galápagos. El Instituto Geofísico EPN dirige desde 1999. El Instituto Geofísico de la Escuela Politécnica Nacional (IGEPN) reportó un rápido aumento en la actividad sísmica, el número de explosiones y una nube de cenizas que alcanzó los 2 km (1.2 millas) de altura, llegando la nube de ceniza a la ciudad de Guayaquil. El 26 de abril de 2011 hubo otra erupción de proporciones considerables, lanzando una columna de ceniza que ascendió hasta los 12 km de altura. El Instituto Geofísico E.P.N dispone de equipos internacionales de Sismología y Vulcanología y dirige volcanes en las islas Galápagos. En agosto de 2015, el Volcán Cotopaxi experimentó un incremento significativo de su actividad, motivando incluso la declaración de un estado de excepción en el territorio nacional. Actualmente se encuentra bajo vigilancia constante por parte del Instituto Geofísico de la EPN. El 25 de mayo de 2015, Isla Wolf (Galápagos) tuvo una erupción volcánica y ahora está siendo dirigida por el Instituto Geofísico de la Escuela Politécnica Nacional En un informe que detalla la erupción, los investigadores del Instituto Geofísico de Ecuador EPN declararon que la columna de humo alcanzó una altitud de 15 kilómetros aproximadamente. Perú Artículo principal: Anexo:Volcanes del Perú El volcán Ubinas, es el volcán más activo del Perú, ha registrado más de 25 erupciones en los últimos 500 años. El Perú está situado en el cinturón de Fuego del Pacífico, región del planeta que se caracteriza por su gran actividad sísmica y volcánica. Como resultado de ello, el sur del Perú está atravesado por más de 400 volcanes que componen el llamado Arco volcánico del Perú y que forman parte de la Zona Volcánica Central de los Andes (ZVC). El Perú cuenta con dos centros de monitoreo volcánico ubicados en la ciudad de Arequipa, los denominados Observatorio Vulcanológico del Sur del Instituto Geofísico del Perú (OVS-IGP) y el Observatorio Vulcanológico del INGEMMET (OVI), que mancomunadamente se han centrado en el objetivo de vigilar permanentemente los 16 volcanes activos y potencialmente activos (Sabancaya, Misti, Ubinas, Coropuna, Tutupaca, Huaynaputina, Ticsani, Chachani, Yucamani, Sara Sara, Ampato, Casiri, Purupuruni, Auquihuato y el Valle de los Volcanes en Andahua y Huambo), para ello cuentan con redes de vigilancia multiparamétricas que proporcionan información valiosa sobre el estado y niveles de actividad de los volcanes a su cargo. El Ubinas es considerado como el volcán más activo del Perú por sus 25 eventos de alta actividad fumarólica y actividad explosiva moderada registrada desde el año de 1550. Está situado en el distrito de Ubinas, departamento de Moquegua. Culmina a 5672 ms y cubre una superficie de 45 km². La más reciente erupción tuvo lugar entre marzo de 2006 a junio de 2009, afectado fuertemente la actividad agrícola en el valle de Ubinas. El inicio de esta crisis eruptiva se presentó dominado por una actividad freática y luego, a partir del 19 de abril de 2006, la actividad deviene en magmática de tipo vulcaniano con emisión de material andesítico básico. Posteriormente, y luego de cuatro años de inactividad, en septiembre de 2013 el volcán Ubinas entró en un nuevo proceso eruptivo, el cual se fue acelerando en febrero de 2014 al tiempo que se registraba Tremores sísmicos de gran energía, eventos de tipo Híbrido, así como emisiones persistentes de gases y ceniza, etc. Finalmente, esta alta actividad sísmica y fumarólica culminó con la ocurrencia de la primera explosión magmática el día 14 de febrero de 2014. A partir de entonces y hasta el presente, la actividad eruptiva del volcán Ubinas ha continuado de manera intermitente."

ksampletext_wikipedia_tech_telefonomovil: str = "Teléfono móvil. Un teléfono móvil o teléfono celular (Esp. móvil; Am. celular) (acortado como móvil o celular) es un dispositivo electrónico portátil, que puede permitir llamadas a través de una onda de radiofrecuencia, mientras el usuario se está moviendo dentro de un área de servicio telefónico. El enlace de radiofrecuencia establece una conexión con los sistemas de conmutación de un operador de telefonía móvil, que proporciona acceso a la red telefónica pública conmutada (PSTN). La mayoría de los servicios de telefonía móvil modernos utilizan una arquitectura de red de celdas (red celular) y por lo tanto los teléfonos móviles son, con frecuencia, llamados celulares. Durante los inicios de la telefonía móvil, estos dispositivos tenían la función de realizar y recibir llamadas de voz, además de ser considerablemente grandes de tamaño, por lo que estos teléfonos eran conocidos como «ladrillos» en su época. Con la evolución de la tecnología celular analógica a la digital, fueron incluyéndose otras funciones como: mensajes de texto (SMS), MMS, comunicaciones inalámbricas de corto alcance (infrarrojos, bluetooth), videojuegos, cámara digital, acceso a Internet, entre otros. Además, con el tiempo, fue cambiado el factor de forma de los teléfonos móviles, pasando de los antiguos ladrillos, a los barras y plegables de los años 1990 y 2000 (con teclado físico), hasta los actuales slates (de pantalla táctil y teclado virtual).[cita requerida] Estos aparatos fueron popularizándose en el mundo desarrollado durante el transcurso de los años 1990 y 2000. El teléfono de la Marca Blackberry también marco una etapa en la evolución de esta telefonía. Recientemente, en los años 2010 se popularizan los hoy día conocidos como «teléfonos inteligentes», que son teléfonos portátiles que pueden cumplir funciones de una computadora u ordenador, aparte de las funciones mencionadas anteriormente. Historia Artículo principal: Historia del teléfono móvil Martin Cooper de Motorola hizo la primera publicidad de una llamada con teléfono móvil portátil en un prototipo de modelo DynaTAC el 3 de abril de 1973. Esta es una dramatización hecha en 2007. En las primeras etapas de la ingeniería de radio, se concibió un servicio de radio móvil de mano. En 1917, el inventor finlandés Eric Tigerstedt presentó una patente para un Teléfono plegable de bolsillo con un micrófono de carbono muy delgado. Los primeros predecesores de los teléfonos móviles incluyen las comunicaciones de radio analógicas de barcos y trenes. La carrera para crear dispositivos telefónicos portátiles realmente comenzó después de la Segunda Guerra Mundial, con la evolución que tiene lugar en muchos países avanzados. En la URSS a finales de los años 1950 , el inventor Leonid Kupriánovich desarrolló un sistema de telefonía inalámbrica, el radioteléfono portátil dúplex portátil LK-1, que se extendió por la URSS y otros países. En 1973 John F. Mitchell y Martin Cooper de empresa estadounidense Motorola presentaron un sistema con terminales de 2 kgs de peso. En 1983 Motorola presentó el DynaTAC 8000x fue el primer teléfono móvil de mano disponible comercialmente. Los avances de la telefonía móvil se han trazado en generaciones sucesivas, empezando por los servicios 0G (generación cero), tales como Servicio de Telefonía Móvil de Sistemas de Bell y su sucesor, el Servicio de Telefonía Móvil Mejorada. Estos sistemas 0G no eran celular, soportaban algunas llamadas simultáneas, y eran muy caros. El Motorola DynaTAC 8000X (1984). El primer teléfono móvil celular disponible comercialmente. El primer teléfono celular de mano fue presentado por Motorola en 1983, aunque el primer teléfono móvil de mano estuvo disponible comercialmente en los años 80. La primera red celular automatizada comercial fue lanzada en Japón por Nippon Telegraph and Telephone en 1979. Esto fue seguido en 1981 por el lanzamiento simultáneo del sistema de Telefonía Móvil Nórdica (NMT) en Dinamarca, Finlandia, Noruega y Suecia. Muchos otros países siguieron lanzando la red celular analógica (1G) en la década de 1980 y principios de la década de 1990. Estos sistemas de primera generación (1G) podían hacer llamadas simultáneas más lejos, pero todavía se utilizaba la tecnología analógica. La primera llamada digital entre teléfonos celulares fue realizada en Estados Unidos en 1990. En 1991, la segunda generación (2G) de tecnología celular digital fue lanzada en Finlandia por Radiolinja, en el estándar GSM. Esto provocó la competencia en el sector, ya que los nuevos operadores desafiaron a los operadores de red 1G existentes. Esta tecnología fue popularizándose en la segunda mitad de la década. Nokia 3310. Diez años más tarde, en el 2001, la tercera generación (3G) fue lanzada en Japón por NTT DoCoMo en el estándar WCDMA. Esto fue seguido de 3.5G (H o 3G+) y luego el HSPAP (H+), que son mejoras basadas en el acceso de paquetes de alta velocidad (HSPA) de la familia, lo que permite a las redes UMTS tienen mayores velocidades de transferencia de datos y la capacidad. Para el año 2009, se hizo evidente que, en algún momento, las redes 3G se verían abrumadas por el crecimiento de las aplicaciones de banda ancha, tales como transmisión multimedia. En consecuencia, la industria comenzó a buscar a las tecnologías de datos de cuarta generación optimizadas, con la promesa de mejorar la velocidad hasta diez veces sobre tecnologías 3G existentes. Las dos primeras tecnologías disponibles en el mercado facturadas como 4G eran el estándar WiMAX, ofrecido en Norteamérica por Sprint, y el estándar LTE, quien se ofreció por primera vez en Escandinavia por TeliaSonera. Posteriormente apareció el 4.5 G (LTE-A). Para el año 2019 se lanzaron las primeras redes comerciales 5G en algunas partes del mundo, aunque más bien sigue siendo una tecnología experimental actualmente, con proyecciones a expandirse en el transcurso de la década de 2020. Desde 1973 a 2005, las suscripciones de teléfonos móviles en todo el mundo crecieron a más de siete mil millones, habiendo más teléfonos móviles que personas en el planeta y llegando hasta el fondo de la pirámide económica. Durante los años 1990 y 2000, los principales fabricantes de celulares eran Nokia, Motorola, entre otros. Actualmente, los principales fabricantes de teléfonos móviles son: Samsung, Apple y Huawei. Características Todos los teléfonos celulares tienen una variedad de características en común, pero los fabricantes buscan diferenciación de producto por añadir funciones para atraer consumidores. Esta competición ha dirigido a una gran innovación en el desarrollo del teléfono celular en los últimos 20 años. Los componentes comunes encontrados en todos los teléfonos son: Una batería, proporcionando la fuente de energía para las funciones del teléfono. Un teléfono moderno generalmente usa una batería de iones de litio (LIB), mientras que los teléfonos más antiguos usaban baterías de hidruro de níquel-metal (Ni-MH). Desde fines de los años 2010 la mayoría de los teléfonos inteligentes las baterías no son extraíbles por el consumidor. Un mecanismo de entrada para dejar interactuar al usuario con el teléfono. La entrada más común son las pantallas táctiles en los teléfonos inteligentes, pero en los más antiguos se usan botones, es decir por medio del teclado físico (del tipo 3x4). Una pantalla que repite al usuario lo que está escribiendo, muestra mensajes de texto, contactos y más. Las pantallas cada vez son de mayor tamaño y resolución, además de ser capacitivas o táctiles (a diferencia de los teléfonos antiguos que solamente actuaban como visualizador). Los servicios básicos de telefonía móvil que permiten a los usuarios hacer llamadas y enviar mensajes de texto. Todos los teléfonos GSM utilizan una tarjeta SIM que permiten tener una cuenta que puede intercambiarse entre los dispositivos. Algunos dispositivos CDMA también tienen una tarjeta similar llamado un R-UIM. Algunos dispositivos GSM, WCDMA, iDEN y algunos teléfonos satelitales se identifican por un número de Identidad del Equipo Móvil Internacional (IMEI). Los teléfonos móviles de gama baja se refieren generalmente como teléfonos con funciones, y ofrecen servicios de telefonía básica. Los terminales con capacidad de computación más avanzada mediante el uso de aplicaciones de software nativas son conocidos como teléfonos inteligentes. Se han introducido varias series de teléfonos para hacer frente a segmentos de mercado específicos, tales como el RIMBlackBerry centrándose en las necesidades de correo electrónico de clientes corporativos/empresariales, la serie Sony-Ericsson Walkman de música/móvil y serie Cyber-shot de cámara/teléfono, el Nokia Nseries de teléfonos multimedia, el Palm Pre, el HTC Dream y el iPhone de Apple. En Argentina cuando se compra un equipo con abono, la empresa de telefonía exige un período mínimo de permanencia, que figura en el contrato. Cuando termina ese plazo, la empresa tiene la obligación de dar el código de desbloqueo del equipo. Si un cliente quiere cambiar de compañía, sin respetar ese período de permanencia, puede hacerlo pagando una suma por esa rescisión anticipada. Calidad de sonido En calidad de sonido, los teléfonos inteligentes y los teléfonos con funciones varían muy poco. Han aparecido nuevos teléfonos inteligentes con algunas características que mejora la calidad de audio, tales como Voz sobre LTE y Voz HD. La calidad del sonido aún continúa siendo un problema ya que esto depende no tanto del propio teléfono, sino de la calidad de la red y, en llamadas de larga distancia, los cuellos de botella encontrados en el camino. Como tal, para llamadas de larga distancia, incluso las características Voz sobre LTE y voz HD puede no mejorar las cosas. En algunos casos, los teléfonos inteligentes pueden mejorar la calidad de audio incluso en llamadas de larga distancia, usando el servicio de telefonía VoIP, con la conexión Wifi/Internet de otra persona. Algunos teléfonos celulares tienen pequeños altavoces de modo que el usuario puede utilizar una función de altavoz y hablar con una persona en el teléfono sin sujetarlo a su oído. También se pueden utilizar pequeños altavoces para escuchar archivos de audio digitales de música o de voz, o ver vídeos con un componente de audio, sin sostener el teléfono cerca de la oreja. Mensajes de texto Artículo principal: SMS La aplicación de datos más utilizada en los teléfonos móviles es el Servicio de Mensajes de texto cortos (SMS). El primer mensaje SMS fue enviado desde un ordenador a un teléfono móvil en 1992 en el Reino Unido, mientras que el primer SMS de persona a persona de un teléfono a otro fue enviado en Finlandia en 1993. El primer servicio móvil de noticias, emitido a través de SMS, fue lanzado en Finlandia en 2000,[cita requerida] y posteriormente, muchas organizaciones proporcionan los servicios de noticias a través de SMS bajo demanda e instantánea. El Servicio de Mensajería Multimedia (MMS) fue introducido en el 2001.[cita requerida] Tarjeta SIM Artículo principal: Tarjeta SIM Típica tarjeta SIM de un teléfono móvil. Los teléfonos con funciones GSM requieren de pequeños microchips llamados Módulo de Identidad de Abonado o tarjeta SIM, para poder funcionar en la red. La tarjeta SIM es aproximadamente del tamaño de un sello de correos pequeño y por lo general se coloca debajo de la batería en la parte trasera del dispositivo. La SIM almacena de forma segura la clave de servicio del abonado (IMSI) y la K¡ usada para identificar y autenticar al usuario del teléfono móvil. La tarjeta SIM permite a los usuarios cambiar de teléfono simplemente retirando la tarjeta SIM de un teléfono móvil e insertándola en otro teléfono móvil o dispositivo de telefonía de banda ancha, siempre que no esté impedido por un bloqueo de SIM. La primera tarjeta SIM fue fabricada en 1991 por el fabricante de tarjetas inteligentes Múnich Giesecke & Devrient para el operador de red inalámbrico finlandés Radiolinja. Existen teléfonos móviles que pueden contener hasta cuatro tarjetas SIM, llamados teléfonos híbridos. Las tarjetas SIM y R-UIM se pueden sincronizar para permitir acceso a las redes GSM y CDMA disponibles. A partir de 2010 este tipo de teléfonos se hizo popular en la India e Indonesia y otros mercados emergentes, esto se atribuyó al deseo de obtener la tasa más baja de llamadas on-net. En el 3T de 2011, Nokia envió 18 millones de teléfonos de doble SIM de su gama de bajo costo en un intento de recuperar el terreno perdido en el mercado de teléfonos inteligentes de alta gama. Tipos Teléfonos inteligentes Artículo principal: Teléfono inteligente Diseño de teléfono inteligente (Galaxy Z Fold2 y Galaxy Z Flip2 de Samsung). El término teléfono inteligente (del inglés smartphone) se utiliza más bien con fines comerciales para distinguir de los teléfonos móviles básicos. En gran parte del mundo, los teléfonos inteligentes superaron el uso de los teléfonos convencionales básicos en la década de 2010. Estos dispositivos funcionan sobre una plataforma informática móvil, con mayor capacidad de almacenar datos y capaz de realizar tareas simultáneamente, tareas que realiza una computadora, y con una mayor conectividad que un teléfono convencional. La mayoría de estos dispositivos cuentan con una pantalla capacitiva (táctil) de alta resolución para poder interactuar por medio de la entrada (teclado) virtual, y visualizar el contenido multimedia en mejor calidad.[cita requerida] Teléfonos básicos Artículo principal: Teléfono básico Los términos teléfono básico (del inglés feature phone) y teléfono convencional son retrónimos aplicados a ciertos teléfonos móviles de baja gama o de características límitadas frente a la introducción de los teléfonos inteligentes. Los teléfonos básicos dominaron el mercado de la telefonía celular desde sus inicios, hasta finales de la década de 2000 con el avance de los teléfonos inteligentes. Hoy día siguen existiendo, aunque en menor medida y con algunas diferencias con respecto a los teléfonos básicos que estaban de moda en los años 1990 y 2000. Poseen funciones esenciales como la posibilidad de llamar o enviar mensajes de texto, y en algunos dispositivos el emplear archivos multimedia o navegar por internet usando conexiones de alta velocidad, GSM o WiFi. La mayoría de estos dispositivos cuentan con teclado físico y pantalla pequeña no capacitiva. Teléfonos Kosher Hay restricciones religiosas del judaísmo ortodoxo, que, según algunas interpretaciones, los teléfonos móviles estándares puedan sobrepasar. Para hacer frente a este problema, algunas organizaciones rabínicas han recomendado que los móviles con capacidad de mensajería de texto no puedan ser utilizados por niños. Es así que, a los teléfonos con funciones limitadas son conocidos como teléfonos kosher y tienen la aprobación para su uso rabínico en Israel y en otros lugares por los judíos ortodoxos practicantes. A pesar de que estos teléfonos estén destinados a evitar contenidos obscenos, algunos vendedores reportan buenas ventas en adultos que prefieren la simplicidad de los dispositivos. Algunos teléfonos están aprobados para su uso por los trabajadores esenciales (como trabajadores de salud, de seguridad y de servicios públicos) el Sabbat, a pesar de que en general el uso de cualquier dispositivo eléctrico esté prohibido durante este tiempo. Operadores de telefonía móvil Artículo principal: Operador de telefonía móvil Crecimiento en suscriptores de teléfono celular por país desde 1980 a 2009 Son las compañías de teléfono que proporcionan la red telefónica pública conmutada (PSTN), para que los usuarios de teléfonos móviles puedan acceder al servicio celular, ya sea por medio de un contrato (pospago) o por medio de recargas (prepago). El mayor operador móvil del mundo por número de suscriptores es China Mobile, el cual tiene más de 500 millones de suscriptores de telefonía móvil. Más de 50 operadores móviles tienen más de diez millones de suscriptores cada uno, y más de 150 operadores móviles han tenido por lo menos un millón de abonados a finales de 2009. En 2014, había más de siete mil millones de abonados de teléfonos móviles en todo el mundo, un número que se espera que siga aumentando. Fabricantes Artículo principal: Fabricantes de teléfonos móviles por país Antes de 2010, Nokia era el líder del mercado. Sin embargo, desde entonces ha emergido la competencia en la región de Asia y el Pacífico, de marcas como Micromax, Nexian e i-Mobile, que han disminuido la cuota de mercado de Nokia. Los teléfonos inteligentes Android también ganaron mucho terreno en toda la región gracias a Nokia. En la India, la cuota de mercado de Nokia se redujo significativamente desde 56 % hasta aproximadamente el 31 % en el mismo período. Su participación fue desplazada por proveedores de China e India de teléfonos móviles de gama baja. En el primer trimestre de 2012, según Strategy Analytics, Samsung superó a Nokia en ventas, de 93,5 millones unidades frente a 82,7 millones de unidades de Nokia. En 2012 Standard & Poors degradó a Nokia a la condición de estatus basura, en BB+/B, con perspectiva negativa debido a la alta pérdida y una mayor disminución esperada debido al insuficiente crecimiento en las ventas de teléfonos inteligentes Lumia para compensar una rápida disminución de los ingresos procedentes de los teléfonos inteligentes basados en Symbian que se pronostica para los próximos trimestres. Los dispositivos más vendidos, como Samsung y Xiaomi, poseen Android como sistema operativo, siendo este último el sistema operativo celular más popular del mundo. En segundo lugar le sigue el sistema operativo iOS de Apple, con cerca del 20% de la cuota. Cuota de mercado de entre los 5 fabricantes de teléfonos móviles en el mundo, 1T-2022 Rango Fabricante Informe de Shipments Market Share 1 Samsung 23 % 2 Apple 18 % 5 Xiaomi 12 % 3 Oppo 9 % 4 VIVO 9 % Otros 30 % Nota: Envíos de proveedores son envíos de marca y se excluyen las ventas OEM para todos los proveedores Otros fabricantes fuera de los primeros cinco lugares incluyen TCL Comunicatión, Lenovo, Sony Mobile Comunications, Motorola y LG Electronics. Pequeños jugadores actuales y pasados incluyen Audiovox (ahora UTStarcom), BenQ-Siemens, BlackBerry, Casio, CECT, Coolpad, Fujitsu, HTC, Just5, Intex, Karbonn Mobiles, Kyocera, Lumigon, Micromax Mobile, Mitsubishi Electric, Modu, NEC, Neonode, Openmoko, Panasonic, Palm, Pantech Wireless Inc., Philips, Sagem, Sanyo, Sharp, Sierra Wireless, SK Teletech, Trium y Toshiba. Uso General Suscriptores de teléfono celular por 100 habitantes. Figura de 2014 estimada. Los teléfonos móviles son usados para una variedad de propósitos, tales como mantener el contacto con miembros de la familia, conducir negocios, y con el fin de tener acceso a un teléfono en el caso de una emergencia. Algunas personas llevan más de un teléfono móvil para diferentes propósitos, tales como para uso comercial y personal. Se pueden usar múltiples tarjetas SIM para tomar ventaja de los beneficios de los diferentes planes de llamadas. Por ejemplo, un plan en particular podría prever llamadas más baratas locales, llamadas de larga distancia, llamadas internacionales, o itinerancia. Suscripciones de banda ancha móviles activas por 100 habitantes El teléfono móvil se ha usado en una variedad de diversos contextos de la sociedad. Por ejemplo: Un estudio realizado por Motorola encontró que uno de cada diez usuarios de telefonía móvil tienen un segundo teléfono que a menudo se mantiene en secreto de otros miembros de la familia. Estos teléfonos se pueden utilizar para participar en actividades tales como relaciones extramaritales o tratos comerciales clandestinos. Algunas organizaciones ayudan a las víctimas de la violencia doméstica, proporcionando teléfonos móviles para su uso en situaciones de emergencia. Estos son a menudo los teléfonos reacondicionados. El advenimiento de la generalizada mensajería de texto ha producido la novela de teléfono celular, el primer género literario que surge en la era celular, a través de mensajes de texto a un sitio web que recopila novelas en su conjunto. La telefonía móvil también facilita el activismo y el periodismo público siendo explorado por Reuters y Yahoo! y pequeñas empresas de noticias independientes como Jasmine News en Sri Lanka. Las Naciones Unidas informaron que los teléfonos móviles se han extendido más rápido que cualquier otra forma de tecnología y pueden mejorar la vida de las personas más pobres en los países en desarrollo, mediante el acceso a la información en los lugares donde la red fija o Internet no están disponibles, especialmente en los países menos desarrollados. El uso de los teléfonos móviles también genera una gran cantidad de microempresas, proporcionando este tipo de trabajo como la venta de tiempo aire en las calles y la reparación o reacondicionamiento de teléfonos. En Malí y otros países africanos, la gente solía viajar de pueblo en pueblo para que sus amigos y familiares sepan sobre las bodas, nacimientos y otros eventos. Esto ahora se puede evitar en áreas con cobertura de telefonía móvil, que suelen ser más extensas que las zonas con la penetración de línea fija. La industria de la televisión recientemente ha empezado a utilizar los teléfonos móviles para impulsar la televisión en vivo a través de la visualización de las aplicaciones móviles, publicidad, televisión social, y televisión móvil. Se estima que el 86% de los estadounidenses utilizan sus teléfonos móviles mientras ve la televisión. En algunas partes del mundo, el intercambio de teléfono móvil es común. Es frecuente en la India urbana, ya que las familias y grupos de amigos a menudo comparten uno o más teléfonos móviles entre sus miembros. Hay evidentes beneficios económicos, pero a menudo las costumbres familiares y los roles tradicionales de género juegan un papel. Es común que un pueblo tenga acceso a un solo teléfono móvil, tal vez propiedad de un maestro o misionero, que está disponible para todos los miembros del pueblo para llamadas necesarias. Para la distribución de contenidos En 1998, uno de los primeros ejemplos de la distribución y venta de contenidos multimedia a través del teléfono móvil fue la venta de tonos de llamada por Radiolinja en Finlandia. Poco después, apareció otro contenido multimedia, tales como noticias, videojuegos, chistes, horóscopos, contenidos de televisión y publicidad. La mayoría del contenido inicial para los teléfonos móviles tienden a ser copias de medios heredados, tales como anuncios publicitarios o informativos de videoclips televisivos más destacados. Recientemente, un contenido único para los teléfonos móviles ha ido surgiendo, desde tonos de llamada y tonos de espera para movisodios, contenido de vídeo producido exclusivamente para teléfonos móviles. En 2006, el valor total de los contenidos de los medios pagados de telefonía móvil superó al contenido multimedia de Internet pagado por 31 000 millones de dólares. El valor de la música en los teléfonos móviles superó los 9300 millones de dólares en 2007, y de juegos fue más de 5000 millones de dólares en el 2007. Al conducir un vehículo Un conductor de Nueva York utilizando dos teléfonos móviles El uso del teléfono móvil mientras se conduce, incluso hablar por teléfono, mensajes de texto u operar otras funciones del teléfono, es muy común pero controvertido. Es ampliamente considerado como peligroso debido a la distracción al manejar. Distraerse mientras se conduce un vehículo motorizado se ha demostrado que aumenta el riesgo de accidentes. En septiembre de 2010, la Administración Nacional de Seguridad del Tráfico de Carreteras (NHTSA) de Estados Unidos informó que 995 personas fueron asesinadas por conductores distraídos por los teléfonos celulares. En marzo de 2011 una compañía de seguros de Estados Unidos, State Farm Insurance, anunció los resultados de un estudio que mostró 19 % de los conductores encuestados accede a Internet en un teléfono inteligente, mientras conduce. Muchas jurisdicciones prohíben el uso de teléfonos móviles mientras conducen. En Egipto, Israel, Japón, Portugal y Singapur está prohibido el uso de ambos, tanto para el uso del dispositivo como del manos libres (que utiliza un altavoz) están prohibidos. En otros países, incluyendo el Reino Unido y Francia y en muchos estados de Estados Unidos, solo el uso de los teléfonos de mano está prohibido, pero se permite el uso de manos libres. Un estudio de 2011 informó que más del 90 % de los estudiantes universitarios encuestados mensajean (iniciar, resporder o leer) mientras conducen. La literatura científica sobre los peligros de conducir mientras se envía un mensaje de texto desde un teléfono móvil, o enviar mensajes de texto mientras se conduce, es limitada. Un estudio de simulación en la Universidad de Utah encontró un aumento de seis veces en accidentes relacionados con la distracción en cuanto a los mensajes de texto. Debido a la complejidad creciente de los teléfonos móviles, que a menudo son más como ordenadores móviles por sus usos disponibles. Esto ha introducido dificultades adicionales para las fuerzas del orden cuando se trata de distinguir una de otra en el uso de los conductores que utilizan sus dispositivos. Esto es más evidente en los países que prohíben el uso tanto del aparato de mano y manos libres, en lugar de aquellas que prohíben el uso del aparato telefónico, ya que las autoridades no pueden saber fácilmente qué función del teléfono móvil se utiliza simplemente mirando al conductor. Esto puede llevar a los conductores de ser detenidos por usar su dispositivo de forma ilegal para una llamada telefónica cuando, de hecho, estaban usando el dispositivo de forma legal, por ejemplo, cuando se utilizan controles incorporadas del teléfono para el estéreo del coche, el GPS o el satnav. Una señal a lo largo de Bellaire Boulevard en Southside Place, Texas (Greater Houston) prohíbe el uso de teléfonos celulares mientras se conduce desde las 7:30 a. m. hasta las 9:00 a. m. y las 14:00 hasta las 16:15 Un estudio de 2010 examinó la incidencia de uso del teléfono móvil durante el ciclismo y sus efectos sobre el comportamiento y la seguridad. En 2013 una encuesta nacional en los EE. UU. informó el número de conductores que reportó el uso de sus teléfonos móviles para acceder a Internet mientras conducían se había elevado a casi uno de cada cuatro. Un estudio realizado por la Universidad de Illinois examinó enfoques para reducir el uso inapropiado y problemático de los teléfonos móviles, tales como el uso de teléfonos móviles durante la conducción. Los accidentes que involucran a un conductor distraído por hablar con un teléfono móvil han comenzado a ser perseguido como una negligencia similar al exceso de velocidad. En el Reino Unido, desde el 27 de febrero de 2007, los automovilistas que sean encontrados usando un teléfono móvil de mano mientras conduce ganarán tres puntos de penalización en su licencia, además de la multa de 60 libras. Esta medida se introdujo para intentar frenar el incremento de conductores que infringían la ley. En Japón se prohíbe el uso de teléfonos móviles mientras conduce, incluyendo el uso de dispositivos de manos libres. Nueva Zelanda ha prohibido el uso de celulares de mano desde el 1 de noviembre de 2009. Muchos estados en los Estados Unidos han prohibido los mensajes de texto en teléfonos celulares mientras se conduce. Illinois se convirtió en el estado americano número 17 en hacer cumplir esta ley. A partir de julio de 2010, 30 estados habían prohibido enviar mensajes de texto mientras se conduce, con Kentucky convirtiéndose en la más reciente adición el 15 de julio. En México desde 2015 también es considerado un delito que puede ser castigado hasta con MXN$2642.15 En España, conducir y usar el teléfono a la vez está penalizado hasta con 200 Euros y la pérdida de tres puntos. Public Health Law Research mantiene una lista de las leyes de distracción al conducir en los Estados Unidos. Esta base de datos de leyes ofrece una visión completa de las provisiones de leyes que restringen el uso de dispositivos de comunicación móvil mientras se conduce para los 50 estados y el Distrito de Columbia entre 1992 (cuando se aprobó la primera ley), hasta el 1 de diciembre de 2010. El conjunto de datos contiene información sobre 22 variables dicotómicas, continuas o categóricas que incluyen, por ejemplo, actividades reguladas (por ejemplo, mensajes de texto versus hablar, manos libres versus sostenerlo), poblaciones específicas y exenciones. Banca móvil y pagos Artículos principales: Banca móvil y Pago móvil. Véase también: Pago sin contacto Sistema de pago móvil En muchos países, los teléfonos móviles se utilizan para proporcionar servicios de banca móvil, que pueden incluir la capacidad de transferir los pagos en efectivo por mensaje de texto SMS seguro. El servicio de banca móvil M-PESA de Kenia, por ejemplo, permite a los clientes del operador de telefonía móvil Safaricom retener saldos de efectivo que son registrados en sus tarjetas SIM. El efectivo puede ser depositado o retirado de las cuentas de M-PESA en los puntos de venta Safaricom de todo el país, y se puede transferir electrónicamente de persona a persona y utilizar para pagar las facturas de las empresas. La banca sin sucursales también ha tenido éxito en Sudáfrica y Filipinas. Un proyecto piloto en Bali se puso en marcha en 2011 por la Corporación Financiera Internacional y un banco de Indonesia, Bank Mandiri. Otra aplicación de la tecnología de la banca móvil es Zidisha, una plataforma de micropréstamos sin fines de lucro con sede en Estados Unidos que permite a los residentes de los países en desarrollo aumentar los préstamos de pequeñas empresas de usuarios Web en todo el mundo. Zidisha utiliza la banca móvil para los desembolsos de préstamos y reembolsos, la transferencia de fondos de los prestamistas en los Estados Unidos a los prestatarios en zonas rurales de África que tienen teléfonos móviles y que pueden utilizar el Internet. Los pagos móviles se pusieron a prueba por primera vez en Finlandia en 1998, cuando se habilitaron dos máquinas expendedoras de Coca-Cola en Espoo para trabajar con pagos SMS. Con el tiempo, la idea se extendió y en 1999, las Filipinas lanzó los primeros sistemas de pagos móviles comerciales del país con los operadores móviles Globe y Smart. Algunos teléfonos móviles pueden realizar pagos móviles a través de programas de facturación móvil directa, o mediante los pagos sin contacto, si el teléfono y el punto de venta soportan Near Field Communication (NFC). La activación de los pagos sin contacto a través de los teléfonos móviles equipados con NFC requiere la cooperación de los fabricantes, operadores de redes y comerciantes al por menor. Seguimiento y privacidad Artículos principales: Vigilancia de teléfonos móviles y Localización GSM. Los teléfonos móviles se utilizan comúnmente para recopilar datos de localización. Mientras que el teléfono está encendido, la ubicación geográfica de un teléfono móvil se puede determinar con facilidad (si se utiliza o no) usando una técnica conocida como multilateración para calcular las diferencias en tiempo que una señal viaja desde el teléfono móvil a cada una de varias torres de telefonía móvil cercanas al propietario del teléfono. Los movimientos de un usuario de teléfono móvil pueden ser rastreados por su proveedor de servicios y, si se desea, por las fuerzas del orden y sus gobiernos. Tanto la tarjeta SIM como el teléfono pueden ser rastreados. China ha propuesto el uso de esta tecnología para realizar un seguimiento de las pautas de movilidad de los residentes de la ciudad de Beijing. En el Reino Unido y los Estados Unidos, la policía y los servicios de inteligencia utilizan teléfonos móviles para realizar operaciones de vigilancia. Poseen tecnología que les permite activar los micrófonos en los teléfonos móviles de forma remota con el fin de escuchar las conversaciones que tienen lugar cerca del teléfono. Los hackers son capaces de rastrear la ubicación de un teléfono, leer los mensajes y grabar las llamadas, con solo saber el número de teléfono. Robos De acuerdo con la Comisión Federal de Comunicaciones, uno de cada tres robos involucran el robo de un teléfono móvil. Datos de la Policía en San Francisco muestran que la mitad de todos los robos en 2012 fueron robos de teléfonos móviles. Una petición en línea en Change.org, llamada Asegurar nuestras smartphones (Secure our Smartphones), instó a fabricantes de teléfonos inteligentes a que instalaran interruptores de apagado (kill switches) en sus dispositivos para que queden inutilizables en caso de robo. La petición es parte de un esfuerzo conjunto por el fiscal general de Nueva York, Eric Schneiderman y el fiscal del distrito de San Francisco, George Gascón, y fue dirigido a los directores generales de los principales fabricantes de teléfonos inteligentes y operadores de telecomunicaciones. En México el robo de celulares aumentó un 500% en el año 2017 convirtiéndolos en los objetos más robados, puesto que permite a los delincuentes ganar dinero con su reventa y las penas por ese delito son mínimas. El lunes 10 de junio de 2013, Apple anunció que instalaría un interruptor de apagado en su próximo sistema operativo iPhone, debido a su debut en octubre de 2013. Por otro lado Android ofrece su aplicación Encontrar mi dispositivo para encontrar el teléfono perdido, bloquearlo o restablecerlo borrando toda su información a distancia. Asimismo, los proveedores ofrecen el bloqueo por IMEI para dejar inservibles los teléfonos robados. También existen formas de evitar este delito como evitar exponer el teléfono en lugares concurridos, contestar llamadas y mensajes solo en lugares seguros, evitar resistirse al asalto, bloquear el teléfono y evitar usarlo en lugares peligrosos. En Argentina, como medida para combatir los robos a los teléfonos celulares y dar más seguridad a los usuarios del servicio, los titulares de líneas de telefonía celular deben registrar sus datos en los registros de las compañías de telefonía celular. Efectos en la salud Artículo principal: Radiación de teléfonos móviles y salud El efecto de la radiación del teléfono móvil en la salud humana es el tema de reciente interés y estudio, como resultado del enorme aumento en el uso de teléfonos móviles en todo el mundo. Los teléfonos móviles utilizan radiación electromagnética en el rango de las microondas, que algunos creen que puede ser perjudicial para la salud humana. Existe una gran cantidad de investigaciones, tanto epidemiológica y experimental, en animales no humanos y en los seres humanos. La mayoría de estas investigaciones no muestran relación causal clara entre la exposición a los teléfonos móviles y los efectos biológicos nocivos en los seres humanos. Esto a menudo se parafraseó simplemente como el balance de evidencia que muestra ningún daño de los teléfonos móviles a los seres humanos, aunque un número significativo de estudios individuales sí sugieren una relación, o no son concluyentes. Otros sistemas inalámbricos digitales, tales como las redes de comunicación de datos, producen radiación similar. El 31 de mayo de 2011, la Organización Mundial de la Salud declaró que el uso de teléfonos móviles, posiblemente, puede representar riesgo para la salud a largo plazo, clasificando la radiación de los teléfonos móviles como Posiblemente cancerígeno para los seres humanos después de que un equipo de científicos revisaran estudios en teléfono móvil seguros. El teléfono móvil está en la categoría 2B, lo que la ubica junto al café y otras posibles sustancias cancerígenas. Algunos estudios recientes han encontrado una asociación entre el uso de teléfonos móviles y algunos tipos de tumores cerebrales y de la glándula salival. Lennart Hardell y otros autores de un metaanálisis del 2009 sobre 11 estudios de revistas revisadas por pares concluyeron que el uso del teléfono celular durante al menos diez años duplica aproximadamente el riesgo de ser diagnosticado con un tumor cerebral en el mismo (ipsilateral) lado de la cabeza que prefiera usar teléfonos celulares. Un estudio anterior sobre el uso del teléfono móvil, muestra un informe del 40% de aumento en el riesgo de gliomas (cáncer cerebral) en la máxima categoría de grandes usuarios (media: 30 minutos por día durante un período de 10 años). Esto es un revés al estudio previo con la posición de que el cáncer es poco probable que sea causado por los teléfonos móviles o sus estaciones base y los comentarios que no habían encontrado evidencia convincente para otros efectos en la salud. Sin embargo, un estudio publicado el 24 de marzo de 2012 en la British Medical Journal cuestionando estas estimaciones, debido a que el aumento en los cánceres de cerebro no ha sido paralelo al aumento del uso de teléfonos móviles. Ciertos países, como Francia, han advertido sobre el uso de teléfonos móviles por los menores de edad en particular, debido a la incertidumbre sobre el riesgo para la salud. La contaminación de móviles mediante la transmisión de ondas electromagnéticas puede disminuir hasta un 90 % adoptando el circuito tal como fue diseñado en el teléfono móvil (MS) y central móvil (BTS, MSC, etc.). En mayo de 2016, los resultados preliminares de un estudio a largo plazo por el gobierno de los Estados Unidos sugirió que la radiación de la radiofrecuencia (RF), del tipo emitida por los teléfonos celulares pueden causar cáncer. Evolución futura Artículo principal: Telefonía móvil 5G 5G es una tecnología y un término utilizado en trabajos y proyectos de investigación para referirse a la siguiente fase importante en las normas de telecomunicaciones móviles más allá de los estándares 4G/IMT-Avanced. El término 5G no se utiliza oficialmente en cualquier especificación o documento aún hecha pública oficial por las empresas de telecomunicaciones o los organismos de normalización, tales como 3GPP, WiMAX Forum o UIT-R. Nuevos estándares más allá de 4G actualmente están siendo desarrolladas por los organismos de normalización, pero son en este momento visto como bajo el paraguas 4G, no para una nueva generación móvil. Deloitte predice un colapso en el rendimiento inalámbrico a venir tan pronto como 2016, a medida que más dispositivos que utilizan cada vez más servicios compiten por el ancho de banda limitado para su operación. En 2019 ya hay a la venta teléfonos con capacidad 5G, como el LG V50 ThinQ 5G, aunque apenas hay redes con cobertura 5G en el mundo. Es una tecnología que se impondrá progresivamente, a partir del año 2022. Impacto ambiental Un quiosco de reparación de teléfonos celulares en Hong Kong Los estudios han demostrado que alrededor del 40-50 % del impacto ambiental de los teléfonos móviles se produce durante la fabricación de sus placas de circuitos impresos y circuitos integrados. El usuario promedio reemplaza su teléfono móvil cada 11 a 18 meses, y los celulares descartados luego contribuyen a la basura electrónica. Fabricantes de teléfonos móviles en Europa están sujetos a la directiva WEEE, y Australia ha introducido un sistema de reciclaje de teléfonos móviles. Apple se ha dado cuenta de cómo sus productos cuando no se reciclan impactan al medio ambiente y los residuos son valiosos recursos. Liam de Apple se introdujo al mundo como un desensamblador robótico avanzado y clasificador diseñada por Ingenieros de Apple en California específicamente para el reciclaje de iPhones obsoletos o rotos. Reutiliza y recicla piezas de productos negociados. Minerales conflictivos La demanda de los metales usados en los teléfonos móviles y otros electrónicos alimentaron la segunda guerra del Congo, que cobró casi 5,5 millones de vidas. En una noticia de 2012, The Guardian informó: Inseguridad en minas subterráneas profundas en el este del Congo, niños trabajan para extraer minerales esenciales para la industria de la electrónica. Los beneficios de los minerales financian el conflicto más sangriento desde la segunda guerra mundial; la guerra ha durado más de 20 años y recientemente ha estallado de nuevo... Durante los últimos 15 años, la República Democrática del Congo ha sido una importante fuente de recursos naturales para la industria de la telefonía móvil. La compañía Fairphone ha intentado desarrollar un teléfono móvil que no contiene minerales conflictivos. Reciclaje Esta sección es un extracto de Reciclaje de teléfonos móviles.[editar] Teléfonos celulares desechados. El reciclaje de teléfonos móviles es el proceso de reciclaje que pueden seguir los teléfonos móviles (también llamados celulares) al final de su vida útil. A causa del frecuente cambio de dispositivos, el bajo costo, e incluso la obsolescencia programada, muchísimos teléfonos son desechados anualmente, lo cual contribuye a la creciente cantidad de residuos electrónicos alrededor del mundo. Los recicladores consideran a este tipo de deshechos un problema que se expande rápidamente. En los Estados Unidos, aproximadamente un 70% de metales pesados en los basureros proviene de tecnología desechada. Los residuos electrónicos apenas representan entre un 2% y un 5% de la basura en los vertederos de aquel país, sin embargo esta cifra crece rápidamente. Los teléfonos celulares son considerados residuos peligrosos en California. Muchas sustancias químicas presentes en tales dispositivos drenan de los basureros al sistema de agua subterránea. La organización Greenpeace advierte de que al estar soldada la batería del iPhone a su carcasa, se dificulta su reciclado. También señala que sus científicos han encontrado tóxicos Ftalatos en los cables de este modelo de teléfono celular, y añade que esto va en contra de la proposición 65, la cual exige advertir de esto en las etiquetas de los productos que exponen a los consumidores a esta sustancia tóxica. Estados Unidos no ha ratificado la Convención de Basilea ni su enmienda de Prohibición, y no posee leyes nacionales que prohíban la exportación de residuos tóxicos. La Basel Action Network estima que, aproximadamente un 80% de los residuos electrónicos que se enviaron a reciclar en los EE. UU. no se reciclaron del todo, sino que se dirigieron en buques portacontenedores a países como China. Algunos lugares como Guiyu en la región de Shantou, China; y Delhi y Bangalore en India, tienen áreas de procesamiento de residuos electrónicos."
ksampletext_wikipedia_tech_inteligenciaartificial: str = "Inteligencia artificial. La inteligencia artificial, abreviado como IA, en el contexto de las ciencias de la computación, es una disciplina y un conjunto de capacidades cognoscitivas e intelectuales expresadas por sistemas informáticos o combinaciones de algoritmos cuyo propósito es la creación de máquinas que imiten la inteligencia humana. Estas tecnologías permiten que las máquinas aprendan de la experiencia, se adapten a nuevas entradas y realicen tareas humanas como el reconocimiento de voz, la toma de decisiones, la traducción de idiomas o la visión por computadora. En la actualidad, la inteligencia artificial abarca una gran variedad de subcampos. Estos van desde áreas de propósito general, aprendizaje y percepción, a otras más específicas como el reconocimiento de voz, el juego de ajedrez, la demostración de teoremas matemáticos, la escritura de poesía y el diagnóstico de enfermedades. La inteligencia artificial sintetiza y automatiza tareas que en principio son intelectuales y, por lo tanto, es potencialmente relevante para cualquier ámbito de actividades intelectuales humanas. En este sentido, es un campo genuinamente universal, además, la IA se encuentra en constante evolución gracias al desarrollo de tecnologías como el aprendizaje profundo, redes neuronales y procesamiento del lenguaje natural, lo cual permite un avance acelerado en su capacidad para resolver problemas complejos. La arquitectura de las inteligencias artificiales y los procesos por los cuales aprenden, se mejoran y se implementan en algún área de interés que varía según el enfoque de utilidad que se les quiera dar, pero de manera general, estos van desde la ejecución de sencillos algoritmos hasta la interconexión de complejas redes neuronales artificiales que intentan replicar los circuitos neuronales del cerebro humano y que aprenden mediante diferentes modelos de aprendizaje tales como el aprendizaje automático, el aprendizaje por refuerzo, el aprendizaje profundo y el aprendizaje supervisado. Por otro lado, el desarrollo y aplicación de la inteligencia artificial en muchos aspectos de la vida cotidiana también ha propiciado la creación de nuevos campos de estudio como la roboética y la ética de las máquinas, que abordan aspectos relacionados con la ética en la inteligencia artificial y que se encargan de analizar cómo los avances en este tipo de tecnologías impactarían en diversos ámbitos de la vida, así como el manejo responsable y ético que se les debería dar a los mismos, además de establecer cuál debería ser la manera correcta de proceder de las máquinas y las reglas que deberían cumplir. En cuanto a su clasificación, tradicionalmente se divide a la inteligencia artificial en inteligencia artificial débil, la cual es la única que existe en la actualidad y que se ocupa de realizar tareas específicas, e inteligencia artificial general, que sería una IA que excediese las capacidades humanas. Algunos expertos creen que si alguna vez se alcanzara este nivel, se podría dar lugar a la aparición de una singularidad tecnológica, es decir, una entidad tecnológica superior que se mejoraría a sí misma constantemente, volviéndose incontrolable para los humanos, dando pie a teorías como el basilisco de Roko. Algunas de las inteligencias artificiales más conocidas y utilizadas en la actualidad alrededor del mundo incluyen inteligencia artificial en el campo de la salud, asistentes virtuales como Alexa, el asistente de Google o Siri, traductores automáticos como el traductor de Google y DeepL, sistemas de recomendación como el de la plataforma digital de YouTube, motores de ajedrez y otros juegos como Stockfish y AlphaZero, chatbots como ChatGPT, creadores de arte de inteligencia artificial como Midjourney, Dall-e, Leonardo y Stable Diffusion, e incluso la conducción de vehículos autónomos como Tesla Autopilot. Denominación Ilustración generada mediante inteligencia artificial, con estilo de Acuarela, de Alan Turing, considerado uno de los padres fundadores de la IA. En 2019 la Comisión Mundial de Ética del Conocimiento Científico y la Tecnología (COMEST) de la UNESCO definió la inteligencia artificial como un campo que implica máquinas capaces de imitar determinadas funcionalidades de la inteligencia humana, incluidas características como la percepción, el aprendizaje, el razonamiento, la resolución de problemas, la interacción lingüística e incluso la producción de trabajos creativos. Coloquialmente, la locución «inteligencia artificial» se aplica cuando una máquina imita las funciones «cognitivas» que los humanos asocian como competencias humanas; por ejemplo: «percibir», «razonar», «aprender» y «resolver problemas». Andreas Kaplan y Michael Haenlein definen la inteligencia artificial como «la capacidad de un sistema para interpretar correctamente datos externos, y así aprender y emplear esos conocimientos para lograr tareas y metas concretas a través de la adaptación flexible». A medida que las máquinas se vuelven cada vez más capaces, se elimina de la definición la tecnología que alguna vez se pensó que requería de inteligencia. Marvin Minsky, uno de los ideadores de la IA, hablaba del término inteligencia artificial como una palabra maleta («suitcase word») porque en él se pueden meter una diversidad de elementos. Por ejemplo, el reconocimiento óptico de caracteres ya no se percibe como un ejemplo de la «inteligencia artificial», habiéndose convertido en una tecnología común. Avances tecnológicos todavía clasificados como inteligencia artificial son los sistemas de conducción autónomos o los capaces de jugar al ajedrez o Go. La inteligencia artificial es una nueva forma de resolver problemas dentro de los cuales se incluyen los sistemas expertos, el manejo y control de robots y los procesadores, que intenta integrar el conocimiento en tales sistemas; en otras palabras, un sistema inteligente capaz de escribir su propio programa. Un sistema experto definido como una estructura de programación capaz de almacenar y utilizar un conocimiento sobre un área determinada que se traduce en su capacidad de aprendizaje. De igual manera, se puede considerar a la IA como la capacidad de las máquinas para usar algoritmos, aprender de los datos y utilizar lo aprendido en la toma de decisiones tal y como lo haría un ser humano. Según Takeyas (2007), la IA es una rama de las ciencias computacionales encargada de estudiar modelos de cómputo capaces de realizar actividades propias de los seres humanos con base en dos de sus características primordiales: el razonamiento y la conducta. En 1956, John McCarthy acuñó la expresión «inteligencia artificial», y la definió como «la ciencia e ingenio de hacer máquinas inteligentes, especialmente programas de cómputo inteligentes». Grau-Luque contrasta diferentes definiciones desde diversas fuentes y autores, destacando que difieren dependiendo de «en qué campo específico se usen».Esto lleva al autor a definir «inteligencia artificial» como «sistemas que llevan a cabo tareas consideradas inteligentes», para luego asociar conceptos como «aprendizaje» y «razonamiento» con el aprendizaje automático como una subdisciplina de la inteligencia artificial. También existen distintos tipos de percepciones y acciones, que pueden ser obtenidas y producidas, respectivamente, por sensores físicos y sensores mecánicos en máquinas, pulsos eléctricos u ópticos en computadoras, tanto como por entradas y salidas de bits de un software y su entorno hardware. Varios ejemplos se encuentran en el área de control de sistemas, planificación automática, la capacidad de responder a diagnósticos y a consultas de los consumidores, reconocimiento de escritura, reconocimiento del habla y reconocimiento de patrones. Los sistemas de IA actualmente son parte de la rutina en campos como economía, medicina, ingeniería, el transporte, las comunicaciones y la milicia, y se ha usado en gran variedad de programas informáticos, juegos de estrategia, como ajedrez de computador, y otros videojuegos. Asimismo la inteligencia artificial se está desarrollando en la plataforma digital cada vez más, evolucionando y creando nuevas herramientas, como la plataforma laboral que existe desde el año 2023 llamada SIVIUM, una herramienta por la cual una persona postula en forma automatizada a todas las ofertas laborales de todos los portales de trabajo, sin necesidad de estar revisando cada oferta laboral que se presente y enviar su CV uno por uno. Tipos Stuart J. Russell y Peter Norvig diferencian varios tipos de inteligencia artificial: Los sistemas que piensan como humanos: Estos sistemas tratan de emular el pensamiento humano; por ejemplo, las redes neuronales artificiales. La automatización de actividades que vinculamos con procesos de pensamiento humano, actividades como la toma de decisiones, resolución de problemas y aprendizaje. Los sistemas que actúan como humanos: Estos sistemas tratan de actuar como humanos; es decir, imitan el comportamiento humano; por ejemplo, la robótica (el estudio de cómo lograr que los computadores realicen tareas que, por el momento, los humanos hacen mejor). Los sistemas que piensan racionalmente: Es decir, con lógica (idealmente), tratan de imitar el pensamiento racional del ser humano; por ejemplo, los sistemas expertos, (el estudio de los cálculos que hacen posible percibir, razonar y actuar). Los sistemas que actúan racionalmente: Tratan de emular de forma racional el comportamiento humano; por ejemplo, los agentes inteligentes, que están relacionados con conductas inteligentes en artefactos. Inteligencia artificial generativa Artículo principal: Inteligencia artificial generativa La inteligencia artificial generativa es un tipo de sistema de inteligencia artificial capaz de generar texto, imágenes u otros medios en respuesta a comandos de texto conocidos como prompts. Un prompt es la instrucción, pregunta o conjunto de indicaciones para que la inteligencia artificial realice la tarea específica proporcione la respuesta requerida. La calidad del prompt influye directamente en la calidad de la respuesta. Los modelos de IA generativa aprenden los patrones y la estructura de sus datos de entrenamiento de entrada, y luego generan nuevos datos que tienen características similares. Los sistemas de IA generativa notables incluyen ChatGPT (y su variante Microsoft Copilot), un bot conversacional creado por OpenAI usando sus modelos de lenguaje grande fundacionales GPT-3 y GPT-4; Gemini (anteriormente llamado Bard), un bot conversacional creado por Google usando el modelo de lenguaje Gemini; y Claude, un bot conversacional creado por Anthropic usando los modelos del mismo nombre.Otros modelos generativos de IA incluyen sistemas de arte de inteligencia artificial como Stable Diffusion, Midjourney y DALL-E, que permiten crear imágenes. Actualmente la IA generativa puede crear texto, código, imágenes, vídeo, música, voces y efectos de sonido. Inteligencia artificial fuerte Artículo principal: Inteligencia artificial fuerte La Inteligencia artificial fuerte (IGA) es un tipo hipotético de inteligencia artificial que iguala o excede la inteligencia humana promedio. Si se hiciera realidad, una IGA podría aprender a realizar cualquier tarea intelectual que los seres humanos o los animales puedan llevar a cabo. Alternativamente, la IGA se ha definido como un sistema autónomo que supera las capacidades humanas en la mayoría de las tareas económicamente valiosas. Algunos sostienen que podría ser posible en años o décadas; otros, que podría tardar un siglo o más; y una minoría cree que quizá nunca se consiga. Existe un debate sobre la definición exacta de IGA y sobre si los grandes modelos de lenguaje (LLM) modernos, como el GPT-4, son formas tempranas pero incompletas de IGA. Inteligencia artificial explicable Artículo principal: Inteligencia artificial explicable La inteligencia artificial explicable se refiere a métodos y técnicas en la aplicación de tecnología de inteligencia artificial por los que el ser humano es capaz de comprender las decisiones y predicciones realizadas por la inteligencia artificial. Inteligencia artificial amigable Artículo principal: Inteligencia artificial amigable La inteligencia artificial amigable es una IA fuerte e hipotética que puede tener un efecto positivo más que uno negativo sobre la humanidad. Amigable es usado en este contexto como terminología técnica y escoge agentes que son seguros y útiles, no necesariamente aquellos que son «amigables» en el sentido coloquial. El concepto es invocado principalmente en el contexto de discusiones de agentes artificiales de automejora recursiva que rápidamente explota en inteligencia, con el argumento de que esta tecnología hipotética pudiera tener una larga, rápida y difícil tarea de controlar el impacto en la sociedad humana. Inteligencia artificial multimodal Artículo principal: Inteligencia artificial multimodal La inteligencia artificial multimodal es un tipo de inteligencia artificial que puede procesar e integrar datos de diferentes modalidades, como texto, imágenes, audio y video, para obtener una comprensión más completa y contextualizada de una situación. La inteligencia artificial multimodal se inspira en la forma en que los humanos usan varios sentidos para percibir e interactuar con el mundo, y ofrece una forma más natural e intuitiva de comunicarse con la tecnología. Inteligencia artificial cuántica Artículo principal: Inteligencia Artificial Cuántica La inteligencia artificial Cuántica es un campo interdisciplinar que se enfoca en construir algoritmos cuánticos para mejorar las tareas computacionales dentro de la IA, incluyendo subcampos como el aprendizaje automático. Existen evidencias que muestran una posible ventaja cuadrática cuántica en operaciones fundamentales de la IA. Escuelas de pensamiento La IA se divide en dos escuelas de pensamiento: La inteligencia artificial convencional. La inteligencia computacional. Inteligencia artificial convencional Se conoce también como IA simbólica-deductiva. Está basada en el análisis formal y estadístico del comportamiento humano ante diferentes problemas: Razonamiento basado en casos: Ayuda a tomar decisiones mientras se resuelven ciertos problemas concretos y, aparte de que son muy importantes, requieren de un buen funcionamiento. Sistemas expertos: Infieren una solución a través del conocimiento previo del contexto en que se aplica y utiliza ciertas reglas o relaciones. Redes bayesianas: Propone soluciones mediante inferencia probabilística. Inteligencia artificial basada en comportamientos: Esta inteligencia contiene autonomía, es decir, puede autorregularse y controlarse para mejorar. Smart process management: Facilita la toma de decisiones complejas, proponiendo una solución a un determinado problema al igual que lo haría un especialista en dicha actividad. Inteligencia artificial computacional Artículo principal: Inteligencia computacional La inteligencia computacional (también conocida como IA subsimbólica-inductiva) implica desarrollo o aprendizaje interactivo (por ejemplo, modificaciones interactivas de los parámetros en sistemas de conexiones). El aprendizaje se realiza basándose en datos empíricos. La inteligencia computacional tiene una doble finalidad. Por un lado, su objetivo científico es comprender los principios que posibilitan el comportamiento inteligente (ya sea en sistemas naturales o artificiales) y, por otro, su objetivo tecnológico consiste en especificar los métodos para diseñar sistemas inteligentes. Historia Artículo principal: Historia de la inteligencia artificial La expresión «inteligencia artificial» fue acuñada formalmente en 1955 en el documento con la propuesta para la Conferencia de Dartmouth como una manera de separar la disciplina del campo de los autómatas y señalar su objetivo prioritario de la creación de máquinas que realizasen tareas similares a la inteligencia humana. El término además parecía más apropiado para obtener subvenciones que el original de «estudios de autómatas» , como explicó uno de sus autores, John McCarthy, en el debate Lighthill de 1973 Las ideas más básicas se remontan a los antiguos griegos. Aristóteles (384-322 a. C.) fue el primero en describir un conjunto de reglas que describen una parte del funcionamiento de la mente para obtener conclusiones racionales, y Ctesibio de Alejandría (250 a. C.) construyó la primera máquina autocontrolada, un regulador del flujo de agua (racional pero sin razonamiento). En 1315 Ramon Llull en su libro Ars magna tuvo la idea de que el razonamiento podía ser efectuado de manera artificial. En 1840 Ada Lovelace previó la capacidad de las máquinas para ir más allá de los simples cálculos y aportó una primera idea de lo que sería el software. En 1912 Leonardo Torres Quevedo, desarrolló un autómata capaz de jugar al ajedrez (el ajedrecista). En 1936 Alan Turing diseña formalmente una Máquina universal que demuestra la viabilidad de un dispositivo físico para implementar cualquier cómputo formalmente definido. En 1943 Warren McCulloch y Walter Pitts presentaron su modelo de neuronas artificiales, el cual se considera el primer trabajo del campo, aun cuando todavía no existía el término. Los primeros avances importantes comenzaron a principios del año 1950 con el trabajo de Alan Turing, a partir de lo cual la ciencia ha pasado por diversas situaciones. En 1955 Herbert Simon, Allen Newell y Joseph Carl Shaw, desarrollan el primer lenguaje de programación orientado a la resolución de problemas, el IPL-11. Un año más tarde desarrollan el LogicTheorist, el cual era capaz de demostrar teoremas matemáticos. En 1955 fue ideada la expresión «inteligencia artificial» por John McCarthy, Marvin Minsky y Claude Shannon en el documento con la propuesta para la Conferencia de Dartmouth de 1956. El término fue introducido como una manera de separar la disciplina del campo de los autómatas y señalar su objetivo prioritario de la creación de máquinas que realizasen tareas similares a la inteligencia humana. El término además parecía más apropiado para obtener subvenciones que el original de «estudios de autómatas» , como explicó uno de sus autores, John McCarthy, en el debate Lighthill de 1973 En la conferencia se hicieron previsiones triunfalistas a diez años que jamás se cumplieron, lo que provocó el abandono casi total de las investigaciones durante quince años. En 1957 Newell y Simon continúan su trabajo con el desarrollo del General Problem Solver (GPS). GPS era un sistema orientado a la resolución de problemas. En 1958 John McCarthy desarrolla en el Instituto Tecnológico de Massachusetts (MIT) el LISP. Su nombre se deriva de LISt Processor. LISP fue el primer lenguaje para procesamiento simbólico. En 1959 Rosenblatt introduce el «perceptrón». A finales de la década de 1950 y comienzos de la de 1960 Robert K. Lindsay desarrolla «Sad Sam», un programa para la lectura de oraciones en inglés y la inferencia de conclusiones a partir de su interpretación. En 1963 Quillian desarrolla las redes semánticas como modelo de representación del conocimiento. En 1964 Bertrand Raphael construye el sistema SIR (Semantic Information Retrieval) el cual era capaz de inferir conocimiento basado en información que se le suministra. Bobrow desarrolla STUDENT. A mediados de los años 60, aparecen los sistemas expertos, que predicen la probabilidad de una solución bajo un set de condiciones. Por ejemplo, DENDRAL, iniciado en 1965 por Buchanan, Feigenbaum y Lederberg, el primer Sistema Experto, que asistía a químicos en estructuras químicas complejas, MACSYMA, que asistía a ingenieros y científicos en la solución de ecuaciones matemáticas complejas. Posteriormente entre los años 1968-1970 Terry Winograd desarrolló el sistema SHRDLU, que permitía interrogar y dar órdenes a un robot que se movía dentro de un mundo de bloques. En 1966 y 1972 Artificial Intelligence Center desarrollo el Robot Shakey, considerado el primer robot inteligente de la historia. El robot incluía visión artificial, y tenía la capacidad de percibir y razonar sobre su entorno. En 1968 Marvin Minsky publica Semantic Information Processing. En 1968 Seymour Papert, Danny Bobrow y Wally Feurzeig desarrollan el lenguaje de programación LOGO. En 1969 Alan Kay desarrolla el lenguaje Smalltalk en Xerox PARC y se publica en 1980. En 1973 Alain Colmenauer y su equipo de investigación en la Universidad de Aix-Marseille crean PROLOG (del francés PROgrammation en LOGique) un lenguaje de programación ampliamente utilizado en IA. En 1973 Shank y Abelson desarrollan los guiones, o scripts, pilares de muchas técnicas actuales en inteligencia artificial y la informática en general. En 1974 Edward Shortliffe escribe su tesis con MYCIN, uno de los Sistemas Expertos más conocidos, que asistió a médicos en el diagnóstico y tratamiento de infecciones en la sangre. En las décadas de 1970 y 1980, creció el uso de sistemas expertos, como MYCIN: R1/XCON, ABRL, PIP, PUFF, CASNET, INTERNIST/CADUCEUS, etc. Algunos permanecen hasta hoy (Shells) como EMYCIN, EXPERT, OPSS. En 1981 Kazuhiro Fuchi anuncia el proyecto japonés de la quinta generación de computadoras. En 1986 McClelland y Rumelhart publican Parallel Distributed Processing (Redes Neuronales). En 1987 Hitachi desarrollo el Sendai Subway 1000, el primer tren autónomo de la historia. Fue el primer de tren del mundo en utilizar lógica difusa para controlar su velocidad, arranques y paradas. En 1988 se establecen los lenguajes Orientados a Objetos. En 1997 Gari Kaspárov, campeón mundial de ajedrez, pierde ante la computadora autónoma Deep Blue. En 2006 se celebró el aniversario con el Congreso en español 50 años de inteligencia artificial - Campus Multidisciplinar en Percepción e Inteligencia 2006. En 2009 ya había en desarrollo sistemas inteligentes terapéuticos que permiten detectar emociones para poder interactuar con niños autistas. En 2011 IBM desarrolló un superordenador llamado Watson, el cual ganó una ronda de tres juegos seguidos de Jeopardy!, venciendo a sus dos máximos campeones, y ganando un premio de 1 millón de dólares que IBM luego donó a obras de caridad. En 2016, un programa informático ganó cinco a cero al triple campeón de Europa de Go. En 2016, el entonces presidente Obama habla sobre el futuro de la inteligencia artificial y la tecnología. Existen personas que al dialogar sin saberlo con un chatbot no se percatan de hablar con un programa, de modo tal que se cumple la prueba de Turing como cuando se formuló: «Existirá inteligencia artificial cuando no seamos capaces de distinguir entre un ser humano y un programa informático en una conversación a ciegas». En 2017 AlphaGo desarrollado por DeepMind derrota 4-1 en una competencia de Go al campeón mundial Lee Sedol. Este suceso fue muy mediático y marcó un hito en la historia de este juego. A finales de ese mismo año, Stockfish, el motor de ajedrez considerado el mejor del mundo con 3 400 puntos ELO, fue abrumadoramente derrotado por AlphaZero con solo conocer las reglas del juego y tras solo 4 horas de entrenamiento jugando contra sí mismo. Como anécdota, muchos de los investigadores sobre IA sostienen que «la inteligencia es un programa capaz de ser ejecutado independientemente de la máquina que lo ejecute, computador o cerebro». En 2017 un grupo de ingenieros en Google inventan la arquitectura de transformador, un modelo de deep learning que alumbró una nueva generación de modelos grandes de lenguaje, empezando por BERT, y luego el revolucionario GPT de OpenAI. En 2018, se lanza el primer televisor con inteligencia artificial por parte de LG Electronics con una plataforma denominada ThinQ. En 2019, Google presentó su Doodle en que, con ayuda de la inteligencia artificial, hace un homenaje a Johann Sebastian Bach, en el que, añadiendo una simple melodía de dos compases la IA crea el resto. En 2020, la OECD (Organización para la Cooperación y el Desarrollo Económico) publica el documento de trabajo intitulado Hola, mundo: La inteligencia artificial y su uso en el sector público, dirigido a funcionarios de gobierno con el afán de resaltar la importancia de la IA y de sus aplicaciones prácticas en el ámbito gubernamental. Al final del año 2022, se lanzó ChatGPT, una inteligencia artificial generativa capaz de escribir textos y responder preguntas en muchos idiomas. Dado que la calidad de las respuestas recordaba inicialmente al nivel humano, se generó un entusiasmo mundial por la IA y ChatGPT alcanzó más de 100 millones de usuarios dos meses después de su lanzamiento. Más tarde, los expertos notaron que ChatGPT proporciona información errónea en áreas donde no tiene conocimiento («alucinaciones de datos»), la cual a primera vista parece creíble debido a su perfecta redacción. En 2023, las fotos generadas por IA alcanzaron un nivel de realismo que las hacía confundirse con fotos reales. Como resultado, hubo una ola de «fotos» generadas por IA que muchos espectadores creyeron que eran reales. Una imagen generada por Midjourney se destacó, mostrando al papa Francisco con un elegante abrigo blanco de invierno. Tendencias En 2024, se realizaron avances significativos en varias áreas de la IA: Aprendizaje automático y profundo: Se espera que estas técnicas permitan a las máquinas aprender de manera más eficiente y precisa de grandes volúmenes de datos, mejorando capacidades en procesamiento del lenguaje natural, visión por computadora y toma de decisiones automatizada. Procesamiento del Lenguaje Natural (PLN): Los avances en PLN permitirán a las máquinas comprender y responder al lenguaje humano de manera más natural y precisa, abriendo nuevas posibilidades en atención al cliente automatizada y generación de contenido. Analítica predictiva y prescriptiva: Estas técnicas utilizarán datos y modelos matemáticos para prever el futuro y recomendar acciones, permitiendo a las organizaciones anticipar y abordar problemas de manera proactiva. Integración del Internet de las cosas (IoT) y la IA: Permitirá a las máquinas recopilar y analizar datos en tiempo real para tomar decisiones autónomas y mejorar la eficiencia. IA Generativa: Estará más al alcance de las personas sin conocimientos técnicos, permitiendo la creación de chatbots personalizados y otros modelos generativos. Implicaciones sociales, éticas y filosóficas Artículo principal: Ética en la inteligencia artificial Ante la posibilidad de crear máquinas dotadas de inteligencia, se volvió importante preocuparse por la cuestión ética de las máquinas para tratar de garantizar que no se produzca ningún daño a los seres humanos, a otros seres vivos e incluso a las mismas máquinas según algunas corrientes de pensamiento. Es así como surgió un amplio campo de estudios conocido como ética de la inteligencia artificial de relativamente reciente aparición y que generalmente se divide en dos ramas, la roboética, encargada de estudiar las acciones de los seres humanos hacia los robots, y la ética de las máquinas encargada del estudio del comportamiento de los robots para con los seres humanos. El acelerado desarrollo tecnológico y científico de la inteligencia artificial que se ha producido en el siglo XXI supone también un importante impacto en otros campos. En la economía mundial durante la segunda revolución industrial se vivió un fenómeno conocido como desempleo tecnológico, que se refiere a cuando la automatización industrial de los procesos de producción a gran escala reemplaza la mano de obra humana. Con la inteligencia artificial podría darse un fenómeno parecido, especialmente en los procesos en los que interviene la inteligencia humana, tal como se ilustraba en el cuento ¡Cómo se divertían! de Isaac Asimov, en el que su autor vislumbra algunos de los efectos que tendría la interacción de máquinas inteligentes especializadas en pedagogía infantil, en lugar de profesores humanos, con los niños en etapa escolar. Este mismo escritor diseñó lo que hoy se conocen como las tres leyes de la robótica, aparecidas por primera vez en el relato Círculo vicioso (Runaround) de 1942, donde establecía lo siguiente: Primera Ley Un robot no hará daño a un ser humano ni, permitirá que un ser humano sufra daño. Segunda Ley Un robot debe cumplir las órdenes dadas por los seres humanos, a excepción de aquellas que entren en conflicto con la primera ley. Tercera Ley Un robot debe proteger su propia existencia en la medida en que esta protección no entre en conflicto con la primera o con la segunda ley. Otras obras de ciencia ficción más recientes también exploran algunas cuestiones éticas y filosóficas con respecto a la Inteligencia artificial fuerte, como las películas Yo, robot o A.I. Inteligencia Artificial, en los que se tratan temas tales como la autoconsciencia o el origen de una conciencia emergente de los robots inteligentes o sistemas computacionales, o si éstos podrían considerarse sujetos de derecho debido a sus características casi humanas relacionadas con la sintiencia, como el poder ser capaces de sentir dolor y emociones o hasta qué punto obedecerían al objetivo de su programación, y en caso de no ser así, si podrían ejercer libre albedrío. Esto último es el tema central de la famosa saga de Terminator, en la que las máquinas superan a la humanidad y deciden aniquilarla, historia que, según varios especialistas, podría no limitarse a la ciencia ficción y ser una posibilidad real en una sociedad posthumana que dependiese de la tecnología y las máquinas completamente. Regulación Artículo principal: Regulación de la inteligencia artificial Cronología de estrategias, planes de acción y documentos de políticas que definen enfoques nacionales, regionales e internacionales para la IA. El Derecho desempeña un papel fundamental en el uso y desarrollo de la IA. Las leyes establecen reglas y normas de comportamiento para asegurar el bienestar social y proteger los derechos individuales, y pueden ayudarnos a obtener los beneficios de esta tecnología mientras minimizamos sus riesgos, que son significativos. De momento no hay normas jurídicas que regulen directamente a la IA. Pero con fecha 21 de abril de 2021, la Comisión Europea ha presentado una propuesta de Reglamento europeo para la regulación armonizada de la inteligencia artificial (IA) en la UE. Su título exacto es Propuesta de Reglamento del Parlamento Europeo y del Consejo por el que se establecen normas armonizadas en materia de inteligencia artificial –Ley de Inteligencia Artificial– y se modifican otros actos legislativos de la Unión. En marzo de 2023, cientos de empresarios como Elon Musk, Steve Wozniak (cofundador de Apple) o los presidentes de numerosas compañías tecnológicas; intelectuales como Yuval Noah Harari y cientos de académicos e investigadores especializados en inteligencia artificial firmaron una carta abierta avisando del peligro de la falta de regulación de la IA, poniendo el foco sobre OpenAI, la empresa que ha desarrollado ChatGPT. Pidieron una pausa de al menos 6 meses para sus experimentos más potentes, hasta que el mundo logre un consenso internacional para que estos sistemas «sean más precisos, seguros, interpretables, transparentes, robustos, neutrales, confiables y leales». Dos meses más tarde, en mayo, 350 ejecutivos de las principales empresas desarrolladoras de IA, académicos e investigadores expertos firmaron un nuevo manifiesto alertando de que la IA avanzada sin regular representa un peligro de extinción para la humanidad: «Mitigar el riesgo de extinción de la IA debería ser una prioridad mundial junto a otros riesgos a escala social como las pandemias y la guerra nuclear» Entre los impulsores de esta petición está toda la plana mayor de OpenAI, el jefe de Tecnología de Microsoft, el líder de Google DeepMind con 38 ejecutivos, investigadores o profesores de universidad relacionados con la empresa, y representantes de desarrolladoras más pequeñas como Anthropic, Stability AI o Inflection AI. Algunos autores han comenzado a explorar la posibilidad de reconocer a las inteligencias artificiales avanzadas no sólo como herramientas, sino como potenciales sujetos jurídicos limitados, capaces de adquirir derechos y asumir cargas fiscales en relación con los bienes digitales o intangibles que producen. Esta propuesta encuentra inspiración en figuras ya reconocidas por el derecho, como las personas jurídicas ficticias, que sin ser humanas, han sido dotadas de capacidad para actuar en el tráfico jurídico. En este contexto, se ha planteado que ciertos autómatas complejos podrían constituir patrimonios propios, asumir responsabilidades civiles por los efectos de sus decisiones autónomas, e incluso tributar, como una forma de responder por su impacto económico y redistribuir los beneficios que generan en sociedades cada vez más digitalizadas. Esta perspectiva no implica una equiparación entre seres humanos e inteligencias artificiales, sino una arquitectura jurídica funcional que permitiría canalizar las consecuencias derivadas de la producción autónoma de bienes y servicios por parte de sistemas de IA. Se propone, por ejemplo, que los ingresos generados por bots creativos o vehículos autónomos puedan ser destinados a fondos de garantía o tributación, aliviando así presiones sobre las economías humanas y evitando zonas grises en la responsabilidad jurídica. Esta visión contribuye al debate sobre la singularidad tecnológica, proponiendo respuestas normativas desde el derecho privado y fiscal. Objetivos Razonamiento y resolución de problemas Una imagen de IA generada por Dall-e tras escribir el texto: «Un edificio arquitectónico moderno con grandes ventanales de vidrio, situado en un acantilado con vista a un océano sereno al atardecer». Los primeros investigadores desarrollaron algoritmos que imitaban el razonamiento paso a paso que los humanos usan cuando resuelven acertijos o hacen deducciones lógicas. A finales de la década de 1981-1990, la investigación de la inteligencia artificial había desarrollado métodos para tratar con información incierta o incompleta, empleando conceptos de probabilidad y economía. Estos algoritmos demostraron ser insuficientes para resolver grandes problemas de razonamiento porque experimentaron una «explosión combinatoria»: se volvieron exponencialmente más lentos a medida que los problemas crecían. De esta manera, se concluyó que los seres humanos rara vez usan la deducción paso a paso que la investigación temprana de la inteligencia artificial seguía; en cambio, resuelven la mayoría de sus problemas utilizando juicios rápidos e intuitivos. Representación abstracta de una canción generada por inteligencia artificial. Representación del conocimiento Artículo principal: Representación del conocimiento La representación del conocimiento y la ingeniería del conocimiento son fundamentales para la investigación clásica de la inteligencia artificial. Algunos «sistemas expertos» intentan recopilar el conocimiento que poseen los expertos en algún ámbito concreto. Además, otros proyectos tratan de reunir el «conocimiento de sentido común» conocido por una persona promedio en una base de datos que contiene un amplio conocimiento sobre el mundo. Entre los temas que contendría una base de conocimiento de sentido común están: objetos, propiedades, categorías y relaciones entre objetos, situaciones, eventos, estados y tiempo causas y efectos; y el conocimiento sobre el conocimiento (lo que sabemos sobre lo que saben otras personas) entre otros. Planificación Otro objetivo de la inteligencia artificial consiste en poder establecer metas y finalmente alcanzarlas. Para ello necesitan una forma de visualizar el futuro, una representación del estado del mundo y poder hacer predicciones sobre cómo sus acciones lo cambiarán, con tal de poder tomar decisiones que maximicen la utilidad (o el «valor») de las opciones disponibles. En los problemas clásicos de planificación, el agente puede asumir que es el único sistema que actúa en el mundo, lo que le permite estar seguro de las consecuencias de sus acciones. Sin embargo, si el agente no es el único actor, entonces se requiere que este pueda razonar bajo incertidumbre. Esto requiere un agente que no solo pueda evaluar su entorno y hacer predicciones, sino también evaluar sus predicciones y adaptarse en función de su evaluación. La planificación de múltiples agentes utiliza la cooperación y la competencia de muchos sistemas para lograr un objetivo determinado. El comportamiento emergente como este es utilizado por algoritmos evolutivos e inteligencia de enjambre. Aprendizaje El aprendizaje automático es un concepto fundamental de la investigación de la inteligencia artificial desde el inicio de los estudios de este campo; consiste en la investigación de algoritmos informáticos que mejoran automáticamente a través de la experiencia. El aprendizaje no supervisado es la capacidad de encontrar patrones en un flujo de entrada, sin que sea necesario que un humano etiquete las entradas primero. El aprendizaje supervisado incluye clasificación y regresión numérica, lo que requiere que un humano etiquete primero los datos de entrada. La clasificación se usa para determinar a qué categoría pertenece algo y ocurre después de que un programa observe varios ejemplos de entradas de varias categorías. La regresión es el intento de producir una función que describa la relación entre entradas y salidas y predice cómo deben cambiar las salidas a medida que cambian las entradas. Tanto los clasificadores como los aprendices de regresión intentan aprender una función desconocida; por ejemplo, un clasificador de spam puede verse como el aprendizaje de una función que asigna el texto de un correo electrónico a una de dos categorías, «spam» o «no spam». La teoría del aprendizaje computacional puede evaluar a los estudiantes por complejidad computacional, complejidad de la muestra (cuántos datos se requieren) o por otras nociones de optimización. El mundo está en constante evolución, y herramientas como ChatGPT están en el centro de esta transformación. Mientras que muchas personas ven a ChatGPT como una oportunidad para mejorar la experiencia de sus negocios o personales, hay quienes se muestran escépticos sobre su implementación. Procesamiento de lenguajes naturales Artículo principal: Procesamiento de lenguajes naturales El procesamiento del lenguaje natural permite a las máquinas leer y comprender el lenguaje humano. Un sistema de procesamiento de lenguaje natural suficientemente eficaz permitiría interfaces de usuario de lenguaje natural y la adquisición de conocimiento directamente de fuentes escritas por humanos, como los textos de noticias. Algunas aplicaciones sencillas del procesamiento del lenguaje natural incluyen la recuperación de información, la minería de textos, la respuesta a preguntas y la traducción automática. Muchos enfoques utilizan las frecuencias de palabras para construir representaciones sintácticas de texto. Las estrategias de búsqueda de «detección de palabras clave» son populares y escalables, pero poco óptimas; una consulta de búsqueda para «perro» solo puede coincidir con documentos que contengan la palabra literal «perro» y perder un documento con el vocablo «caniche». Los enfoques estadísticos de procesamiento de lenguaje pueden combinar todas estas estrategias, así como otras, y a menudo logran una precisión aceptable a nivel de página o párrafo. Más allá del procesamiento de la semántica, el objetivo final de este es incorporar una comprensión completa del razonamiento de sentido común. En 2019, las arquitecturas de aprendizaje profundo basadas en transformadores podían generar texto coherente. Percepción La detección de características (en la imagen se observa la detección de bordes) ayuda a la inteligencia artificial a componer estructuras abstractas informativas a partir de datos sin procesar. La percepción de la máquina es la capacidad de utilizar la entrada de sensores (como cámaras de espectro visible o infrarrojo, micrófonos, señales inalámbricas y lidar, sonar, radar y sensores táctiles) para entender aspectos del mundo. Las aplicaciones incluyen reconocimiento de voz, reconocimiento facial y reconocimiento de objetos. La visión artificial es la capacidad de analizar la información visual, que suele ser ambigua; un peatón gigante de cincuenta metros de altura muy lejos puede producir los mismos píxeles que un peatón de tamaño normal cercano, lo que requiere que la inteligencia artificial juzgue la probabilidad relativa y la razonabilidad de las diferentes interpretaciones, por ejemplo, utilizando su «modelo de objeto» para evaluar que los peatones de cincuenta metros no existen. Importancia de la inteligencia artificial La gran importancia de la IA radica en el hecho de que tiene una amplia gama de aplicaciones, desde la automatización de tareas tediosas hasta la creación de sistemas avanzados de asistencia médica y diagnóstico de enfermedades, la detección de fraudes y la optimización de procesos empresariales. En muchos casos, la IA puede hacer cosas que los humanos no pueden hacer, como el procesamiento de datos en grandes cantidades y la localización de patrones e interrelaciones entre estos que serían difíciles o imposibles de detectar de otra manera. Esta herramienta ayuda a automatizar el aprendizaje y descubrimiento repetitivo a través de datos, realiza tareas computarizadas frecuentes de manera confiable, sin embargo, necesita intervención humana para la configuración del sistema. Analiza datos más profundos y agrega inteligencia ya que no se puede vender como una aplicación individual, por lo que es un valor agregado a los productos. Tiene una gran precisión a través de redes neuronales profundas; por ejemplo, en medicina se puede utilizar la IA para detectar cáncer con MRIs (imágenes ppr resonancia magnética). Se adapta a través de algoritmos de aprendizaje progresivo, encuentra estructura y regularidades en los datos de modo que el algoritmo se convierte en un clasificador o predictor. Y, por último, la inteligencia artificial, saca el mayor provecho de datos. Además, una de las principales razones por las que la IA es importante es porque puede automatizar tareas repetitivas y monótonas, liberando tiempo y recursos para que las personas se centren en tareas más creativas y valiosas. Por ejemplo, la IA puede ayudar a las empresas a automatizar tareas de back office, como la contabilidad y el procesamiento de facturas, lo que puede reducir los costos y mejorar la eficiencia. De manera similar, la IA puede ayudar a los trabajadores a realizar tareas más complejas y creativas, como el diseño y la planificación estratégica. Otra razón por la que la IA es importante es porque puede ayudar a las empresas a tomar decisiones informadas y precisas. Así mismo, la IA puede procesar grandes cantidades de datos y proporcionar información valiosa para la toma de decisiones empresariales, lo que puede ayudar a las empresas a identificar oportunidades comerciales, predecir tendencias de mercado y mejorar la eficiencia del mercado financiero. Además, la IA puede ayudar a los trabajadores a tomar decisiones informadas en tiempo real, como en el caso de la atención médica, donde la IA puede ayudar a los médicos a identificar enfermedades y personalizar el tratamiento. La IA también es importante en el campo de la ciberseguridad. La IA puede ayudar a detectar y prevenir amenazas, desde ciberataques hasta la detección de comportamientos sospechosos. La IA puede analizar grandes cantidades de datos en tiempo real y detectar patrones y anomalías que podrían indicar una amenaza de seguridad. Además, la IA puede aprender de los patrones de comportamiento y mejorar su capacidad para detectar amenazas en el futuro. En el campo de la seguridad cibernética, la IA puede ayudar a proteger los sistemas y las redes de los ataques de virus informáticos y la infiltración de malware. Otra área donde la IA es importante es en el descubrimiento de conocimientos. La IA puede descubrir patrones y relaciones en los datos que los humanos no podrían detectar, lo que puede llevar a nuevas ideas y avances en diversos campos. Por ejemplo, la IA puede ayudar a los investigadores a identificar nuevos tratamientos para enfermedades, o ayudar a los científicos a analizar datos de sensores y satélites para entender mejor el calentamiento global. Ventajas de la inteligencia Artificial La inteligencia artificial (IA) ofrece múltiples ventajas a la sociedad, muchas de las cuales ya comienzan a hacerse evidentes. Una de las principales áreas beneficiadas es la educación, ya que permite que las clases sean más dinámicas y comprensibles, facilitando el aprendizaje y promoviendo un pensamiento más autónomo en los estudiantes. Desventajas de la inteligencia artificial A pesar de sus beneficios, la IA también presenta algunas desventajas. Una de las principales preocupaciones es la pérdida de empleos, ya que algunas profesiones se han visto afectadas, como los diseñadores gráficos, analistas financieros y matemáticos, cuyos trabajos pueden ser reemplazados por sistemas automatizados. Controversias Sophia, un robot humanoide controlado por IA. Sophia En marzo de 2016, se hizo popular el comentario que la robot humanoide llamada Sophia de la empresa Hanson Robotics hizo durante su presentación cuando su creador, David Hanson, le preguntara si estaba dispuesta a destruir a la humanidad, a lo que la robot contestó: «Está bien, voy a destruir a la humanidad». Posteriormente, Sophía se ganó el reconocimiento y la atención mediática mundial debido a sus conductas casi humanas, siendo entrevistada en muchas ocasiones por distintos medios y sosteniendo conversaciones con personalidades famosas y reconocidas. En 2017, Sophia obtuvo la ciudadanía saudí, convirtiéndose así en la primera robot en ser reconocida como ciudadana por un país, lo cual levantó la controversia sobre si se les debería otorgar los mismos derechos y obligaciones a los robots como si se trataran de sujetos de derecho. Alice y Bob A finales de julio de 2017, varios medios internacionales dieron a conocer que el laboratorio de investigación de inteligencia artificial del Instituto Tecnológico de Georgia, en conjunto con el Grupo de Investigación de inteligencia artificial (FAIR) de Facebook, ahora Meta, tuvieron que apagar dos inteligencias artificiales de tipo chatbot denominadas Bob y Alice, ya que habían desarrollado un lenguaje propio más eficiente que el inglés, idioma en el que habían sido entrenados para aprender a negociar, desarrollando finalmente un tipo de comunicación incomprensible que se alejaba de las reglas gramaticales del lenguaje natural y que favorecía el uso de abreviaturas. El lenguaje creado por estas IA mostraba características de un inglés corrupto y patrones repetitivos, en especial de pronombres y determinantes. Este inesperado suceso fue visto con pánico en los medios de comunicación, ya que se aseguraba que los chatbots supuestamente habían salido del control humano y habían desarrollado la capacidad de comunicarse entre sí. Sin embargo, posteriormente esto también fue desmentido, pues se argumentó que en realidad Facebook no apagó las inteligencias artificiales, sino que simplemente las puso en pausa y cambió los parámetros de los chatbots, desechando el experimento al final por no tener ningún interés práctico o útil dentro de la investigación sobre IA. Ameca A principios del 2022, en la Feria de Electrónica de Consumo (CES) que tomó lugar en Las Vegas, el robot desarrollado por Engineered Arts nombrado Ameca causó duda y miedo a los espectadores durante su exposición principalmente por la semejanza de su rostro a uno de un ser humano, la compañía expresó que el desarrollo de este robot humanoide aún se encontraba en proceso y hasta septiembre del mismo año el robot aún no era capaz de caminar ni tener interacción alguna con las personas. Por otro lado, en septiembre de 2023 la compañía volvió a exponer a Ameca al público mostrando al robot en videos en donde se le puede ver frente a un espejo haciendo 25 expresiones humanas, así como dibujando un gato al ya contar con brazos y piernas que le otorgaron movilidad y, de igual manera, empleando ironía en conversaciones con personas e incluso declarando que realizó una broma al ser cuestionada sobre su capacidad de soñar como un humano siendo un robot al decir «soñé con dinosaurios luchando una guerra contra alienígenas en Marte» esto lo desmintió momentos después explicando cómo es que la IA implementada en su sistema le permitía crear escenarios sobre hechos de la humanidad e iba aprendiendo sobre ellos mientras se encontraba apagada; estos hechos impactaron a la sociedad sobre la semejanza que este robot humanoide estaba teniendo con el ser humano y sobre el avance tecnológico que está permitiendo que este robot esté cada vez más cercano a vivir entre las personas como un miembro más de la comunidad. Falsos desnudos La utilización de aplicaciones gratuitas de IA para transformar fotografías de personas en falsos desnudos está generando problemas que afectan a menores. El caso saltó a los medios de comunicación en septiembre de 2023 cuando en Almendralejo (Badajoz, España) aparecieron varias fotografías de niñas y jóvenes (entre 11 y 17 años) que habían sido modificadas mediante inteligencia artificial para aparecer desnudas. Las imágenes fueron obtenidas de los perfiles de Instagram y de la aplicación WhatsApp de al menos 20 niñas de la localidad. Las fotografías de niñas desnudas habían circulado después mediante Whatsapp y a partir de ellas se había creado un vídeo que también había circulado entre menores. Los autores de dicha transformación también eran menores y compañeros de colegio o instituto. La Agencia Española de Protección de Datos abrió una investigación y se comunicó con el Ayuntamiento de Almendralejo y con la Junta de Extremadura informándoles de que se podía solicitar la retirada de cualquier imagen circulando en internet en el canal prioritario de la agencia. Críticas La «revolución digital» y, más concretamente, el desarrollo de la inteligencia artificial, está suscitando temores y preguntas, incluso en el ámbito de personalidades relevantes en estas cuestiones. En esta imagen, se observa a Bill Gates, exdirector general de Microsoft; el citado y Elon Musk (director general de Tesla) opinan que se debe ser «muy cauteloso con la inteligencia artificial»; si tuviéramos que «apostar por lo que constituye nuestra mayor amenaza a la existencia», serían precisamente ciertas aplicaciones sofisticadas del citado asunto, que podrían llegar a tener derivaciones por completo impensadas. Uno de los mayores críticos de la denominación de estos procesos informáticos con el término de inteligencia artificial es Jaron Lanier. Para ello, objeta la idea de que esta sea realmente inteligente y de que podríamos estar en competencia con un ente artificial. «Esta idea de superar la capacidad humana es ridícula porque está hecha de habilidades humanas». Las principales críticas a la inteligencia artificial tienen que ver con su capacidad de imitar por completo a un ser humano. Sin embargo, hay expertosen el tema que indican que ningún humano individual tiene capacidad para resolver todo tipo de problemas, y autores como Howard Gardner han teorizado sobre la solución. En los humanos, la capacidad de resolver problemas tiene dos aspectos: los aspectos innatos y los aspectos aprendidos. Los aspectos innatos permiten, por ejemplo, almacenar y recuperar información en la memoria, mientras que en los aspectos aprendidos reside el saber resolver un problema matemático mediante el algoritmo adecuado. Del mismo modo que un humano debe disponer de herramientas que le permitan solucionar ciertos problemas, los sistemas artificiales deben ser programados para que puedan llegar a resolverlos. Muchas personas consideran que la prueba de Turing ha sido superada, citando conversaciones en que al dialogar con un programa de inteligencia artificial para chat no saben que hablan con un programa. Sin embargo, esta situación no es equivalente a una prueba de Turing, que requiere que el participante se encuentre sobre aviso de la posibilidad de hablar con una máquina. Otros experimentos mentales como la habitación china, de John Searle, han mostrado cómo una máquina podría simular pensamiento sin realmente poseerlo, pasando la prueba de Turing sin siquiera entender lo que hace, tan solo reaccionando de una forma concreta a determinados estímulos (en el sentido más amplio de la palabra). Esto demostraría que la máquina en realidad no está pensando, ya que actuar de acuerdo con un programa preestablecido sería suficiente. Si para Turing el hecho de engañar a un ser humano que intenta evitar que le engañen es muestra de una mente inteligente, Searle considera posible lograr dicho efecto mediante reglas definidas a priori. Uno de los mayores problemas en sistemas de inteligencia artificial es la comunicación con el usuario. Este obstáculo es debido a la ambigüedad del lenguaje, y se remonta a los inicios de los primeros sistemas operativos informáticos. La capacidad de los humanos para comunicarse entre sí implica el conocimiento del lenguaje que utiliza el interlocutor. Para que un humano pueda comunicarse con un sistema inteligente hay dos opciones: o bien que el humano aprenda el lenguaje del sistema como si aprendiese a hablar cualquier otro idioma distinto al nativo, o bien que el sistema tenga la capacidad de interpretar el mensaje del usuario en la lengua que el usuario utiliza. También puede haber desperfectos en las instalaciones de los mismos. Un humano, durante toda su vida, aprende el vocabulario de su lengua nativa o materna, siendo capaz de interpretar los mensajes (a pesar de la polisemia de las palabras) y utilizando el contexto para resolver ambigüedades. Sin embargo, debe conocer los distintos significados para poder interpretar, y es por esto que lenguajes especializados y técnicos son conocidos solamente por expertos en las respectivas disciplinas. Un sistema de inteligencia artificial se enfrenta con el mismo problema, la polisemia del lenguaje humano, su sintaxis poco estructurada y los dialectos entre grupos. Los desarrollos en inteligencia artificial son mayores en los campos disciplinares en los que existe mayor consenso entre especialistas. Un sistema experto es más probable que sea programado en física o en medicina que en sociología o en psicología. Esto se debe al problema del consenso entre especialistas en la definición de los conceptos involucrados y en los procedimientos y técnicas a utilizar. Por ejemplo, en física hay acuerdo sobre el concepto de velocidad y cómo calcularla. Sin embargo, en psicología se discuten los conceptos, la etiología, la psicopatología, y cómo proceder ante cierto diagnóstico. Esto dificulta la creación de sistemas inteligentes porque siempre habrá desacuerdo sobre la forma en que debería actuar el sistema para diferentes situaciones. A pesar de esto, hay grandes avances en el diseño de sistemas expertos para el diagnóstico y toma de decisiones en el ámbito médico y psiquiátrico (Adaraga Morales, Zaccagnini Sancho, 1994). Al desarrollar un robot con inteligencia artificial se debe tener cuidado con la autonomía, hay que tener en cuenta el no vincular el hecho de que el robot tenga interacciones con seres humanos a su grado de autonomía. Si la relación de los humanos con el robot es de tipo maestro esclavo, y el papel de los humanos es dar órdenes y el del robot obedecerlas, entonces sí cabe hablar de una limitación de la autonomía del robot. Pero si la interacción de los humanos con el robot es de igual a igual, entonces su presencia no tiene por qué estar asociada a restricciones para que el robot pueda tomar sus propias decisiones. Con el desarrollo de la tecnología de inteligencia artificial, muchas compañías de software como el aprendizaje profundo y el procesamiento del lenguaje natural han comenzado a producirse y la cantidad de películas sobre inteligencia artificial ha aumentado. Stephen Hawking advirtió sobre los peligros de la inteligencia artificial y lo consideró una amenaza para la supervivencia de la humanidad. Problemas de privacidad y derechos de autor Los algoritmos de aprendizaje automático requieren grandes cantidades de datos. Las técnicas utilizadas para adquirir estos datos generan preocupaciones sobre temas de privacidad y vigilancia. Las empresas tecnológicas recopilan un gran número de datos de sus usuarios, incluida la actividad en internet, los datos de geolocalización, video y audio. Por ejemplo, para construir algoritmos de reconocimiento de voz, Amazon, entre otros, ha grabado millones de conversaciones privadas y han permitido que trabajadores temporales las escuchen para transcribirlas algunas de ellas. Las opiniones sobre esta vigilancia generalizada van desde aquellos que la ven como un mal necesario hasta aquellos para quienes no es ética y constituye una violación del derecho a la intimidad. Los desarrolladores de IA argumentan que esta es la única forma de ofrecer aplicaciones valiosas y han desarrollado varias técnicas que intentan preservar la privacidad mientras se obtienen los datos, como la agregación de datos, la desidentificación y la privacidad diferencial. Desde 2016, algunos expertos en privacidad, como Cynthia Dwork, comenzaron a ver la privacidad desde la perspectiva de la equidad: Brian Christian escribió que los expertos han cambiado «de la pregunta de “qué saben” a la pregunta de “qué están haciendo con ello”». La IA generativa a menudo se entrena con obras protegidas por derechos de autor no autorizadas, incluidos dominios como imágenes o código informático; la salida se utiliza luego bajo una justificación de uso justo. Los expertos no están de acuerdo sobre la validez de esta justificación durante un proceso legal, ya que podría depender del propósito y el carácter del uso de la obra protegida por derechos de autor y del efecto sobre el mercado potencial de la obra protegida.En 2023, escritores como John Grisham y Jonathan Franzen demandaron a las empresas de IA por usar sus obras para entrenar IA generativa. En 2024, 200 artistas escribieron una carta abierta que solicitaba «parar el asalto a la creatividad humana». Normativa para su uso en el entorno educativo La normativa tiene como objetivo regular y reglamentar el uso de la IA en el entorno educativo, específicamente en el aula. La IA ha experimentado un rápido desarrollo y se ha convertido en una herramienta potencialmente beneficiosa para mejorar la enseñanza y el aprendizaje. No obstante, su implementación plantea desafíos éticos, de privacidad y equidad que deben ser abordados de manera efectiva. Esta normativa se establece en respuesta a la necesidad de garantizar que la IA se utilice de manera ética, responsable y equitativa en el ámbito educativo. Los objetivos de esta normativa son: Promover el uso de la IA como una herramienta complementaria en el proceso de enseñanza-aprendizaje. Garantizar la protección de datos y la privacidad de los estudiantes. Fomentar la equidad y la inclusión en el acceso y el uso de la IA. Establecer principios éticos que rijan el uso de la IA en el aula. Definir responsabilidades y procedimientos claros para el uso de la IA. Esta normativa se aplica a todas las instituciones educativas y docentes que utilizan la IA en el aula, así como a los proveedores de tecnología educativa que ofrecen soluciones basadas en IA. Organizaciones como UNESCO Ethics AI (2020), UNESCO Education & AI (2021), Beijin Consensus, OCDE (2021), Comisión Europea (2019), European Parliament Report AI Education (2021), UNICEF (2021) y Foro Económico Mundial (2019) han mostrado preocupación por implementar lineamientos sobre la ética y la IA en el entorno educativo. En 2024, la Universidad Nacional de Costa rica elaboró y publicó la Declaración de Heredia: principios sobre el uso de inteligencia artificial en la edición científica, referida a la publicación científica, en la que propone una serie de consideraciones para el uso responsable de la inteligencia artificial (IA), se alienta a transparentar el uso de la IA para un ejercicio claro, trazable y reproducible del conocimiento y se llama la atención sobre los retos que supone la incorporación de la IA a la edición científica en cuanto a la diversidad de opciones, el evitar la propagación de sesgos y desinformación, y el respeto a la propiedad intelectual. El uso de la IA en el entorno educativo debe regirse por los siguientes principios éticos y valores: Transparencia: Las decisiones tomadas por algoritmos de IA deben ser comprensibles y explicables. Equidad: La IA no debe discriminar a ningún estudiante ni grupo de estudiantes. Privacidad: Los datos de los estudiantes deben ser protegidos y utilizados de manera responsable. Responsabilidad: Los docentes y las instituciones son responsables de las decisiones tomadas con la ayuda de la IA. Honestidad: El contenido creado por los estudiantes debe ser original sin caer en el plagio. Mejora del aprendizaje: La IA debe utilizarse para mejorar la calidad de la educación y el aprendizaje. Capacitación: Los docentes deben recibir formación sobre el uso de la IA y su aplicación en el aula. Evaluación: Las soluciones de IA deben ser evaluadas en términos de su eficacia y su impacto en el aprendizaje. Protección de datos: Los datos de los estudiantes deben ser protegidos de acuerdo con las leyes de privacidad aplicables. Supervisión: Se debe establecer un proceso de supervisión para garantizar que la IA se utilice de manera ética y responsable. Riesgos de las IA en el entorno educativo Así como tiene muchos beneficios también nos encontramos con diferentes riesgos a los que la educación está expuesta con su uso. Sesgos y discriminación: Al solo recoger información de las bases de datos y textos que procesa de Internet corre el riesgo de aprender cualquier sesgo cognitivo que se encuentre en dicha información. La no privacidad de los datos: El riesgo de un ciberataque se incrementa cuando no hay protocolos de seguridad adecuados en el manejo de la IA. Dependencia: Los estudiantes corren el riesgo de volverse dependientes de la tecnología y no se fomenta la creatividad ni el pensamiento propio. Confiabilidad: La IA puede generar respuestas coherentes pero inexactas además muchas IA no brindan fuentes de información. Falta de habilidades orales y escritas. Desinterés por la investigación por cuenta propia. Dependencia por parte del docente: Los docentes pueden generar dependencia a estas herramientas al momento de dar retroalimentación a las asignaciones además del riesgo de usar la información de las IA para su material didáctico sin antes consultar las fuentes. Los estudiantes no tendrían a cabo un aprendizaje natural este sería más artificial, generando en ellos que a largo plazo establezcan una falta posible falta de conocimiento. Los docentes perderían la costumbre de realizar presentaciones más llamativas, interactivas volviéndolas estas así más monótonas y perdiendo su propia autovía de aquella. Consideración de Diversidad e Inclusión Se debe prestar especial atención a la diversidad de estudiantes y garantizar que la IA sea accesible y beneficiosa para todos, independientemente de su origen étnico, género, discapacidad u orientación sexual. Las soluciones de IA deben ser diseñadas teniendo en cuenta la accesibilidad y la inclusión. Esta normativa se basa en investigaciones académicas, recomendaciones de organizaciones educativas y en las mejores prácticas establecidas en el uso de la IA en la educación. Se alienta a las instituciones a mantenerse al día con la literatura científica y las directrices relevantes. Aunque la IA puede ser una herramienta poderosa en el aula, no debe reemplazar la creatividad, la originalidad y el juicio humano en el proceso educativo. La IA debe ser utilizada de manera complementaria para enriquecer la experiencia educativa. Esta normativa se presenta como un marco general que deberá ser adaptado y ampliado por las instituciones educativas de acuerdo a sus necesidades y contextos específicos. Debe ser comunicada de manera efectiva a todos los involucrados en el proceso educativo y revisada periódicamente para asegurar su vigencia. Esta normativa tiene como objetivo garantizar que la IA sea utilizada de manera ética y responsable en el aula, promoviendo el beneficio de los estudiantes y el avance de la educación. Su cumplimiento es esencial para lograr una implementación exitosa de la IA en el entorno educativo. Aprendizaje automatizado y aprendizaje profundo Artículos principales: Aprendizaje automático y Aprendizaje profundo. En cuanto a la naturaleza del aprendizaje, la IA puede subdividirse en dos campos conceptualmente distintos: El aprendizaje automático, que se enfoca en desarrollar algoritmos de regresión, árboles de decisión y modelos que puedan aprender de datos existentes y realizar predicciones o decisiones basadas en esos datos. En el aprendizaje automático, se utilizan técnicas de estadística matemática para encontrar patrones y relaciones en los datos y, a partir de ellos, desarrollar modelos que puedan hacer predicciones sobre nuevos datos. El aprendizaje profundo, que se centra en la creación de redes neuronales artificiales capaces de aprender y realizar tareas de manera similar a como lo hacen los seres humanos. En el aprendizaje profundo, se utilizan capas de neuronas artificiales para procesar los datos de entrada y aprender a través de un proceso iterativo de ajuste de los pesos de las conexiones entre neuronas. Este tipo de aprendizaje es capaz de procesar y analizar grandes cantidades de datos de manera más eficiente y precisa que el primero, especialmente cuando se trata de datos no estructurados, como imágenes, texto y audio. Además, tiene la capacidad de identificar patrones y características más complejas en los datos, lo que puede llevar a mejores resultados en aplicaciones como el reconocimiento de voz, la visión por computadora y el procesamiento del lenguaje natural. Propiedad intelectual de la inteligencia artificial Imagen de una ciudad futurista generada por la IA Midjourney. La composición está en el dominio público al no ser de un autor humano. Al hablar acerca de la propiedad intelectual atribuida a creaciones de la inteligencia artificial, se forma un debate fuerte alrededor de si una máquina puede tener derechos de autor. Según la Organización Mundial de la Propiedad Intelectual (OMPI), cualquier creación de la mente puede ser parte de la propiedad intelectual, pero no especifica si la mente debe ser humana o puede ser una máquina, dejando la creatividad artificial en la incertidumbre. Alrededor del mundo han comenzado a surgir distintas legislaciones con el fin de manejar la inteligencia artificial, tanto su uso como creación. Los legisladores y miembros del gobierno han comenzado a pensar acerca de esta tecnología, enfatizando el riesgo y los desafíos complejos de esta. Observando el trabajo creado por una máquina, las leyes cuestionan la posibilidad de otorgarle propiedad intelectual a una máquina, abriendo una discusión respecto a la legislación relacionada con IA. El 5 de febrero de 2020, la Oficina del Derecho de Autor de los Estados Unidos y la OMPI asistieron a un simposio donde observaron de manera profunda cómo la comunidad creativa utiliza la inteligencia artificial (IA) para crear trabajo original. Se discutieron las relaciones entre la inteligencia artificial y el derecho de autor, qué nivel de involucramiento es suficiente para que el trabajo resultante sea válido para protección de derechos de autor; los desafíos y consideraciones de usar inputs con derechos de autor para entrenar una máquina; y el futuro de la inteligencia artificial y sus políticas de derecho de autor. El director general de la OMPI, Francis Gurry, presentó su preocupación ante la falta de atención que hay frente a los derechos de propiedad intelectual, pues la gente suele dirigir su interés hacia temas de ciberseguridad, privacidad e integridad de datos al hablar de la inteligencia artificial. Así mismo, Gurry cuestionó si el crecimiento y la sostenibilidad de la tecnología IA nos guiaría a desarrollar dos sistemas para manejar derechos de autor- uno para creaciones humanas y otro para creaciones de máquinas. Aún hay una falta de claridad en el entendimiento alrededor de la inteligencia artificial. Los desarrollos tecnológicos avanzan a paso rápido, aumentando su complejidad en políticas, legalidades y problemas éticos que se merecen la atención global. Antes de encontrar una manera de trabajar con los derechos de autor, es necesario entenderlo correctamente, pues aún no se sabe cómo juzgar la originalidad de un trabajo que nace de una composición de una serie de fragmentos de otros trabajos. La asignación de derechos de autor alrededor de la inteligencia artificial aún no ha sido regulada por la falta de conocimientos y definiciones. Aún hay incertidumbre sobre si, y hasta qué punto, la inteligencia artificial es capaz de producir contenido de manera autónoma y sin ningún humano involucrado, algo que podría influenciar si sus resultados pueden ser protegidos por derechos de autor. El sistema general de derechos de autor aún debe adaptarse al contexto digital de inteligencia artificial, pues están centrados en la creatividad humana. Los derechos de autor no están diseñados para manejar cualquier problema en las políticas relacionado con la creación y el uso de propiedad intelectual, y puede llegar a ser dañino estirar excesivamente los derechos de autor para resolver problemas periféricos, dado que: «Usar los derechos de autor para gobernar la inteligencia artificial es poco inteligente y contradictorio con la función primordial de los derechos de autor de ofrecer un espacio habilitado para que la creatividad florezca». La conversación acerca de la propiedad intelectual tendrá que continuar hasta asegurarse de que la innovación sea protegida, pero también tenga espacio para florecer. En la cultura popular En la literatura A continuación se incluye alguna obra que tiene como motivo central la inteligencia artificial. Yo, Robot (1950), de Isaac Asimov: novela que consta de nueve historias ambientas entre los años de 1940 y 1950, cada uno cuenta con personajes distintos pero que siguen la misma temática a través del seguimiento de las Tres Leyes de la Robótica, en donde se plantea tanto su cumplimiento como la creación de problemas alternos que los mismos robots generan y de esta manera demostrar que la tecnología siempre puede estar un paso adelante del pensamiento y lógica humana. También sigue el hilo argumentativo a través de una entrevista con una psicóloga de robots la cual va relatando el surgimiento de los robots y suponiendo cómo será el desenvolvimiento del ser humano en un mundo en donde la tecnología se esté superando cada vez más. Galatea 2.2 (1995) de Richard Powers: novela que explora la relación entre inteligencia artificial y literatura. La trama sigue al protagonista, quien participa en un experimento con un modelo computacional llamado «Helen» para enseñarle a comunicarse como un humano. A través de esta interacción, se plantean cuestiones profundas sobre la conciencia y la emoción en un entorno tecnológico. La era del diamante (1996) de Neal Stephenson, la inteligencia artificial juega un papel crucial en la trama a través del Manual ilustrado para jovencitas diseñado por John Percival Hackworth. Este instrumento interactivo es capaz de adaptarse dinámicamente a las circunstancias de la niña mediante la inteligencia artificial. El primer libro (2013), de Antonio Palacios Rojo: una novela dialogada que satiriza el uso de la IA en la creación artística unos diez años antes de la irrupción de estas herramientas inteligentes. En el cine Véase también: Computadoras en la ciencia ficción La inteligencia artificial está cada vez más presente en la sociedad, la evolución de la tecnología es una realidad y con ello, la producción de películas sobre esta temática. Cabe destacar, que lleva habiendo piezas audiovisuales sobre inteligencia artificial desde hace mucho tiempo, ya sea incluyendo personajes o mostrando un trasfondo moral y ético. A continuación, se muestra una lista de algunas de las principales películas que tratan este tema: The Terminator (1984): En esta película el argumento se basa en el desarrollo de un microchip capaz de dotar de inteligencia artificial a robots que luego se rebelan contra la humanidad. Se trata de una de las películas más populares sobre una hipotética guerra entre humanos y robots inteligentes capaces de crearse a sí mismos. Matrix (1999): En esta película Keanu Reeves interpreta a Thomas Anderson / Neo, un programador de día y hacker de noche que trata de desentrañar la verdad oculta tras una simulación conocida como «Matrix». Esta realidad simulada es producto de programas de inteligencia artificial que terminan esclavizando a la humanidad y utilizando sus cuerpos como fuente de energía. El hombre bicentenario (1999) Inteligencia artificial (2001): Un trabajador de Cybertronics Manufacturing adopta a David de forma momentánea para, así, estudiar su comportamiento. Tanto él como su esposa acaban por tratar al niño artificial como a su propio hijo biológico. A pesar del cariño que le profesan, David siente la necesidad de escapar de su hogar e iniciar un viaje que le ayude a descubrir a quién pertenece realmente. Ante sus perplejos ojos, se abrirá un nuevo mundo oscuro, injusto, violento, insensible... Algo que le resultará difícil aceptar. Se pregunta cosas como: ¿cómo es posible que sienta algo tan real como el amor y que él sea artificial? y fue nominado al Premio Oscar. Minority Report (2002): La película sobre IA de Steven Spielberg, Minority Report, sigue a John (Tom Cruise), un agente de la ley, que es acusado de un asesinato que cometerá en el futuro. En esta película de principios de los años 2000, el protagonista utiliza una tecnología del futuro que permite a la policía atrapar a los criminales antes de que hayan cometido un delito. En Minority Report, la IA se representa a través de los Precogs, los gemelos que poseen habilidades psíquicas. Los Precogs ven los asesinatos antes de que se produzcan, lo que permite a las fuerzas del orden perseguir el crimen antes de que se cometa. En lugar de los robots físicos de IA tipo cyborg, aquí explora la IA mediante el uso de seres humanos. Yo, robot (2004): Esta película de ciencia ficción protagonizada por Will Smith está ambientada en 2035, en una sociedad donde los humanos viven en perfecta armonía con robots inteligentes en los que confían para todo. Los problemas emergen a la superficie cuando un error en la programación de un superordenador llamado VIKI le lleva a creer que los robots deben tomar las riendas para proteger a la humanidad de sí misma. Her (2013): Esta película de Spike Jonze relata la historia de un escritor de cartas quien está solo y a punto de divorciarse. Este personaje lo representó el galardonado Joaquin Phoenix. Este hombre compró un sistema operativo con inteligencia artificial para utilizarlo a fin de complacer a todos los usuarios y adaptarse a sus necesidades. Sin embargo, el resultado es que desarrolla un sentimiento romántico con Samantha. Quien es la voz femenina del sistema operativo. Avengers: Era de Ultrón (2015): En esta segunda entrega de las películas de Avengers, dirigidas por Joseph Hill Whedon y basadas en los cómics escritos por Stan Lee, se demuestra como es que la inteligencia artificial albergada dentro del cetro de Loki, la cual se tenía como objetivo el convertirla en una protección para la Tierra y recibió por nombre Ultrón, al ser conectada con JARVIS, la IA desarrollada por Stark, pudo obtener la suficiente información para comenzar a pensar de manera independiente y ser capaz de ir actualizando tanto su sistema como su cuerpo logrando controlar un ejército de robots con el objetivo de destruir a la humanidad y así ser lo único que quedara en la Tierra para, posteriormente, dominarla y controlarla. Ex Machina (2015): En la interpretación de Alicia Vikander, increíblemente editada, como Ava, encontramos un probable robot a prueba de Turing escondido en la mansión de un genio, Nathan, un poco loco. Y es que, hablamos de una creación extraña que se siente totalmente real y a la vez inhumana. Está considerada como una de las mejores películas que tratan la inteligencia artificial. Esto se debe principalmente a que parece cubrir todo el concepto IA integrado en una película: el protagonista es un sustituto del ser humano y nos adentra en multitud de argumentos morales que rodean a esta, al tiempo que vemos un arco narrativo de thriller que, desde luego, acaba enganchándonos. Desde luego aquí la representación del personaje de la IA no es blanco o negro. Ava no es buena, pero tampoco es del todo mala. Y en esto, el público se queda reflexionando sobre cuestiones profundas sobre la naturaleza de la IA."
ksampletext_wikipedia_tech_computadora: str = "Computadora. Una computadora, computador u ordenador es una máquina programable que ejecuta una serie de comandos para procesar los datos de entrada, obteniendo convenientemente información que posteriormente se envía a las unidades de salida. Una computadora está compuesta por numerosos y diversos circuitos integrados y varios elementos de apoyo, extensión y accesorios, que en conjunto pueden ejecutar tareas diversas con suma rapidez y bajo el control de un programa (software). La constituyen dos partes esenciales, el hardware, que es su estructura física (circuitos electrónicos, cables, gabinete, teclado, ratón, etc.), y el software, que es su parte intangible (programas, datos, información, documentación, etc). Desde el punto de vista funcional es una máquina que posee, al menos, una unidad central de procesamiento (CPU), una unidad de memoria y otra de entrada/salida (periférico). Los periféricos de entrada permiten el ingreso de datos, la CPU se encarga de su procesamiento (operaciones aritmético-lógicas) y los dispositivos de salida los comunican a los medios externos. Es así, que la computadora recibe datos, los procesa y emite la información resultante, la que luego puede ser interpretada, almacenada, transmitida a otra máquina o dispositivo o sencillamente impresa; todo ello a criterio de un operador o usuario y bajo el control de un programa de computación. El hecho de que sea programable le permite realizar una gran variedad de tareas sobre la base de datos de entrada ya que puede realizar operaciones y resolver problemas en diversas áreas de la actividad humana (administración, ciencia, diseño, ingeniería, medicina, comunicaciones, música, etc). Básicamente, la capacidad de una computadora depende de sus componentes hardware, en tanto que la diversidad de tareas radica mayormente en el software que admita ejecutar y contenga instalado. Si bien esta máquina puede ser de dos tipos, computadora analógica o sistema digital, el primer tipo es usado para pocos y muy específicos propósitos; la más difundida, utilizada y conocida es la computadora digital (de propósitos generales); de tal modo que en términos generales (incluso populares), cuando se habla de «la computadora» se está refiriendo a una computadora digital. Las hay de arquitectura mixta, llamadas computadoras híbridas, siendo también estas de propósitos especiales. En la Segunda Guerra Mundial se utilizaron computadoras analógicas mecánicas, orientadas a aplicaciones militares, y durante la misma época se desarrolló la primera computadora digital, que se llamó ENIAC; ella ocupaba un enorme espacio y consumía grandes cantidades de energía, que equivalen al consumo de cientos de computadoras actuales (PC). Las computadoras modernas están basadas en circuitos integrados, miles de millones de veces más veloces que las primeras máquinas, y ocupan una pequeña fracción de su espacio. Computadoras simples son lo suficientemente pequeñas para residir en los dispositivos móviles. Las computadoras portátiles, tales como tabletas, netbooks, notebooks, ultrabooks, pueden ser alimentadas por pequeñas baterías. Las computadoras personales en sus diversas formas son iconos de la llamada era de la información y son lo que la mayoría de la gente considera como «computadora». Sin embargo, los sistemas embebidos también constituyen computadoras, y se encuentran en muchos dispositivos actuales, tales como reproductores MP4, teléfonos inteligentes, aviones de combate, juguetes, robots industriales, etc. Etimología Computadora/Computador Una «computadora humana» (eufemismo para personal de apoyo que efectuaba cálculos largos) con un microscopio y una calculadora mecánica. En el español que se habla en América se utilizan términos derivados del inglés computer y este a su vez del latín computare calcular. A partir de la raíz latina, también surgieron computator (lit., «computador»; c. 1600), el que calcula, y computist («computista»; finales del siglo XIV), experto en cómputo calendárico o cronológico. Según el Oxford English Dictionary, el primer uso conocido de la palabra computer en la lengua inglesa se encuentra en el libro The Yong Mans Gleanings (1613), del escritor Richard Braithwait, para referirse a un arithmetician (aritmético): «I haue [sic] read the truest computer of Times, and the best Arithmetician that euer [sic] breathed, and he reduceth thy dayes into a short number». Este término aludía a un “computador humano”, una persona que realizaba cálculos o cómputos. Computer continuó con el mismo significado hasta mediados del siglo XX. A finales de este período, se contrataban a mujeres como computadoras porque se les podían pagar menos que a sus colegas masculinos. En 1943, la mayoría de las computadoras humanas eran mujeres; para referirse a ellas existía la forma femenina computress (“computadora”), que con el tiempo se cambió a programmer (“programadora”). El Oxford English Dictionary registra que, a finales del siglo XIX, computer empezó a utilizarse con el significado de «máquina calculadora». El uso moderno del término para «computador electrónico digital programable» data de 1945, basándose en el concepto teórico de máquina de Turing publicado en 1937. ENIAC (1946), sigla de «computador e integrador numérico electrónico» (Electronic Numerical Integrator And Computer), generalmente se considera el primero de este tipo. Ordenador En el español que se habla en España es predominante el uso del vocablo «ordenador», que proviene del término francés ordinateur y este a su vez del término latino ordinator. La palabra «ordenador» fue introducida por IBM Francia en 1955, después de que François Girard, entonces jefe del departamento de publicidad de la empresa tuvo la idea de consultar a su antiguo profesor de literatura en París, Jacques Perret. Junto con Christian de Waldner, entonces presidente de IBM Francia, pidieron al profesor Perret que sugiriera un nombre en francés para su nueva máquina electrónica de tratamiento de la información (IBM 650), evitando la traducción literal de la palabra inglesa computer (calculadora o calculatriz), que en aquella época estaba reservada para las máquinas científicas. En 1911, una descripción de la máquina analítica de Babbage utilizaba la palabra ordonnateur para describir su fuerza motriz: «Pour aller prendre et reporter les nombres… et pour les soumettre à lopération demandée, il faut quil y ait dans la machine un organe spécial et variable : cest lordonnateur. Cet ordonnateur est constitué simplement par des feuilles de carton ajourées, analogues à celle des métiers Jacquard…». «Para tomar y transferir los números... y someterlos a la operación requerida, debe haber un órgano especial y variable en la máquina: este es el ordenador. Este ordenador se compone simplemente de láminas de cartón con agujeros, similares a los que se utilizan en los telares de Jacquard...». Manual de la máquina Babbage (en francés) Perret propuso una palabra compuesta centrada en el ordonnateur: el que pone orden y tiene la noción del orden eclesiástico en la Iglesia católica (ordinant). Sugirió, más precisamente, ordinatrice électronique, de manera que el femenino, según él, pudiese distinguir mejor el uso religioso del uso contable de la palabra. IBM Francia conservó la palabra ordinateur e inicialmente trató de proteger este nombre como marca registrada, pero, como los usuarios adoptaron fácil y rápidamente la palabra ordinateur, la empresa decidió dejarla en el dominio público. En 1984, académicos franceses, en el debate Les jeunes, la technique et nous, que el uso del sustantivo ordonnateur es incorrecto, porque la función del aparato es procesar datos, no dar órdenes. Mientras que otros ,como también el creador del término, Jacques Perret, conocedores del origen religioso del término, lo consideran el más correcto; se habló del hecho de que la palabra «ordenador» guarda más relación con la función ordenar que con dar órdenes lo que sería más correcto para la función moderna de estos aparatos. El uso de la palabra «ordenador» se ha exportado a los idiomas de España: el aragonés, el asturiano, el gallego, el castellano, el catalán y el euskera. Historia Artículo principal: Historia de la computación Lejos de ser un invento de una persona en particular, la computadora es el resultado evolutivo de ideas de muchas personas relacionadas con áreas tales como la electrónica, la mecánica, los materiales semiconductores, la lógica, el álgebra y la programación. Cronología Los principales hitos en la historia de la computación, desde las primeras herramientas manuales para hacer cálculos hasta las modernas computadoras de bolsillo. 500 a. C.: se utiliza el ábaco en antiguas civilizaciones como la China o la Sumeria, la primera herramienta para realizar sumas y restas. Hacia 830: el matemático e ingeniero persa Musa al-Juarismi desarrolló la teoría del algoritmo, es decir, la resolución metódica de problemas de álgebra y cálculo numérico mediante una lista bien definida, ordenada y finita de operaciones. 1614: el escocés John Napier inventa el logaritmo neperiano, que consiguió simplificar el cálculo de multiplicaciones y divisiones reduciéndolo a un cálculo con sumas y restas. 1620: el inglés Edmund Gunter inventa la regla de cálculo, instrumento manual utilizado desde entonces hasta la aparición de la calculadora electrónica para hacer operaciones aritméticas. 1623: el alemán Wilhelm Schickard inventa la primera máquina de calcular, cuyo prototipo desapareció poco después. 1642: el científico y filósofo francés Blaise Pascal inventa una máquina de sumar (la pascalina), que utilizaba ruedas dentadas, y de la que todavía se conservan algunos ejemplares originales. 1671: el filósofo y matemático alemán Gottfried Wilhelm Leibniz inventa una máquina capaz de multiplicar y dividir. 1801: el francés Joseph Jacquard inventa para su máquina de tejer brocados una tarjeta perforada que controla el patrón de funcionamiento de la máquina, una idea que sería empleada más adelante por los primeros computadores. 1833: el matemático e inventor británico Charles Babbage diseña e intenta construir la primera computadora, de funcionamiento mecánico, a la que llamó la «máquina analítica». Sin embargo, la tecnología de su época no estaba lo suficientemente avanzada para hacer realidad su idea. 1841 : la matemática Ada Lovelace comienza a trabajar junto a Charles Babbage en lo que sería el primer algoritmo destinado a ser procesado por una máquina, por lo que se la considera como la primera programadora de computadores. 1890: el estadounidense Herman Hollerith inventa una máquina tabuladora aprovechando algunas de las ideas de Babbage, que se utilizó para elaborar el censo de Estados Unidos. Hollerith fundó posteriormente la compañía que después se convertiría en IBM. 1893: el científico suizo Otto Steiger desarrolla la primera calculadora automática que se fabricó y empleó a escala industrial, conocida como la Millonaria. 1936: el matemático y computólogo inglés Alan Turing formaliza los conceptos de algoritmo y de máquina de Turing, que serían claves en el desarrollo de la computación moderna. 1938: el ingeniero alemán Konrad Zuse completa la Z1, la primera computadora que se puede considerar como tal. De funcionamiento electromecánico y utilizando relés, era programable (mediante cinta perforada) y usaba sistema binario y lógica booleana. A ella le seguirían los modelos mejorados Z2, Z3 y Z4. 1944: en Estados Unidos la empresa IBM construye la computadora electromecánica Harvard Mark I, diseñada por un equipo encabezado por Howard H. Aiken. Fue la primera computadora creada en Estados Unidos. 1944: en Reino Unido se construyen los computadores Colossus (Colossus Mark I y Colossus Mark 2), con el objetivo de descifrar las comunicaciones de los alemanes durante la Segunda Guerra Mundial. 1946: en la Universidad de Pensilvania se pone en funcionamiento la ENIAC (Electronic Numerical Integrator And Calculator), que funcionaba a válvulas y fue la primera computadora electrónica de propósito general. 1947: en Bell Labs, John Bardeen, Walter Houser Brattain y William Shockley inventan el transistor. 1948: en Reino Unido se construye el primer computador del mundo con programa almacenado, el Manchester Baby, diseñado por Frederic C. Williams, Tom Kilburn y Geoff Tootill, de la Universidad de Mánchester. 1949: el MESM fue la primera computadora en Rusia y la segunda programable en Europa continental, creada por un equipo de científicos bajo la dirección de Serguéi Alekseevich Lébedev. 1950: Kathleen Booth, crea el Lenguaje ensamblador para hacer operaciones en la computadora sin necesidad de cambiar los cables de conexión, sino a través de tarjetas perforadoras (programa u operación guardada para usarla cuando sea necesario) las cuales eran propensas a dañarse por esta razón, a finales de este año se comienza a desarrollar el lenguaje de programación. 1951 BESM El desarrollo de las máquinas BESM comenzó en el Instituto de Mecánica de Precisión y Ciencias de la Computación de la URSS (ITM y VT) en Moscú bajo la supervisión de Serguéi Alekseevich Lébedev en 1950 como una continuación de sus trabajos en Kiev con la computadora MESM. Esta serie de máquinas se dejaron de fabricar en 1987. 1951: comienza a operar la EDVAC, concebida por John von Neumann, que a diferencia de la ENIAC no era decimal, sino binaria, y tuvo el primer programa diseñado para ser almacenado. 1953: IBM fabrica su primera computadora a escala industrial, la IBM 650. Se amplía el uso del lenguaje ensamblador para la programación de las computadoras. Las computadoras con transistores reemplazan a las de válvulas, marcando el comienzo de la segunda generación de computadoras. 1957: Jack S. Kilby construye el primer circuito integrado. 1961: en Reino Unido se construye la Calculadora Anita en dos modelos, siendo las primeras calculadoras electrónicas de escritorio del mundo. 1964: la aparición del IBM 360 marca el comienzo de la tercera generación de computadoras, en la que las placas de circuito impreso con múltiples componentes elementales pasan a ser reemplazadas con placas de circuitos integrados. 1965: Olivetti lanza, Programma 101, la primera computadora de escritorio. 1971: Nicolet Instruments Corp. lanza al mercado la Nicolet 1080, una computadora de uso científico basada en registros de 20 bits. 1971: Intel presenta el primer microprocesador comercial, el Intel 4004, diseñado por Federico Faggin, Marcian Hoff y Masatoshi Shima. 1975: Bill Gates y Paul Allen fundan Microsoft. 1976: Steve Jobs, Steve Wozniak, Mike Markkula fundan Apple. 1977: Apple presenta el primer computador personal que se vende a gran escala, el Apple II, desarrollado por Steve Jobs y Steve Wozniak. 1981: se lanza al mercado el IBM PC, que se convertiría en un éxito comercial, marcaría una revolución en el campo de la computación personal y definiría nuevos estándares. 1982: Microsoft presenta su sistema operativo MS-DOS, por encargo de IBM. 1983: ARPANET se separa de la red militar que la originó, pasando a un uso civil y convirtiéndose así en el origen de Internet. 1983: Richard Stallman anuncia públicamente el proyecto GNU. 1985: Microsoft presenta el sistema operativo Windows 1.0. 1990: Tim Berners-Lee idea el hipertexto para crear el World Wide Web (WWW), una nueva manera de interactuar con Internet. 1991: Linus Torvalds comenzó a desarrollar Linux, un sistema operativo compatible con Unix. 2000: aparecen a comienzos del siglo XXI los computadores de bolsillo, primeras PDA. 2007: presentación del primer iPhone, por la empresa Apple, un teléfono inteligente o smartphone. Componentes Artículo principal: Arquitectura de computadoras Imagen ilustrativa Las tecnologías utilizadas en computadoras digitales han evolucionado mucho desde la aparición de los primeros modelos en los años 1940, aunque la mayoría todavía utiliza la Arquitectura de von Neumann, publicada por John von Neumann a principios de esa década, que otros autores atribuyen a John Presper Eckert y John William Mauchly. La arquitectura de Von Neumann describe una computadora con cuatro (4) secciones principales: la unidad aritmético lógica, la unidad de control, la memoria primaria, principal o central, y los dispositivos de entrada y salida (E/S). Estas partes están interconectadas por canales de conductores denominados buses. Unidad central de procesamiento Artículo principal: Unidad central de procesamiento La unidad central de procesamiento (CPU, por sus siglas del inglés: Central Processing Unit) consta de manera básica de los siguientes tres elementos: Un típico símbolo esquemático para una ALU: A y B son operandos; R es la salida; F es la entrada de la unidad de control; D es un estado de la salida. La unidad aritmética lógica (ALU: Arithmetic Logic Unit) es el dispositivo diseñado y construido para llevar a cabo las operaciones elementales como las operaciones aritméticas (suma, resta), operaciones lógicas (AND, OR, XOR, inversiones, desplazamientos y rotaciones). La unidad de control (UC: Control Unit) sigue la dirección de las posiciones en memoria que contienen la instrucción que el computador va a realizar en ese momento; recupera la información poniéndola en la ALU para la operación que debe desarrollar. Transfiere luego el resultado a ubicaciones correspondientes en la memoria. Una vez que ocurre lo anterior, la unidad de control va a la siguiente instrucción, pudiendo ser la siguiente físicamente (procedente del Contador de Programa) u otra (a través de una instrucción de salto). Los registros: no accesibles (de instrucción, de bus de datos y bus de dirección) y accesibles, uso específico (contador programa, puntero pila, acumulador, flags, etc.) o de uso general. Memoria primaria Véanse también: Jerarquía de memoria, Memoria principal, Memoria (Informática) y RAM. La memoria principal, conocida como memoria de acceso aleatorio (RAM, por sus siglas del inglés: Random-Access Memory), es un conjunto de celdas de almacenamiento organizadas de tal forma que se pueden acceder numéricamente a la dirección de memoria. Cada celda corresponde a un bit o unidad mínima de información. Se accede por secuencias de 8 bits. Una instrucción es una determinada acción operativa, una secuencia que indica a la ALU la operación a realizar (suma, resta, operaciones lógicas, etc). En los bytes de memoria principal se almacenan tanto los datos como los códigos de operación que se necesitan para llevar a cabo las instrucciones. La capacidad de la memoria viene dada por el número de celdas que contiene, medido en bytes o múltiplos. Las tecnologías empleadas para fabricar las memorias han cambiado bastante; desde los relés electromecánicos de las primeras computadoras, tubos con mercurio en los que se formaban los pulsos acústicos, matrices de imanes permanentes, transistores individuales hasta los actuales circuitos integrados con millones de celdas en un solo chip. Se subdividen en memorias estáticas (SRAM) con seis transistores integrados por bit y la mucho más utilizada memoria dinámica (DRAM), de un transistor y un condensador integrados por bit. La memoria RAM puede ser reescrita varios millones de veces; a diferencia de la memoria ROM, que solo puede ser grabada una única vez. Periféricos de entrada, de salida o de entrada/salida Véanse también: Periférico (informática), Periférico de entrada y Periférico de entrada/salida. Los dispositivos de entrada permiten el ingreso de datos e información mientras que los de salida son los encargados de exteriorizar la información procesada por la computadora. Hay periféricos que son a la vez de entrada y de salida. Como ejemplo, un dispositivo típico de entrada es el teclado, uno de salida es el monitor, uno de entrada/salida es el disco rígido. Hay una gama muy extensa de dispositivos E/S, como teclado, monitor, impresora, ratón, unidad de disco flexible, cámara web, etc. Computadora de escritorio Buses Las tres unidades básicas en una computadora, la CPU, la memoria y el elemento de E/S, están comunicadas entre sí por buses o canales de comunicación: Bus de direcciones : permite seleccionar la dirección del dato o del periférico al que se quiere acceder, Bus de control : controla el funcionamiento externo e interno de la CPU. Bus de datos :contiene la información (datos) que circula por el sistema. Otros datos y conceptos En los computadores modernos, un usuario tiene la impresión de que los computadores pueden ejecutar varios programas «al mismo tiempo», esto se conoce como multitarea. En realidad, la CPU ejecuta instrucciones de un programa y después tras un breve periodo de tiempo, cambia la ejecución a un segundo programa y ejecuta algunas de sus instrucciones. Dado que este proceso es muy rápido, crea la ilusión de que se están ejecutando varios programas simultáneamente; en realidad se está repartiendo el tiempo de la CPU entre los programas, uno a la vez. El sistema operativo es el que controla el reparto del tiempo. El procesamiento realmente simultáneo se realiza en computadoras que poseen más de un CPU, lo que da origen al multiprocesamiento. El sistema operativo es el programa que gestiona y administra todos los recursos del ordenador, controla, por ejemplo, qué programas se ejecutan y cuándo, administra la memoria y los accesos a los dispositivos E/S, provee las interfases entre dispositivos, incluso entre el computador y el usuario. Actualmente se suele incluir en las distribuciones del sistema operativo algunos programas muy usados; como navegadores de Internet, procesadores de texto, programas de correo electrónico, interfaces de red, reproductores de películas y otros programas que antes se tenían que conseguir e instalar separadamente. Los primeros computadores digitales, de gran tamaño y coste, se utilizaban principalmente para hacer cálculos científicos. ENIAC se creó con el propósito de resolver los problemas de balística del ejército de Estados Unidos. El CSIRAC, el primer ordenador australiano, permitió evaluar patrones de precipitaciones para un gran proyecto de generación hidroeléctrica. Con la fabricación comercial de computadoras, los gobiernos y las empresas sistematizaron muchas de sus tareas de recolección y procesamiento de datos, que antes eran realizadas manualmente. En el mundo académico, los científicos de todos los campos empezaron a utilizar los computadores para hacer sus análisis y cálculos; el descenso continuado de los precios de estos aparatos permitió su uso por empresas cada vez más pequeñas. Las empresas, las organizaciones y los gobiernos empezaron a emplear un gran número de pequeños ordenadores para realizar tareas que antes eran hechas por computadores centrales grandes y costosos. Con la invención del microprocesador en 1970, fue posible fabricar ordenadores cada vez más baratos. Nació el microcomputador y luego apareció la computadora personal, estos últimos se hicieron populares para llevar a cabo tareas rutinarias como escribir e imprimir documentos, calcular probabilidades, realizar análisis y cálculo con hojas de cálculo, comunicarse mediante correo electrónico e Internet. La gran disponibilidad de computadores y su fácil adaptación a las necesidades de cada persona, han hecho que se utilicen para una variedad de tareas, que incluyen los más diversos campos de aplicación. Al mismo tiempo, los computadores pequeños de programación fija (sistemas embebidos) empezaron a abrirse camino entre las aplicaciones para el hogar, los automóviles, los aviones y la maquinaria industrial. Estos procesadores integrados controlaban el comportamiento de los aparatos más fácilmente, permitiendo el desarrollo de funciones de control más complejas, como por ejemplo los sistemas de freno antibloqueo (ABS). A principios del siglo XXI, la mayoría de los aparatos eléctricos, casi todos los tipos de transporte eléctrico y la mayoría de las líneas de producción de las fábricas funcionan con un computador. PC con interfaz táctil. Hacia finales del siglo XX y comienzos del XXI, los computadores personales son usados tanto para la investigación como para el entretenimiento (videojuegos), mientras que los grandes computadores se utilizan para cálculos matemáticos complejos, tecnología, modelado, astronomía, medicina, etc. Como resultado del cruce entre el concepto de la computadora personal y los llamados «supercomputadores» surge la estación de trabajo; este término, originalmente utilizado para equipos y máquinas de registro, grabación y tratamiento digital de sonido, ahora hace referencia a estaciones de trabajo, que son sistemas de gran capacidad de cómputo, normalmente dedicados a labores de cálculo científico o procesos en tiempo real. Una estación de trabajo es, en esencia, un equipo de trabajo personal con capacidad elevada de cálculo, rendimiento y almacenamiento, superior a los computadores personales convencionales."
ksampletext_wikipedia_tech_software: str = "Software. Se conoce como software logicial, soporte lógico o programática al sistema formal de un sistema informático, que comprende el conjunto de los componentes lógicos necesarios que hace posible la realización de tareas específicas, en contraposición a los componentes físicos que son llamados hardware. La interacción entre el software y el hardware hace operativo un ordenador (u otro dispositivo), es decir, el software envía instrucciones que el hardware ejecuta, haciendo posible su funcionamiento. Los componentes lógicos incluyen, entre muchos otros, las aplicaciones informáticas, tales como el procesador de texto, que permite al usuario realizar todas las tareas concernientes a la edición de textos; el llamado software de sistema, tal como el sistema operativo, que básicamente permite al resto de los programas funcionar adecuadamente, facilitando también la interacción entre los componentes físicos y el resto de las aplicaciones, y proporcionando una interfaz con el usuario. El software, en su gran mayoría, está escrito en lenguajes de programación de alto nivel, ya que son más fáciles y eficientes para que los programadores los usen, porque son más cercanos al lenguaje natural respecto del lenguaje de máquina. Los lenguajes de alto nivel se traducen a lenguaje de máquina utilizando un compilador o un intérprete, o bien una combinación de ambos. El software también puede estar escrito en lenguaje ensamblador, que es de bajo nivel y tiene una alta correspondencia con las instrucciones de lenguaje máquina; se traduce al lenguaje de la máquina utilizando un ensamblador. El anglicismo software es el más ampliamente difundido al referirse a este concepto, especialmente en la jerga técnica, en tanto que el término sinónimo «logicial», derivado del término francés logiciel, es utilizado mayormente en países y zonas de influencia francesa. Etimología Software es una palabra proveniente del inglés, que en español no posee una traducción adecuada al contexto, por lo cual se la utiliza asiduamente sin traducir y así fue admitida por la Real Academia Española (RAE). Aunque puede no ser estrictamente lo mismo, suele sustituirse por expresiones tales como programas (informáticos), aplicaciones informáticas o soportes lógicos. Software es lo que se denomina producto en ingeniería de software. El término «logicial» es un calco léxico del término francés logiciel, neologismo que se formó en 1969 a partir de las palabras logique (lógica) y matériel (material) como traducción de la Delegación de la informática responsable del Plan Calcul. Definición de software Existen varias definiciones similares aceptadas para software, pero probablemente la más formal sea la siguiente: Es el conjunto de los programas de cómputo, procedimientos, reglas, documentación y datos asociados, que forman parte de las operaciones de un sistema de computación. Extraído del estándar 729 del IEEE Considerando esta definición, el concepto de software va más allá de los programas de computación en sus distintos estados: código fuente, binario o ejecutable; también su documentación, los datos a procesar e incluso la información de usuario forman parte del software: es decir, abarca todo lo intangible, todo lo «no físico» relacionado. El término software fue usado por primera vez en este sentido por John W. Tukey en 1957. En la ingeniería de software y las ciencias de la computación, el software es toda la información procesada por los sistemas informáticos: programas y datos. El concepto de leer diferentes secuencias de instrucciones (programa) desde la memoria de un dispositivo para controlar los cálculos fue introducido por Charles Babbage como parte de su máquina diferencial. La teoría que forma la base de la mayor parte del software moderno fue propuesta por Alan Turing en su ensayo de 1936, «Los números computables», con una aplicación al problema de decisión. Clasificación del software Buscador de Programas en Ubuntu 13.10 Si bien esta distinción es, en cierto modo arbitraria, y a veces confusa, a los fines prácticos se puede clasificar al software en tres tipos: Software de sistema: Su objetivo es vincular adecuadamente al usuario y al programador de los detalles del sistema informático en particular que se use, aislándolo especialmente del procesamiento referido a las características internas de: memoria, discos, puertos y dispositivos de comunicaciones, impresoras, pantallas, teclados, etc. El software de sistema le procura al usuario y programador adecuadas interfaces de alto nivel, controladores, herramientas y utilidades de apoyo que permiten el mantenimiento del sistema global. Incluye entre otros: Sistemas operativos Controladores de dispositivos Herramientas de diagnóstico Herramientas de corrección y optimización Servidores Utilidades Software de programación: Es el conjunto de herramientas que permiten al programador desarrollar programas de informática, usando diferentes alternativas y lenguajes de programación, de una manera práctica. Incluyen en forma básica: Editores de texto Compiladores Intérpretes Enlazadores Depuradores Entornos de desarrollo integrados (IDE): Agrupan las anteriores herramientas, usualmente en un entorno visual, de forma tal que el programador no necesite introducir múltiples comandos para compilar, interpretar, depurar, etc. Habitualmente cuentan con una avanzada interfaz gráfica de usuario (GUI). Software de aplicación: Es aquel que permite a los usuarios llevar a cabo una o varias tareas específicas, en cualquier campo de actividad susceptible de ser automatizado o asistido, con especial énfasis en los negocios. Incluye entre muchos otros: Aplicaciones para Control de sistemas y automatización industrial Aplicaciones ofimáticas Software educativo Software empresarial Bases de datos Telecomunicaciones (por ejemplo Internet y toda su estructura lógica) Videojuegos Software médico Software de cálculo numérico y simbólico. Software de diseño asistido (CAD) Software de control numérico (CAM) Proceso de creación del software Artículo principal: Proceso para el desarrollo de software Se define como «proceso» al conjunto ordenado de pasos a seguir para llegar a la solución de un problema u obtención de un producto, en este caso particular, para lograr un producto software que resuelva un problema específico. El proceso de creación de software puede llegar a ser muy complejo, dependiendo de su porte, características y criticidad del mismo. Por ejemplo la creación de un sistema operativo es una tarea que requiere proyecto, gestión, numerosos recursos y todo un equipo disciplinado de trabajo. En el otro extremo, si se trata de un sencillo programa (por ejemplo, la resolución de una ecuación de segundo orden), este puede ser realizado por un solo programador (incluso aficionado) fácilmente. Es así que normalmente se dividen en tres categorías según su tamaño (líneas de código) o costo: de «pequeño», «mediano» y «gran porte». Existen varias metodologías para estimarlo, una de las más populares es el sistema COCOMO que provee métodos y un software (programa) que calcula y provee una aproximación de todos los costos de producción en un «proyecto software» (relación horas/hombre, costo monetario, cantidad de líneas fuente de acuerdo a lenguaje usado, etc.). Considerando los de gran porte, es necesario realizar complejas tareas, tanto técnicas como de gerencia, una fuerte gestión y análisis diversos (entre otras cosas), la complejidad de ello ha llevado a que desarrolle una ingeniería específica para tratar su estudio y realización: es conocida como ingeniería de Software. En tanto que en los de mediano porte, pequeños equipos de trabajo (incluso un avezado analista-programador solitario) pueden realizar la tarea. Aunque, siempre en casos de mediano y gran porte (y a veces también en algunos de pequeño porte, según su complejidad), se deben seguir ciertas etapas que son necesarias para la construcción del software. Tales etapas, si bien deben existir, son flexibles en su forma de aplicación, de acuerdo a la metodología o proceso de desarrollo escogido y utilizado por el equipo de desarrollo o por el analista-programador solitario (si fuere el caso). Los «procesos de desarrollo de software» poseen reglas preestablecidas, y deben ser aplicados en la creación del software de mediano y gran porte, ya que en caso contrario lo más seguro es que el proyecto no logre concluir o termine sin cumplir los objetivos previstos, y con variedad de fallos inaceptables (fracasan, en pocas palabras). Entre tales «procesos» los hay ágiles o livianos (ejemplo XP), pesados y lentos (ejemplo RUP), y variantes intermedias. Normalmente se aplican de acuerdo al tipo y porte del software a desarrollar, a criterio del líder (si lo hay) del equipo de desarrollo. Algunos de esos procesos son Programación Extrema (en inglés eXtreme Programming o XP), Proceso Unificado de Rational (en inglés Rational Unified Process o RUP), Feature Driven Development (FDD), etc. Cualquiera sea el «proceso» utilizado y aplicado al desarrollo del software (RUP, FDD, XP, etc), y casi independientemente de él, siempre se debe aplicar un «modelo de ciclo de vida». Se estima que, del total de proyectos software grandes emprendidos, un 28 % fracasan, un 46 % caen en severas modificaciones que lo retrasan y un 26 % son totalmente exitosos. Cuando un proyecto fracasa, rara vez es debido a fallas técnicas, la principal causa de fallos y fracasos es la falta de aplicación de una buena metodología o proceso de desarrollo. Entre otras, una fuerte tendencia, desde hace pocas décadas, es mejorar las metodologías o procesos de desarrollo, o crear nuevas y concientizar a los profesionales de la informática a su utilización adecuada. Normalmente los especialistas en el estudio y desarrollo de estas áreas (metodologías) y afines (tales como modelos y hasta la gestión misma de los proyectos) son los ingenieros en software, es su orientación. Los especialistas en cualquier otra área de desarrollo informático (analista, programador, Lic. en informática, ingeniero en informática, ingeniero de sistemas, etc.) normalmente aplican sus conocimientos especializados pero utilizando modelos, paradigmas y procesos ya elaborados. Es común para el desarrollo de software de mediano porte que los equipos humanos involucrados apliquen «metodologías propias», normalmente un híbrido de los procesos anteriores y a veces con criterios propios. El proceso de desarrollo puede involucrar numerosas y variadas tareas, desde lo administrativo, pasando por lo técnico y hasta la gestión y el gerenciamiento. Pero, casi rigurosamente, siempre se cumplen ciertas etapas mínimas; las que se pueden resumir como sigue: Captura, elicitación, especificación y análisis de requisitos (ERS) Diseño Codificación Pruebas (unitarias y de integración) Instalación y paso a producción Mantenimiento En las anteriores etapas pueden variar ligeramente sus nombres, o ser más globales, o contrariamente, ser más refinadas; por ejemplo indicar como una única fase (a los fines documentales e interpretativos) de «análisis y diseño»; o indicar como «implementación» lo que está dicho como «codificación»; pero en rigor, todas existen e incluyen, básicamente, las mismas tareas específicas. En el apartado 4 del presente artículo se brindan mayores detalles de cada una de las etapas indicadas. Modelos de proceso o ciclo de vida Para cada una de las fases o etapas listadas en el ítem anterior, existen subetapas (o tareas). El modelo de proceso o modelo de ciclo de vida utilizado para el desarrollo, define el orden de las tareas o actividades involucradas, también define la coordinación entre ellas, y su enlace y realimentación. Entre los más conocidos se puede mencionar: modelo en cascada o secuencial, modelo espiral, modelo iterativo incremental. De los antedichos hay a su vez algunas variantes o alternativas, más o menos atractivas según sea la aplicación requerida y sus requisitos. Modelo cascada Este, aunque es más comúnmente conocido como modelo en cascada es también llamado «modelo clásico», «modelo tradicional» o «modelo lineal secuencial». El modelo en cascada puro «difícilmente se utiliza tal cual», pues esto implicaría un previo y absoluto conocimiento de los requisitos, la no volatilidad de los mismos (o rigidez) y etapas subsiguientes libres de errores; ello solo podría ser aplicable a escasos y pequeños sistemas a desarrollar. En estas circunstancias, el paso de una etapa a otra de las mencionadas sería sin retorno, por ejemplo pasar del diseño a la codificación implicaría un diseño exacto y sin errores ni probable modificación o evolución: «codifique lo diseñado sin errores, no habrá en absoluto variantes futuras». Esto es utópico; ya que intrínsecamente «el software es de carácter evolutivo», cambiante y difícilmente libre de errores, tanto durante su desarrollo como durante su vida operativa. Figura 2: Modelo cascada puro o secuencial para el ciclo de vida del software. Algún cambio durante la ejecución de una cualquiera de las etapas en este modelo secuencial podría implicar reiniciar desde el principio todo el ciclo completo, lo cual redundaría en altos costos de tiempo y desarrollo. La Figura 2 muestra un posible esquema del modelo en cuestión. Sin embargo, el modelo cascada en algunas de sus variantes es uno de los actualmente más utilizados, por su eficacia y simplicidad, más que nada en software de pequeño y algunos de mediano porte; pero nunca (o muy rara vez) se lo usa en su forma pura, como se dijo anteriormente. En lugar de ello, siempre se produce alguna realimentación entre etapas, que no es completamente predecible ni rígida; esto da oportunidad al desarrollo de productos software en los cuales hay ciertas incertezas, cambios o evoluciones durante el ciclo de vida. Así por ejemplo, una vez capturados y especificados los requisitos (primera etapa) se puede pasar al diseño del sistema, pero durante esta última fase lo más probable es que se deban realizar ajustes en los requisitos (aunque sean mínimos), ya sea por fallas detectadas, ambigüedades o bien porque los propios requisitos han cambiado o evolucionado; con lo cual se debe retornar a la primera o previa etapa, hacer los reajustes pertinentes y luego continuar nuevamente con el diseño; esto último se conoce como realimentación. Lo normal en el modelo cascada es entonces la aplicación del mismo con sus etapas realimentadas de alguna forma, permitiendo retroceder de una a la anterior (e incluso poder saltar a varias anteriores) si es requerido. De esta manera se obtiene el «modelo cascada realimentado», que puede ser esquematizado como lo ilustra la Figura 3. Figura 3: Modelo cascada realimentado para el ciclo de vida. Lo dicho es, a grandes rasgos, la forma y utilización de este modelo, uno de los más usados y populares. El modelo cascada realimentado resulta muy atractivo, hasta ideal, si el proyecto presenta alta rigidez (pocos cambios, previsto no evolutivo), los requisitos son muy claros y están correctamente especificados. Hay más variantes similares al modelo: refino de etapas (más etapas, menores y más específicas) o incluso mostrar menos etapas de las indicadas, aunque en tal caso la faltante estará dentro de alguna otra. El orden de esas fases indicadas en el ítem previo es el lógico y adecuado, pero adviértase, como se dijo, que normalmente habrá realimentación hacia atrás. El modelo lineal o en cascada es el paradigma más antiguo y extensamente utilizado, sin embargo las críticas a él (ver desventajas) han puesto en duda su eficacia. Pese a todo, tiene un lugar muy importante en la ingeniería de software y continúa siendo el más utilizado; y siempre es mejor que un enfoque al azar. Desventajas del modelo cascada: Los cambios introducidos durante el desarrollo pueden confundir al equipo profesional en las etapas tempranas del proyecto. Si los cambios se producen en etapa madura (codificación o prueba) pueden ser catastróficos para un proyecto grande. No es frecuente que el cliente o usuario final explicite clara y completamente los requisitos (etapa de inicio); y el modelo lineal así lo requiere. La incertidumbre natural en los comienzos es luego difícil de acomodar. El cliente debe tener paciencia ya que el software no estará disponible hasta muy avanzado el proyecto. Un error importante detectado por el cliente (en fase de operación) puede ser desastroso, implicando reinicio del proyecto, con altos costos. Modelos evolutivos El software evoluciona con el tiempo. Los requisitos del usuario y del producto suelen cambiar conforme se desarrolla el mismo. Las fechas de mercado y la competencia hacen que no sea posible esperar a poner en el mercado un producto absolutamente completo, por lo que se aconseja introducir una versión funcional limitada de alguna forma para aliviar las presiones competitivas. En esas u otras situaciones similares, los desarrolladores necesitan modelos de progreso que estén diseñados para acomodarse a una evolución temporal o progresiva, donde los requisitos centrales son conocidos de antemano, aunque no estén bien definidos a nivel detalle. En el modelo cascada y cascada realimentado no se tiene demasiado en cuenta la naturaleza evolutiva del software, se plantea como estático, con requisitos bien conocidos y definidos desde el inicio. Los evolutivos son modelos iterativos, permiten desarrollar versiones cada vez más completas y complejas, hasta llegar al objetivo final deseado; incluso evolucionar más allá, durante la fase de operación. Los modelos «iterativo incremental» y «espiral» (entre otros) son dos de los más conocidos y utilizados del tipo evolutivo. Modelo iterativo incremental En términos generales, se puede distinguir, en la figura 4, los pasos generales que sigue el proceso de desarrollo de un producto software. En el modelo de ciclo de vida seleccionado, se identifican claramente dichos pasos. La descripción del sistema es esencial para especificar y confeccionar los distintos incrementos hasta llegar al producto global y final. Las actividades concurrentes (especificación, desarrollo y validación) sintetizan el desarrollo pormenorizado de los incrementos, que se hará posteriormente. Figura 4: Diagrama genérico del desarrollo evolutivo incremental. El diagrama de la figura 4 muestra en forma muy esquemática, el funcionamiento de un ciclo iterativo incremental, el cual permite la entrega de versiones parciales a medida que se va construyendo el producto final. Es decir, a medida que cada incremento definido llega a su etapa de operación y mantenimiento. Cada versión emitida incorpora a los anteriores incrementos las funcionalidades y requisitos que fueron analizados como necesarios. El incremental es un modelo de tipo evolutivo que está basado en varios ciclos cascada realimentados aplicados repetidamente, con una filosofía iterativa.En la figura 5 se muestra un refino del diagrama previo, bajo un esquema temporal, para obtener finalmente el esquema del modelo de ciclo de vida iterativo incremental, con sus actividades genéricas asociadas. Aquí se observa claramente cada ciclo cascada que es aplicado para la obtención de un incremento; estos últimos se van integrando para obtener el producto final completo. Cada incremento es un ciclo cascada realimentado, aunque, por simplicidad, en la figura 5 se muestra como secuencial puro. Figura 5: Modelo iterativo incremental para el ciclo de vida del software. Se observa que existen actividades de desarrollo (para cada incremento) que son realizadas en paralelo o concurrentemente, así por ejemplo, en la Figura, mientras se realiza el diseño detalle del primer incremento ya se está realizando en análisis del segundo. La Figura 5 es solo esquemática, un incremento no necesariamente se iniciará durante la fase de diseño del anterior, puede ser posterior (incluso antes), en cualquier tiempo de la etapa previa. Cada incremento concluye con la actividad de «operación y mantenimiento» (indicada como «Operación» en la figura), que es donde se produce la entrega del producto parcial al cliente. El momento de inicio de cada incremento es dependiente de varios factores: tipo de sistema; independencia o dependencia entre incrementos (dos de ellos totalmente independientes pueden ser fácilmente iniciados al mismo tiempo si se dispone de personal suficiente); capacidad y cantidad de profesionales involucrados en el desarrollo; etc. Bajo este modelo se entrega software «por partes funcionales más pequeñas», pero reutilizables, llamadas incrementos. En general cada incremento se construye sobre aquel que ya fue entregado. Como se muestra en la Figura 5, se aplican secuencias Cascada en forma escalonada, mientras progresa el tiempo calendario. Cada secuencia lineal o Cascada produce un incremento y a menudo el primer incremento es un sistema básico, con muchas funciones suplementarias (conocidas o no) sin entregar. El cliente utiliza inicialmente ese sistema básico, intertanto, el resultado de su uso y evaluación puede aportar al plan para el desarrollo del/los siguientes incrementos (o versiones). Además también aportan a ese plan otros factores, como lo es la priorización (mayor o menor urgencia en la necesidad de cada incremento en particular) y la dependencia entre incrementos (o independencia). Luego de cada integración se entrega un producto con mayor funcionalidad que el previo. El proceso se repite hasta alcanzar el software final completo. Siendo iterativo, con el modelo incremental se entrega un producto parcial pero completamente operacional en cada incremento, y no una parte que sea usada para reajustar los requisitos (como si ocurre en el modelo de construcción de prototipos). El enfoque incremental resulta muy útil cuando se dispone de baja dotación de personal para el desarrollo; también si no hay disponible fecha límite del proyecto por lo que se entregan versiones incompletas pero que proporcionan al usuario funcionalidad básica (y cada vez mayor). También es un modelo útil a los fines de versiones de evaluación. Nota: Puede ser considerado y útil, en cualquier momento o incremento incorporar temporalmente el paradigma MCP como complemento, teniendo así una mixtura de modelos que mejoran el esquema y desarrollo general. Ejemplo: Un procesador de texto que sea desarrollado bajo el paradigma incremental podría aportar, en principio, funciones básicas de edición de archivos y producción de documentos (algo como un editor simple). En un segundo incremento se le podría agregar edición más sofisticada, y de generación y mezcla de documentos. En un tercer incremento podría considerarse el agregado de funciones de corrección ortográfica, esquemas de paginado y plantillas; en un cuarto capacidades de dibujo propias y ecuaciones matemáticas. Así sucesivamente hasta llegar al procesador final requerido. Así, el producto va creciendo, acercándose a su meta final, pero desde la entrega del primer incremento ya es útil y funcional para el cliente, el cual observa una respuesta rápida en cuanto a entrega temprana; sin notar que la fecha límite del proyecto puede no estar acotada ni tan definida, lo que da margen de operación y alivia presiones al equipo de desarrollo. Como se dijo, el iterativo incremental es un modelo del tipo evolutivo, es decir donde se permiten y esperan probables cambios en los requisitos en tiempo de desarrollo; se admite cierto margen para que el software pueda evolucionar. Aplicable cuando los requisitos son medianamente bien conocidos pero no son completamente estáticos y definidos, cuestión esa que si es indispensable para poder utilizar un modelo Cascada. El modelo es aconsejable para el desarrollo de software en el cual se observe, en su etapa inicial de análisis, que posee áreas bastante bien definidas a cubrir, con suficiente independencia como para ser desarrolladas en etapas sucesivas. Tales áreas a cubrir suelen tener distintos grados de apremio por lo cual las mismas se deben priorizar en un análisis previo, es decir, definir cual será la primera, la segunda, y así sucesivamente; esto se conoce como «definición de los incrementos» con base en la priorización. Pueden no existir prioridades funcionales por parte del cliente, pero el desarrollador debe fijarlas de todos modos y con algún criterio, ya que basándose en ellas se desarrollarán y entregarán los distintos incrementos. El hecho de que existan incrementos funcionales del software lleva inmediatamente a pensar en un esquema de desarrollo modular, por tanto este modelo facilita tal paradigma de diseño. En resumen, un modelo incremental lleva a pensar en un desarrollo modular, con entregas parciales del producto software denominados «incrementos» del sistema, que son escogidos según prioridades predefinidas de algún modo. El modelo permite una implementación con refinamientos sucesivos (ampliación o mejora). Con cada incremento se agrega nueva funcionalidad o se cubren nuevos requisitos o bien se mejora la versión previamente implementada del producto software. Este modelo brinda cierta flexibilidad para que durante el desarrollo se incluyan cambios en los requisitos por parte del usuario, un cambio de requisitos propuesto y aprobado puede analizarse e implementarse como un nuevo incremento o, eventualmente, podrá constituir una mejora/adecuación de uno ya planeado. Aunque si se produce un cambio de requisitos por parte del cliente que afecte incrementos previos ya terminados (detección/incorporación tardía) se debe evaluar la factibilidad y realizar un acuerdo con el cliente, ya que puede impactar fuertemente en los costos. La selección de este modelo permite realizar entregas funcionales tempranas al cliente (lo cual es beneficioso tanto para él como para el grupo de desarrollo). Se priorizan las entregas de aquellos módulos o incrementos en que surja la necesidad operativa de hacerlo, por ejemplo para cargas previas de información, indispensable para los incrementos siguientes. El modelo iterativo incremental no obliga a especificar con precisión y detalle absolutamente todo lo que el sistema debe hacer, (y cómo), antes de ser construido (como el caso del cascada, con requisitos congelados). Solo se hace en el incremento en desarrollo. Esto torna más manejable el proceso y reduce el impacto en los costos. Esto es así, porque en caso de alterar o rehacer los requisitos, solo afecta una parte del sistema. Aunque, lógicamente, esta situación se agrava si se presenta en estado avanzado, es decir en los últimos incrementos. En definitiva, el modelo facilita la incorporación de nuevos requisitos durante el desarrollo. Con un paradigma incremental se reduce el tiempo de desarrollo inicial, ya que se implementa funcionalidad parcial. También provee un impacto ventajoso frente al cliente, que es la entrega temprana de partes operativas del software. El modelo proporciona todas las ventajas del modelo en cascada realimentado, reduciendo sus desventajas solo al ámbito de cada incremento. El modelo incremental no es recomendable para casos de sistemas de tiempo real, de alto nivel de seguridad, de procesamiento distribuido, o de alto índice de riesgos. Modelo espiral El modelo espiral fue propuesto inicialmente por Barry Boehm. Es un modelo evolutivo que conjuga la naturaleza iterativa del modelo MCP con los aspectos controlados y sistemáticos del Modelo Cascada. Proporciona potencial para desarrollo rápido de versiones incrementales. En el modelo espiral el software se construye en una serie de versiones incrementales. En las primeras iteraciones la versión incremental podría ser un modelo en papel o bien un prototipo. En las últimas iteraciones se producen versiones cada vez más completas del sistema diseñado. El modelo se divide en un número de Actividades de marco de trabajo, llamadas «regiones de tareas». En general existen entre tres y seis regiones de tareas (hay variantes del modelo). En la figura 6 se muestra el esquema de un modelo espiral con seis regiones. En este caso se explica una variante del modelo original de Boehm, expuesto en su tratado de 1988; en 1998 expuso un tratado más reciente. Figura 6: Modelo espiral para el ciclo de vida del software. Las regiones definidas en el modelo de la figura son: Región 1 , Tareas requeridas para establecer la comunicación entre el cliente y el desarrollador. Región 2 , Tareas inherentes a la definición de los recursos, tiempo y otra información relacionada con el proyecto. Región 3 , Tareas necesarias para evaluar los riesgos técnicos y de gestión del proyecto. Región 4 , Tareas para construir una o más representaciones de la aplicación software. Región 5 , Tareas para construir la aplicación, instalarla, probarla y proporcionar soporte al usuario o cliente (Ej. documentación y práctica). Región 6 , Tareas para obtener la reacción del cliente, según la evaluación de lo creado e instalado en los ciclos anteriores. Las actividades enunciadas para el marco de trabajo son generales y se aplican a cualquier proyecto, grande, mediano o pequeño, complejo o no. Las regiones que definen esas actividades comprenden un «conjunto de tareas» del trabajo: ese conjunto sí se debe adaptar a las características del proyecto en particular a emprender. Nótese que lo listado en los ítems de 1 a 6 son conjuntos de tareas, algunas de las ellas normalmente dependen del proyecto o desarrollo en sí. Proyectos pequeños requieren baja cantidad de tareas y también de formalidad. En proyectos mayores o críticos cada región de tareas contiene labores de más alto nivel de formalidad. En cualquier caso se aplican actividades de protección (por ejemplo, gestión de configuración del software, garantía de calidad, etc.). Al inicio del ciclo, o proceso evolutivo, el equipo de ingeniería gira alrededor del espiral (metafóricamente hablando) comenzando por el centro (marcado con ๑ en la figura 6) y en el sentido indicado; el primer circuito de la espiral puede producir el desarrollo de una especificación del producto; los pasos siguientes podrían generar un prototipo y progresivamente versiones más sofisticadas del software. Cada paso por la región de planificación provoca ajustes en el plan del proyecto; el coste y planificación se realimentan en función de la evaluación del cliente. El gestor de proyectos debe ajustar el número de iteraciones requeridas para completar el desarrollo. El modelo espiral puede ir adaptándose y aplicarse a lo largo de todo el Ciclo de vida del software (en el modelo clásico, o cascada, el proceso termina a la entrega del software). Una visión alternativa del modelo puede observarse examinando el «eje de punto de entrada de proyectos». Cada uno de los circulitos (๏) fijados a lo largo del eje representan puntos de arranque de los distintos proyectos (relacionados); a saber: Un proyecto de «desarrollo de conceptos» comienza al inicio de la espiral, hace múltiples iteraciones hasta que se completa, es la zona marcada con verde. Si lo anterior se va a desarrollar como producto real, se inicia otro proyecto: «Desarrollo de nuevo Producto». Que evolucionará con iteraciones hasta culminar; es la zona marcada en color azul. Eventual y análogamente se generarán proyectos de «mejoras de productos» y de «mantenimiento de productos», con las iteraciones necesarias en cada área (zonas roja y gris, respectivamente). Cuando la espiral se caracteriza de esta forma, está operativa hasta que el software se retira, eventualmente puede estar inactiva (el proceso), pero cuando se produce un cambio el proceso arranca nuevamente en el punto de entrada apropiado (por ejemplo, en «mejora del producto»). El modelo espiral da un enfoque realista, que evoluciona igual que el software; se adapta muy bien para desarrollos a gran escala. El Espiral utiliza el MCP para reducir riesgos y permite aplicarlo en cualquier etapa de la evolución. Mantiene el enfoque clásico (cascada) pero incorpora un marco de trabajo iterativo que refleja mejor la realidad. Este modelo requiere considerar riesgos técnicos en todas las etapas del proyecto; aplicado adecuadamente debe reducirlos antes de que sean un verdadero problema. El Modelo evolutivo como el Espiral es particularmente apto para el desarrollo de Sistemas Operativos (complejos); también en sistemas de altos riesgos o críticos (Ej. navegadores y controladores aeronáuticos) y en todos aquellos en que sea necesaria una fuerte gestión del proyecto y sus riesgos, técnicos o de gestión. Desventajas importantes: Requiere mucha experiencia y habilidad para la evaluación de los riesgos, lo cual es requisito para el éxito del proyecto. Es difícil convencer a los grandes clientes que se podrá controlar este enfoque evolutivo. Este modelo no se ha usado tanto, como el Cascada (Incremental) o MCP, por lo que no se tiene bien medida su eficacia, es un paradigma relativamente nuevo y difícil de implementar y controlar. Modelo espiral Win & Win Una variante interesante del Modelo Espiral previamente visto (Figura 6) es el «Modelo espiral Win-Win» (Barry Boehm). El Modelo Espiral previo (clásico) sugiere la comunicación con el cliente para fijar los requisitos, en que simplemente se pregunta al cliente qué necesita y él proporciona la información para continuar; pero esto es en un contexto ideal que rara vez ocurre. Normalmente cliente y desarrollador entran en una negociación, se negocia coste frente a funcionalidad, rendimiento, calidad, etc. «Es así que la obtención de requisitos requiere una negociación, que tiene éxito cuando ambas partes ganan». Las mejores negociaciones se fuerzan en obtener «Victoria & Victoria» (Win & Win), es decir que el cliente gane obteniendo el producto que lo satisfaga, y el desarrollador también gane consiguiendo presupuesto y fecha de entrega realista. Evidentemente, este modelo requiere fuertes habilidades de negociación. El modelo Win-Win define un conjunto de actividades de negociación al principio de cada paso alrededor de la espiral; se definen las siguientes actividades: Identificación del sistema o subsistemas clave de los directivos * (saber qué quieren). Determinación de «condiciones de victoria» de los directivos (saber qué necesitan y los satisface). Negociación de las condiciones «victoria» de los directivos para obtener condiciones «Victoria & Victoria» (negociar para que ambos ganen). * Directivo: Cliente escogido con interés directo en el producto, que puede ser premiado por la organización si tiene éxito o criticado si no. El modelo Win & Win hace énfasis en la negociación inicial, también introduce 3 hitos en el proceso llamados «puntos de fijación», que ayudan a establecer la completitud de un ciclo de la espiral, y proporcionan hitos de decisión antes de continuar el proyecto de desarrollo del software. Etapas en el desarrollo del software Captura, análisis y especificación de requisitos Al inicio de un desarrollo (no de un proyecto), esta es la primera fase que se realiza, y, según el modelo de proceso adoptado, puede casi terminar para pasar a la próxima etapa (caso de Modelo Cascada Realimentado) o puede hacerse parcialmente para luego retomarla (caso Modelo Iterativo Incremental u otros de carácter evolutivo). En simple palabras y básicamente, durante esta fase, se adquieren, reúnen y especifican las características funcionales y no funcionales que deberá cumplir el futuro programa o sistema a desarrollar. Las bondades de las características, tanto del sistema o programa a desarrollar, como de su entorno, parámetros no funcionales y arquitectura dependen enormemente de lo bien lograda que esté esta etapa. Esta es, probablemente, la de mayor importancia y una de las fases más difíciles de lograr certeramente, pues no es automatizable, no es muy técnica y depende en gran medida de la habilidad y experiencia del analista que la realice. Involucra fuertemente al usuario o cliente del sistema, por tanto tiene matices muy subjetivos y es difícil de modelar con certeza o aplicar una técnica que sea «la más cercana a la adecuada» (de hecho no existe «la estrictamente adecuada»). Si bien se han ideado varias metodologías, incluso software de apoyo, para captura, elicitación y registro de requisitos, no existe una forma infalible o absolutamente confiable, y deben aplicarse conjuntamente buenos criterios y mucho sentido común por parte del o los analistas encargados de la tarea; es fundamental también lograr una fluida y adecuada comunicación y comprensión con el usuario final o cliente del sistema. El artefacto más importante resultado de la culminación de esta etapa es lo que se conoce como especificación de requisitos software o simplemente documento ERS. Como se dijo, la habilidad del analista para interactuar con el cliente es fundamental; lo común es que el cliente tenga un objetivo general o problema que resolver, no conoce en absoluto el área (informática), ni su jerga, ni siquiera sabe con precisión qué debería hacer el producto software (qué y cuantas funciones) ni, mucho menos, cómo debe operar. En otros casos menos frecuentes, el cliente «piensa» que sabe precisamente lo que el software tiene que hacer, y generalmente acierta muy parcialmente, pero su empecinamiento entorpece la tarea de elicitación. El analista debe tener la capacidad para lidiar con este tipo de problemas, que incluyen relaciones humanas; tiene que saber ponerse al nivel del usuario para permitir una adecuada comunicación y comprensión. Escasas son las situaciones en que el cliente sabe con certeza e incluso con completitud lo que requiere de su futuro sistema, este es el caso más sencillo para el analista. Las tareas relativas a captura, elicitación, modelado y registro de requisitos, además de ser sumamente importante, puede llegar a ser dificultosa de lograr acertadamente y llevar bastante tiempo relativo al proceso total del desarrollo; al proceso y metodologías para llevar a cabo este conjunto de actividades normalmente se las asume parte propia de la ingeniería de software, pero dada la antedicha complejidad, actualmente se habla de una ingeniería de requisitos, aunque ella aún no existe formalmente. Hay grupos de estudio e investigación, en todo el mundo, que están exclusivamente abocados a idear modelos, técnicas y procesos para intentar lograr la correcta captura, análisis y registro de requisitos. Estos grupos son los que normalmente hablan de la ingeniería de requisitos; es decir se plantea esta como un área o disciplina pero no como una carrera universitaria en sí misma. Algunos requisitos no necesitan la presencia del cliente, para ser capturados o analizados; en ciertos casos los puede proponer el mismo analista o, incluso, adoptar unilateralmente decisiones que considera adecuadas (tanto en requisitos funcionales como no funcionales). Por citar ejemplos probables: Algunos requisitos sobre la arquitectura del sistema, requisitos no funcionales tales como los relativos al rendimiento, nivel de soporte a errores operativos, plataformas de desarrollo, relaciones internas o ligas entre la información (entre registros o tablas de datos) a almacenar en caso de bases o bancos de datos, etc. Algunos funcionales tales como opciones secundarias o de soporte necesarias para una mejor o más sencilla operatividad; etc. La obtención de especificaciones a partir del cliente (u otros actores intervinientes) es un proceso humano muy interactivo e iterativo; normalmente a medida que se captura la información, se la analiza y realimenta con el cliente, refinándola, puliéndola y corrigiendo si es necesario; cualquiera sea el método de ERS utilizado. El analista siempre debe llegar a conocer la temática y el problema que resolver, dominarlo, hasta cierto punto, hasta el ámbito que el futuro sistema a desarrollar lo abarque. Por ello el analista debe tener alta capacidad para comprender problemas de muy diversas áreas o disciplinas de trabajo (que no son específicamente suyas); así por ejemplo, si el sistema a desarrollar será para gestionar información de una aseguradora y sus sucursales remotas, el analista se debe compenetrar en cómo ella trabaja y maneja su información, desde niveles muy bajos e incluso llegando hasta los gerenciales. Dada a gran diversidad de campos a cubrir, los analistas suelen ser asistidos por especialistas, es decir gente que conoce profundamente el área para la cual se desarrollará el software; evidentemente una única persona (el analista) no puede abarcar tan vasta cantidad de áreas del conocimiento. En empresas grandes de desarrollo de productos software, es común tener analistas especializados en ciertas áreas de trabajo. Contrariamente, no es problema del cliente, es decir él no tiene por qué saber nada de software, ni de diseños, ni otras cosas relacionadas; solo se debe limitar a aportar objetivos, datos e información (de mano propia o de sus registros, equipos, empleados, etc) al analista, y guiado por él, para que, en primera instancia, defina el «Universo de Discurso», y con posterior trabajo logre confeccionar el adecuado documento ERS. Es bien conocida la presión que sufren los desarrolladores de sistemas informáticos para comprender y rescatar las necesidades de los clientes/usuarios. Cuanto más complejo es el contexto del problema más difícil es lograrlo, a veces se fuerza a los desarrolladores a tener que convertirse en casi expertos de los dominios que analizan. Cuando esto no sucede es muy probable que se genere un conjunto de requisitos erróneos o incompletos y por lo tanto un producto de software con alto grado de desaprobación por parte de los clientes/usuarios y un altísimo costo de reingeniería y mantenimiento. Todo aquello que no se detecte, o resulte mal entendido en la etapa inicial provocará un fuerte impacto negativo en los requisitos, propagando esta corriente degradante a lo largo de todo el proceso de desarrollo e incrementando su perjuicio cuanto más tardía sea su detección (Bell y Thayer 1976)(Davis 1993). Procesos, modelado y formas de elicitación de requisitos Siendo que la captura, elicitación y especificación de requisitos, es una parte crucial en el proceso de desarrollo de software, ya que de esta etapa depende el logro de los objetivos finales previstos, se han ideado modelos y diversas metodologías de trabajo para estos fines. También existen herramientas software que apoyan las tareas relativas realizadas por el ingeniero en requisitos. El estándar IEEE 830-1998 brinda una normalización de las «Prácticas recomendadas para la especificación de requisitos software». A medida que se obtienen los requisitos, normalmente se los va analizando, el resultado de este análisis, con o sin el cliente, se plasma en un documento, conocido como ERS o Especificación de requisitos software, cuya estructura puede venir definida por varios estándares, tales como CMMI. Un primer paso para realizar el relevamiento de información es el conocimiento y definición acertada lo que se conoce como «Universo de Discurso» del problema, que se define y entiende por: Universo de Discurso (UdeD): es el contexto general en el cual el software deberá ser desarrollado y deberá operar. El UdeD incluye todas las fuentes de información y todas las personas relacionadas con el software. Esas personas son conocidas también como actores de ese universo. El UdeD es la realidad circunstanciada por el conjunto de objetivos definidos por quienes demandaron el software. A partir de la extracción y análisis de información en su ámbito se obtienen todas las especificaciones necesarias y tipos de requisitos para el futuro producto software. El objetivo de la ingeniería de requisitos (IR) es sistematizar el proceso de definición de requisitos permitiendo elicitar, modelar y analizar el problema, generando un compromiso entre los ingenieros de requisitos y los clientes/usuarios, ya que ambos participan en la generación y definición de los requisitos del sistema. La IR aporta un conjunto de métodos, técnicas y herramientas que asisten a los ingenieros de requisitos (analistas) para obtener requisitos lo más seguros, veraces, completos y oportunos posibles, permitiendo básicamente: Comprender el problema Facilitar la obtención de las necesidades del cliente/usuario Validar con el cliente/usuario Garantizar las especificaciones de requisitos Si bien existen diversas formas, modelos y metodologías para elicitar, definir y documentar requisitos, no se puede decir que alguna de ellas sea mejor o peor que la otra, suelen tener muchísimo en común, y todas cumplen el mismo objetivo. Sin embargo, lo que si se puede decir sin dudas es que es indispensable utilizar alguna de ellas para documentar las especificaciones del futuro producto software. Así por ejemplo, hay un grupo de investigación argentino que desde hace varios años ha propuesto y estudia el uso del LEL (Léxico Extendido del Lenguaje) y Escenarios como metodología, aquí se presenta una de las tantas referencias y bibliografía sobre ello. Otra forma, más ortodoxa, de capturar y documentar requisitos se puede obtener en detalle, por ejemplo, en el trabajo de la Universidad de Sevilla sobre «Metodología para el Análisis de Requisitos de Sistemas Software». En la Figura 7 se muestra un esquema, más o menos riguroso, aunque no detallado, de los pasos y tareas a seguir para realizar la captura, análisis y especificación de requisitos software. También allí se observa qué artefacto o documento se obtiene en cada etapa del proceso. En el diagrama no se explicita metodología o modelo a utilizar, sencillamente se pautan las tareas que deben cumplirse, de alguna manera. Figura 7: Diagrama de tareas para captura y análisis de requisitos. Una posible lista, general y ordenada, de tareas recomendadas para obtener la definición de lo que se debe realizar, los productos a obtener y las técnicas a emplear durante la actividad de elicitación de requisitos, en fase de Especificación de requisitos software es: Obtener información sobre el dominio del problema y el sistema actual (UdeD). Preparar y realizar las reuniones para elicitación/negociación. Identificar/revisar los objetivos del usuario. Identificar/revisar los objetivos del sistema. Identificar/revisar los requisitos de información. Identificar/revisar los requisitos funcionales. Identificar/revisar los requisitos no funcionales. Priorizar objetivos y requisitos. Algunos principios básicos a tener en cuenta: Presentar y entender cabalmente el dominio de la información del problema. Definir correctamente las funciones que debe realizar el software. Representar el comportamiento del software a consecuencias de acontecimientos externos, particulares, incluso inesperados. Reconocer requisitos incompletos, ambiguos o contradictorios. Dividir claramente los modelos que representan la información, las funciones y comportamiento y características no funcionales. Clasificación e identificación de requisitos Se pueden identificar dos formas de requisitos: Requisitos de usuario: Los requisitos de usuario son frases en lenguaje natural junto a diagramas con los servicios que el sistema debe proporcionar, así como las restricciones bajo las que debe operar. Requisitos de sistema: Los requisitos de sistema determinan los servicios del sistema y pero con las restricciones en detalle. Sirven como contrato. Es decir, ambos son lo mismo, pero con distinto nivel de detalle. Ejemplo de requisito de usuario: El sistema debe hacer préstamos. Ejemplo de requisito de sistema: Función préstamo: entrada código socio, código ejemplar; salida: fecha devolución; etc. Se clasifican en tres los tipos de requisitos de sistema: Requisitos funcionales Los requisitos funcionales describen: Los servicios que proporciona el sistema (funciones). La respuesta del sistema ante determinadas entradas. El comportamiento del sistema en situaciones particulares. Requisitos no funcionales Los requisitos no funcionales son restricciones de los servicios o funciones que ofrece el sistema (ej. cotas de tiempo, proceso de desarrollo, rendimiento, etc.) Ejemplo 1. La biblioteca Central debe ser capaz de atender simultáneamente a todas las bibliotecas de la Universidad Ejemplo 2. El tiempo de respuesta a una consulta remota no debe ser superior a 1/2 s A su vez, hay tres tipos de requisitos no funcionales: Requisitos del producto. Especifican el comportamiento del producto (Ej. prestaciones, memoria, tasa de fallos, etc.) Requisitos organizativos. Se derivan de las políticas y procedimientos de las organizaciones de los clientes y desarrolladores (Ej. estándares de proceso, lenguajes de programación, etc.) Requisitos externos. Se derivan de factores externos al sistema y al proceso de desarrollo (Ej. requisitos legislativos, éticos, etc.) Requisitos del dominio. Los requisitos del dominio se derivan del dominio de la aplicación y reflejan características de dicho dominio. Pueden ser funcionales o no funcionales. Ej. El sistema de biblioteca de la Universidad debe ser capaz de exportar datos mediante el Lenguaje de Intercomunicación de Bibliotecas de España (LIBE). Ej. El sistema de biblioteca no podrá acceder a bibliotecas con material censurado. Diseño del sistema En ingeniería de software, el diseño es una fase de ciclo de vida del software. Se basa en la especificación de requisitos producido por el análisis de los requisitos (fase de análisis), el diseño define cómo estos requisitos se cumplirán, la estructura que debe darse al sistema de software para que se haga realidad. El diseño sigue siendo una fase separada de la programación o codificación, esta última corresponde a la traducción en un determinado lenguaje de programación de las premisas adoptadas en el diseño. Las distinciones entre las actividades mencionadas hasta ahora no siempre son claras cómo se quisiera en las teorías clásicas de ingeniería de software. El diseño, en particular, puede describir el funcionamiento interno de un sistema en diferentes niveles de detalle, cada una de ellos se coloca en una posición intermedia entre el análisis y codificación. Normalmente se entiende por diseño de la arquitectura al diseño de muy alto nivel, que solo define la estructura del sistema en términos de la módulos de software de que se compone y las relaciones macroscópicas entre ellos. A este nivel de diseño pertenecen fórmulas como cliente-servidor o “tres niveles”, o, más generalmente, las decisiones sobre el uso de la arquitectura de hardware especial que se utilice, el sistema operativo, DBMS, Protocolos de red, etc. Un nivel intermedio de detalle puede definir la descomposición del sistema en módulos, pero esta vez con una referencia más o menos explícita al modo de descomposición que ofrece el particular lenguaje de programación con el que el desarrollo se va a implementar, por ejemplo, en un diseño realizado con la tecnología de objetos, el proyecto podría describir al sistema en términos de clases y sus interrelaciones. El diseño detallado, por último, es una descripción del sistema muy cercana a la codificación (por ejemplo, describir no solo las clases en abstracto, sino también sus atributos y los métodos con sus tipos). Debido a la naturaleza intangible del software, y dependiendo de las herramientas que se utilizan en el proceso, la frontera entre el diseño y la codificación también puede ser virtualmente imposible de identificar. Por ejemplo, algunas herramientas CASE son capaces de generar código a partir de diagramas UML, los que describen gráficamente la estructura de un sistema software. Codificación del software Durante esta etapa se realizan las tareas que comúnmente se conocen como programación; que consiste, esencialmente, en llevar a código fuente, en el lenguaje de programación elegido, todo lo diseñado en la fase anterior. Esta tarea la realiza el programador, siguiendo por completo los lineamientos impuestos en el diseño y en consideración siempre a los requisitos funcionales y no funcionales (ERS) especificados en la primera etapa. Es común pensar que la etapa de programación o codificación (algunos la llaman implementación) es la que insume la mayor parte del trabajo de desarrollo del software; sin embargo, esto puede ser relativo (y generalmente aplicable a sistemas de pequeño porte) ya que las etapas previas son cruciales, críticas y pueden llevar bastante más tiempo. Se suele hacer estimaciones de un 30 % del tiempo total insumido en la programación, pero esta cifra no es consistente ya que depende en gran medida de las características del sistema, su criticidad y el lenguaje de programación elegido. En tanto menor es el nivel del lenguaje mayor será el tiempo de programación requerido, así por ejemplo se tardaría más tiempo en codificar un algoritmo en lenguaje ensamblador que el mismo programado en lenguaje C. Mientras se programa la aplicación, sistema, o software en general, se realizan también tareas de depuración, esto es la labor de ir liberando al código de los errores factibles de ser hallados en esta fase (de semántica, sintáctica y lógica). Hay una suerte de solapamiento con la fase siguiente, ya que para depurar la lógica es necesario realizar pruebas unitarias, normalmente con datos de prueba; claro es que no todos los errores serán encontrados solo en la etapa de programación, habrá otros que se encontrarán durante las etapas subsiguientes. La aparición de algún error funcional (mala respuesta a los requisitos) tarde o temprano puede llevar a retornar a la fase de diseño antes de continuar la codificación. Durante la fase de programación, el código puede adoptar varios estados, dependiendo de la forma de trabajo y del lenguaje elegido, a saber: Código fuente: es el escrito directamente por los programadores en editores de texto, lo cual genera el programa. Contiene el conjunto de instrucciones codificadas en algún lenguaje de alto nivel. Puede estar distribuido en paquetes, procedimientos, bibliotecas fuente, etc. Código objeto: es el código binario o intermedio resultante de procesar con un compilador el código fuente. Consiste en una traducción completa y de una sola vez de este último. El código objeto no es inteligible por el ser humano (normalmente es formato binario) pero tampoco es directamente ejecutable por la computadora. Se trata de una representación intermedia entre el código fuente y el código ejecutable, a los fines de un enlace final con las rutinas de biblioteca y entre procedimientos o bien para su uso con un pequeño intérprete intermedio [a modo de distintos ejemplos véase EUPHORIA, (intérprete intermedio), FORTRAN (compilador puro) MSIL (Microsoft Intermediate Language) (intérprete) y BASIC (intérprete puro, intérprete intermedio, compilador intermedio o compilador puro, depende de la versión utilizada)]. El código objeto no existe si el programador trabaja con un lenguaje a modo de intérprete puro, en este caso el mismo intérprete se encarga de traducir y ejecutar línea por línea el código fuente (de acuerdo al flujo del programa), en tiempo de ejecución. En este caso tampoco existe el o los archivos de código ejecutable. Una desventaja de esta modalidad es que la ejecución del programa o sistema es un poco más lenta que si se hiciera con un intérprete intermedio, y bastante más lenta que si existe el o los archivos de código ejecutable. Es decir no favorece el rendimiento en velocidad de ejecución. Pero una gran ventaja de la modalidad intérprete puro, es que él está forma de trabajo facilita enormemente la tarea de depuración del código fuente (frente a la alternativa de hacerlo con un compilador puro). Frecuentemente se suele usar una forma mixta de trabajo (si el lenguaje de programación elegido lo permite), es decir inicialmente trabajar a modo de intérprete puro, y una vez depurado el código fuente (liberado de errores) se utiliza un compilador del mismo lenguaje para obtener el código ejecutable completo, con lo cual se agiliza la depuración y la velocidad de ejecución se optimiza. Código ejecutable: Es el código binario resultado de enlazar uno o más fragmentos de código objeto con las rutinas y bibliotecas necesarias. Constituye uno o más archivos binarios con un formato tal que el sistema operativo es capaz de cargarlo en la memoria RAM (eventualmente también parte en una memoria virtual), y proceder a su ejecución directa. Por lo anterior se dice que el código ejecutable es directamente «inteligible por la computadora». El código ejecutable, también conocido como código máquina, no existe si se programa con modalidad de «intérprete puro». Pruebas (unitarias y de integración) Entre las diversas pruebas que se le efectúan al software se pueden distinguir principalmente: Prueba unitarias: Consisten en probar o testear piezas de software pequeñas; a nivel de secciones, procedimientos, funciones y módulos; aquellas que tengan funcionalidades específicas. Dichas pruebas se utilizan para asegurar el correcto funcionamiento de secciones de código, mucho más reducidas que el conjunto, y que tienen funciones concretas con cierto grado de independencia. Pruebas de integración: Se realizan una vez que las pruebas unitarias fueron concluidas exitosamente; con éstas se intenta asegurar que el sistema completo, incluso los subsistemas que componen las piezas individuales grandes del software funcionen correctamente al operar e interoperar en conjunto. Las pruebas normalmente se efectúan con los llamados datos de prueba, que es un conjunto seleccionado de datos típicos a los que puede verse sometido el sistema, los módulos o los bloques de código. También se escogen: Datos que llevan a condiciones límites al software a fin de probar su tolerancia y robustez; datos de utilidad para mediciones de rendimiento; datos que provocan condiciones eventuales o particulares poco comunes y a las que el software normalmente no estará sometido pero pueden ocurrir; etc. Los «datos de prueba» no necesariamente son ficticios o «creados», pero normalmente sí lo son los de poca probabilidad de ocurrencia. Generalmente, existe un fase probatoria final y completa del software, llamada Beta Test, durante la cual el sistema instalado en condiciones normales de operación y trabajo es probado exhaustivamente a fin de encontrar errores, inestabilidades, respuestas erróneas, etc. que hayan pasado los previos controles. Estas son normalmente realizadas por personal idóneo contratado o afectado específicamente a ello. Los posibles errores encontrados se transmiten a los desarrolladores para su depuración. En el caso de software de desarrollo «a pedido», el usuario final (cliente) es el que realiza el Beta Test, teniendo para ello un período de prueba pactado con el desarrollador. Instalación y paso a producción La instalación del software es el proceso por el cual los programas desarrollados son transferidos apropiadamente al computador destino, inicializados, y, finalmente, configurados; todo ello con el propósito de ser ya utilizados por el usuario final. Constituye la etapa final en el desarrollo propiamente dicho del software. Luego de esta el producto entrará en la fase de funcionamiento y producción, para el que fuera diseñado. La instalación, dependiendo del sistema desarrollado, puede consistir en una simple copia al disco rígido destino (casos raros actualmente); o bien, más comúnmente, con una de complejidad intermedia en la que los distintos archivos componentes del software (ejecutables, bibliotecas, datos propios, etc.) son descomprimidos y copiados a lugares específicos preestablecidos del disco; incluso se crean vínculos con otros productos, además del propio sistema operativo. Este último caso, comúnmente es un proceso bastante automático que es creado y guiado con herramientas software específicas (empaquetado y distribución, instaladores). En productos de mayor complejidad, la segunda alternativa es la utilizada, pero es realizada o guiada por especialistas; puede incluso requerirse la instalación en varios y distintos computadores (instalación distribuida). También, en software de mediana y alta complejidad normalmente es requerido un proceso de configuración y chequeo, por el cual se asignan adecuados parámetros de funcionamiento y se testea la operatividad funcional del producto. En productos de venta masiva las instalaciones completas, si son relativamente simples, suelen ser realizadas por los propios usuarios finales (tales como sistemas operativos, paquetes de oficina, utilitarios, etc.) con herramientas propias de instalación guiada; incluso la configuración suele ser automática. En productos de diseño específico o «a medida» la instalación queda restringida, normalmente, a personas especialistas involucradas en el desarrollo del software en cuestión. Una vez realizada exitosamente la instalación del software, el mismo pasa a la fase de producción (operatividad), durante la cual cumple las funciones para las que fue desarrollado, es decir, es finalmente utilizado por el (o los) usuario final, produciendo los resultados esperados. Mantenimiento El mantenimiento de software es el proceso de control, mejora y optimización del software ya desarrollado e instalado, que también incluye depuración de errores y defectos que puedan haberse filtrado de la fase de pruebas de control y beta test. Esta fase es la última (antes de iterar, según el modelo empleado) que se aplica al ciclo de vida del desarrollo de software. La fase de mantenimiento es la que viene después de que el software está operativo y en producción. De un buen diseño y documentación del desarrollo dependerá cómo será la fase de mantenimiento, tanto en costo temporal como monetario. Modificaciones realizadas a un software que fue elaborado con una documentación indebida o pobre y mal diseño puede llegar a ser tanto o más costosa que desarrollar el software desde el inicio. Por ello, es de fundamental importancia respetar debidamente todas las tareas de las fases del desarrollo y mantener adecuada y completa la documentación. El período de la fase de mantenimiento es normalmente el mayor en todo el ciclo de vida. Esta fase involucra también actualizaciones y evoluciones del software; no necesariamente implica que el sistema tuvo errores. Uno o más cambios en el software, por ejemplo de adaptación o evolutivos, puede llevar incluso a rever y adaptar desde parte de las primeras fases del desarrollo inicial, alterando todas las demás; dependiendo de cuán profundos sean los cambios. El modelo cascada común es particularmente costoso en mantenimiento, ya que su rigidez implica que cualquier cambio provoca regreso a fase inicial y fuertes alteraciones en las demás fases del ciclo de vida. Durante el período de mantenimiento, es común que surjan nuevas revisiones y versiones del producto; que lo liberan más depurado, con mayor y mejor funcionalidad, mejor rendimiento, etc. Varias son las facetas que pueden ser alteradas para provocar cambios deseables, evolutivos, adaptaciones o ampliaciones y mejoras. Básicamente se tienen los siguientes tipos de cambios: Perfectivos: Aquellos que llevan a una mejora de la calidad interna del software en cualquier aspecto: Reestructuración del código, definición más clara del sistema y su documentación; optimización del rendimiento y eficiencia. Evolutivos: Agregados, modificaciones, incluso eliminaciones, necesarias en el software para cubrir su expansión o cambio, según las necesidades del usuario. Adaptivos: Modificaciones que afectan a los entornos en los que el sistema opera, tales como: Cambios de configuración del hardware (por actualización o mejora de componentes electrónicos), cambios en el software de base, en gestores de base de datos, en comunicaciones, etc. Correctivos: Alteraciones necesarias para corregir errores de cualquier tipo en el producto de software desarrollado. Carácter evolutivo del software El software es el producto derivado del proceso de desarrollo, según la ingeniería de software. Este producto es intrínsecamente evolutivo durante su ciclo de vida: en general, evoluciona generando versiones cada vez más completas, complejas, mejoradas, optimizadas en algún aspecto, adecuadas a nuevas plataformas (sean de hardware o sistemas operativos), etc. Cuando un sistema deja de evolucionar, tarde o temprano cumplirá con su ciclo de vida, entrará en obsolescencia e inevitablemente, tarde o temprano, será reemplazado por un producto nuevo. El software evoluciona sencillamente porque se debe adaptar a los cambios del entorno, sean funcionales (exigencias de usuarios), operativos, de plataforma o arquitectura hardware. La dinámica de evolución del software es el estudio de los cambios del sistema. La mayor contribución en esta área fue realizada por Meir M. Lehman y Belady, comenzando en los años 70 y 80. Su trabajo continuó en la década de 1990, con Lehman y otros investigadores de relevancia en la realimentación en los procesos de evolución (Lehman, 1996; Lehman et al., 1998; lehman et al., 2001). A partir de esos estudios propusieron un conjunto de leyes (conocidas como leyes de Lehman) respecto de los cambios producidos en los sistemas. Estas leyes (en realidad son hipótesis) son invariantes y ampliamente aplicables. Lehman y Belady analizaron el crecimiento y la evolución de varios sistemas software de gran porte; derivando finalmente, según sus medidas, las siguientes ocho leyes: Cambio continuo: Un programa que se usa en un entorno real necesariamente debe cambiar o se volverá progresivamente menos útil en ese entorno. Complejidad creciente: A medida que un programa en evolución cambia, su estructura tiende a ser cada vez más compleja. Se deben dedicar recursos extras para preservar y simplificar la estructura. Evolución prolongada del programa: La evolución de los programas es un proceso autorregulativo. Los atributos de los sistemas, tales como tamaño, tiempo entre entregas y la cantidad de errores documentados son aproximadamente invariantes para cada entrega del sistema. Estabilidad organizacional: Durante el tiempo de vida de un programa, su velocidad de desarrollo es aproximadamente constante e independiente de los recursos dedicados al desarrollo del sistema. Conservación de la familiaridad: Durante el tiempo de vida de un sistema, el cambio incremental en cada entrega es aproximadamente constante. Crecimiento continuado: La funcionalidad ofrecida por los sistemas tiene que crecer continuamente para mantener la satisfacción de los usuarios. Decremento de la calidad: La calidad de los sistemas software comenzará a disminuir a menos que dichos sistemas se adapten a los cambios de su entorno de funcionamiento. Realimentación del sistema: Los procesos de evolución incorporan sistemas de realimentación multiagente y multibucle y estos deben ser tratados como sistemas de realimentación para lograr una mejora significativa del producto."

ksampletext_wikipedia_anth_antropologia: str = "Antropología. La antropología es la ciencia que estudia al ser humano de una forma integral, en sus características físicas como animales y de su cultura. Para abarcar la materia de su estudio, la antropología recurre a herramientas y conocimientos producidos por las ciencias sociales y las ciencias naturales. La aspiración de la disciplina antropológica es producir conocimiento sobre el ser humano en diversas esferas, intentando abarcar tanto las estructuras sociales de la actualidad, la evolución biológica de nuestra especie, el desarrollo y los modos de vida de pueblos que han desaparecido y la diversidad de expresiones culturales y lingüísticas que caracterizan a la humanidad. Las facetas diversas del ser humano llevaron a una especialización de los campos de la antropología. Cada uno de los campos de estudio del ser humano implicó el desarrollo de disciplinas que mantienen constante diálogo entre ellas. Se trata de la antropología física, la arqueología, la lingüística y la antropología social. Con mucha frecuencia, el término «antropología» solo se aplica a esta última, que a su vez se ha diversificado en numerosas ramas, dependiendo de la orientación teórica, la materia de su estudio o bien, como resultado de la interacción entre la antropología social y otras disciplinas. La antropología se constituyó como disciplina independiente durante la segunda mitad del siglo XIX. Uno de los factores que favoreció su aparición fue la difusión de la teoría de la evolución, que en el campo de los estudios sobre la sociedad dio origen al evolucionismo social, entre cuyos principales autores se encuentra Herbert Spencer. Los primeros antropólogos pensaban que así como las especies evolucionaban de organismos sencillos a otros más complejos, las sociedades y las culturas de los humanos debían seguir el mismo proceso de evolución hasta producir estructuras complejas como su propia sociedad. Varios de los antropólogos pioneros eran abogados de profesión, de modo que las cuestiones jurídicas aparecieron frecuentemente como tema central de sus obras. A esta época corresponde el descubrimiento de los sistemas de parentesco por parte de Lewis Henry Morgan. Desde el final del siglo XIX el enfoque adoptado por los primeros antropólogos fue puesto en tela de juicio por las siguientes generaciones. Después de la crítica de Franz Boas a la antropología evolucionista del siglo XIX, la mayor parte de las teorías producidas por los antropólogos de la primera generación se considera obsoleta. A partir de entonces, la antropología vio la aparición de varias corrientes durante el siglo XIX y el XX, entre ellas la escuela culturalista de Estados Unidos, el estructural-funcionalismo, el estructuralismo antropológico, la antropología marxista, el procesualismo, el indigenismo, etc. La antropología es, sobre todo, una ciencia integradora que estudia al ser humano en el marco de la sociedad y cultura a las que pertenece, y, al mismo tiempo, como producto de estas. Se le puede definir como la ciencia que se ocupa de estudiar el origen y desarrollo de toda la gama de la variabilidad humana y los modos de comportamientos sociales a través del tiempo y el espacio; es decir, del proceso biosocial de la existencia de la especie humana. Antecedentes Fruto de la meticulosa investigación de Bernardino de Sahagún es el Códice Florentino. Se considera como antecedente de la etnografía. En la imagen, un folio de esta obra escrito en náhuatl. Se atribuye al explorador François Péron haber sido quien usó por primera ocasión el término antropología. Péron recogió en esa obra un conjunto de datos sobre los aborígenes de Tasmania, que fueron casi exterminados en los años que siguieron al paso de Péron por la isla. Sin embargo, Péron no fue el primero ni el más antiguo de quienes estaban interesados en la cuestión de la diversidad humana y sus manifestaciones. El estudio del ser humano es muy antiguo. Heródoto (484-425 a. C.) en sus Historias nos cuenta las diferencias entre los distintos habitantes del mundo (Libia, Egipto, Grecia, Asia Menor), y nos habla de las diferencias de cráneo entre egipcios y persas. Hipócrates (460-377 a. C.) lanza la teoría de que el medio influye en los caracteres físicos del ser humano, y llama la atención sobre las diferencias de quienes habitan climas distintos. Aristóteles (384-322 a. C.) estudia al ser humano por ser el animal más complejo. Llama la atención sobre el tamaño de su cráneo, mucho mayor que en el resto de animales, así como sobre su carácter bípedo y que es el único animal capaz de deliberar y reflexionar. Define al hombre como zoon politikón o «animal político». Algunos autores consideran a fray Bernardino de Sahagún como uno de los antecedentes más notables de la etnografía. De la misma manera que otros misioneros del siglo XVI, Sahagún estaba preocupado por las diversas maneras en que la religión de los indígenas podría confundirse con el cristianismo recién implantado. En el afán de comprender mejor a los pueblos nahuas del centro de Nueva España, Sahagún investigó de manera muy detallada la historia, las costumbres y las creencias de los nahuas antes de la llegada de los españoles. Para hacerlo tuvo que aprender náhuatl. Luego, con el apoyo de algunos de sus informantes, organizó la información obtenida en una obra pensada para un público más o menos amplio. El resultado fue el Códice Florentino, de vital importancia en el conocimiento de la civilización mesoamericana precolombina. Georges-Louis Leclerc de Buffon, naturalista quien escribió Histoire Naturelle (1749), enlaza las ciencias naturales y la diversidad física de la especie humana (anatomía comparada) con la inquietud por comprender la diversidad de las expresiones culturales de los pueblos. De manera análoga, algunos pensadores de la Ilustración como Montesquieu, Rousseau e incluso el matemático DAlembert abordaron la materia, y propusieron algunas hipótesis sobre el origen de las relaciones sociales, las formas de gobierno y los temperamentos de las naciones. Historia Esta sección es un extracto de Historia de la antropología. Durante el siglo XIX, la llamada entonces «antropología general» incluida un amplísimo espectro de intereses, desde la paleontología del cuaternario al folclor europeo, pasando por el estudio comparado de los pueblos aborígenes. Fue por ello una rama de la Historia Natural y del historicismo cultural alemán que se propuso el estudio científico de la historia de la diversidad humana. Tras la aparición de los modelos evolucionistas y el desarrollo del método científico en las ciencias naturales, muchos autores pensaron que los fenómenos históricos también seguirían pautas deducibles por observación. El desarrollo inicial de la antropología como disciplina más o menos autónoma del conjunto de las Ciencias Naturales coincide con el auge del pensamiento ilustrado y posteriormente del positivismo que elevaba la razón como una capacidad distintiva de los seres humanos. Su desarrollo se pudo vincular muy pronto a los intereses del colonialismo europeo derivado de la Revolución Industrial. Por razones que tienen que ver con el proyecto de la New Republic, y sobre todo con el problema de la gestión de los asuntos indios, la antropología de campo empezó a tener bases profesionales en Estados Unidos en el último tercio del siglo. XIX, a partir del Bureau of American Ethnology y de la Smithsonian Institution. El antropólogo alemán Franz Boas, inicialmente vinculado a este tipo de tarea, institucionalizó académica y profesionalmente la Antropología en Estados Unidos. En la Gran Bretaña victoriana, Edward Burnett Tylor y posteriormente autores como William Rivers y más tarde Bronisław Malinowski y Alfred Reginald Radcliffe-Brown desarrollaron un modelo profesionalizado de Antropología académica. Lo mismo sucedió en Alemania antes de 1918. En todas las potencias coloniales de principios de siglo hay esbozos de profesionalización de la Antropología que no acabaron de cuajar hasta después de la II Guerra Mundial. En el caso de España puede citarse a Julio Caro Baroja y a diversos africanistas y arabistas que estudiaron las culturas del Norte de África. En todos los países occidentales se incorporó el modelo profesional de la Antropología anglosajona. Por este motivo, la mayor parte de la producción de la Antropología social o cultural antes de 1960 ,lo que se conoce como «modelo antropológico clásico», se basa en etnografías producidas en América, Asia, Oceanía y África, pero con un peso muy inferior de Europa. La razón es que en el continente europeo prevaleció una etnografía positivista, destinada a apuntalar un discurso sobre la identidad nacional, tanto en los países germánicos como en los escandinavos y los eslavos. Históricamente hablando, el proyecto de Antropología general se componía de cuatro ramas: la lingüística, la arqueología, la antropología biológica y la antropología social, referida esta última como antropología cultural o etnología en algunos países. Estas últimas ponen especial énfasis en el análisis comparado de la cultura ,término sobre el que no existe consenso entre las corrientes antropológicas,, que se realiza básicamente por un proceso trifásico, que comprende, en primera instancia, una investigación de gabinete; en segundo lugar, una inmersión cultural que se conoce como etnografía o trabajo de campo y, por último, el análisis de los datos obtenidos mediante el trabajo de campo. El modelo antropológico clásico de la antropología social fue abandonado en la segunda mitad del siglo XX. Actualmente los antropólogos trabajan prácticamente todos los ámbitos de la cultura,la sociedad y la lingüística. El objeto de estudio antropológico El cráneo del niño de Taung, descubierto en Sudáfrica. Este niño era un Australopithecus africanus, una forma intermedia de hominino. La materia de estudio de la antropología ha sido materia de debate desde el nacimiento de la disciplina, aunque es común a todas las posturas el compartir la preocupación por producir conocimiento sobre el ser humano. La manera en que se aborda la cuestión es lo que plantea el desacuerdo, porque la materia puede abordarse desde diversos puntos de vista. Sin embargo, desde el inicio la configuración epistemológica de la antropología consistió en la pregunta por el Otro. Esta es una cuestión central en las ciencias y disciplinas antropológicas que se va configurando desde el Renacimiento. Tras el desarrollo de diferentes tradiciones teóricas en diversos países, entró en debate cuál era el aspecto de la vida humana que correspondía estudiar a la antropología. Para esa época, los lingüistas y arqueólogos ya habían definido sus propios campos de acción. Edward Burnett Tylor, en las primeras líneas del capítulo primero de su obra Cultura primitiva, había propuesto que el objeto era la cultura o civilización, entendida como un «todo complejo» que incluye las creencias, el arte, la moral, el derecho, las costumbres y cualesquiera otros hábitos adquiridos por el ser humano como miembro de una sociedad. Esta propuesta está presente en todas las corrientes de la antropología, ya sea que se declaren a favor o en contra. Sin embargo, a partir del debate se presenta un fenómeno de constante atomización en la disciplina, a tal grado que para muchos autores ,por citar el ejemplo más conocido,, el estudio de la cultura sería el campo de la antropología cultural; el de las estructuras sociales sería facultad de la antropología social propiamente dicha. De esta suerte, Radcliffe-Brown (antropólogo social) consideraba como una disciplina diferente (y errada, por lo demás) la que realizaban Franz Boas y sus alumnos (antropólogos culturales). Según Clifford Geertz, el objeto de la antropología es el estudio de la diversidad cultural. La antropología es una ciencia que estudia las respuestas del ser humano ante el medio, las relaciones interpersonales y el marco sociocultural en que se desenvuelven, cuyo objeto va a ser el estudio del ser humano en sus múltiples relaciones; además estudia la cultura como elemento diferenciador de los demás seres humanos. Estudia al ser humano en su totalidad, incluyendo los aspectos biológicos y socioculturales como parte integral de cualquier grupo o sociedad. Se convirtió en una ciencia empírica que reunió mucha información, además fue la primera ciencia que introdujo el trabajo de campo y surge de los relatos de viajeros, misioneros, etc. Autores como Manuel Marzal (1998: 16), sostienen que antropología cultural, antropología social y etnología son la misma disciplina. Campos de acción de la antropología Excavación del yacimiento de Gran Dolina, en Atapuerca (España). La antropología social se orientó en su inicio a la investigación de las sociedades no occidentales. En la imagen Alfred Kroeber e Ishi, el último yahi, en 1911. Saussure (en la imagen) sentó los antecedentes del gran desarrollo de la lingüística moderna, cuyos aportes han sido aprovechados especialmente por los antropólogos sociales. Londres, fue una de las primeras ciudades analizadas desde el enfoque de la Antropología Urbana. La Antropología, como ciencia que abarca los fenómenos del ser humano como parte de una sociedad, se ha diversificado en sus métodos y sus teorías. La diversificación obedece al interés por rendir mejor cuenta de los procesos que enfrenta la especie en diversas dimensiones. De acuerdo con la American Anthropological Association (AAA), los cuatro campos de la Antropología son la Antropología biológica, la Antropología cultural, la Arqueología y la Antropología lingüística. La Antropología biológica o física es el campo de la Antropología que se especializa en el estudio de los seres humanos desde el punto de vista evolutivo y adaptativo. Al adoptar una postura evolucionista, los antropólogos físicos pretenden dar cuenta no solo de los grandes cambios en los aspectos biológicos del ser humano ,lo que se llama hominización,, sino en los pequeños cambios que se observan entre poblaciones humanas. La diversidad física del ser humano incluye cuestiones como la pigmentación de la piel, las formas de los cráneos, la talla promedio de un grupo, tipo de cabello y otras cuestiones numerosas. Para abordar esta diversidad, la Antropología física no solo echa mano de estudios propiamente anatómicos, sino las interacciones entre los seres humanos y otras especies, animales y vegetales, el clima, cuestiones relativas a la salud y la interacción entre distintas sociedades. El campo de la Antropología biológica también es interés de otras ciencias con las que mantiene un diálogo, por ejemplo, con la Primatología (estudio científico de los primates), la Demografía, la Ecología o las ciencias de la salud. Cuenta entre sus especializaciones a la Paleoantropología y la Antropología médica. La Arqueología es una de las ciencias antropológicas con mayor difusión entre el público no especializado. Se trata del estudio científico de los vestigios del pasado humano. Podría decirse que este interés se ha encontrado en diversas épocas y lugares, aunque la Arqueología tiene un antecedente muy claro en el coleccionismo de antigüedades en las sociedades europeas. Para lograr sus propósitos, los arqueólogos indagan en depósitos de estos materiales que son llamados yacimientos arqueológicos ,o «sitios arqueológicos», calcado del inglés archaeological site, a los que se accede normalmente por excavaciones. A pesar de los estereotipos sobre los arqueólogos ,a los que se suele imaginar como una especie de Indiana Jones, y los lugares comunes sobre lo que es la Arqueología, el método arqueológico no comprende únicamente las técnicas de excavación. Ante todo se trata de interpretar los hallazgos, tanto en relación con su contexto arqueológico como en relación con los conocimientos ya comprobados, la historia del yacimiento y otros elementos. La Antropología social, cultural o Etnología estudia el comportamiento humano, la cultura, las estructuras de las relaciones sociales. En la actualidad la antropología social se ha volcado al estudio de Occidente y su cultura. Aunque para los antropólogos de los países centrales (EE. UU., Gran Bretaña, Francia, etc.) este es un enfoque nuevo, hay que señalar que esta práctica es común en la antropología de muchos países latinoamericanos (como ejemplo, la obra de Darcy Ribeiro sobre el Brasil, la de Guillermo Bonfil Batalla y Gonzalo Aguirre Beltrán sobre México, etc.). Dependiendo de si surge de la tradición anglosajona se conoce como antropología cultural y, si parte de la escuela francesa, entonces se le denomina etnología. Quizá se haya distinguido de la antropología social en tanto que su estudio es esencialmente dirigido al análisis de la otredad (condición de ser otro) en tanto que el trabajo de la antropología social resulta generalmente más inmediato. Uno de sus principales exponentes es Claude Lévi-Strauss, quien propone un análisis del comportamiento del ser humano basado en un enfoque estructural en el que las reglas de comportamiento de todos los sujetos de una determinada cultura son existentes en todos los sujetos a partir de una estructura invisible que ordena a la sociedad.[cita requerida] La Antropología lingüística o Lingüística antropológica estudia los lenguajes humanos. Dado que el lenguaje es una amplia parte constitutiva de la cultura, los antropólogos la consideran como una disciplina separada. Los lingüistas se interesan en el desarrollo de las lenguas. Así mismo, se ocupan en las diferencias de los lenguajes vivos, cómo se vinculan o difieren, y en ciertos procesos que explican las migraciones y la difusión de la información. También se preguntan sobre las formas en que el lenguaje se opone o refleja otros aspectos de la cultura. Dentro de las ciencias sociales, disciplinas como la lingüística y la antropología han mantenido una relación que ha tomado la forma de un complejo proceso articulatorio influido a lo largo del tiempo por las distintas condiciones históricas, sociales y teóricas imperantes. La lingüística, al igual que la etnología, la arqueología, la antropología social, la antropología física y la historia, es una de las disciplinas que conforman el campo de la antropología desde algunas perspectivas. La lingüística estudia el lenguaje para encontrar sus principales características y así poder describir, explicar o predecir los fenómenos lingüísticos. Dependiendo de sus objetivos, estudia las estructuras cognitivas de la competencia lingüística humana o la función y relación del lenguaje con factores sociales y culturales. La relación entre la lingüística y la antropología ha respondido a distintos intereses. Durante el siglo XIX y la primera mitad del XX, la antropología y la lingüística comparativa intentaban trazar las relaciones genéticas y el desarrollo histórico de las lenguas y familias lingüísticas. Posteriormente, la relación entre las dos disciplinas tomó otra perspectiva por la propuesta desde el estructuralismo. Los modelos lingüísticos fueron adoptados como modelos del comportamiento cultural y social en un intento por interpretar y analizar los sistemas socioculturales, dentro de las corrientes de la antropología. La tendencia estructural pudo proponerse por la influencia de la lingüística, tanto en lo teórico como en lo metodológico. Sin embargo, al excluir las condiciones materiales y el desarrollo histórico, se cuestionó que la cultura y la organización social pudieran ser analizadas del mismo modo que un código lingüístico, tomando al lenguaje como el modelo básico sobre el que se estructura todo el pensamiento o clasificación. No obstante estos puntos de vista diferentes, se puede llegar a acercamientos productivos reconociendo que la cultura y la sociedad son producto tanto de condiciones objetivas o materiales como de construcciones conceptuales o simbólicas. De esta forma, la interacción entre estas dos dimensiones nos permite abordar a los sistemas socioculturales como una realidad material a la vez que una construcción conceptual. Las lenguas implican o expresan teorías del mundo y, por tanto, son objetos ideales de estudio para los científicos sociales. El lenguaje, como herramienta conceptual, aporta el más complejo sistema de clasificación de experiencias, por lo que cada teoría, sea ésta antropológica, lingüística o la unión de ambas, contribuye a nuestra comprensión de la cultura como un fenómeno complejo, ya que «el lenguaje es lo que hace posible el universo de patrones de entendimiento y comportamiento que llamamos cultura. Es también parte de la cultura, ya que es transmitido de una generación a otra a través del aprendizaje y la imitación, al igual que otros aspectos de la cultura».[cita requerida] Roman Jakobson plantea que «los antropólogos nos prueban, repitiéndolo sin cesar, que lengua y cultura se implican mutuamente, que la lengua debe concebirse como parte integrante de la vida de la sociedad y que la lingüística está en estrecha conexión con la antropología cultural». Para él, la lengua, como el principal sistema semiótico, es el fundamento de la cultura: «Ahora sólo podemos decir con nuestro amigo McQuown que no se da igualdad perfecta entre los sistemas de signos, y que el sistema semiótico primordial, básico y más importante, es la lengua: la lengua es, a decir verdad, el fundamento de la cultura. Con relación a la lengua, los demás sistemas de símbolos no pasan de ser o concomitantes o derivados. La lengua es el medio principal de comunicación informativa». Ramas y subramas A su vez, cada una de estas cuatro ramas principales se subdivide en innumerables subramas que muchas veces interactúan entre sí. De la antropología cultural o social (también conocida como antropología sociocultural), se desprenden: Antropología urbana: Hace referencia el estudio etnográfico y transcultural de la urbanización global y de la vida en las ciudades. Es una subdisciplina enseñada en la mayoría de las universidades del mundo. Las Áreas Metropolitanas se han constituido en los lugares objeto de estudio de las investigaciones sobre temas como la etnicidad, la pobreza, el espacio público, las clases y las variaciones subculturales. Antropología del parentesco: esta rama se enfoca en las relaciones de parentesco, entendido como un fenómeno social, y no como mero derivado de las relaciones biológicas que se establecen entre un individuo, sus progenitores y los consanguíneos de éstos; se trata de una de las especialidades más antiguas de la antropología, y de hecho está relacionada con el quehacer de los primeros antropólogos evolucionistas del siglo XIX. Antropología de la religión: Estudia los sistemas religiosos y de creencias. Antropología filosófica: es una rama de la filosofía alemana y no de la Antropología científica que, principalmente, se ocupa de las incertidumbres de índole ontológica, centrado su atención en el ser humano, tomando en cuenta una variedad de aspectos de la existencia humana, pasada y presente, combinando estos materiales diversos en un abordaje íntegro del problema de la existencia humana. Además, se pregunta por la naturaleza fundamental de su ser, se pregunta lo que diferencia al ser humano de todos los demás seres, cómo se define a través de su existencia histórica, etc. Tales interrogantes fundamentales de la antropología filosófica pueden ser condensadas en una pregunta radical: ¿Qué es el ser humano? Además de: antropología económica, antropología política, aplicada, rural, urbana, visual, todas las que deben entenderse como enfoques o puntos de partida diversos para analizar los fenómenos sociales. De la antropología física (también como antropología biológica), se desprenden: Antropología forense: Se encarga de la identificación de restos humanos esqueletizados dado su amplia relación con la biología y variabilidad del esqueleto humano. También puede determinar, en el caso de que hayan dejado marcas sobre los huesos, las causas de la muerte, para tratar de reconstruir la mecánica de hechos y la mecánica de lesiones, conjuntamente con el arqueólogo forense, el criminalista de campo y médico forense, así como aportar, de ser posible, elementos sobre la conducta del victimario por medio de indicios dejados en el lugar de los hechos y el tratamiento perimortem y post mortem dado a la víctima. Paleoantropología: Se ocupa del estudio de la evolución humana y sus antepasados fósiles u homínidos antiguos. A veces, también puede ser conocida como paleontología humana. Antropología genética: Se la define como la aplicación de técnicas moleculares para poder entender la evolución homínida, en particular la humana, relacionándolas con otras criaturas no humanas. Autores como Lorena Campo (2008: 38), consideran a la arqueología como rama que se desprende de la antropología cultural. En todo caso, de la arqueología se pueden citar las siguientes subramas: Arqueoastronomía: Es el estudio de yacimientos arqueológicos relacionados con el estudio de la astronomía por culturas antiguas. También estudia el grado de conocimientos astronómicos poseído por los diferentes pueblos antiguos. Uno de los aspectos de esta disciplina es el estudio del registro histórico de conocimientos astronómicos anterior al desarrollo de la moderna astronomía. Arqueología subacuática: Sigue los preceptos de la arqueología terrestre pero se dedica, a través de las técnicas de buceo, a desentrañar antiguas culturas cuyos restos materiales que, por una u otra razón, se encuentran actualmente bajo el agua. Antropología evolucionista: es el estudio interdisciplinario de la evolución de la fisiología humana y el comportamiento humano y la relación entre los homínidos y los primates no homínidos. La antropología evolucionista, se basa en las ciencias naturales y las ciencias sociales. Varios campos y disciplinas incluyen: La antropología de la evolución humana y la antropogenía. La paleoantropología y la paleontología. La primatología de etología y paleontología de los primates. La evolución cultural del comportamiento humano. El estudio arqueológico de la tecnología humana y el cambio sobre tiempo y espacio. La genética humana evolucionista y los cambios en el genoma humano durante el tiempo. La neurociencia cognitiva y neuroantropología de la cognición, las acciones y las capacidades de los primates y humanos. La ecología del comportamiento y la interacción entre humanos y el medio ambiente. Los estudios de la anatomía humana ósea, la endocrinología y la neurobiología y las diferencias y cambios entre especies, la variación entre grupos humanos y relaciones a factores culturales. La antropología evolucionista está relacionada con la evolución biológica y cultural de los humanos, pasados y presentes. Está basada en un enfoque científico, y une campos como la arqueología, la ecología del comportamiento, la psicología, la primatología y la genética. Es un campo dinámico e interdisciplinario, aprovechándose de muchas líneas de evidencia para comprender la experiencia humana, pasada y presente. Generalmente los estudios de la evolución biológica están relacionados con la evolución de la forma humana. La evolución cultural supone el estudio del cambio cultural sobre el tiempo y el espacio e incorpora los modelos de transmisión cultural con frecuencia. Nota que la evolución cultural no es la misma que la evolución biológica, y que la cultura humana supone la transmisión de información cultural, que comporte en maneras distintas de la biología humana y la genética. El estudio del cambio cultural se realiza cada vez más tras cladística y los modelos genéticos. Cada una de las ramas ha tenido un desarrollo propio en mayor o menor medida. La diversificación de las disciplinas no impide, por otro lado, que se hallen en interacción permanente unas con otras. Los edificios teóricos de las disciplinas antropológicas comparten como base su interés por el estudio de la humanidad. Sin embargo, metonímicamente en la actualidad, cuando se habla de antropología, por antonomasia se hace referencia a la antropología social. El origen de la pregunta antropológica Este artículo o sección necesita referencias que aparezcan en una publicación acreditada. Busca fuentes: «Antropología» – noticias · libros · académico · imágenes Este aviso fue puesto el 18 de abril de 2023. La pregunta antropológica es ante todo una pregunta por el otro. Y en términos estrictos, está presente en todo individuo y en todo grupo humano, en la medida en que ninguna de las dos entidades puede existir como aislada, sino en relación con el otro. Ese otro es el referente para la construcción de la identidad, puesto que ésta se construye por «oposición a» y no «a favor de». La preocupación por aquello que genera las variaciones de sociedad en sociedad es el interés fundador de la antropología moderna. De esa manera, para Krotz el «asombro» es el pilar del interés por lo «otro» (alter), y son las «alteridades» las que marcan tal contraste binario entre los seres humanos. A pesar de que todos los pueblos comparten esta inquietud, es en Occidente donde, por condiciones históricas y sociales particulares se ha documentado de manera más notable. Es innegable que ya Hesíodo, Heródoto, y otros clásicos indagaban en estas diferencias. Sin embargo, cuando Europa se halló frente a pueblos desconocidos y que resultaban tan extraordinarios, interpretó estas exóticas formas de vida ora fascinada, ora sobrecogida. Colón toma posesión del «Nuevo Mundo». La Conquista de América constituye un gran hito de la pregunta antropológica moderna. Los escritos de Cristóbal Colón y otros navegantes revelan el choque cultural en que se vio inmersa la vieja Europa. Especial importancia tienen los trabajos de los misioneros indianos en México, Perú, Colombia y Argentina en los primeros acercamientos a las culturas aborígenes. De entre ellos destaca Bernardino de Sahagún, quien emplea en sus investigaciones un método sumamente riguroso, y lega una obra donde hay una separación bien clara entre su opinión eclesiástica y los datos de sus «informantes» sobre su propia cultura. Esta obra es la Historia general de las cosas de la Nueva España. Con los nuevos descubrimientos geográficos se desarrolló el interés hacia las sociedades que encontraban los exploradores. En el siglo XVI el ensayista francés Montaigne se preocupó por los contrastes entre las costumbres en diferentes pueblos. En 1724 el misionero jesuita Lafitau publicó un libro en el que comparaba las costumbres de los indios americanos con las del mundo antiguo. En 1760 Charles de Brosses describe el paralelismo entre la religión africana y la del Antiguo Egipto. En 1748 Montesquieu publica El espíritu de las leyes basándose en lecturas sobre costumbres de diferentes pueblos. En el siglo XVIII, fue común la presencia de relatores históricos, los cuales, a modo de crónica, describían sus experiencias a través de viajes de gran duración a través del mundo. El siglo XIX vio el comienzo de viajes emprendidos con el fin de observar otras sociedades humanas. Viajeros famosos de este siglo fueron Bastian (1826-1905) y Ratzel (1844-1904). Ratzel fue el padre de la teoría del difusionismo que consideraba que todos los inventos se habían extendido por el mundo por medio de migraciones, esta teoría fue llevada al absurdo por su discípulo Frobenius (1873-1938) que pensaba que todos los inventos básicos se hicieron en un solo sitio: Egipto. En la era moderna, Charles Darwin y sucesos históricos como la Revolución industrial contribuirían al desarrollo de la antropología como una disciplina científica. Antropología moderna Para el establecimiento de una ciencia que incorporase las teorías filosóficas y los programas generales ya elaborados, serían necesarios ciertos avances metodológicos que no tuvieron lugar hasta finales del siglo XVIII y comienzos del siglo XIX. En esta época se produjeron las primeras clasificaciones raciales sistemáticas, como las de Linneo (1707-1778) y J. Blumenbach (1752-1840). Durante este mismo período surgió la lingüística moderna, dominada durante el s. XIX por la idea de que los idiomas podían clasificarse en familias y que los pertenecientes a una misma familia eran ramas de un tronco común más antiguo. Ello dio lugar al desarrollo de métodos comparativos sistemáticos con el fin de poder reconstruir el idioma ancestral. Aquí se aprecia a Boas posando como la “danza canibal” de los indígenas Kwaklutl, durante una exhibición en el National Museum of Natural History, en 1895. La regularidad de las correspondencias fonéticas en idiomas emparentados fue presentada primero por R. Rask (1787-1832) y divulgada por J. Grimm (1785-1863) a comienzos del s. XIX, con lo que contribuyeron a consolidar la idea general de la existencia de regularidades en el cambio cultural humano. Otro tipo de descubrimientos realizados en este período ampliaron de manera importante el horizonte temporal del desarrollo humano y otorgaron legitimidad a la idea de un progreso cultural gradual. Por una parte, el desciframiento de la escritura egipcia por Jean-François Champollion (1790-1832), en 1821, alteró de forma radical las ideas tradicionales acerca de la antigüedad del ser humano. Posteriormente, a mediados del s. XIX, se reconoció la validez del descubrimiento de Boucher de Perthes (1788- 1868) de utensilios humanos del Paleolítico, contemporáneos de mamíferos ya extinguidos. De este modo, la arqueología y las teorías de Darwin concurrían en ofrecer una imagen del ser humano como la de un ser sólidamente anclado entre las demás especies animales del pasado, que pasa de ser un antropoide carente de atributos culturales a transformarse en ser humano a lo largo de un prolongado período de cientos de miles de años. Durante la primera mitad del s. XIX la antropología comenzó a adquirir el rango de disciplina científica independiente y se crean las primeras sociedades etnológicas o antropológicas en Inglaterra, Francia y Alemania. En este último país, la palabra «Kultur» adquiere el sentido técnico que reviste en la actualidad. Posteriormente el término fue introducido en el mundo de habla inglesa por E.B. Tylor en su obra clásica La cultura primitiva (Primitive Culture), publicada en 1871. En una tan detallada como amplia panorámica de la evolución cultural humana y con una clara exposición de las perspectivas teóricas de una ciencia de la cultura, el libro de Tylor representa una obra fundacional en el desarrollo de la antropología moderna. Código de ética y política en antropología Este artículo o sección necesita referencias que aparezcan en una publicación acreditada. Busca fuentes: «Antropología» – noticias · libros · académico · imágenes Este aviso fue puesto el 20 de mayo de 2024. Logo de la Asociación Americana de Antropología. Algunos problemas éticos surgen de la sencilla razón de que los antropólogos tienen más poder que los pueblos que estudian. Se ha argumentado que la disciplina es una forma de colonialismo en la cual los antropólogos obtienen poder a expensas de los sujetos. Según esto, los antropólogos adquieren poder explotando el conocimiento y los artefactos de los pueblos que investigan. Estos, por su parte, no obtienen nada a cambio, y en el colmo, llevan la pérdida en la transacción. De hecho, la llamada escuela británica estuvo ligada explícitamente, en su origen, a la administración colonial. Otros problemas son derivados también del énfasis en el relativismo cultural de la antropología estadounidense y su añeja oposición al concepto de raza. El desarrollo de la sociobiología hacia finales de la década de 1960 fue objetado por antropólogos culturales como Marshall Sahlins, quien argumentaba que se trataba de una posición reduccionista. Algunos autores, como John Randal Baker, continuaron con el desarrollo del concepto biológico de raza hasta la década de 1970, cuando el nacimiento de la genética se volvió central en este frente. En tanto que la genética ha avanzado como ciencia, algunos genetistas como Luca Cavalli-Sforza han desterrado el concepto de raza de acuerdo con los nuevos descubrimientos (tales como el trazo de las migraciones antiguas por medio del ADN de la mitocondria y del cromosoma Y). La antropología estadounidense tiene una historia de asociaciones con las agencias gubernamentales de inteligencia y la política antibelicosa. Boas rechazó públicamente la participación de los Estados Unidos en la Primera Guerra Mundial, lo mismo que la colaboración de algunos antropólogos con el servicio de inteligencia de Estados Unidos. En contraste, muchos antropólogos contemporáneos de Boas fueron activos participantes en estas guerras de múltiples formas. Entre ellos se cuentan las docenas de antropólogos que sirvieron en la Oficina de Servicios Estratégicos y la Oficina de Información de Guerra. Como ejemplo, Ruth Benedict escribió El crisantemo y la espada, un informe sobre la cultura japonesa realizado a pedido del Ejército de los Estados Unidos. Fotografía del antropólogo Josef Mengele. A veces la antropología puede ser utilizada con fines perversos, tal y como hizo durante el Holocausto. En 1950 la Asociación Antropológica Estadounidense (AAA) proveyó a la CIA información especializada de sus miembros, y muchos participaron en la Operación Camelot en Latinoamérica y la guerra de Vietnam. Aunque en aquellos años, varios otros antropólogos estuvieron sumamente activos en el movimiento pacifista e hicieron pública su oposición en la American Anthropological Association, condenando el involucramiento del gremio en operaciones militares encubiertas. Hoy en día, los colegios profesionales de antropólogos censuran el servicio estatal de la antropología y su deontología les puede impedir a los antropólogos dar conferencias secretas con fines colonizadores. La Asociación Británica de Antropología Social, ha calificado ciertas becas éticamente peligrosas, por ejemplo, ha condenado el programa de la CIA «Pat Roberts Intelligence Scholars Program», que patrocina a estudiantes de antropología en las universidades de Estados Unidos en preparación a tareas de espionaje para el gobierno. La Declaración de Responsabilidad Profesional de la American Anthropological Association afirma claramente que «en relación con el gobierno propio o anfitrión (...) no deben aceptarse acuerdos de investigaciones secretas, reportes secretos o informes de ningún tipo». Los antropólogos, junto con otros científicos sociales, han trabajado con los militares de EE. UU. como parte de la estrategia del Ejército de EE. UU. en Afganistán, este programa de intervención se denomina: Human Terrain System. Ramas principales Antropología cultural o social o etnología Arqueología Antropología física o biológica Lingüística antropológica Teorías, corrientes, escuelas antropológicas Positivismo Evolucionismo social Evolucionismo cultural Difusionismo Determinismo Particularismo histórico Escuela Sociológica Francesa Funcionalismo Funcionalismo estructuralista Estructuralismo Antipositivismo Neoevolucionismo Relativismo cultural Marxismo antropológico Escuela culturalista Etnometodología Indigenismo Interaccionismo simbólico Postestructuralismo Deconstrucción Teoría de sistemas Métodos y técnicas de investigación Investigación cualitativa Etnografía Investigación cuantitativa Metodología arqueológica Trabajo de campo Holismo Diario de campo Etnología Entrevista (investigación) Observación participante Osteología Excavación Antropometría Autores principales Alberto Rex González Alfred Kroeber Alfred Reginald Radcliffe-Brown Anne Chapman Bronisław Malinowski Bruno Latour Carlos Martínez Sarasola Carlota Sempé Clark Wissler Claude Lévi-Strauss Clifford Geertz Daniel Miller Edward Burnett Tylor Edward Evan Evans-Pritchard Edward Sapir Esther Hermitte Evelia Edith Oyhenart Federico Kauffmann Doig Francisco Raúl Carnese Franz Boas Georges Balandier Gustavo Politis Helen Fisher Herbert Spencer Héctor Mario Pucciarelli Hugo Ratier James George Frazer Johann Jakob Bachofen Julian Steward Karl Polanyi Lewis Henry Morgan Leslie White Louis Leakey Manuel Gamio Marcel Mauss Margaret Mead Marshall Sahlins Marvin Harris Mary Douglas Mary Leakey Maurice Godelier Melville Herskovits Néstor García Canclini Philippe Descola Pierre Bourdieu Pierre Clastres Ralph Linton Renato Rosaldo Rita Segato Rodolfo Kusch Rosana Guber Ruth Benedict Sherry Ortner Tim Ingold Revistas científicas de antropología Categoría principal: Revistas de antropología Temas más estudiados Aculturación Alteridad Brujería Capacidad intercultural Cerámica Chamanismo Choque cultural Colonialismo Comportamiento Comunicación intercultural Contacto cultural Cosmovisión Costumbre Cultura Desigualdad Diversidad cultural Drogas ancestrales Ecología cultural Educación intercultural Endoculturación Etnabotánica Etnia Etnolingüística Etnomusicología Etnoarqueología Etnocentrismo Evolución humana Exotismo Excavación Familia Genocidio Globalización Hominización *Humanización Identidad cultural Institución Interculturalidad Magia Matriarcado Mito Modernidad Netnografía Neuroantropología Parentesco Posmodernidad Proxémica Razas humanas Ritual Rito Sexualidad humana Síndrome cultural Sociedad Organización social Tabú Transculturación Género."
ksampletext_wikipedia_anth_homosapiens: str = "Homo sapiens. Homo sapiens (en latín hombre sabio), comúnmente llamado ser humano, persona u hombre ,este último en el sentido de ser racional, que no distingue entre ambos sexos,, es una especie de primate catarrino perteneciente a la familia de los homínidos, nativo originalmente de África, aunque luego se fue expandiendo hacia el resto del mundo. El conjunto de personas o el género humano también se conoce con la denominación genérica de humanos y humanidad. Los seres humanos poseen capacidades mentales que les permiten inventar, aprender, utilizar estructuras lingüísticas complejas, adquirir y mejorar sus habilidades lógicas, matemáticas, de escritura, musicales, entre otras. Los seres humanos son animales sociales, capaces de concebir, transmitir y aprender conceptos totalmente abstractos. Se considera Homo sapiens de manera indiscutible a los que poseen las características anatómicas de las poblaciones humanas actuales. Los restos más antiguos atribuidos a Homo sapiens, datados en 315 000 años, se encontraron en Marruecos. Las evidencias más antiguas de comportamiento moderno son las de Pinnacle Point (Sudáfrica), con 165 000 años de antigüedad. Pertenece al género Homo, que fue más diversificado y durante el último millón y medio de años incluía otras especies ya extintas. Desde la extinción de Homo neanderthalensis, hace 28 000 años, es la única especie conocida del género Homo que aún perdura. Hasta hace poco, en la biología se utilizaba un nombre trinomial ,Homo sapiens sapiens, para esta especie, pero sigue en debate el nexo filogenético entre el neandertal y la actual humanidad. Homo sapiens pertenece a una estirpe de primates, los hominoideos. Aunque el descubrimiento de Homo sapiens idaltu en 2003 haría necesario volver al sistema trinomial, la posición taxonómica de este último es aún incierta. Evolutivamente se diferenció en África y de ese ancestro surgió la familia de la que forman parte los homínidos. Filosóficamente, el ser humano se ha definido y redefinido a sí mismo de numerosas maneras a través de la historia, otorgándose de esta manera un propósito positivo o negativo respecto de su propia existencia. Existen diversos sistemas religiosos e ideales filosóficos que, de acuerdo con una diversa gama de culturas e ideales individuales, tienen como propósito y función responder a algunas de esas interrogantes existenciales. Los seres humanos tienen la capacidad de ser conscientes de sí mismos, así como de su pasado; saben que tienen el poder de planear, realizar y transformar proyectos de diversos tipos. En función de esta capacidad, han creado diversos códigos morales y dogmas orientados directamente al manejo de estas capacidades. Además, pueden ser conscientes de responsabilidades y peligros provenientes de la naturaleza, así como de otros seres humanos. En la actualidad, aproximadamente 8000 millones de seres humanos habitan la Tierra. Nombre científico Hombre de Vitruvio, por Leonardo da Vinci. El nombre científico asignado por el naturalista sueco Carlos Linneo (1707-1778) en 1758 alude al rasgo biológico más característico (sapiens significa «sabio» o «capaz de conocer») y se refiere a la consideración del ser humano como «animal racional», al contrario que todas las otras especies, siendo la descripción que aportó para Homo sapiens simplemente: Nosce te ipsum («Conócete a ti mismo»). Es precisamente la capacidad del ser humano de realizar operaciones conceptuales y simbólicas muy complejas ,que incluyen, por ejemplo, el uso de sistemas lingüísticos muy sofisticados, el razonamiento abstracto y las capacidades de introspección y especulación, uno de sus rasgos más destacados. Posiblemente esta complejidad, fundada neurológicamente en un aumento del tamaño del cerebro y, sobre todo, en el desarrollo del lóbulo frontal, es también una de las causas, a la vez que producto, de las muy complejas estructuras sociales que el ser humano ha desarrollado, y que forman una de las bases de la cultura, entendida biológicamente como la capacidad para transmitir información y hábitos por imitación e instrucción, en vez de por herencia genética. Esta propiedad no es exclusiva de esta especie y es importante también en otros primates. Linneo clasificó al hombre y a los monos en un grupo que llamó antropomorfos, como subconjunto del grupo cuadrúpedos, pues entonces no reconocía signos orgánicos que le permitieran ubicar al ser humano en un lugar privilegiado de la escala de los seres vivientes. Años más tarde, en el prefacio de Fauna suecica, manifestó que había clasificado al hombre como cuadrúpedo porque no era planta ni piedra, sino un animal, tanto por su género de vida como por su locomoción y porque además, no había podido encontrar un solo carácter distintivo por el cual el hombre se diferenciara del mono; en otro contexto afirmó sin embargo que considera al hombre como el fin último de la creación. A partir de la décima edición de Systema naturae reemplazó a los cuadrúpedos por los mamíferos y como primer orden de estos, puso a los primates, entre los cuales colocó al hombre. Linneo tuvo el mérito de dar origen a un nuevo e inmenso campo epistemológico, el de la antropología, si bien se limitó a enunciarlo y no lo cultivó. A él tendrán que remitirse todos los científicos posteriores, tanto para retomar sus definiciones como para criticarlas. En 1758 se definió al Homo sapiens linneano como una especie diurna que cambiaba por la educación y el clima. Linneo no designó un holotipo para Homo sapiens, pero en 1959 William Stearn propuso al propio Linneo, padre de la moderna taxonomía, como lectotipo para la especie. Con posterioridad se difundió la idea de que había sido sustituido por Edward Cope, pero esta propuesta no llegó a formalizarse, así que siguen siendo los restos de Linneo enterrados en Uppsala el tipo nomenclatural ,que debe considerarse simbólico, para la especie Homo sapiens. En la actualidad existen defensores de incluir al ser humano, chimpancé (Pan troglodytes) y bonobo (Pan paniscus) en el mismo género, dada la cercanía filogenética, que es más estrecha que la que se encuentra entre otras especies animales que sí están agrupadas genéricamente. Sin embargo, la inmensa mayoría de los especialistas no consideran correcto incluirlos dentro del mismo género, debido a que los linajes evolutivos que condujeron al ser humano y al chimpancé divergieron hace entre 6 y 10 millones de años y se diversificaron posteriormente, como argumenta Sandy Harcourt, y debido a las significativas diferencias entre los planes corporales de ambas líneas, especialmente en la de los Hominina, que permiten justificar varios géneros (Ardipithecus, Paranthropus, Australopithecus u Homo). Taxonomía y cladística En la siguiente tabla se muestra la clasificación cladística y taxonómica de la especie Homo sapiens desde el origen de la vida: N.º de clado Rango Nombre Contenido Biología El ser humano es un ser vivo, y como tal está compuesto por sustancias químicas llamadas biomoléculas, por células y realiza las tres funciones vitales: nutrición, relación y reproducción. Además, el humano es un organismo pluricelular; es decir, está formado por muchas células, entre las cuales existen diferencias de estructura y de función. Por otra parte, el ser humano es un animal, pues tiene células eucariotas, es decir, presenta orgánulos celulares especializados en una función determinada y su material genético se encuentra protegido por una envoltura; y presenta nutrición heterótrofa; es decir, que para obtener su propia materia orgánica se alimenta de otros seres vivos. Cuerpo humano Elementos principales de la anatomía externa de la mujer y el varón u hombre Artículo principal: Anatomía humana En cuanto a su locomoción y movimiento, es uno de los más plásticos del reino animal, pues existe una amplia gama de movimientos posibles, lo que le capacita para actividades como el arte escénico y la danza, el deporte y un sinnúmero de actividades cotidianas. Asimismo destaca la habilidad de manipulación, gracias a los pulgares oponibles, que le facilitan la fabricación y uso de instrumentos. La especie humana posee un notorio dimorfismo sexual en el nivel anatómico, siendo los machos adultos (o varones) más altos y más pesados que las mujeres (hembras de la especie) en promedio, aunque se ha notado una «tendencia secular» al aumento de las tallas en ambos sexos (especialmente durante el siglo XX). El ser humano adulto contemporáneo promedio mide entre: 1,50 m a 1,70 m (mujeres), y entre 1,60 m a 1,80 m (varones). El peso depende de la contextura del individuo y del sexo, generalmente rondando los 45 kg a 70 kg (mujeres), y 65 kg a 100 kg (varones). Los cuerpos humanos difieren entre sí según la estatura, peso, musculatura, nivel de grasa, entre otros. Véanse también: Cuerpo humano, Fisiología humana y Genética humana. Mente Artículo principal: Mente La mente se refiere colectivamente a aspectos del entendimiento y consciencia que son combinaciones de capacidades como el raciocinio, la percepción, la emoción, la memoria, la imaginación y la voluntad. La mente, según la neurociencia, es un resultado de la actividad del cerebro. El término pensamiento define todos los productos que la mente puede generar incluyendo las actividades racionales del intelecto y las abstracciones de la imaginación; todo aquello que sea de naturaleza mental es considerado pensamiento, bien sean estos abstractos, racionales, creativos, artísticos, etc. Junto con los cetáceos superiores (delfines y ballenas), los homininos de los géneros Gorilla y Pan, y los elefantes, alcanzan el mayor desarrollo y aun muchas de sus interacciones nos son desconocidas. Los seres humanos, a diferencia del resto del reino animal, son los únicos con capacidad de razonar. Además poseen capacidades mentales que les permiten inventar, aprender y utilizar estructuras lingüísticas complejas, lógicas, matemáticas, escritura, música, ciencia y tecnología. Los seres humanos son animales sociales, capaces de concebir, transmitir y aprender conceptos totalmente abstractos. Véanse también: Cerebro humano, Consciente, Inteligencia, Pensamiento y Psicología. Nutrición humana Artículo principal: Nutrición Véanse también: Régimen alimenticio, Alimentación humana, Omnívoro y Vegetarianismo. El ser humano es un animal omnívoro. En las primeras especies del género Homo, el paso de una alimentación eminentemente vegetariana a la inclusión de carne y grasas animales en la dieta no se debió a cuestiones culturales, sino a los desajustes metabólicos provocados por un mayor desarrollo cerebral. Sin embargo, en el humano, una dieta demasiado rica en proteínas necesita el complemento de carbohidratos y grasas; de lo contrario pueden aparecer carencias nutricionales importantes que pueden incluso provocar la muerte. Por ello, la alimentación del ser humano se basa en la combinación de carne con materia vegetal. Al igual que el mismo ser humano, su alimentación ha evolucionado a lo largo de los años, cambiando para adaptarse al mundo que lo rodea. El ser humano cazaba y recolectaba, sin embargo, con la invención de la agricultura y de la ganadería, su alimentación evolucionó. Etología Artículo principal: Comportamiento humano Ciclo vital Feto, por Leonardo da Vinci. La especie humana es entre los seres vivos pluricelulares actuales una de las más longevas; se tienen documentados casos de longevidad que sobrepasan los cien años. Tal longevidad es un carácter genotípico que, sin embargo, debe ser coadyuvado por condiciones vivenciales favorables. En el Imperio romano, hacia el año 2 d. C., la esperanza de vida rondaba solo los 27 años, debido en gran parte a la elevada mortalidad infantil. A principios del siglo XXI, la esperanza de vida global era de unos 70 años aproximadamente, siendo más elevada en países desarrollados y más baja en países subdesarrollados. Se supone que el ser humano, en óptimas condiciones, pueda vivir cien años o un poco más. Sin embargo a pesar del avance en la salud y calidad de vida en el último siglo, las costumbres humanas como el consumo de drogas, alcohol, azúcar, comida basura, sedentarismo, estrés, enfermedades de todo tipo, exposición a elementos tóxicos, entre otros, disminuye los años de vida de los seres humanos. Se cree también que pueda ser genético. La infancia humana es una de las más prolongadas en comparación con otras especies cercanas, siendo la edad de la pubertad aproximadamente a los once años en las mujeres y a los trece años en los varones, aunque las edades varían según la persona. Véanse también: Biología del desarrollo, Longevidad, Crecimiento humano y Desarrollo (biología). Sexualidad Artículo principal: Sexualidad Como todos los mamíferos, el ser humano tiene comportamientos reproductivos y sexuales. Pero a diferencia de la mayoría de ellos no tiene una época reproductiva estacional determinada, manteniendo actividad sexual y fertilidad en las hembras a lo largo de todo el año. Las mujeres tienen un ciclo de ovulación aproximadamente mensual, durante el cual producen óvulos y pueden ser fecundadas; en caso contrario tienen la menstruación, que es la eliminación a través de la vagina de los tejidos y sustancias relacionados con la producción de células sexuales. Pero el comportamiento sexual humano no está únicamente supeditado a las funciones reproductivas, sino que, de modo similar a otros simios antropoides, tiene fines recreativos y sociales. En el contacto sexual se busca tanto la reproducción como el placer y la comunicación afectiva. Es una parte importante de las relaciones de pareja y también se considera importante en las necesidades psicológicas del individuo aunque no tenga una relación de pareja. Cabe destacar la importancia del lenguaje simbólico en Homo sapiens, que hace que los significantes sean los soportes del pensar o los pensamientos. En nuestra especie, el pensar humano, a partir de los tres años y medio de edad se hace prevalentemente simbólico. Asociado con lo anterior, debe notarse que la especie humana es prácticamente la única que se mantiene en celo sexual continuo: es realmente destacable que en la especie humana no exista un estro propiamente dicho. En las mujeres existe un ciclo de actividad ovárica en virtud del cual existen cambios fisiológicos en todo su sistema reproductivo y del cual derivan ciertos cambios de conducta. Sin embargo, como en las mujeres la aceptación sexual no se circunscribe a una parte del ciclo reproductivo, no se debería usar los vocablos «estro» y «celo» en el ser humano, dado que la aceptación sexual es independiente de su ciclo reproductivo. Ya entre chimpancés y, sobre todo, bonobos, se nota una conducta próxima. Ahora bien, dada la dificultad de vivir «solamente» practicando relaciones sexuales, un «mecanismo» evolutivo compensatorio habría sido el de la sublimación –la cual se considera asociada a la existencia de un lenguaje y un pensar simbólicos–. Si se da una sublimación, esto parece significar que también se da una «represión» (en el sentido freudiano) que origina a lo inconsciente. Homo sapiens es, en este sentido, un ««animal pulsional». Según la ley del reflejo condicional de Pavlov Homo sapiens «no» se restringe a un «primer sistema de señales» (el de estímulo/respuesta y respuesta a un estímulo substitutivo), sino que el ser humano se encuentra en un nivel de «segundo sistema de señales». Este segundo sistema es, principalmente, el del lenguaje simbólico que permite una heurística, que es la capacidad para realizar de forma inmediata innovaciones positivas para sus fines. Por otra parte, la especie humana es de las pocas, junto con el bonobo (Pan paniscus), en el reino animal que copula cara a cara (entre otras múltiples formas), lo cual tiene implicaciones emocionales de gran relevancia para la especie. Cabe anotar que con el surgimiento de la teoría de la inteligencia emocional, desde la psicología sistémica, el ser humano no debe reducirse a sus pulsiones, las cuales sublima o reprime, sino que se entiende como un ser sexuado, que vive esta dimensión en relación con la formación recibida en la familia y la sociedad. La sexualidad se forma entonces desde los primeros años y se va entendiendo como una vivencia procesual acorde a su ciclo vital y su contexto sociocultural. A diferencia de lo que ocurre en la mayor parte de las otras especies sexuadas, la mujer sigue viviendo mucho tiempo tras la menopausia. En las otras especies la hembra suele fenecer al poco tiempo de su llegada. Por la indicada precocidad, la madurez sexo-genital es –con relación a otras especies– muy tardía entre los individuos de la especie humana. Actualmente en muchas zonas la menarquia está ocurriendo a los once años; esto significa que, aunque la madurez sexo-genital es siempre lenta en la especie humana, existe un adelantamiento de la misma respecto a épocas pasadas (del mismo modo suele darse una menopausia cada vez más tardía). Pero si la madurez sexo-genital es tardía en la especie humana, aún más suele serlo la madurez intelectual y, en especial, la madurez emotiva. Véanse también: Amor, Sexualidad humana y Sistema reproductivo. Origen y evolución Artículos principales: Evolución humana y Origen del hombre. Mitos sobre los orígenes Artículo principal: Mitos de la creación A lo largo de la historia se han ido desarrollando distintas concepciones míticas, religiosas, filosóficas y científicas respecto del ser humano, cada una con su propia explicación sobre el origen del hombre, trascendencia y misión en la vida. Esqueleto reconstruido de Proconsul, un primate hominoideo. De los simios del Viejo Mundo Artículo principal: Catarrinos Evolutivamente, en cuanto perteneciente al orden taxonómico Catarrhini, Homo sapiens parece tener su ancestro, junto con todos los primates catarrinos, en un período que va de los 50 a 33 millones de años antes del presente (AP). Uno de los primeros catarrinos, quizás el primero, es Propliopithecus, incluyendo a Aegyptopithecus. En este sentido, el ser humano actual, al igual que primates del «Viejo Mundo» con características más primitivas, probablemente descienda de esa antigua especie. Homínidos bípedos Artículo principal: Hominina Australopithecus africanus. En cuanto a la bipedestación, esta se observa en ciertos primates a partir del Mioceno. Ya se encuentran ejemplos de bipedación en Oreopithecus bambolii y la bipedestación parece haber sido común en Orrorin y Ardipithecus. Las mutaciones que llevaron a la bipedación fueron exitosas porque dejaban libres las manos para agarrar objetos y, particularmente, porque en la marcha un homínido ahorra mucha más energía andando sobre dos piernas que sobre cuatro patas, puede acarrear objetos durante la marcha y otear más lejos. Sin embargo, de remontarse la bipedestación a quizás a unos seis millones de años AP, la andadura o forma de marcha típica del humano se consolida aproximadamente hace al menos unos cuatro millones de años con Australopithecus. Previamente los primates antropoides apoyaban toda la planta del pie haciendo una flexión y descargando el peso en el calcáneo; en cambio, Australopithecus logra una marcha bípeda eficiente, pues se notan claramente los cambios anatómicos a nivel del pie, en especial del dedo gordo; también ajustando el ángulo del fémur con el cuerpo para el equilibrio, la cadera o pelvis cambia a más robusta, corta y cóncava (forma de cuenco); la columna pasó de ser un arco en forma de C a una forma de S y el agujero de la base del cráneo que conecta con la columna se desplazó hacia adelante como dirigiéndose al centro de gravedad de la cabeza. Hace 1.5 millones de años con Homo erectus o con Homo ergaster, la andadura moderna implica la existencia de un pequeño ángulo entre el dedo gordo y el eje del pie, así como la presencia del arco longitudinal de la planta y una distribución medial del peso (nótese que en las mujeres la andadura distribuye el peso más hacia las partes internas del pie debido a la mayor anchura de la pelvis). Todos los cambios reseñados han sucedido en un periodo relativamente breve (aunque se mida en millones de años). Esto explica la susceptibilidad de nuestra especie a afecciones en la columna vertebral y en la circulación sanguínea y linfática (por ejemplo, el corazón recibe ,relativamente, «poca» sangre). Aparece el ser humano Artículo principal: Homo Homo erectus. Lo que denominamos propiamente «humano» es una referencia a la aparición de la capacidad de fabricar herramientas de piedra en un homínido bípedo, Homo habilis, considerado por la mayoría como la especie humana más primitiva, mostrando además incremento en la capacidad craneana con respecto a Australopithecus. Es así como se establece que hace unos dos millones y medio de años, con la aparición del género Homo, se toma como punto de inicio para el Paleolítico o Edad de Piedra. Mayor éxito evolutivo tendrá Homo erectus, quien logrará expandirse por toda Eurasia. Véanse también: Prehistoria y Paleolítico inferior. Evolución de la nutrición Probablemente cuando los ancestros de Homo sapiens vivían en selvas comiendo frutos, bayas y hojas, abundantes en vitamina C, pudieron perder la capacidad metabólica que tiene la mayoría de los animales de sintetizar en su propio organismo tal vitamina; ya antes parecen haber perdido la capacidad de digerir la celulosa. Tales pérdidas durante la evolución han implicado sutiles pero importantes determinaciones: cuando las selvas originales se redujeron o, por crecimiento demográfico, resultaron superpobladas, los primitivos homínidos (y luego los humanos) se vieron forzados a recorrer importantes distancias, migrar, para obtener nuevas fuentes de nutrientes. La pérdida de la capacidad de metabolizar ciertos nutrientes como la vitamina C habría sido compensada por una mutación favorable que permite a Homo sapiens una metabolización óptima (ausente en primates) del almidón, y así una rápida y «barata» obtención de energía, particularmente útil para el cerebro. Homo sapiens parece ser una criatura bastante indefensa, y como respuesta satisfactoria la única solución evolutiva que ha tenido es su complejísimo sistema nervioso central, espoleado principalmente por la búsqueda de nuevas fuentes de alimentación. Se ha sugerido la hipótesis de que la cefalización aumentó paralelamente al incremento de consumo de carne,[cita requerida] aunque dicha hipótesis no concuerda con el grado de cefalización desarrollada por los animales carnívoros. La habilidad humana para digerir alimentos con alto contenido de almidón podría explicar el éxito del Homo sapiens en el planeta, y sugiere un estudio genético. Humanos arcaicos Artículo principal: Humanos arcaicos Homo neanderthalensis. Se denomina «humanos arcaicos», «Homo sapiens arcaico» o también «pre-sapiens», a un cierto número de especies de Homo que aun no son considerados anatómicamente modernos. Poseen hasta 600 000 años de antigüedad y tienen un tamaño cerebral cercano al del ser humano moderno. El antropólogo Robin Dunbar opina que es en esta etapa cuando aparece el lenguaje humano. La filiación de estos individuos dentro de nuestro género resulta aun controvertida. Entre los humanos arcaicos están considerados Homo heidelbergensis, Homo rhodesiensis, Homo neanderthalensis y a veces Homo antecessor. En 2010 se ha añadido a estos el denominado «hombre de Denísova», y en 2012 el denominado «hombre del ciervo rojo» en China. Ya que no son sapiens, algunos especialistas prefieren llamarlos simplemente arcaicos antes que H. sapiens arcaico. Humanos anatómicamente modernos Artículo principal: Humanos anatómicamente modernos Mujer de Qafzeh (anatómicamente moderna). Se denomina propiamente Homo sapiens o anatómicamente modernos a individuos con una apariencia similar a la del ser humano moderno. Estos humanos pueden clasificarse como «premodernos», pues en ellos no se observa todavía el conjunto de características de un cráneo moderno, casi esférico, con la bóveda alta y la frente vertical. La similitud se aprecia a nivel del esqueleto del cuerpo y cavidad craneana, pero esta similitud no es total pues el rostro aun mantiene características arcaicas como los arcos superciliares (grandes cejas) y prognatismo maxilar (proyección bucal), aunque menos desarrollados que en los neandertales. Se considera dentro de este grupo a los restos de Florisbad en Sudáfrica (260 000 años), los de Herto en Etiopía, que corresponde a Homo sapiens idaltu (160 000 años), los de Jebel Irhoud en Marruecos (315 000 años) y los de Skhul/Qafzeh al norte de Israel (100 000 años). También se considera anatómicamente modernos a los hombres de Kibish; sin embargo, estos se enmarcan mejor dentro del hombre moderno. Ser humano moderno Artículo principal: Origen de los humanos modernos Véase también: Adán cromosómico Véase también: Eva mitocondrial Ascendencia mitocondrial africana. Se considera Homo sapiens sapiens de forma indiscutible a los que poseen las características principales que definen al ser humano moderno: primero la equiparación anatómica con las poblaciones humanas actuales y luego lo que se define como «comportamiento moderno». Actualmente, gracias a los análisis científicos, se sabe que en la genealogía de la evolución humana habría existido un antepasado común masculino y uno femenino, a los cuales se les nombró como sus símiles religiosos. Los restos más antiguos son los de Omo I, llamados Hombres de Kibish, encontrados en Etiopía con 195 000 años, y restos en cuevas del río Klasies en Sudáfrica con 125 000 años y con indicios de una conducta más moderna. Esta antigüedad coincide con lo estimado para la Eva mitocondrial, la cual está considerada la antecesora de todos los seres humanos actuales y de la que se cree que vivió en el África Oriental (probablemente Tanzania) hace unos 200 000 años. Por otra parte, la línea patrilineal nos lleva hasta el Adán cromosómico, quien nos confirma un origen para el hombre moderno en el África subsahariana y se le calcula unos 140 000 años de antigüedad. Pigmentación Es casi seguro que la Eva mitocondrial y el Adán cromosómico, los primeros Homo sapiens eran melanodérmicos, esto es, de tez oscura. Esto se debe a que la piel oscura es una excelente adaptación a la exposición solar alta de las zonas intertropicales del planeta Tierra; la tez oscura (por la melanina) protege de las radiaciones UV (ultravioletas) y obtiene de ellas por metabolismo un nutriente llamado folato, indispensable para el desarrollo del embrión y del feto; pero, a medida que las poblaciones humanas migraron a latitudes más allá de los 45° (tanto norte como sur) la melanina paulatinamente fue menos necesaria, más aún, en las cercanías de las latitudes de los 50° la casi total falta de este pigmento en la dermis, cabello y ojos ha sido una adaptación para captar más radiaciones U.V. ,relativamente escasas en tales latitudes, salvo que se produzcan huecos de ozono,; en tales latitudes la tez muy clara posibilita una mayor metabolización de vitamina D a partir de las radiaciones UV. Comportamiento moderno Indígenas karajá de Brasil. El uso de adornos personales es un comportamiento humano ampliamente extendido. La aparición del comportamiento humano moderno significó el más importante cambio en la evolución de la mente humana, dando lugar a que el ingenio creativo humano le permitiese dominar su entorno paulatinamente. Las innovaciones que fueron apareciendo consisten en una gran diversidad de herramientas de piedra, en el uso de hueso, asta y marfil, en entierros con bienes funerarios y rituales, construcción de viviendas, diseño de las fogatas, evidencia de pesca, cacería compleja, aparición del arte figurativo y el uso de adornos personales. Las evidencias más antiguas se encuentran en África; herramientas elaboradas hace 165 000 años se encontraron en la cueva de Pinnacle Point (Sudáfrica). Restos de puntas de flechas y herramientas de hueso para pescar se encontraron en el Congo y tienen 90 000 años. Igualmente antiguos son unos símbolos sombreados con ocre rojo en costas al sur de África. Véase también: Paleolítico superior Expansión de la humanidad Artículo principal: Expansión de la humanidad Mapa de las migraciones humanas fuera de África, versión de Naruya Saitou y Masatoshi Nei (2002) del Instituto Nacional de la Genética del Japón que coincide con la versión de Göran Burenhult (2000). Según la teoría fuera de África, hubo una gran migración de África hacia Eurasia hace 70 000 años que produjo la paulatina dispersión por todos los continentes. Según los estudios genéticos y los descubrimientos paleontológicos, se estima que hace 60 000 años hubo una migración costera por el Sur de Asia, de pocos miles de años, que posibilitó la colonización posterior de Australia, Extremo Oriente y Europa. En Occidente hubo un centro de expansión en Oriente Medio que está relacionado con el hombre de Cromañón y la población temprana de Europa, probable causa de la extinción del hombre de Neandertal. Según algunos estudios genéticos, en Europa hubo tres migraciones: la primera, proveniente del Asia Central hace 40 000 años que colonizó la Europa Oriental. Una segunda oleada hace 22 000 años, proveniente del Oriente Medio, que se instaló en la Europa del sur y del oeste. El 80 % de los europeos actuales son descendientes de estas dos migraciones, que durante el transcurso del máximo glaciar de hace 20 000 años se refugiaron en la península ibérica y en los Balcanes, para volver a expandirse por el resto de Europa cuando llegó el clima favorable. La tercera migración se habría producido hace 9000 años, proveniente del Oriente Medio, durante el transcurso del Neolítico, y solo el 20 % de los europeos actuales llevan marcadores genéticos correspondientes a esos emigrantes. Otros estudios dicen lo contrario, afirmando que en Europa el componente neolítico desde Oriente Próximo es el más importante. Lo cierto por ahora es que el acervo genético europeo prehistórico proviene mayoritariamente del Cercano Oriente, y una menor parte proviene de África, Asia Central y Siberia. En Oriente la población es igualmente antigua. El pliegue epicántico de los párpados existente en gran parte de las poblaciones del Asia y de América, el pliegue que hace bridados en su aspecto externo a los ojos, ha sido una especialización de poblaciones que durante las glaciaciones debieron pervivir en lugares con abundancia de nieve; los ojos vulgarmente llamados «rasgados» entonces fueron el modo de adaptación para que los ojos no padecieran un excesivo reflejo de la luz solar reflejada por la nieve.[cita requerida] Sin embargo, una publicación de julio de 2019 en la revista Nature puso en tela de juicio las teorías e ideas previas acerca del momento del poblamiento de Europa por Homo sapiens desde África. El hallazgo y datación de un cráneo de Homo sapiens de 210 000 años de antigüedad en Grecia significaría un poblamiento de Europa 60 000 años más temprano que lo que se suponía. Véanse también: Neolítico, Civilización, Historia y Arqueología. Cultura Artículo principal: Cultura Lenguaje y semiótica Artículo principal: Lenguaje humano El lenguaje designa todas las comunicaciones basadas en la interpretación, incluyendo el lenguaje humano, pero la mayoría de las veces la locución se refiere a lo que los humanos utilizan para comunicarse, es decir, a los idiomas. El lenguaje es universal y es usado por naturaleza en las personas y en el resto de animales. Sin embargo, filósofos como Martin Heidegger consideran que el lenguaje propiamente tal es solo privativo del hombre. Es famosa la tesis de Heidegger según la cual el lenguaje es la casa del ser (Haus des Seins) y la morada de la esencia humana. Este criterio es similar al de Ernst Cassirer, quien ha definido a Homo sapiens como el animal simbólico por excelencia; tan es así que es casi imposible suponer un pensamiento humano sin la ayuda de los símbolos, particularmente de los significantes que subyacen como fundamentos elementales para todo pensar complejo y que transcienda a lo instintivo. Actualmente la especie humana muestra esta faceta hablando en torno a 6000 idiomas diferentes, si bien más del 50 % de los 8000 millones de personas que actualmente conforman la colectividad humana, sabe hablar al menos uno de los siguientes: chino mandarín, español, inglés, francés, árabe, hindi, portugués, alemán, bengalí o ruso. Véanse también: Lenguaje, Familias de lenguas, Idioma, Lingüística y Evolución del lenguaje. Espiritualidad y trascendencia Artículo principal: Religión En muchas civilizaciones los seres humanos se han visto a sí mismos como diferentes de los demás animales, y en ciertos ámbitos culturales (como las religiones del Libro o buena parte de la metafísica del Occidente) la diferencia se asigna a una entidad inmaterial llamada alma, en la que residirían la mente y la personalidad, y que algunos creen que puede existir con independencia del cuerpo. La espiritualidad del ser humano está ligada a la idea de un Dios, el cual abre el paso a muchas interpretaciones sobre la inmortalidad, el alma y sobre todo el más allá. Véase también: Familias de religiones Arte y cultura Artículo principal: Arte Posiblemente, la manifestación más clara de humanidad es el arte ,en el sentido amplio del término,, que produce la cultura. Por ejemplo, los individuos de una determinada especie de ave fabrican un nido, o emiten un canto, cuyas características son específicas, comunes a todos los individuos de esa especie. En cambio, cada hombre puede imprimir a sus acciones los rasgos propios de su individualidad; por eso, cuando se analiza un cuadro, una forma de escribir, una manera de fabricar herramientas, etc., se puede deducir quién es su autor, su artífice, su artista.[cita requerida] En 2011, en la revista Science, se publicó un trabajo de Francesco dErrico, de la Universidad de Burdeos, donde afirma haber encontrado uno de los rastros más antiguos de un taller de pintura, en la cueva Blombos en Cape Coast, 300 km al este de Ciudad del Cabo. Este hecho muestra un modo sistemático para obtener pigmentos, pues reunir todos los elementos necesarios para una preparación de este tipo es indicativo de un elevado nivel de pensamiento, que se puede llamar pensamiento simbólico. «La capacidad de tener estos pensamientos es considerada un gran paso en la evolución humana, precisamente lo que nos diferenció del mundo animal». Paralelamente, también es la única especie que dedica su tiempo y energía a algo aparentemente inútil desde el punto de vista puramente práctico. El arte es una de las manifestaciones de la creatividad humana, pero una manifestación vacía y negativa desde el punto de vista de la supervivencia. Si bien esta actividad es en principio dañina, en realidad es la herramienta con la cual Homo sapiens desarrolla su cultura, unión y fuerza como pueblo.[aclaración requerida][cita requerida] Véanse también: Humanidades, Música, Danza, Historia del arte y Filosofía. Ciencia Esta sección es un extracto de Ciencia.[editar] Ciencia a través de escalas. De arriba abajo, en el sentido de las agujas del reloj: investigadores en un laboratorio; el Observatorio Paranal y la Vía Láctea; bacteria Escherichia coli bajo el microscopio; y funciones de onda del electrón en un átomo de hidrógeno. La ciencia es una disciplina sistemática que construye y organiza conocimiento en forma de hipótesis y predicciones comprobables sobre el universo. La ciencia moderna se divide típicamente en dos o tres ramas principales: las ciencias naturales (por ejemplo, física, química y biología), que estudian el mundo físico; y las ciencias sociales (por ejemplo, economía, psicología y sociología), que estudian a los individuos y las sociedades. Las ciencias formales (por ejemplo, lógica, matemáticas y ciencia computacional teórica), que estudian sistemas formales regidos por axiomas y reglas, a veces también se describen como ciencias; sin embargo, a menudo se consideran un campo separado porque dependen del razonamiento deductivo en lugar del método científico o de la evidencia empírica como su principal metodología. Las ciencias aplicadas son disciplinas que utilizan el conocimiento científico con fines prácticos, como la ingeniería y la medicina. La historia de la ciencia abarca la mayoría del registro histórico, siendo los primeros precursores identificables de la ciencia moderna datados en la Edad de Bronce en Egipto y Mesopotamia (aproximadamente 3000–1200 a. C.). Sus contribuciones a las matemáticas, la astronomía y la medicina entraron y moldearon la filosofía natural griega de la antigüedad clásica, mediante la cual se intentaron proporcionar explicaciones de los eventos en el mundo físico basadas en causas naturales. La investigación científica decayó en estas regiones después de la caída del Imperio romano de Occidente durante la Alta Edad Media (400–1000 d. C.), pero en los renacimientos medievales (Renacimiento carolingio, Renacimiento otoniano y el Renacimiento del siglo XII) la erudición floreció nuevamente. Algunos manuscritos griegos perdidos en Europa Occidental fueron preservados y ampliados en el Medio Oriente durante la Edad de Oro del islam, junto con los esfuerzos posteriores de los eruditos griegos bizantinos, quienes llevaron manuscritos griegos desde el moribundo Imperio Bizantino a Europa Occidental al comienzo del Renacimiento. La recuperación y asimilación de obras griegas e investigaciones islámicas en Europa Occidental desde los siglos X al XIII revivió la filosofía natural, que luego fue transformada por la Revolución Científica que comenzó en el siglo XVI cuando nuevas ideas y descubrimientos se apartaron de las concepciones y tradiciones griegas anteriores. El método científico pronto desempeñó un papel más importante en la creación del conocimiento y no fue hasta el siglo XIX cuando muchas de las características institucionales y profesionales de la ciencia comenzaron a tomar forma, junto con el cambio de filosofía natural a ciencias naturales. El nuevo conocimiento en la ciencia avanza mediante la investigación de científicos motivados por la curiosidad sobre el mundo y el deseo de resolver problemas. La investigación científica contemporánea es altamente colaborativa y suele realizarse por equipos en instituciones académicas y de investigación, agencias gubernamentales y empresas. El impacto práctico de su trabajo ha llevado al surgimiento de políticas científicas que buscan influir en la empresa científica, priorizando el desarrollo ético y moral de productos comerciales, armamentos, atención médica, infraestructura pública y protección del medio ambiente. Véase también: Tecnología Sociedad Artículo principal: Sociedad Una sociedad humana es aquella que se considera a sí misma, a los habitantes y a su entorno, todo ello interrelacionado con un proyecto común, que les da una identidad de pertenencia. Asimismo, el término connota un grupo con lazos económicos, ideológicos y políticos. Tal sociedad supera al concepto de nación-estado, planteando a la sociedad occidental como una sociedad de naciones, organizaciones nacionales, etc.[cita requerida] Véanse también: Sistema social, Comercio, Estado, Gobierno y Guerra. Hábitat Cuenca. El ser humano es capaz de modificar enteramente su entorno para adaptarlo a sus necesidades. Artículo principal: Geografía humana En relación con la capacidad para realizar grandes modificaciones ambientales, cabe decir que Homo sapiens es actualmente un poderoso agente geomorfológico; es en este y otros sentidos que el ser humano es actualmente el mayor superpredador y la especie más poderosa del planeta. Sin embargo, sigue siendo frágil ante posibles acontecimientos cataclísmicos que pudieran afectar a su hábitat, como las glaciaciones. Homo sapiens, por ser un animal muy vulnerable en el medio natural, es muy dependiente de la tecnología (ergo: es dependiente de la ciencia por primitiva que esta sea), así es que se dice de Homo sapiens que es homo faber. Quizás, dado que todo sistema retroalimentado de forma natural llega a su fin, el fin de un ecosistema llega cuando la vida ha logrado evolucionar hasta lograr seres con un grado de consciencia capaz de programarse en función de la educación recibida y no según lo termodinámicamente sostenible.[cita requerida] La educación es, por tanto, la demostración evidente de si somos parte de un sistema aun mayor o intentamos independizarnos de todo, estableciendo nuestras formas de obtener nuestros recursos, sin tener en cuenta los ya establecidos por la propia naturaleza. Por ejemplo, la naturaleza le dota de capacidades físicas para buscar alimentos en el medio que les rodea de una manera termodinámicamente eficaz. Los humanos establecen que lo mejor es racionalizar los medios que la naturaleza les da y replicarlos de forma industrial, aplicando procesos que no se dan de forma natural, aumentando el consumo energético por redundar algo que ya existe y ampliándolo a algo totalmente termodinámicamente innecesario, como es el hecho de que se le entregue alimento en casa, de intervenir los códigos genéticos de las especies alimentarias para hacerlas resistentes a enfermedades, de influir en qué alimentos contendrán semillas y cuáles no y un largo etcétera, que a día de hoy nos hace la vida más cómoda, pero que ignoran cómo les afectan esos cambios en su estructura genética y, por lo tanto, si su descendencia portará características fundamentales para sobrevivir a un medio natural o, por el contrario, nacerán y dependerán tan íntimamente del medio artificial que cualquier modificación a ese medio le incapacite de tal manera que provoque su extinción."
ksampletext_wikipedia_anth_cultura: str = "Cultura. La cultura es un concepto que abarca el comportamiento social, las instituciones y las normas que se encuentran en las sociedades humanas, así como el conocimiento, las creencias, el arte, las leyes, las costumbres, las actitudes y los hábitos de los individuos de estos grupos. La cultura a menudo se origina en una región o ubicación específica, o se atribuye a ella. Los humanos adquieren cultura a través de los procesos de aprendizaje de enculturación y socialización, que se muestran en la diversidad de culturas en las sociedades. Una norma cultural codifica la conducta aceptable en la sociedad; sirve como guía para el comportamiento, la vestimenta, el lenguaje y el comportamiento en una situación, que sirve como plantilla para las expectativas en un grupo social. El cambio cultural es la reconstrucción de un concepto cultural de una sociedad. Las culturas se ven afectadas internamente tanto por fuerzas que fomentan el cambio como por fuerzas que se resisten al cambio, y externamente a través del contacto entre sociedades. Organizaciones como la Unesco intentan preservar la cultura y el patrimonio cultural. Naturaleza y cultura Una primera distinción en el conocimiento científico es la que se establece entre naturaleza y cultura. Esa distinción significa que el mundo de la naturaleza es el que no ha sido creado por el hombre, al menos en sus orígenes; mientras que la cultura es el mundo creado por los seres humanos, tal como se explica extensamente en el libro The Man-Made World (). Cuando el término cultura surgió en Europa, entre los siglos XVIII y XIX, se refería a un proceso de tecnología o mejora, como en la agricultura u horticultura. En el siglo XIX, pasó primero a referirse al mejoramiento o refinamiento de lo individual, especialmente a través de la educación, y luego al logro de las aspiraciones o ideales nacionales. A mediados del siglo XIX, algunos científicos utilizaron el término «cultura» para referirse a la capacidad humana universal. Para el antipositivista y sociólogo alemán Georg Simmel, la cultura se refería a «la cultivación de los individuos a través de la injerencia de formas externas que se han materializado en el transcurso de la historia». En el siglo XX, la «cultura» surgió como un concepto central de la antropología, abarcando todos los fenómenos humanos que no son el total resultado de la genética. Específicamente, el término «cultura» en la antropología americana tiene dos significados: (1) la evolucionada capacidad humana de clasificar y representar las experiencias con símbolos y actuar de forma imaginativa y creativa; y (2) las distintas maneras en que la gente vive en diferentes partes del mundo, clasificando y representando sus experiencias y actuando creativamente. Después de la Segunda Guerra Mundial, el término se volvió importante, aunque con diferentes significados, en otras disciplinas como estudios culturales, psicología organizacional, sociología de la cultura y estudios gerenciales. Algunos etólogos han hablado de «cultura» para referirse a costumbres, actividades o comportamientos transmitidos de una generación a otra en grupos de animales por imitación consciente de dichos comportamientos.[cita requerida] Las creencias y prácticas de una cultura determinada pueden ser ejercidas como mecanismos de control que limitan la conducta social. La cultura se asocia con la libertad, ya que es el vehículo entre el conocimiento y nuevas formas de conciencia que permiten una desestabilización en la hegemonía. Además puede reconocerse como conjuntos o modos de vida y costumbres de una época o grupo social. El término cultura puede alcanzar extensión y usos diversos, como diversidad cultural, objeto del conocimiento empírico, y la diferencia cultural. Otros conceptos de cultura La palabra cultura se asocia con la acción de cultivar o practicar algo, también según la RAE puede ser el resultado o efecto de prevalecer conocimientos humanos y conjuntos de modos de vida. La cultura ha sido vista dentro de los proyectos de modernidad. Una dimensión y expresión de la vida humana, se realiza mediante la utilización de símbolos y artefactos, en los que hay un campo de producción, circulación y consumo de signos y como una praxis que se articula en una teoría. En el diccionario se nombran diferentes tipos de culturas y entre ellas las dos más emblemáticas son la cultura popular y la cultura de masas. Formación del concepto de cultura Etimología La etimología del concepto moderno “cultura” tiene un origen antiguo. En varias lenguas europeas, la palabra “cultura” está basada en el término latino utilizado por Cicerón, en su Tusculanae Disputationes, quien escribió acerca de un cultivo del alma o “cultura animi”, utilizando así una metáfora agrícola para describir el desarrollo de un alma filosófica, que fue comprendida teleológicamente como uno de los ideales más altos posibles para el desarrollo humano. Samuel Pufendorf llevó esta metáfora a un concepto moderno, con un significado similar, pero ya sin asumir que la filosofía es la perfección natural del hombre. Para este autor, los significados de cultura, que muchos escritores posteriores retoman, “se refieren a todas las formas en la que los humanos comienzan a superar su barbarismo original y, a través de artificios, se vuelven completamente humanos”. Como lo describe Velkley: El término “cultura”, que originalmente significaba la cultivación del alma o la mente, adquiere la mayoría de sus posteriores significados en los escritos de los pensadores alemanes del siglo XVIII, quienes en varios niveles desarrollaron la crítica de Rousseau al liberalismo moderno y la Ilustración. Además, un contraste entre “cultura” y “civilización” está usualmente implícito por estos autores, aun cuando no lo expresen así. Dos significados primarios de cultura surgen de este período: cultura como un espíritu folclórico con una identidad única, y cultura como la cultivación de la espiritualidad o la individualidad libre. El primer significado es predominante dentro de nuestro uso actual del término “cultura”, pero el segundo juega todavía un importante rol en lo que creemos debería lograr la cultura, como la “expresión” plena del ser único y “auténtico”. Concepción clásica de la cultura En sus primeras acepciones, cultura designaba el cultivo de los campos. El término cultura proviene del latín cultus que a su vez deriva de la voz colere que significa cuidado del campo o del ganado. Hacia el siglo XIII, el término se empleaba para designar una parcela cultivada, y tres siglos más tarde había cambiado su sentido de estado de una cosa a la propia acción que lleva a dicho estado: el cultivo de la tierra o el cuidado del ganado (Cuche, 1999: 10), aproximadamente en el sentido en que se emplea en el español de nuestros días en vocablos como agricultura, apicultura, piscicultura y otros. Por la mitad del siglo XVI, el término adquiere una connotación metafórica, como el cultivo de cualquier facultad. De cualquier manera, la acepción figurativa de cultura no se extenderá hasta el siglo XVII, cuando también aparece en ciertos textos académicos. El Siglo de las Luces (siglo XVIII) es la época en que el sentido figurado del término como “cultivo del espíritu” se impone en amplios campos académicos. Por ejemplo, el Dictionnaire de lAcadémie Française de 1718. Y aunque la Enciclopedia lo incluye solo en su sentido restringido de cultivo de tierras, no desconoce el sentido figurado, que aparece en los artículos dedicados a la literatura, la pintura, la filosofía y las ciencias. Con el paso del tiempo, como cultura se entenderá la formación de la mente. Es decir, se convierte nuevamente en una palabra que designa un estado, aunque en esta ocasión es el estado de la mente humana, y no el estado de las parcelas. Voltaire, uno de los pocos pensadores franceses del siglo XVIII que se mostraban partidarios de una concepción relativista de la historia humana. La clásica oposición entre cultura y naturaleza también tiene sus raíces en esta época. En 1798, el Dictionnaire incluye una acepción de cultura en que se estigmatiza el “espíritu natural”. Para muchos de los pensadores de la época, como Jean Jacques Rousseau, la cultura es un fenómeno distintivo de los seres humanos, que los coloca en una posición diferente a la del resto de animales. La cultura es el conjunto de los conocimientos y saberes acumulados por la humanidad a lo largo de sus milenios de historia. En tanto una característica universal (el vocablo), se emplea en número singular, puesto que se encuentra en todas las sociedades sin distinción de etnias, ubicación geográfica o momento histórico. Cultura y civilización También es en el contexto de la Ilustración cuando surge otra de las clásicas oposiciones en que se involucra a la cultura, esta vez, como sinónimo de la civilización. Esta palabra aparece por primera vez en la lengua francesa del siglo XVIII, y con ella se significaba la refinación de las costumbres. Civilización es un término relacionado con la idea de progreso. Según esto, la civilización es un estado de la Humanidad en el cual la ignorancia ha sido abatida y las costumbres y relaciones sociales se hallan en su más elevada expresión. La civilización no es un proceso terminado, es constante, e implica el perfeccionamiento progresivo de las leyes, las formas de gobierno, el conocimiento. Como la cultura, también es un proceso universal que incluye a todos los pueblos, incluso a los más atrasados en la línea de la evolución social. Desde luego, los parámetros con los que se medía si una sociedad era más civilizada o más salvaje eran los de su propia sociedad. En los albores del siglo XIX, ambos términos, cultura y civilización eran empleados casi del mismo modo, sobre todo en francés e inglés (Thompson, 2002: 186). Johann Gottfried Herder. Según él, la cultura podía entenderse como la realización del genio nacional (Volksgeist). Es necesario señalar que no todos los intelectuales franceses emplearon el término. Rousseau y Voltaire se mostraron reticentes a esta concepción progresista de la historia. Intentaron proponer una versión más relativista de la historia, aunque sin éxito, pues la corriente dominante era la de los progresistas. No fue en Francia, sino en Alemania donde las posturas relativistas ganaron mayor prestigio. El término Kultur en sentido figurado aparece en Alemania hacia el siglo XVII -aproximadamente con la misma connotación que en francés. Para el siglo XVIII goza de gran prestigio entre los pensadores burgueses alemanes. Esto se debió a que fue empleado para denostar a los aristócratas, a los que acusaban de tratar de imitar las maneras “civilizadas” de la corte francesa. Por ejemplo, Immanuel Kant apuntaba que “nos cultivamos por medio del arte y de la ciencia, nos civilizamos [al adquirir] buenos modales y refinamientos sociales” (Thompson, 2002: 187). Por lo tanto, en Alemania el término civilización fue equiparado con los valores cortesanos, calificados de superficiales y pretenciosos. En sentido contrario, la cultura se identificó con los valores profundos y originales de la burguesía (Cuche, 1999:13). En el proceso de crítica social, el acento en la dicotomía cultura/civilización se traslada de las diferencias entre estratos sociales a las diferencias nacionales. Mientras Francia era el escenario de una de las revoluciones burguesas más importantes de la historia, Alemania estaba fragmentada en múltiples Estados. Por ello, una de las tareas que se habían propuesto los pensadores alemanes era la unificación política. La unidad nacional pasaba también por la reivindicación de las especificidades nacionales, que el universalismo de los pensadores franceses pretendía borrar en nombre de la civilización. Ya en 1774, Johann Gottfried Herder proclamaba que el genio de cada pueblo (Volksgeist) se inclinaba siempre por la diversidad cultural, la riqueza humana y en contra del universalismo. Por ello, el orgullo nacional radicaba en la cultura, a través de la que cada pueblo debía cumplir un destino específico. La cultura, como la entendía Herder, era la expresión de la humanidad diversa, y no excluía la posibilidad de comunicación entre los pueblos. Durante el siglo XIX, en Alemania el término cultura evoluciona bajo la influencia del nacionalismo. Mientras tanto, en Francia, el concepto se amplió para incluir no solo el desarrollo intelectual del individuo, sino el de la humanidad en su conjunto. De aquí, el sentido francés de la palabra presenta una continuidad con el de civilización: no obstante la influencia alemana, persiste la idea de que más allá de las diferencias entre “cultura alemana” y “cultura francesa” (por poner un ejemplo), hay algo que las unifica a todas: la cultura humana. Definiciones de cultura Para efecto de las ciencias sociales, las primeras acepciones de cultura fueron construidas a finales del siglo XIX. Por esta época, la sociología y la antropología eran disciplinas relativamente nuevas, y la pauta en el debate sobre el tema que aquí nos ocupa la llevaba la filosofía. Los primeros sociólogos, como Émile Durkheim, rechazaban el uso del término. Hay que recordar que en su perspectiva, la ciencia de la sociedad debía abordar problemas relacionados con la estructura social. Si bien es opinión generalizada que Karl Marx dejó de lado a la cultura, ello se ve refutado por las mismas obras del autor, sosteniendo que las relaciones sociales de producción (la organización que adoptan los seres humanos para el trabajo y la distribución social de sus frutos) constituyen la base de la superestructura jurídico-política e ideológica, pero en ningún caso un aspecto secundario de la sociedad. No es concebible una relación social de producción sin reglas de conducta, sin discursos de legitimación, sin prácticas de poder, sin costumbres y hábitos permanentes de comportamiento, sin objetos valorados tanto por la clase dominante como por la clase dominada. El desvelo de las obras juveniles de Marx, tanto de La ideología alemana (1845-1846) en 1932 por la célebre edición del Instituto Marx-Engels de la URSS bajo dirección de David Riazanov, como de los Manuscritos económicos y filosóficos (1844) posibilitó que varios partidarios de sus propuestas teóricas desarrollaran una teoría de la cultura marxista (véase más adelante). Generalmente, el significado de cultura se relaciona con la antropología, una de las ramas más importantes de la disciplina social que se encarga precisamente del estudio comparativo de la cultura. Quizá por la centralidad que la palabra tiene en la teoría de la antropología, el término ha sido desarrollado de diversas maneras, que suponen el uso de una metodología analítica basada en premisas que en ocasiones distan mucho las unas de las otras. Fue Franz Boas, frente a esta empresa etnocentrista, quien opera el gran cambio epistemológico en la antropología. A partir de Boas, padre del relativismo cultural, el antropólogo se hace traductor, pudiendo entrar en la cosmovisión del estudiado y entender el mundo de sus significaciones. Ese mundo de la significación, irá conduciendo, lentamente, hacia un concepto que no es antropológico, y es el de la construcción social del sentido. Ante este aumento de la sensibilidad vinculada con las cuestiones del lenguaje, se desarrollarán disciplinas nuevas, vinculadas con el mundo de la significación humana y del lenguaje, que completarán la idea de la cultura entendida desde el mundo de la significación. De acuerdo con la Declaración Universal sobre la Diversidad Cultural de la UNESCO la cultura debe ser considerada como el conjunto de los rasgos distintivos espirituales y materiales, intelectuales y afectivos que caracterizan a una sociedad o a un grupo social y que abarca, además de las artes y las letras, los modos de vida, las maneras de vivir juntos, los sistemas de valores, las tradiciones y las creencias Los etnólogos y antropólogos británicos y estadounidenses de las postrimerías del siglo XIX retomaron el debate sobre el contenido de cultura. Estos autores tenían casi siempre una formación profesional en derecho, pero estaban particularmente interesados en el funcionamiento de las sociedades exóticas con las que Occidente se encontraba en ese momento. En la opinión de estos pioneros de la etnología y la antropología social (como Bachoffen, McLennan, Maine y Morgan), la cultura es el resultado del devenir histórico de la sociedad. Pero la historia de la humanidad en estos escritores era fuertemente deudora de las teorías ilustradas de la civilización, y sobre todo, del darwinismo social de Spencer. Definiciones descriptivas de cultura Definición de Tylor E. B. Tylor, etnólogo británico, dijo: “La principal tendencia de la cultura desde los orígenes a los tiempos modernos ha sido del salvajismo hacia la civilización” (1995:43). Como señala Thompson (2002:190), la definición descriptiva de cultura se encontraba presente en esos primeros autores de la antropología decimonónica. El interés principal en la obra de estos autores (que abordaba problemáticas tan disímbolas como el origen de la familia y el matriarcado, y las supervivencias de culturas antiquísimas en la civilización occidental de su tiempo) era la búsqueda de los motivos que llevaban a los pueblos a comportarse de tal o cual modo. En esas exploraciones, meditarente, o entre la tecnología y el resto del sistema social. Uno de los más importantes etnógrafos de la época fue Gustav Klemm. En los diez tomos de su obra Allgemeine Kulturgeschichte der Menschheit (1843-1852) intentó mostrar el desarrollo gradual de la humanidad por medio del análisis de la tecnología, costumbres, arte, herramientas, prácticas religiosas. Una obra monumental, pues incluía ejemplos etnográficos de pueblos de todo el mundo. El trabajo de Klemm habría de tener eco en sus contemporáneos, empeñados en definir el campo de una disciplina científica que estaba naciendo. Unos veinte años más tarde, en 1871, Edward B. Tylor publicó en Primitive Culture una de las definiciones más ampliamente aceptadas de cultura. Según Tylor, la cultura es: ...aquel todo complejo que incluye el conocimiento, las creencias, el arte, la moral, el derecho, las costumbres, y cualesquiera otros hábitos y capacidades adquiridos por el hombre. La situación de la cultura en las diversas sociedades de la especie humana, en la medida en que puede ser investigada según principios generales, es un objeto apto para el estudio de las leyes del pensamiento y la acción del hombre. (Tylor, 1995: 29) De esta suerte, uno de los principales aportes de Tylor fue la elevación de la cultura como materia de estudio sistemático. A pesar de este notable avance conceptual, la propuesta de Tylor adolecía de dos grandes debilidades. Por un lado, sacó del concepto su énfasis humanista al convertir a la cultura en objeto de ciencia. Por el otro, su procedimiento analítico era demasiado descriptivo. En el texto citado arriba, Tylor plantea que “un primer paso para el estudio de la civilización consiste en diseccionarla en detalles, y clasificar éstos en los grupos adecuados” (Tylor, 1995:33). Según esta premisa, la mera recopilación de los “detalles” permitiría el conocimiento de una cultura. Una vez conocida, sería posible clasificarla en una graduación de más a menos civilizada, premisa que heredó de los darwinistas sociales. Definición de los culturalistas Una mujer hopi arregla el peinado de una joven soltera de su tribu. Los antropólogos estadounidenses de la primera mitad del siglo XX estaban muy interesados en la documentación etnográfica de los pueblos indios, algunos de los cuales estaban en proceso de extinción. La propuesta teórica de Tylor fue retomada y reelaborada posteriormente, tanto en Gran Bretaña como en Estados Unidos. En este último país, la antropología evolucionaba hacia una posición relativista, representada en primera instancia por Franz Boas. Esta posición representaba un rompimiento con las ideas anteriores sobre la evolución cultural, en especial las propuestas por los autores británicos y el estadounidense Lewis Henry Morgan. Para este último, contra quien Boas dirigió sus críticas en uno de sus pocos textos teóricos, el proceso de la evolución social humana (tecnología, relaciones sociales y cultura) podía ser equiparado con el proceso de crecimiento de un individuo de la especie. Por lo tanto, Morgan comparaba el salvajismo con la “infancia de la especie humana”, y la civilización, con la madurez. Boas fue sumamente duro con las propuestas de Morgan y el resto de los antropólogos evolucionistas contemporáneos. A lo que sus autores llamaban “teorías” sobre la evolución de la sociedad, Boas las calificó de “puras conjeturas” sobre el ordenamiento histórico de “fenómenos observados conforme a principios admitidos [de antemano]” (1964:184). La crítica de Boas en contra de los evolucionistas es un eco de la perspectiva de los filósofos alemanes como Herder y Wilhelm Dilthey. El núcleo de la propuesta radica en su inclinación a considerar la cultura como un fenómeno plural. En otras palabras, más que hablar de cultura, Boas hablaba de culturas. Para la mayor parte de los antropólogos y etnólogos adscritos a la escuela culturalista estadounidense, el estado del arte etnográfico al principio del siglo XX no permitía la conformación de una teoría general sobre la evolución de las culturas. Por lo tanto, la labor más importante de los estudiosos del fenómeno debía ser la documentación etnográfica. De hecho, Boas escribió muy pocos textos teóricos, en comparación con sus monografías sobre los pueblos indígenas de la costa pacífica de América del Norte. Los antropólogos formados por Robin Reid hubieron de heredar muchas de las premisas de su maestro. Entre otros casos notables, están el de Ruth Benedict. En su obra Patterns of culture (1939), Benedict señala que cada cultura es un todo comprensible solo en sus propios términos y constituye una suerte de matriz que da sentido a la actuación de los individuos en una sociedad. Alfred Kroeber, retomando la oposición entre cultura y naturaleza, también señalaba que las culturas son fenómenos sui generis pero, en sentido estricto, eran de una categoría exterior a la naturaleza. Por lo tanto, según Kroeber, el estudio de las culturas debía salirse del dominio de las ciencias naturales y encarar a las primeras como lo que eran: fenómenos superorgánicos. Melville Herskovits y Clyde Kluckhohn retomaron de Tylor su definición cientificista del estudio de la cultura. Para el primero, también la recolección de rasgos definitorios de las culturas permitiría su clasificación. Aunque, en este caso, la clasificación no se realizaba en sentido diacrónico, sino espacial-geográfico que habría de permitir el conocimiento de las relaciones entre los diferentes pueblos asentados en un área cultural. Kluckhonn, por su parte, resume en su texto Antropología la mayor parte de los postulados vistos en esta sección, y reclama el dominio de lo cultural como el campo específico de la actividad antropológica. Definición funcionalista-estructural La característica más peculiar del concepto funcionalista de cultura se refiere precisamente a la función social de la misma. El supuesto básico es que todos los elementos de una sociedad (entre los que la cultura es uno más) existen porque son necesarios. Esta perspectiva ha sido desarrollada tanto en antropología como en sociología aunque, sin duda, sus primeras características fueron delineadas involuntariamente por Émile Durkheim. Este sociólogo francés muy pocas veces empleó el término como unidad analítica principal de su disciplina. En su libro Las reglas del método sociológico (1895), plantea que la sociedad está compuesta por entidades que tienen una función específica, integradas en un sistema análogo al de los seres vivos, donde cada órgano está especializado en el cumplimiento de una función vital. Del mismo modo en que los órganos de un cuerpo son susceptibles a la enfermedad, las instituciones y costumbres, las creencias y las relaciones sociales también pueden caer en un estado de anomia. Durkheim y sus seguidores, sin embargo, no se ocupan exclusiva ni principalmente de la cultura como objeto de estudio, sino de hechos sociales. A pesar de ellos, sus propuestas analíticas fueron retomadas por autores conspicuos de la antropología social británica y la sociología de la cultura de Estados Unidos. Más tarde, el polaco Bronislaw Malinowski retomó tanto la descripción de cultura de Tylor como algunos de los planteamientos de Durkheim relativos a la función social. Para Malinowski, la cultura podía ser entendida como una «realidad sui generis» que debía estudiarse como tal (en sus propios términos). En la categoría de cultura incluía artefactos, bienes, procesos técnicos, ideas, hábitos y valores heredados (Thompson, 2002: 193). También consideraba que la estructura social podía ser entendida análogamente a los organismos vivos pero, a diferencia de Durkheim, Malinowski tenía una tendencia más holística. Malinowski creía que todos los elementos de la cultura poseían una función que les daba sentido y hacía posible su existencia. Pero esta función no era dada únicamente por lo social, sino por la historia del grupo y el entorno geográfico, entre muchos otros elementos. El reflejo más claro de este pensamiento aplicado al análisis teórico fue el libro Los argonautas del Pacífico Occidental (1922), una extensa y detallada monografía sobre las distintas esferas de la cultura de los isleños trobriandeses, un pueblo que habitaba en las islas Trobriand, al oriente de Nueva Guinea. Años más tarde, Alfred Reginald Radcliffe-Brown, también antropólogo británico, retomaría algunas de las propuestas de Malinowski, y muy especialmente las que se referían a la función social. Radcliffe-Brown rechazaba que el campo de análisis de la antropología fuera la cultura, más bien se encargaba del estudio de la estructura social, un entramado de relaciones entre las personas de un grupo. Sin embargo, también analizó aquellas categorías que habían sido descritas con anterioridad por Malinowski y Tylor, siguiendo siempre el principio del análisis científico de la sociedad. En su libro Estructura y función en la sociedad primitiva (1975) Radcliffe-Brown establece que la función más importante de las creencias y prácticas sociales es la del mantenimiento del orden social, el equilibrio en las relaciones y la trascendencia del grupo en el tiempo. Sus propuestas fueron retomadas más tarde por muchos de sus alumnos, especialmente por Edward Evan Evans-Pritchard etnógrafo de los nuer y los azande, pueblos del centro de África. En ambos trabajos etnográficos, la función reguladora de las creencias y prácticas sociales está presente en el análisis de esas sociedades, a la primera de las cuales, Evans-Pritchard llamó “anarquía ordenada”. Definiciones simbólicas Los orígenes de las concepciones simbólicas de cultura se remontan a Leslie White, antropólogo estadounidense formado en la tradición culturalista de Boas. A pesar de que en su libro La ciencia de la cultura afirma, en un principio, que esta es «el nombre de un tipo preciso o clase de fenómenos, es decir, las cosas y los sucesos que dependen del ejercicio de una habilidad mental, exclusiva de la especie humana, que hemos llamado simbolizante», en el transcurso de su texto, White irá abandonando la idea de la cultura como símbolos para orientarse hacia una perspectiva ecológica. Definición estructuralista Según la teoría estructuralista, la mente humana clasifica todos los fenómenos del mundo, estableciendo conjuntos clasificatorios a los que se adhieren cargas semánticas (se convierten en símbolos). Por ejemplo, Héritier proponía que un par de grupos clasificatorios universal es el que distingue varones de mujeres, basado en las diferencias fisiológicas. Lo que cambia son las atribuciones de cada grupo: en algunas culturas, como la occidental, la mujer se encarga de criar a los niños; en otras, esta tarea corresponde a los varones. El estructuralismo es una corriente más o menos extendida en las ciencias sociales. Sus orígenes se remontan a Ferdinand de Saussure, lingüista, quien propuso grosso modo que la lengua es un sistema de signos. Tras su conversión a la antropología (tal como la llama en Tristes trópicos), Claude Lévi-Strauss –influido por Roman Jakobson– habría de retomar este concepto para el estudio de los hechos de interés antropológico, entre los que la cultura era solo uno más. De acuerdo con Lévi-Strauss, la cultura es básicamente un sistema de signos producidos por la actividad simbólica de la mente humana (tesis que comparte con White). En Antropología estructural (1958) Lévi-Strauss irá definiendo las relaciones que existen entre los signos y símbolos del sistema, y su función en la sociedad, sin prestar demasiada atención a este último punto. En resumen, se puede decir que en la teoría estructuralista, la cultura es un mensaje que puede ser decodificado tanto en sus contenidos, como en sus reglas. El mensaje de la cultura habla de la concepción del grupo social que la crea, habla de sus relaciones internas y externas. En El pensamiento salvaje (1962), Lévi-Strauss apunta que todos los símbolos y signos de que está hecha la cultura son productos de la misma capacidad simbólica que poseen todas las mentes humanas. Esta capacidad, básicamente consiste en la clasificación de las cosas del mundo en grupos, a los que se atribuyen ciertas cargas semánticas. No existe un grupo de símbolos o signos (campo semántico) que no tenga uno complementario. Los signos y sus significados pueden ser asociados por metáfora (como en el caso de las palabras) o metonimia (como en el caso de los emblemas de la realeza) a fenómenos significativos para el grupo creador del sistema cultural. Las asociaciones simbólicas no necesariamente son las mismas en todas las culturas. Por ejemplo, mientras en la cultura occidental, el rojo es el color del amor, en Mesoamérica es el de la muerte. Según la propuesta estructuralista, las culturas de los pueblos “primitivos” y “civilizados” están hechas de la misma materia y, por tanto, los sistemas del conocimiento del mundo exterior dominantes en cada uno ,magia en los primeros, ciencia en los segundos,– no son radicalmente diferentes. Aunque son varias las distinciones que se pueden establecer entre culturas primitivas y modernas: una de las más importantes es el modo en que manipulan los elementos del sistema. En tanto que la magia improvisa, la ciencia procede sobre la base del método científico. El uso del método científico no quiere decir ,según Lévi-Strauss, que las culturas donde la ciencia es dominante sean superiores, o que aquellas donde la magia juega un papel fundamental sean menos rigurosas o metódicas en su manera de conocer el mundo. Simplemente, son de índole distinta unas de otras, pero la posibilidad de comprensión entre ambos tipos de culturas radica básicamente en una facultad universal del género humano. En la perspectiva estructuralista, el papel de la historia en la conformación de la cultura de una sociedad no es tan importante. Lo fundamental es llegar a dilucidar las reglas que subyacen en la articulación de los símbolos en una cultura, y observar la manera en que estos dotan de sentido la actuación de una sociedad. En varios textos, Lévi-Strauss y sus seguidores (como Edmund Leach) parecen insinuar, como Ruth Benedict, que la cultura es una suerte de patrón que pertenece a todo el grupo social pero no se encuentra en nadie en particular. Esta idea también fue retomada del concepto de lenguaje propuesto por Saussure. Definición de la antropología simbólica La antropología simbólica es una rama de las ciencias sociales cuyo desarrollo se relaciona con la crítica al estructuralismo lévi-straussiano. Uno de los principales exponentes de esta corriente es Clifford Geertz. Comparte con el estructuralismo francés la tesis de la cultura como un sistema de símbolos pero, a diferencia de Lévi-Strauss, Geertz señala que no es posible para los investigadores el conocimiento de sus contenidos: Al creer tal como Max Weber que el hombre es un animal suspendido en tramas de significación tejidas por él mismo, consideró que la cultura se compone de tales tramas, y que el análisis de ésta no es, por tanto, una ciencia experimental en busca de leyes, sino una ciencia interpretativa en busca de significado. (Geertz, 1988:) Bajo la premisa anterior, Geertz y la mayor parte de los antropólogos simbólicos ponen en duda la autoridad de la etnografía. Señalan que a lo que pueden limitarse los antropólogos es a hacer “interpretaciones plausibles” del significado de la trama simbólica que es la cultura, a partir de la descripción densa de la mayor cantidad de puntos de vista que sea posible conocer respecto a un mismo suceso. En otro sentido, los simbólicos no creen que todos los elementos de la trama cultural posean el mismo sentido para todos los miembros de una sociedad. Más bien creen que pueden ser interpretados de modos diferentes, dependiendo, ya de la posición que ocupen en la estructura social, ya de condicionamientos sociales y psíquicos anteriores, o bien, del mismo contexto. Definiciones marxistas Tal como se señaló anteriormente, Karl Marx a pesar de la opinión generalizada, puso atención en el análisis de las cuestiones culturales, específicamente en su relación con el resto de la estructura social. Según la propuesta teórica de Marx, el dominio de lo cultural (constituido sobre todo por la ideología) es un reflejo de las relaciones sociales de producción, es decir, de la organización que adoptan los seres humanos frente a la actividad económica. La gran aportación del marxismo en el análisis de la cultura es que esta es entendida como el producto de las relaciones de producción, como un fenómeno que no está desligado del modo de producción de una sociedad. Asimismo, la considera como uno de los medios por los cuales se reproducen las relaciones sociales de producción, que permiten la permanencia en el tiempo de las condiciones de desigualdad entre las clases. En sus interpretaciones más simplistas, la definición de la ideología en Marx ha dado lugar a una tendencia a explicar las creencias y el comportamiento social en función de las relaciones que se establecen entre quienes dominan el sistema económico y sus subalternos. Sin embargo, son muchas las posturas donde la relación entre la base económica y la superestructura cultural es analizada en enfoques más amplios. Por ejemplo, Antonio Gramsci llama la atención a la hegemonía, un proceso por medio del cual, un grupo dominante se legitima ante los dominados, y estos terminan por ver natural y asumir como deseable la dominación. Louis Althusser propuso que el ámbito de la ideología (el principal componente de la cultura) es un reflejo de los intereses de la élite, y que a través de los aparatos ideológicos del Estado se reproducen en el tiempo. Así mismo, Michel Foucault –en el conocido debate de noviembre de 1971 en Países Bajos con Noam Chomsky– respondiendo la pregunta de que si la sociedad capitalista era democrática, además de contestar negativamente –argumentando que una sociedad democrática se basa en el efectivo ejercicio del poder por una población que no esté dividida u ordenada jerárquicamente en clases– sostiene que, de manera general, todos los sistemas de enseñanza –los cuales aparecen simplemente como transmisores de conocimientos aparentemente neutrales–, están hechos para mantener a cierta clase social en el poder, y excluir de los instrumentos de poder a otras clases sociales. Definición neoevolucionista o ecofuncionalista Plataforma petrolera en el mar del Norte. White proponía que la energía de que dispusiera una sociedad es la que determina en buena medida la cultura. Occidente, por ejemplo, ha modificado sus tecnologías para poder aprovechar diversas fuentes energéticas a lo largo de su historia. La mayor cantidad de energía disponible ha permitido a su vez el desarrollo de nuevas tecnologías, creencias y formas de relaciones sociales. Sin embargo, como señalan Rappaport y Morán, es posible que la expansión en el consumo energético produzca una desadaptación ecológica y conduzca a la civilización Occidental a su desaparición. Si bien el estudio de la cultura nació como una inquietud por el cambio de las sociedades a lo largo del tiempo, el desprestigio en el que cayeron los primeros autores de la antropología fue un terreno fértil para que arraigaran en la reflexión sobre la cultura las concepciones ahistóricas. Salvo los marxistas, interesados en el proceso revolucionario hacia el socialismo, el resto de las disciplinas sociales no prestaron mayor atención al problema de la evolución cultural. Para introducir las definiciones neoevolucionistas de cultura, es necesario recordar que los evolucionistas sociales de finales del siglo XIX (representados, entre otros, por Tylor), pensaban que las sociedades “primitivas” de su época eran residuos de antiguas formas culturales, por las que necesariamente habría pasado la civilización de Occidente antes de llegar a ser lo que era en ese momento. Como se indicó antes, Boas y sus discípulos echaron por tierra estos argumentos, señalando que nada probaba la veracidad de estas suposiciones. Sin embargo, en Estados Unidos, hacia la década de 1940 tuvo lugar un nuevo viraje del enfoque temporal de la antropología. Este nuevo rumbo es el neoevolucionista, interesado entre otras cosas, por el cambio sociocultural y las relaciones entre cultura y medio ambiente. White y Steward Según el neoevolucionismo, la cultura es el producto de las relaciones históricas entre un grupo humano y su medio ambiente. De esta manera se pueden resumir las definiciones de cultura propuestas por Leslie White (1992) y Julian Steward (1992), quienes encabezaron la corriente neoevolucionista en su nacimiento. El énfasis de la nueva corriente antropológica se movió del funcionamiento de la cultura a su carácter dinámico. Este cambio de paradigma representa una clara oposición al funcionalismo estructuralista, interesado en el funcionamiento actual de la sociedad; y el culturalismo, que aplazaba el análisis histórico para un momento en que los datos etnográficos lo permitieran. Tanto Steward como White concuerdan en que la cultura es solo uno de los ámbitos de la vida social. Para White, la cultura no es un fenómeno que deba entenderse en sus propios términos, como proponían los culturalistas. El aprovechamiento energético es el motor de las transformaciones culturales: estimula la transformación de la tecnología disponible, tendiendo siempre a mejorar. Así, la cultura está determinada por la forma en la que el grupo humano aprovecha su entorno. Este aprovechamiento se traduce a su vez en energía. El desarrollo de la cultura de un grupo es proporcional la cantidad de energía que la tecnología disponible le permite aprovechar. La tecnología determina las relaciones sociales y esencialmente la división del trabajo como una prístina forma de organización. A su vez, la estructura social y la división del trabajo se reflejan en el sistema de creencias del grupo, que formula conceptos que le permiten comprender el entorno que le rodea. Una modificación en la tecnología y la cantidad de energía aprovechada se traduce, por tanto, en modificaciones en todo el conjunto. Steward, por su parte, retomaba de Kroeber la concepción de la cultura como un hecho que se encontraba por encima y fuera de la naturaleza. Sin embargo, Steward sostenía que había un diálogo entre ambos dominios. Opinaba que la cultura es un fenómeno o capacidad del ser humano que le permite adaptarse a su medio biológico. Uno de los principales conceptos en su obra es el de evolución. Steward planteaba que la cultura sigue un proceso de evolución multilineal (es decir, no todas las culturas pasan de un estado salvaje a la barbarie, y de ahí a la civilización), y que este proceso se basa en el desarrollo de tipos culturales derivados de las adaptaciones culturales al medio físico de una sociedad. Steward introduce en las ciencias sociales el término de ecología, señalando con él: el análisis de las relaciones existentes entre todos los organismos que comparten un mismo nicho ecológico. Marvin Harris y el materialismo cultural Dentro del tipo de ideas introducidas por White y Steward, cabe señalar el materialismo cultural propugnado por Marvin Harris y otros antropólogos estadounidenses. Esta corriente puede ser asimilada a una forma de ecofuncionalismo en el que se encajan ciertas divisiones introducidas por Marx. Para el materialismo cultural, entender la evolución cultural y la configuración de las sociedades depende básicamente de condiciones materiales, tecnológicas e infraestructurales. El materialismo cultural establece una triple división entre grupos de conceptos que atiende a su relación causal. Esos grupos se llaman: infraestructura (modo de producción, tecnología, condiciones geográficas, etc.), estructura (modo de organización social, estructura jerárquica, etc.) y supraestructura (valores religiosos y morales, creaciones artísticas, leyes, etc.). Evolución cultural Había por lo menos una gran distancia conceptual entre la propuesta de White y de Steward. El primero se inclinaba por el estudio de la cultura como fenómeno total, en tanto que el segundo se mantenía más proclive al relativismo. Por ello, entre las limitaciones que tuvieron que superar sus sucesores estuvo la de concatenar ambas posturas, para unificar la teoría de los estudios de la ecología cultural. De esta suerte, Marshall Sahlins propuso que la evolución cultural sigue dos direcciones. Por un lado, crea diversidad “a través de una modificación de adaptación: las nuevas formas se diferencian de las viejas. Por otra parte, la evolución genera progreso: las formas superiores surgen de las inferiores y las sobrepasan”. La idea de que la cultura se transforma siguiendo dos líneas simultáneas fue desarrollada por Darcy Ribeiro, que introdujo el concepto de proceso civilizatorio para comprender las transformaciones de la cultura. Con el tiempo, el neoevolucionismo sirvió como una de las principales bisagras entre las ciencias sociales y las ciencias naturales, especialmente como puente con la biología y la ecología. De hecho, su propia vocación como enfoque holístico le ha convertido en una de las corrientes más interdisciplinarias de las disciplinas que estudian la humanidad. A partir de la década de 1960, la ecología entró en una relación muy estrecha con los estudios culturales de corte evolutivo. Los biólogos habían descubierto que los seres humanos no son los únicos animales que poseen cultura: se habían encontrado indicios de ella entre algunos cetáceos, pero especialmente entre los primates. Roy Rappaport introdujo en la discusión de lo social la idea de que la cultura forma parte de la misma biología del ser humano, y que la evolución misma del ser humano se debe a la presencia de la cultura. Señalaba que: ...superorgánica o no, se debe tener presente que la cultura en sí pertenece a la naturaleza. Emergió en el curso de la evolución mediante procesos de selección natural diferentes sólo en parte de aquellos que produjeron los tentáculos del pulpo […] Aunque la cultura está altamente desarrollada en los seres humanos, estudios etológicos recientes han indicado alguna capacidad simbólica entre otros animales. […] Aunque las culturas pueden imponerse a los sistemas ecológicos, hay límites para esas imposiciones, ya que las culturas y sus componentes están sujetos a su vez a procesos selectivos. (Rappaport, 1998: 273-274) Los nuevos descubrimientos en la etología (ciencia que estudia el comportamiento de los animales) animaron a muchos biólogos a intervenir en el debate sociológico de la cultura. Algunos de ellos buscaban establecer relaciones entre la cultura humana y las formas primitivas de cultura observadas, por ejemplo, entre los macacos de Japón. Uno de los ejemplos más conocidos es el de Sherwood Washburn, profesor de antropología de la Universidad de California. Al frente de un equipo multidisciplinario, emprendió la tarea de buscar cuáles eran los orígenes de la cultura humana. Como primera parte de su proyecto, analizó el comportamiento social de los primates superiores. En segundo lugar, suponiendo que los bosquimanos !kung eran los últimos reductos de las formas más primitivas de cultura humana, procedió al estudio de su cultura. La tercera etapa del programa de Washburn (en el que colaboraron Richard Lee e Irven de Vore, y que se prolongó durante la primera mitad de los años sesenta) fue proceder a la comparación de los resultados de ambas investigaciones, y especuló sobre esta base acerca de la importancia de la cacería en la construcción de la sociedad y la cultura. Esta hipótesis fue presentada en un congreso llamado Man, the Hunter, realizado en la Universidad de Chicago en 1966. Fuera porque la investigación se apoyaba en premisas sobre la evolución cultural que fueron desechadas desde los tiempos de Boas, o porque era una tesis que negaba la importancia de la mujer en la construcción de la cultura, la tesis de Washburn, Lee y De Vore no fue bien recibida. Esta definición, atiende a la característica principal de la cultura, que es una obra estrictamente de creación humana, a diferencia de los procesos que realiza la naturaleza, por ejemplo, el movimiento de la tierra, las estaciones del año, los ritos de apareamiento de las especies, las mareas e incluso la conducta de las abejas que hacen sus panales, elaboran miel, se orientan para encontrar el camino de regreso pero, que a pesar de eso, no constituyen una cultura, pues todas las abejas del mundo hacen exactamente lo mismo, de manera mecánica, y no pueden cambiar nada. Exactamente lo contrario ocurren en el caso de las obras, ideas y actos humanos, ya que estos transforman o se agregan a la naturaleza, por ejemplo, el diseño de una casa, la receta de un dulce de miel o de chocolate, la elaboración de un plano, la simple idea de las relaciones matemáticas, son cultura y sin la creación humana no existirían por obra de la naturaleza. En 1998, Jesús Mosterín publicó su libro ¡Vivan los animales!, donde explica qué es la cultura: La cultura no es un fenómeno exclusivamente humano, sino que está bien documentada en muchas especies de animales superiores no humanos. Y el criterio para decidir hasta qué punto cierta pauta de comportamiento es natural o cultural no tiene nada que ver con el nivel de complejidad o de importancia de dicha conducta, sino sólo con el modo como se trasmite la información pertinente a su ejecución. […] Los chimpancés son animales muy culturales. Aprenden a distinguir cientos de plantas y sustancias, y a conocer sus funciones alimentarias y astringentes. Así logran alimentarse y contrarrestar los efectos de los parásitos. Tienen muy poco comportamiento instintivo o congénito. No existe una cultura de los chimpancés común a la especie. Cada grupo tiene sus propias tradiciones sociales, venatorias, alimentarias, sexuales, instrumentales, etc. […] La cultura es tan importante para los chimpancés, que todos los intentos de reintroducir en la selva a los chimpancés criados en cautividad fracasan lamentablemente. Los chimpancés no sobreviven. Les falta la cultura. No saben qué comer, cómo actuar, cómo interaccionar con los chimpancés silvestres, que los atacan y matan. Ni siquiera saben cómo hacer cada noche su alto nido-cama para dormir sin peligro en la copa de un árbol. Durante los cinco años que el pequeño chimpancé duerme con su madre tiene unas 2.000 oportunidades de observar cómo se hace el nido-cama. Los chimpancés hembras separados de su grupo y criados con biberón en el zoo ni siquiera saben cómo cuidar a sus propias crías, aunque lo aprenden si ven películas o vídeos de otros chimpancés criando (Jesús Mosterín, ¡Vivan los animales! 1998: 146-7, 151-2) Definición de cultura en la Iglesia católica La definición clásica de cultura en la Iglesia católica se encuentra en el concilio Vaticano II: Con la palabra cultura se indica, en sentido general, todo aquello con lo que el hombre afina y desarrolla sus innumerables cualidades espirituales y corporales; procura someter el mismo orbe terrestre con su conocimiento y trabajo; hace más humana la vida social, tanto en la familia como en toda la sociedad civil, mediante el progreso de las costumbres e instituciones; finalmente, a través del tiempo expresa, comunica y conserva en sus obras grandes experiencias espirituales y aspiraciones para que sirvan de provecho a muchos, e incluso a todo el género humano. (Constitución dogmática Gaudium et spes, 1965, n. 53) En la definición destacan dos aspectos: el poner al individuo al centro, siendo la cultura un producto del hombre y al servicio del hombre; y el conjugar la formación de cada persona a través de la cultura, con la contribución específica de una comunidad al progreso de la humanidad. Este concepto de cultura es la base para explicar el proceso de la inculturación o inserción de la Iglesia católica en una cultura y expresión del cristianismo en una nueva modalidad y culturalidad. El concepto científico de cultura El concepto científico de cultura hizo uso desde el principio de ideas procedentes de la teoría de la información, de la noción de meme introducida por Richard Dawkins, de los métodos matemáticos desarrollados en la genética de poblaciones por autores como Luigi Luca Cavalli-Sforza y de los avances en la comprensión del cerebro y del aprendizaje. Diversos antropólogos, como William Durham, y filósofos, como Daniel Dennett y Jesús Mosterín, han contribuido decisivamente al desarrollo de la concepción científica de la cultura. Mosterín define la cultura como la información transmitida por aprendizaje social entre animales de la misma especie. Como tal, se contrapone a la naturaleza, es decir, a la información transmitida genéticamente. Si los memes son las unidades o trozos elementales de información adquirida, la cultura actual de un individuo en un momento determinado sería el conjunto de los memes presentes en el cerebro de ese individuo en ese momento. A su vez, la noción vaga de cultura de un grupo social es analizada por Mosterín en varias nociones precisas distintas, definidas todas ellas en función de los memes presentes en los cerebros de los miembros del grupo. Industria cultural La industria cultural la define la UNESCO como aquella que produce y distribuye bienes o servicios culturales que, «considerados desde el punto de vista de su calidad, utilización o finalidad específicas, encarnan o transmiten expresiones culturales, independientemente del valor comercial que puedan tener. Las actividades culturales pueden constituir una finalidad de por sí, o contribuir a la producción de bienes y servicios culturales». Socialización de la cultura La importante aportación de la psicología humanista de, por ejemplo, Erik Erikson con una teoría psicosocial para explicar los componentes socioculturales del desarrollo personal. Cada miembro de la especie podría acceder a ella desde una fuente común, sin limitarse, ejemplo de ello: el conocimiento transmitido por los padres. Debe poder ser incrementada en las ulteriores generaciones. Ha de resultar universalmente compartible por todos aquellos que poseen un lenguaje racional y significativo. Así, el ser humano tiene la facultad de enseñar al animal, desde el momento en que es capaz de entender su rudimentario aparato de gestos y sonidos, llevando a cabo nuevos actos de comunicación; pero los animales no pueden hacer algo parecido con nosotros. De ellos podemos aprender por la observación, como objetos, pero no mediante el intercambio cultural, es decir, como sujetos. Clasificación La cultura se clasifica, respecto a sus definiciones, de la siguiente manera: Tópica: La cultura consiste en una lista de tópicos o categorías, tales como organización social, religión o economía. Histórica: La cultura es la herencia social, es la manera que los seres humanos solucionan problemas de adaptación al ambiente o a la vida en común. Mental: La cultura es un complejo de ideas, o los hábitos aprendidos, que inhiben impulsos y distinguen a las personas de los demás. Estructural: La cultura consiste en ideas, símbolos o comportamientos, modelados o pautados e interrelacionados. Simbólica: La cultura se basa en los significados arbitrariamente asignados que son compartidos por una sociedad. La cultura puede también ser clasificada del siguiente modo: Según su extensión Universal: cuando es tomada desde el punto de vista de una abstracción a partir de los rasgos que son comunes en las sociedades del mundo. Por ej., el saludo. Total: conformada por la suma de todos los rasgos particulares a una misma sociedad. Particular: igual a la subcultura; conjunto de pautas compartidas por un grupo que se integra a la cultura general y que a su vez se diferencia de ellas. Ej.: las diferentes culturas en un mismo país. Según su desarrollo Primitiva: aquella cultura que mantiene rasgos precarios de desarrollo técnico y que por ser conservadora no tiende a la innovación. Civilizada: cultura que se actualiza produciendo nuevos elementos que le permitan el desarrollo a la sociedad. Analfabeta o prealfabeta: se maneja con lenguaje oral y no ha incorporado la escritura ni siquiera parcialmente. Alfabeta: cultura que ha incorporado el lenguaje tanto escrito como oral. Según su carácter dominante Sensista: cultura que se manifiesta exclusivamente por los sentidos y es conocida a partir de los mismos. Racional: cultura donde impera la razón y es conocido a través de sus productos tangibles. Ideal: se construye por la combinación de la sensista y la racional. Según su dirección Posfigurativa: aquella cultura que mira al pasado para repetirlo en el presente. Cultura tomada de nuestros mayores sin variaciones. Es generacional y se da particularmente en pueblos primitivos. Configurativa: la cultura cuyo modelo no es el pasado, sino la conducta de los contemporáneos. Los individuos imitan modos de comportamiento de sus pares y recrean los propios. Elementos de la cultura La cultura forma todo lo que implica transformación y seguir un modelo de vida. Los elementos de la cultura se dividen en: a) Materiales: Son todos los objetos, en su estado natural o transformados por el trabajo humano, que un grupo esté en condiciones de aprovechar en un momento dado de su devenir histórico: tierra, materias primas, fuentes de energía, herramientas, utensilios, productos naturales y manufacturados, etcétera. b) De organización: Son las formas de relación social sistematizadas, a través de las cuales se hace posible la participación de los miembros del grupo cuya intervención es necesaria para cumplir la acción. La magnitud y otras características demográficas de la población son datos importantes que deben tomarse en cuenta al estudiar los elementos de organización de cualquier sociedad o grupo. c) De conocimiento: Son las experiencias asimiladas y sistematizadas que se elaboran, es decir, los conocimientos, las ideas y las creencias que se acumulan y trasmiten de generación a generación y en el marco de las cuales se generan o incorporan nuevos conocimientos. d) De conducta: Son los comportamientos o las pautas de conducta comunes a un grupo humano. e) Simbólicos: Son los diferentes códigos que permiten la comunicación necesaria entre los participantes en los diversos momentos de una acción. El código fundamental es el lenguaje, pero hay otros sistemas simbólicos significativos que también deben ser compartidos para que sean posibles ciertas acciones y resulten eficaces. f) Emotivos: que también pueden llamarse subjetivos. Son las representaciones colectivas, las creencias y los valores integrados que motivan a la participación y/o la aceptación de las acciones: la subjetividad como un elemento cultural indispensable. g) Pautada: Son sistemas integrados. Una persona no representa una cultura, pero si todo un grupo amplio. Dentro de toda cultura hay dos elementos a tener en cuenta: Rasgos culturales: porción más pequeña y significativa de la cultura, que da el perfil de una sociedad. Todos los rasgos se transmiten siempre al interior del grupo y cobran fuerza para luego ser exteriorizados. Complejos culturales: contienen en sí los rasgos culturales en la sociedad. Cambios culturales Artículo principal: Evolución cultural Los cambios culturales: son los cambios a lo largo del tiempo de todos o algunos de los elementos culturales de una sociedad (o una parte de la misma). Enculturación: es el proceso en el que el individuo se culturiza, es decir, el proceso en el que el ser humano, desde que es niño o niña, se culturiza. Este proceso es parte de la cultura, y como la cultura cambia constantemente, también lo hacen la forma y los medios con los que se culturaliza. Aculturación: se da normalmente en momento de conquista o de invasión. Es normalmente de manera forzosa e impuesta, como la conquista de América, la invasión de Irak. Ejemplos de resultados de este fenómeno: comida (potaje, pozole), huipil. El fenómeno contrario recibe el nombre de deculturación, y consiste en la pérdida de características culturales propias a causa de la incorporación de otras foráneas. Transculturación: La transculturación es un fenómeno que ocurre cuando un grupo social recibe y adopta las formas culturales que provienen de otro grupo. Inculturación: se da cuando la persona se integra a otras culturas, las acepta y dialoga con la gente de esa determinada cultura. La cultura está basada en todos nosotros. Historia del arte cultural Pintura rupestre: La historia del arte comenzó desde la Edad de Piedra dividiéndose en el Paleolítico, mesolítico y neolítico, hasta la Edad de los Metales. Se considera un periodo en el que surgieron las primeras manifestaciones consideradas como artísticas por parte del Homo sapiens, llamadas “pinturas rupestres”, en las cuevas a mediados del ( 25000-8000 a. C. ). En estas pinturas se veían reflejadas la cacería, la agricultura y la divinidad, como muestra de ello su arte era reflejada en hueso, madera y en esculturas de piedra. Escritura jeroglífica: En la Edad Antigua se desarrollan las primeras muestras de escritura por la necesidad de llevar registros comerciales y económicos. Su primer indicio fue la escritura cuneiforme que era practicada en tablillas de arcilla, basada con elementos pictográficos e ideográficos, seguido de esto aparece la escritura jeroglífica basada en la lengua hebrea la cual se utilizó como método de escritura del alfabeto que relacionaban los fonemas con cada símbolo. En esta etapa se presenta el arte mesopotámico, arte del antiguo Egipto, arte precolombino, arte africano, arte de la india, y el arte de china. Arte clásico: En el arte clásico es desarrollado en Grecia y Roma, donde se representa la naturaleza y la armonía humana, sus bases provienen del arte occidental. El arte griego se divide en tres periodos: arcaico, clásico y helenístico. Sus pinturas eran desarrolladas más que todo en cerámica. Sus mitos griegos llamaron la atención con la fusión de elementos indogermánicos y mediterráneos. Arte medieval: En el arte medieval se marca por la caída del imperio romano de occidente. El arte clásico es reinterpretado de manera que el cristianismo como nueva religión se encarga de la mayor parte de la producción artística medieval. Este arte se divide en el arte paleocristiano, el arte germánico, el arte prerrománico, el arte bizantino, el arte islámico, el arte románico, el arte gótico. Arte moderno: En el arte de la Edad Moderna suele darse como sinónimo del arte contemporáneo el cual se desarrolló en el siglo XV y XVIII, sus cambios se realizaron a nivel político, económico, social, y cultural. Su primera aparición fue con las vidas de girgio vasari en el texto inaugural del estudio del arte con carácter historiográfico. Arte hilemorfista: En este momento de nuestra historia estamos viviendo una gran variedad de cambios no solo sociopolíticos sino culturales. Así mismo, la visión que tenemos hoy en día del arte, ha sido modificada por todos esos cambios, y hoy conocemos lo que sería el “Arte Hilemorfista” que pretende romper la relación convencional entre materia y forma ha sido adoptado por las culturas latinoamericanas como una medida de protesta. Ya que en esencia este sentido hilemorfista lo que busca es deconstruir la concepción de conceptos convencionales creados por esta relación entre materia y forma, y darnos la capacidad de pensar en otras posibilidades del mundo. Y así como lo dice el famoso pintor y artista, Luis Camnitzer en su “Concepto latino”: El arte debe ir a la realidad y transformarla, reemplazarla. Un ejemplo claro de este arte hilemorfista actual es Lotty Rosenfeld, una artista visual chilena, adscrita al neo-vanguardismo. Quien por medio de sus obras modifica la realidad, llevando su arte a las calles y transformando la relación convencional que existe entre materia y forma. Tal es “Un millar de cruces sobre el pavimento” Que fue una obra hecha en la época de la dictadura militar chilena en la que Rosenfeld dibujaba una cruz utilizando las líneas de señalización de las calles y colocándose una línea transversal por cada desaparecido que había de la dictadura, haciendo una contra información a los medios tradicionales de ese momento que negaban la realidad en la que estaba consumido el país, y que no daban cifras o estadísticas de los desaparecidos que había. Relación entre cultura y lingüística Un elemento esencial de la cultura es el lenguaje, hay conceptos culturales dentro de los diferentes sistemas en los que encontramos características específicas de gramática y léxico. Por tanto, incluir la lingüística en el estudio de las culturas antiguas y contemporáneas es indispensable. El análisis de la cultura desde la lingüística en los últimos sesenta años ha evolucionado desde el pensamiento estructuralista hasta la variación cultural. Los lingüistas también han desarrollado una investigación sobre la comunicación intercultural, y recientemente han creado conceptos nuevos tales como multilingüismo y multiculturalismo para definir nuevos fenómenos culturales. Véase también -cultura Progreso Teorías sobre la cultura Antropología cultural Asimilación cultural Comunicación intercultural Difusión cultural Etnocentrismo Evolución cultural Geografía cultural Materialismo cultural Meme Relativismo cultural Revolución Cultural Sociología de la cultura Otras cuestiones culturales Categoría:Subculturas Alta cultura Artes y tradiciones populares Cultura de la violación Cibercultura Civilización Cultura científica Cultura organizacional Cultura popular Decondicionamiento Las dos culturas Leyenda urbana Neocolonialismo Subcultura Tecnociencia."
ksampletext_wikipedia_anth_etnia: str = "Etnia. Una etnia del griego clásico ethnos, pueblo o nación es un conjunto de personas que se identifican con un origen genético, comunidad lingüística, cultural, etc. Según Anthony D. Smith, «las etnias se pueden definir como poblaciones humanas que comparten unos mitos sobre la ascendencia, unas historias y unas culturas y que se asocian con un territorio específico y tienen un sentimiento de solidaridad». La pertenencia étnica tiende a definirse por una herencia cultural compartida, ascendencia, mito de origen, historia, patria, idioma o dialecto, sistemas simbólicos como religión, mitología y ritual, cocina, estilo de vestir, arte o apariencia física. Los grupos étnicos pueden compartir un espectro estrecho o amplio de ascendencia genética, según la identificación del grupo, y muchos grupos tienen ascendencia genética mixta. Este concepto esencialista de etnia es actualmente discutido por la ciencia como definidor de las sociedades humanas, ya que no explica bien la realidad empírica, de la misma manera que sucede con el concepto de raza aplicada a seres humanos. No es posible delimitar de manera nítida y objetiva grupos humanos en base al conjunto de estos criterios étnicos, ya que en realidad existe una intrincada y abigarrada red de rasgos culturales.Se ha propuesto en cambio otro concepto diferente que asimila este término al de grupo social, en el que existe una conciencia de diferenciación de otros grupos con base en algún elemento que puede ser muy diverso. Desde la religión, el grupo de edad, el territorio o la lengua, a la ideología política o la ocupación económica. Otro término relacionado con esta materia es el de etnicidad. Terminología El término étnico deriva del griego ἔθνος ethnos (más precisamente, del adjetivo ἐθνικός ethnikos, que fue importado al latín como ethnicus). En español y hasta mediados del siglo XIX, étnico se usaba para significar pagano (en el sentido de naciones dispares que aún no participaban en la ecúmene cristiana), ya que la Septuaginta utilizaba ta ethne (las naciones) para traducir el hebreo goyim las naciones extranjeras, no hebreas, no judías. El término griego en la antigüedad temprana (griego homérico) podía referirse a cualquier grupo grande, una hueste de hombres, una banda de camaradas, así como un enjambre o un rebaño de animales. En griego clásico, el término adquirió un significado comparable al concepto expresado actualmente por grupo étnico, traducido mayoritariamente como nación, tribu, un grupo único de personas; sólo en griego helenístico el término tendió a estrecharse aún más para referirse a naciones extranjeras o bárbaros en particular (de ahí el significado posterior de pagano, pagano). En el siglo XIX, el término pasó a utilizarse en el sentido de peculiar de una tribu, raza, pueblo o nación, en un retorno al significado griego original. El sentido de grupos culturales diferentes, y en español grupo minoritario tribal, racial, cultural o nacional surge en las décadas de 1930 a 1940, sirviendo como sustituto del término raza que anteriormente había tomado este sentido pero que ahora estaba quedando en desuso debido a su asociación con el racismo ideológico. El término abstracto etnicidad se había utilizado como sinónimo de paganismo en el siglo XVIII, pero ahora pasó a expresar el significado de carácter étnico (registrado por primera vez en 1953). El término grupo étnico se registró por primera vez en 1935 y entró en el Oxford English Dictionary en 1972. Dependiendo del contexto, el término nacionalidad puede utilizarse como sinónimo de etnia o como sinónimo de ciudadanía (en un Estado soberano). El proceso que da lugar a la aparición de una etnia se denomina etnogénesis, término utilizado en la literatura etnológica desde 1950, aproximadamente. El término también puede usarse con la connotación de algo único e inusualmente exótico (cf. un restaurante étnico, etc.), generalmente relacionado con culturas de inmigrantes más recientes, que llegaron después de que se estableciera la población dominante de una zona. Dependiendo de qué fuente de identidad de grupo se enfatice para definir la pertenencia, se pueden identificar los siguientes tipos de grupos (a menudo superpuestos mutuamente): Etnolingüístico, que hace hincapié en la lengua compartida, dialecto (y posiblemente escritura) – ejemplo: canadienses franceses Etnonacional, que hace hincapié en una entidad política o sentido de identidad nacional – ejemplo: austriacos Etnorracial, que hace hincapié en la apariencia física compartida basada en el fenotipo – ejemplo: afroamericanos Etnorregional, que enfatiza un sentido local distintivo de pertenencia derivado de un relativo aislamiento geográfico – ejemplo: isleños del sur de Nueva Zelanda Etnorreligioso, que hace hincapié en la afiliación compartida a una religión, confesión o secta en particular – ejemplo: sijs Etnocultural, que hace hincapié en una cultura o tradición compartida, que a menudo se solapa con otras formas de etnicidad – ejemplo: grupos itinerantes en Europa En muchos casos, más de un aspecto determina la pertenencia: por ejemplo, la etnia armenios puede definirse por la ciudadanía armenia, tener herencia armenia, el uso nativo de la lengua armenia o la pertenencia a la Iglesia Apostólica Armenia. Definición Históricamente, la palabra «etnia» proviene del adjetivo griego ethnikos. El adjetivo se deriva del sustantivo ethnos, que significa gente o nación extranjera. Etnia twa de Uganda. Las personas que se consideran miembros de un grupo étnico se sienten vinculados entre sí por un origen genético común y se sienten por ello parte de una comunidad familiar extendida que les impone formas de lealtad y solidaridad típicas de los vínculos familiares. Lingüística La lengua suele utilizarse como primer factor clasificado de los grupos sin embargo,esta herramienta no ha estado exenta de manipulación política o error. Se debe señalar que existe un gran número de lenguas multiétnicas a la par que determinadas etnias son multilingües. Cultura Dos hombres kurdos de Turquía y un sacerdote ortodoxo. Fotografía de 1873. La delimitación cultural de un grupo étnico respecto a los grupos culturales de sus fronteras se hace un proceso de gran dificultad para el etnólogo (antropólogo), en especial en los grupos humanos altamente comunicados con grupos vecinos. Elie Kedourie es quizás el autor que más ha profundizado en el análisis de las diferencias entre multietnicidad y pluriculturalidad. Generalmente, se aprecia que los grupos étnicos comparten una creencia en un origen común y tienen una continuidad en el tiempo, es decir, una historia o tradición común y proyectan un futuro como pueblo. Esto se alcanza a través de la transmisión generacional de una lengua común, unas instituciones, unos valores y unos usos y costumbres que los distinguen de otras etnias. Si bien en determinadas culturas se entremezclan los factores étnicos y los políticos, no es imprescindible que un grupo étnico cuente con instituciones propias de gobierno para ser considerado como tal. La soberanía, por tanto, no define la etnia, si bien se admite la necesidad de una cierta proyección social común. Genética Mapa detallado de las etnias del mundo. Las etnias generalmente se remontan a mitos de fundación que revelan un parentesco más o menos remoto entre sus miembros. La genética actual puede confirmar o negar la existencia de esa relación genética. Clasificación Entre los principales grupos de pueblos se incluyen: Pueblos aborígenes australianos* Pueblos afroasiáticos Pueblos semitas Pueblo árabe Pueblo judío Pueblos bereberes Pueblos chádicos Pueblos cusitas Pueblos egipcios Pueblos omóticos Pueblos altaicos Pueblos mongoles Pueblos tunguses Pueblos túrquicos Pueblos amerindios* Pueblos guaraníes* Pueblos quechuas* Pueblos andamaneses* Pueblos austrasiáticos Pueblos austronesios Pueblos caucásicos Pueblos chucoto-camchatcos Pueblos dravídicos Pueblos esquimo-aleutianos Pueblos hurrito-urartianos Pueblos indoeuropeos Pueblo albanés Pueblo esloveno Pueblos armenios Pueblos bálticos Pueblos celtas + Pueblo gitano Pueblos dacios Pueblo magiar Pueblos eslavos Pueblos frigios Pueblos germánicos + Pueblos helénicos + Pueblos íberos + Pueblos indo-iranios Pueblos itálicos + Pueblos nenéticos Pueblos tocarios + Pueblos tracios Pueblos indoarios Pueblos iranios Pueblos anatolios Pueblos ilirios + Pueblos lusitanos + Pueblos peonios + Pueblos joisanos Pueblos na-dené Pueblo persa Pueblos níger-congo Pueblos atlánticos Pueblos cordofanos Pueblos dogón Pueblos ijoi Pueblos mandé Pueblos Volta-Congo Pueblos Adamawa-Ubangi Pueblos Benue-Congo Pueblos bantúes Pueblos yoruba Pueblos gur Pueblos kru Pueblos kwa Pueblos senufos Pueblos nilo-saharianos Pueblos paleosiberianos Pueblos papúes Pueblos sino-tibetanos Pueblos birmanotibetanos Etnias chinas Pueblos tai-kadai Pueblos tirsénicos Pueblos urálicos Pueblo kurdo extinto en peligro de extinción Por idioma Artículo principal: Familia de lenguas Mapa con la situación de las diferentes familias lingüísticas. Etnicidad y raza La diversidad racial de los grupos étnicos de Asia (título original: Asiatiska folk), Nordisk familjebok (1904). La etnia se utiliza como una cuestión de identidad cultural de un grupo, a menudo basada en la ascendencia, la lengua y las tradiciones culturales compartidas, mientras que la raza se aplica como una agrupación taxonómica, basada en las similitudes físicas entre los grupos. La raza es un tema más controvertido que la etnia, debido al uso político común del término. Ramón Grosfoguel (Universidad de California, Berkeley) sostiene que la identidad racial/étnica es un concepto y que los conceptos de raza y etnicidad no pueden utilizarse como categorías separadas y autónomas. Antes de Weber (1864-1920), la raza y la etnia se consideraban principalmente dos aspectos de la misma cosa. Alrededor de 1900 y antes, predominaba la concepción primordialista de la etnicidad: se consideraba que las diferencias culturales entre los pueblos eran el resultado de rasgos y tendencias heredados. Con la introducción por parte de Weber de la idea de etnicidad como construcción social, la raza y la etnicidad se dividieron más entre sí. En 1950, la declaración de la UNESCO La cuestión racial, firmada por algunos de los eruditos de renombre internacional de la época (entre ellos Ashley Montagu, Claude Lévi-Strauss, Gunnar Myrdal, Julian Huxley, etc.), decía: Los grupos nacionales, religiosos, geográficos, lingüísticos y culturales no coinciden necesariamente con los grupos raciales: y los rasgos culturales de tales grupos no tienen ninguna conexión genética demostrada con los rasgos raciales. Dado que habitualmente se cometen graves errores de este tipo cuando se utiliza el término raza en el lenguaje popular, sería mejor, cuando se habla de razas humanas, abandonar por completo el término raza y hablar de grupos étnicos. En 1982, el antropólogo David Craig Griffith resumió cuarenta años de investigación etnográfica, argumentando que las categorías raciales y étnicas son marcadores simbólicos de las diferentes formas en que las personas de distintas partes del mundo han sido incorporadas a una economía global: Los intereses opuestos que dividen a las clases trabajadoras se refuerzan aún más mediante apelaciones a distinciones raciales y étnicas. Tales apelaciones sirven para asignar diferentes categorías de trabajadores a los peldaños de la escala de los mercados laborales, relegando a las poblaciones estigmatizadas a los niveles inferiores y aislando a los escalones superiores de la competencia desde abajo. El capitalismo no creó todas las distinciones de etnia y raza que funcionan para separar a unas categorías de trabajadores de otras. Sin embargo, es el proceso de movilización laboral bajo el capitalismo el que confiere a estas distinciones sus valores efectivos. Según Wolf, las categorías raciales se construyeron e incorporaron durante el período del expansión mercantil europea, y las agrupaciones étnicas durante el período del expansión capitalista. En 1977, Wallman escribió sobre el uso del término étnico en el lenguaje ordinario de Gran Bretaña y Estados Unidos. El término étnico connota popularmente [raza] en Gran Bretaña, sólo que con menos precisión, y con una carga de valor más ligera. En Norteamérica, en cambio, [raza] suele significar color, y los étnicos son los descendientes de inmigrantes relativamente recientes de países de habla no inglesa. [Etnia] no es un sustantivo en Gran Bretaña. En efecto, no hay etnias; sólo hay relaciones étnicas. En Estados Unidos, la OMB afirma que la definición de raza utilizada a efectos del censo estadounidense no es científica ni antropológica y tiene en cuenta características sociales y culturales, así como la ascendencia, utilizando metodologías científicas apropiadas que no son de referencia principalmente biológica o genética."

ksampletext_wikipedia_hist_historia: str = "Historia. La historia es la narración de los sucesos del pasado; generalmente los de la humanidad, aunque también puede no estar centrada en el humano. Hay quien más breve y concisamente afirma que la historia es el conocimiento del pasado humano. Asimismo, es una disciplina académica que estudia dichos acontecimientos. A la ciencia o disciplina académica también se le denomina historiografía para distinguirla de la historia entendida como los hechos objetivos sucedidos. Es una ciencia social debido a su clasificación y método; pero, si no se centra en el humano, puede ser considerada como una ciencia natural, especialmente en un marco de la interdisciplinariedad; de cualquier forma, forma parte de la clasificación de la ciencia que engloba las anteriores dos, es decir, una ciencia fáctica (también llamada factual). Su propósito es averiguar los hechos y procesos que ocurrieron y se desarrollaron en el pasado e interpretarlos ateniéndose a criterios de la mayor objetividad posible; aunque la posibilidad de cumplimiento de tales propósitos y el grado en que sean posibles son en sí mismos objetos de estudio de la historiología o teoría de la historia, como epistemología o conocimiento científico de la historia.[cita requerida] Se denomina historiador o historiadora a la persona encargada del estudio de la historia. Al historiador profesional se le concibe como el especialista en la disciplina académica de la historia, y al historiador no profesional se le suele denominar cronista. Etimología Heródoto, padre de la Historia. La palabra historia deriva del griego ἱστορία (léase historia, traducible por «investigación» o «información», conocimiento adquirido por investigación), del verbo ἱστορεῖν («investigar»). De allí pasó al latín historia, que en castellano antiguo evolucionó a estoria (como atestigua el título de la Estoria de España de Alfonso X el Sabio, 1260-1284) y se reintrodujo posteriormente en el castellano como un cultismo en su forma latina original. La etimología remota procede del protoindoeuropeo *wid-tor- (de la raíz *weid-, «saber, ver» ,construcción hipotética,) presente también en las palabras latinas idea o visión, en las germánicas wit, wise o wisdom, la sánscrita veda, y las eslavas videti o vedati, y en otras lenguas de la familia indoeuropea. La palabra antigua griega ἱστορία fue usada por Aristóteles en su Περὶ τὰ ζῷα ἱστορίαι (léase Perí ta zóa jistoríai, traducido Historia acerca de los animales, latinizado Historia animalium, traducible por Historia de los animales [el título griego es plural y el latino es singular]). El término se derivaba de ἵστωρ (léase jístōr, traducible por «hombre sabio», «testigo» o «juez»). Se pueden encontrar usos de ἵστωρ en los himnos homéricos, Heráclito, el juramento de los efebos atenienses y en las inscripciones beocias (en un sentido legal, con un significado similar a «juez» o «testigo»). El rasgo aspirado es problemático, y no se presenta en la palabra cognata griega εἴδομαι («aparecer»). La forma ἱστορεῖν («inquirir»), es una derivación jónica, que se expandió primero en la Grecia clásica y más tarde en la civilización helenística. Definición A su vez, se llama «historia» al pasado mismo, e incluso puede hablarse de una «historia natural» en que la humanidad no estaba presente,[cita requerida] que se utilizaba en oposición a la historia social, para referirse no solo a la geología y la paleontología, sino también a muchas otras ciencias naturales ,las fronteras entre el campo al que se refiere tradicionalmente este término y el de la prehistoria y la arqueología son imprecisas, a través de la paleoantropología,, y que se pretende complementar con la historia ambiental o ecohistoria,[n. 3] y actualizarse con la denominada «Gran Historia». Ese uso del término «historia» lo hace equivalente a «cambio en el tiempo»[n. 4] En ese sentido, se contrapone al concepto de filosófico equivalente a esencia o permanencia (lo que permite hablar de una filosofía natural en textos clásicos y en la actualidad, sobre todo en medios académicos anglosajones, como equivalente a la física). Para cualquier campo del conocimiento, se puede tener una perspectiva histórica ,el cambio, o bien filosófica ,su esencia,. De hecho, puede hacerse eso para la historia misma (véase tiempo histórico[n. 5]) y para el tiempo mismo. En este sentido, todo pasado en relación con el presente hace alusión al tiempo y a su cronología, y por lo tanto tener historia.[cita requerida] Estudio de la historia Como ciencia Véase también: Historiología Véase también: Historiografía Véase también: Ciencias Históricas Dentro de la popular división entre ciencias y letras o humanidades, se tiende a clasificar a la historia entre las disciplinas humanísticas junto con otras ciencias sociales (también denominadas ciencias humanas), o incluso se la llega a considerar como un puente entre ambos campos, al incorporar la metodología de estas a aquellas. No todos los historiadores aceptan la identificación de la historia con una ciencia social, al considerarla una reducción en sus métodos y objetivos, comparables con los del arte si se basan en la imaginación (postura adoptada en mayor o menor medida por Hugh Trevor-Roper, John Lukacs, Donald Creighton, Gertrude Himmelfarb o Gerhard Ritter). Los partidarios de su condición científica son la mayor parte de los historiadores de la segunda mitad del siglo XX y del XXI (incluyendo, de entre los muchos que han explicitado sus preocupaciones metodológicas, a Fernand Braudel, E. H. Carr, Fritz Fischer, Emmanuel Le Roy Ladurie, Hans-Ulrich Wehler, Bruce Trigger, Marc Bloch, Karl Dietrich Bracher, Peter Gay, Robert Fogel, Lucien Febvre, Henri Marrou, Lawrence Stone, E. P. Thompson, Eric Hobsbawm, Carlo Cipolla, Jaume Vicens Vives, Manuel Tuñón de Lara o Julio Caro Baroja). Buena parte de ellos, lo hicieron desde una perspectiva multidisciplinar (Braudel combinaba historia con geografía, Bracher con ciencia política, Fogel con economía, Gay con psicología, Trigger con arqueología), mientras los demás citados lo hacían a su vez con las anteriores y con otras, como la sociología y la antropología. Esto no quiere decir que entre ellos hayan alcanzado una posición común sobre las consecuencias metodológicas de la aspiración de la historia al rigor científico, ni mucho menos que propongan un determinismo que (al menos desde la revolución einsteniana de comienzos del siglo XX) no proponen ni las llamadas ciencias duras.[n. 6] Por su parte, los historiadores menos proclives a considerar científica su actividad tampoco defienden un relativismo estricto que imposibilitaría de forma total el conocimiento de la historia y su transmisión, y de hecho de un modo general aceptan y se someten a los mecanismos institucionales, académicos y de práctica científica existentes en la historia y comparables a los de otras ciencias (ética de la investigación, publicación científica, revisión por pares, debate y consenso científico, etcétera).[cita requerida] La utilización que hace la historia de otras disciplinas como instrumentos para obtener, procesar e interpretar datos del pasado permite hablar de ciencias auxiliares de la historia de metodología muy diferente, cuya subordinación o autonomía depende de los fines a los que estas mismas se apliquen.[cita requerida] Como disciplina académica Véanse también: Cronista e Historiografía. La Historia de Italia de Francesco Guicciardini, 1561 Historia General de los Hechos de los Castellanos en las Islas y Tierra Firme del Mar Océano, de Antonio de Herrera, edición de 1601 El registro de anales y crónicas fue en muchas civilizaciones un oficio ligado a un cargo institucional público, controlado por el Estado. Sima Qian (denominado padre de la Historia, en la cultura china) inauguró en esa civilización los registros históricos oficiales burocratizados (siglo II a. C.). La crítica del musulmán Ibn Jaldún (Muqaddima ,Prolegómenos a la Historia Universal,, 1377) a la manera tradicional de hacer historia no tuvo consecuencias inmediatas, y se le consideró un precedente de la renovación de la metodología de la historia y de la filosofía de la historia que no se inició sino hasta el siglo XIX, fruto de la evolución de la historiografía en Europa occidental. Entretanto, los cronistas oficiales castellanos y de Indias dieron paso en la España ilustrada del siglo XVIII a la fundación de la Real Academia de la Historia; instituciones similares existen en otros países. La docencia de la historia en la enseñanza obligatoria fue una de las bases de la construcción nacional desde el siglo XIX, proceso simultáneo a la proliferación de las cátedras de historia en las universidades (inicialmente en las facultades de letras o Filosofía y Letras, y con el tiempo, en facultades propias o de Geografía e Historia ,disciplinas cuya proximidad científica y metodológica es una característica de la tradición académica francesa y española,) y la creación de todo tipo de instituciones públicas y privadas (clubes históricos o sociedades históricas, muy habitualmente medievalistas, respondiendo al historicismo propio del gusto romántico, empeñado en la búsqueda de elementos de identificación nacional); así como publicaciones dedicadas a la historia. En la enseñanza media de la mayor parte de los países, los programas de historia se diseñaron como parte esencial del currículo. En especial la agregación de historia presente en los lycées franceses desde 1830 adquirió con el tiempo un prestigio social incomparable con los cargos similares en otros sistemas educativos y que caracterizó el elitismo de la escuela laica republicana hasta finales del siglo XX. A ese proceso de institucionalización, siguió la especialización y subdivisión de la disciplina con diferentes sesgos temporales (de cuestionable aplicación fuera de la civilización occidental: historia antigua, medieval, moderna, contemporánea ,estas dos últimas, habituales en la historiografía francesa o española, no suelen subdividirse en la historiografía anglosajona: era moderna,), espaciales (historia nacional, regional, local, continental ,de África, de Asia, de América, de Europa, de Oceanía,), temáticos (historia política, militar, de las instituciones, económica y social, de los movimientos sociales y de los movimientos políticos, de las civilizaciones, de las mujeres, de la vida cotidiana, de las mentalidades, de las ideas, cultural), historias sectoriales ligadas a otras disciplinas (historia del arte, de la música, natural, de las religiones, del derecho, de la ciencia, de la medicina, de la economía, de la ciencia política, de las doctrinas políticas, de la tecnología), o centrada en cualquier tipo de cuestión particular (historia de la electricidad, de la democracia, de la Iglesia, de los sindicatos, de los sistemas operativos, de las formas ,literarias de la Biblia,, etc). Ante la atomización del campo de estudio, también se han realizado distintas propuestas que consideran la necesidad de superar esas subdivisiones con la búsqueda de una perspectiva holística (historia de las civilizaciones, historia total o historia universal) o su enfoque inverso (microhistoria); sin olvidar el nuevo campo académico e interdisciplinar de la Gran Historia como «el intento de comprender de manera unificada, la Historia del Cosmos o Universo, la Tierra, la Vida y la Humanidad», cubriendo la historia desde el Big Bang hasta la Historia del mundo actual. Examina los tiempos de larga duración utilizando un enfoque multidisciplinar basado en la combinación de numerosas disciplinas de la ciencia y las humanidades que estudian el pasado, las Ciencias-Históricas, y explora la existencia humana en el contexto de un panorama más amplio, que en relación con el presente hace alusión al tiempo y la cronología, enseñándose en universidades y escuelas. El Premio Nacional de Historia (de Chile ,bianual, a una personalidad, y de España ,a una obra publicada cada año,) y el Premio Príncipe de Asturias de Ciencias Sociales (a una personalidad del ámbito de la historia, la geografía u otras ciencias sociales) son los más altos reconocimientos de la investigación histórica en el ámbito hispanohablante, mientras que en el ámbito anglosajón existe una de las versiones del Premio Pulitzer. El Premio Nobel de Literatura, que puede recaer en historiadores, solo lo hizo en dos ocasiones (Theodor Mommsen, en 1902, y Winston Churchill, en 1953). Desde una perspectiva más propia de la consideración actual de la historia como una ciencia social, el Premio Nobel de economía fue concedido a Robert Fogel y Douglass North en 1993. Por otra parte, el Premio Pfizer de la History of Science Society se estableció en 1958. El premio consiste en una medalla y una cantidad en metálico. Este premio se otorga en reconocimiento a un libro extraordinario sobre la historia de la ciencia. Cada año, un centenar de autores compiten por este premio, que es considerado el más importante para libros de historia de la ciencia. El Premio Internacional de Ciencias Históricas, es el premio internacional más prestigioso de Historia otorgado por el Comité Internacional de Ciencias Históricas (International International Committee of Historical Sciences / Comité international des sciences historiques), la asociación internacional de Ciencias Históricas fundada en Ginebra el 14 de mayo de 1926, que concede desde 2015 el Premio Internacional de Historia del CICH, Jaeger-LeCoultre, al «historiador que se ha distinguido en el campo de la Historia por sus obras, publicaciones o docencia, y haya contribuido significativamente al desarrollo del conocimiento histórico». Considerado el Premio Nobel en Ciencias Históricas, el jurado del Consejo del CISH, que cuenta con 12 miembros de diferentes países, selecciona al ganador dentro de un grupo de candidatos excelentes y altamente calificados. Solo los miembros colectivos del CISH (sus comités nacionales o sus organizaciones afiliadas internacionales) pueden presentar candidatos. Historiador Artículo principal: Historiador Véase también: Historiógrafo Véase también: Historiólogo Perspectivas: justificación, importancia y objetivo Historia de Nikolaos Gysis (1892) Tampoco deben confundirse los supuestos fines teleológicos del hombre en la historia con los fines de la historia, es decir, la justificación de la propia historia como memoria de la humanidad. La historia, al ser una ciencia social, no puede abstraerse del porqué se encarga de estudiar los procesos sociales: explicar los hechos y eventos del pasado, sea por el conocimiento mismo, sea porque nos ayudan a comprender el presente. Cicerón bautizó a la historia como maestra de la vida, y como él Cervantes, que también la llamó madre de la verdad.[n. 7] Benedetto Croce remarcó la fuerte implicación del pasado en el presente con su toda historia es historia contemporáea. La historia, al estudiar los hechos y procesos del pasado humano, es un útil para la comprensión del presente y plantear posibilidades para el futuro. Salustio llegó a decir que entre las distintas ocupaciones que se ejercitan con el ingenio, el recuerdo de los hechos del pasado ocupa un lugar destacado por su gran utilidad. Un tópico muy difundido (atribuido a Jorge Santayana) advierte que los pueblos que no conocen su historia están condenados a repetirla, aunque otro tópico (atribuido a Karl Marx) indique a su vez que cuando se repite lo hace una vez como tragedia y la segunda como farsa. La radical importancia de ello se basa en que la historia, como la medicina, es una de las ciencias en que el sujeto investigador coincide con el objeto a estudiar. De ahí la gran responsabilidad del historiador: la historia tiene una proyección al futuro por su potencia transformadora como herramienta de cambio social; y a los profesionales que la manejan, los historiadores, les es aplicable lo que Marx dijo de los filósofos (hasta ahora se han encargado de interpretar el mundo y de lo que se trata es de transformarlo). No obstante, desde otra perspectiva se pretende una investigación desinteresada para la objetividad en la ciencia histórica. Aunque llegar a conocer los hechos tal como fueron, como pretendía Leopold Ranke, es imposible, sí es un imperativo de la investigación histórica acercarse al máximo a ese objetivo, y además hacerlo con una perspectiva tal que sitúe los hechos en su contexto, de modo que al conocimiento factual se añada el entendimiento de lo que realmente pasó; y aunque sea inevitable que sesgos de todo tipo alteren la forma en que tal entendimiento se produce, al menos ser conscientes de cuáles pueden ser y en qué grado actúan. Ramas Historiografía Artículo principal: Historiografía La historiografía es el conjunto de técnicas y métodos propuestos para describir los hechos históricos acontecidos y registrados, entendida como la ciencia que se encarga del estudio de la historia. La correcta praxis de la historiografía requiere el empleo correcto del método histórico y el sometimiento a los requerimientos típicos del método científico. También se denomina historiografía a la producción literaria de los historiadores, y a las escuelas, agrupaciones o tendencias de los historiadores mismos. El escriba sentado (Saqqara iii milenio a. C. ,IV o V dinastía de Egipto,). Representa a un funcionario en actitud de comenzar a escribir, o sea, a registrar un hecho o una interpretación más o menos interesada de hechos seleccionados ,económicos, militares, legislativos, religiosos,; una función de consecuencias trascendentales: sirve tanto para el ejercicio y la justificación del poder en su presente como para la preservación de la memoria histórica hacia la posteridad. La identificación del concepto de historia con la narración escrita del pasado produce, por un lado, su confusión con el término historiografía (historia se llama a la vez al objeto estudiado, a la ciencia que lo estudia y al documento resultado de ese estudio); y por otro justifica el empleo del término prehistoria para el período anterior a la aparición de la escritura, reservándose el nombre historia para el periodo posterior. Según ese uso restrictivo, la mayor parte de la humanidad queda fuera de la historia, no tanto porque no accede personalmente a la lectura y la escritura (el analfabetismo fue la condición común de la inmensa mayoría de la población, incluso para las clases dominantes, hasta la imprenta), sino porque los reflejados en el discurso histórico han sido siempre muy pocos, y grupos enteros quedan invisibilizados (las clases bajas, las mujeres, los discrepantes que no pueden acceder al registro escrito), con lo que ha sido objeto de preocupación de algunos historiadores la reconstrucción de la visión de los vencidos y la historia desde abajo. Lo mismo ocurre con gran número de pueblos y culturas (las consideradas como culturas primitivas, en una terminología ya desfasada de la antropología antigua) que no tienen historia. El tópico los idealiza al considerar que son pueblos felices.[n. 8] Entran en ella cuando se produce su contacto, habitualmente destructivo (aculturación), con civilizaciones (sociedades complejas, con escritura). Incluso en ese momento no son propiamente objeto de la historia sino de la protohistoria (historia realizada a partir de las fuentes escritas producidas por los que generalmente son sus pueblos colonizadores por oposición a los pueblos indígenas). No obstante, independientemente de que los historiadores y los antropólogos ideológicamente tengan una tendencia etnocentrista (eurocentrista, sinocentrista o indigenista) o, de forma opuesta, multiculturalista o relativista cultural, existe la posibilidad de obtener o reconstruir un relato fiable de los acontecimientos que afectan a un grupo humano utilizando otras metodologías: fuentes arqueológicas (cultura material) o historia oral. En buena parte, esta diferencia es artificial, y no necesariamente novedosa: el mismo Heródoto no puede sino usar ese tipo de fuentes documentales cuando redacta la que se considera la primera Historia, o al menos acuña el término, en la Grecia del siglo V a. C. para que el tiempo no abata el recuerdo de las acciones de los hombres y que las grandes empresas acometidas, ya sea por los griegos, ya por los bárbaros, no caigan en olvido; da también razón del conflicto que puso a estos dos pueblos en la lid. Así comienza su obra titulada Ἱστορίαι (léase históriai, literalmente «investigaciones», «exploraciones», latinizado Historiae ,«Historias», en plural,), seminal para la ciencia histórica, y que suele denominarse en castellano Los nueve libros de historia. La lid citada son las guerras médicas y los bárbaros, persas. Historiología Artículo principal: Historiología La historiología o «teoría de la historia» es el conjunto de explicaciones, métodos y teorías sobre cómo, por qué y en qué medida se dan cierto tipo de hechos históricos y tendencias sociopolíticas en determinados lugares y no en otros. El término fue introducido por José Ortega y Gasset y el DRAE lo define como el estudio de la estructura, leyes y condiciones de la realidad histórica. Ramas de otras ciencias relacionadas Filosofía de la historia Artículo principal: Filosofía de la historia La filosofía de la historia no debe confundirse ni con la historiología, ni con la historiografía, de los que se separa claramente. La filosofía de la historia es la rama de la filosofía que concierne al significado de la historia humana, si es que lo tiene. En su origen especuló si era posible un fin teleológico de su desarrollo, o sea, se pregunta si hay un diseño, propósito, principio director o finalidad en el proceso de la historia humana. En la actualidad se discute más sobre la función del conocimiento histórico dentro del conocimiento y las implicaciones del mismo. También se ha discutido sobre si el objeto de la historia debe ser una verdad histórica, el deber ser, o si la historia es en algún sentido es cíclica o lineal y el devenir histórico se aparta indefinidamente del punto de partida. También se ha discutido si es posible hablar de la idea de progreso positivo en ella. Áreas de estudio por región geográfica Historia universal Artículo principal: Historia universal Periodización tradicional Artículo principal: Periodización Véanse también: Tiempo histórico y Tiempo geológico. No hay un acuerdo universal sobre la periodización de la historia, aunque sí un consenso académico sobre los periodos de la historia de la civilización occidental, basado en los términos acuñados inicialmente por Cristóbal Celarius (Edades Antigua, Media y Moderna), que ponía al mundo clásico grecorromano y su Renacimiento como los hechos determinantes para la división; y que actualmente es de aplicación general.[n. 9] La acusación de eurocentrismo que se hace a tal periodización no impide que sea la más utilizada, por ser la que responde precisamente al desarrollo de los procesos históricos que produjeron el mundo contemporáneo. En cuanto a la división del tiempo prehistórico en Edad de la Piedra y Edad de los Metales, fue propuesta en 1836 por el arqueólogo danés Christian Jürgensen Thomsen.[n. 10] La evolución tecnológica presenta dos grandes cesuras en el pasado de la humanidad: la revolución neolítica y la revolución industrial, lo que permite hablar de tres grandes periodos: el caracterizado por la exclusividad de sociedades cazadoras-recolectoras, el preindustrial y el industrial (a veces se emplea el adjetivo postindustrial para el periodo de la historia más reciente).[n. 11] El problema de cualquier periodización es hacerla coherente en términos sincrónicos y diacrónicos, es decir: que sea válida tanto para el transcurso del tiempo en un único lugar, como para lo que ocurre al mismo tiempo en distintos ámbitos espaciales. Cumplir ambos requisitos resulta difícil cuando los fenómenos que originan el comienzo de un periodo en un lugar (especialmente el Próximo Oriente, Asia Central o China) tardan en difundirse o surgir endógenamente en otros lugares, que a su vez pueden estar más o menos próximos y conectados (como Europa Occidental o el África subsahariana), o más o menos lejanos y desconectados (como América u Oceanía). Para responder a todo ello, los modelos de periodización incluyen términos intermedios y periodos de solapamiento (yuxtaposición de características distintas) o transición (aparición paulatina de las novedades o características mixtas entre el periodo que empieza y el que termina). La didáctica de la historia se ayuda frecuentemente de diferentes tipos de representación gráfica de la sucesión de hechos y procesos en el tiempo y en el espacio.[n. 12] Véanse también: Tiempo histórico y Mapa histórico. Prehistoria Edad de Piedra Edad de los Metales Paleolítico Mesolítico Neolítico Edad del Cobre Edad del Bronce Edad del Hierro Paleolítico Inferior Paleolítico Medio Paleolítico Superior Epi- paleolítico Proto- neolítico Historia de Europa Protohistoria Edad Antigua Edad Media siglo XV Edad Moderna siglo XVIII Edad Contemporánea Antigüedad clásica Antigüedad tardía Alta Edad Media Baja Edad Media Plena Edad Media Crisis siglo XVI siglo XVII siglo XIX siglo XX siglo XXI Pinturas rupestres de Cueva de las Manos (Río Pinturas, Argentina, cerca de 9000 años de antigüedad). Representan esquemáticamente a un hombre y a grupos de animales; también se observan otros símbolos, destacadamente las manos que dan el nombre al lugar. Esta forma de arte prehistórico, aunque es un testimonio valiosísimo para la reconstrucción del pasado, no es una fuente histórica en el sentido clásico de la palabra, sino arqueológica. Stonehenge, un monumento megalítico tipo crómlech construido en Gran Bretaña en el iii milenio a. C. por un pueblo en transición del neolítico a la Edad de los Metales, contemporáneo de las Pirámides de Egipto. Su olvidada función religiosa y astronómica es objeto en la actualidad de revivals espiritualistas. Espada de bronce (Saint-Germain-en-Laye, Francia, hacia 800 a. C., periodo protohistórico en el que los héroes griegos, que usarían armas semejantes, ya son cantados por Homero). Prehistoria. Desde la aparición del hombre (diferenciación de las distintas especies del género Homo, subtribu hominina, superfamilia Hominoidea, orden de los primates), de fechas inciertas, hace más de dos millones de años; hasta la aparición de la escritura, en torno al iv milenio a. C. Se considera un campo académico o especialidad muy ligada a la Arqueología. Paleolítico (etimológicamente Antigua Edad de Piedra, por la piedra tallada). Los hechos más decisivos son los ligados a la evolución humana, en lo físico, y a la evolución cultural primitiva (utilización de herramientas y del fuego y desarrollo de distintos tipos de colaboración y conducta social primitiva; destacadamente el lenguaje). Los grupos sociales no superarían el tamaño de hordas, con una densidad de población inferior a un habitante por kilómetro cuadrado. La economía se limitaba a una relación depredadora con el medio ambiente (caza, pesca y recolección), lo que no impedía un impacto notable (primera humanización del paisaje natural y extinciones provocadas por la presión de la actividad humana en los ecosistemas donde se introduce). Paleolítico inferior. Primeros modos de talla lítica de instrumentos (Olduvayense o modo 1 y Achelense o modo 2), asociados a restos fósiles de homínidos: Australopitecus, Homo habilis y Homo ergaster (África sudoriental), Homo erectus (extendido por todo el Viejo Continente); Homo antecessor y Homo heidelbergensis (específicos de Europa ,yacimiento de Atapuerca,). Paleolítico medio. Ligado a cambios en la cultura material (Musteriense o modo 3) y en las especies de homínidos (Hombre de Neanderthal en Europa, Homo sapiens arcaico en África ,Hombres de Kibish,), desde hace 130 000 años hasta hace 35 000 años aproximadamente. Paleolítico superior. Ligado a la cultura material asociada al Homo sapiens moderno: el modo 4 (Auriñaciense, Gravetiense, Solutrense, Magdaleniense ,en Europa,, Clovis y Monte Verde ,en América, donde por primera vez aparecen homínidos,); desde hace 35 000 años hasta hace 10 000 años aproximadamente. Ya no hay cambios significativos para la paleoantropología en el registro fósil; las variaciones entre distintos grupos son mucho más sutiles: las estudiadas tradicionalmente por la antropología física y que se conocían como razas humanas, y que la moderna genética de poblaciones estudia con renovadas metodologías (genética molecular). Junto con la paleo-lingüística pretende reconstruir las migraciones primitivas.[n. 13] Mesolítico/Epipaleolítico/Protoneolítico. Periodo de transición, ligado a los cambios que produjo el fin de la última glaciación. Desde el x milenio a. C. hasta el viii milenio a. C., aproximadamente. En las zonas en las que significó una transición hacia el neolítico se denomina mesolítico, mientras que en el resto, en las que solo significa una fase de continuación del paleolítico, se denomina epipaleolítico. Neolítico (etimológicamente «nueva Edad de Piedra», por la piedra pulimentada: modo 5). Del viii milenio a. C. al iv milenio a. C. aproximadamente. Su inicio en cada zona está ligado al desarrollo de la denominada Revolución Neolítica: sustitución de la economía depredadora (caza, pesca y recolección) por la economía productora (agricultura y ganadería), lo que intensificó extraordinariamente la densidad de población (de crecimiento limitado ,régimen demográfico antiguo,) y el impacto en el medio ambiente. Aparición de la cerámica, sustitución del nomadismo por el sedentarismo (asentamientos estables o aldeas). Tuvo lugar a partir del viii milenio a. C. en el Creciente fértil del Oriente Próximo, y se difundió hacia el norte de África y Europa (en España a partir del vi milenio a. C.) y Asia. La aparición de la agricultura y la ganadería se produjo de forma endógena en otras zonas del mundo (con seguridad en América, de forma menos clara en otras zonas). Edad de los Metales. Desde el iv milenio a. C. (o más tarde, según la zona), que aunque es una época ya histórica en el Próximo Oriente Antiguo, es aún prehistórica en la mayor parte del mundo. Innovaciones tecnológicas de difusión paulatina (metalurgia, rueda, arado, vela). Algunas aldeas se amurallan y aumentan de tamaño hasta transformarse en ciudades. La economía y la sociedad se hace más compleja (excedentes, comercio a larga distancia, especialización del trabajo, estratificación social con una élite dirigente caracterizada por la exhibición de riqueza en forma de armas y monumentos funerarios). El tránsito a la historia se dará cuando se complete la formación de las sociedades complejas (civilizaciones) con estado y religión institucionalizada, que producirán la escritura. Calcolítico o Edad del Cobre (iii milenio a. C. aproximadamente, en Europa Occidental). Edad del Bronce (ii milenio a. C. aproximadamente, en Europa Occidental). Edad del Hierro (i milenio a. C. aproximadamente, en Europa Occidental, hasta la romanización). Los miles de guerreros del ejército de terracota (Xian, siglo III a. C.) servían para garantizar el eterno mandato de Qin Shi Huang, autoproclamado primer emperador de China, temeroso de los innumerables enemigos cuya venganza esperaba en la vida después de la muerte. Las civilizaciones extremo-orientales se caracterizaron por su continuidad, que no se vio interrumpida por la discontinuidad entre Edad Antigua y Edad Media propia de la civilización occidental. Especialmente la civilización china, el ejemplo más estable de imperio hidráulico, vio la repetición aparentemente perpetua de ciclos dinásticos de auge (interpretado tradicionalmente como premio por respetar el equilibrio del mandato del cielo), descomposición interna (interpretada como consecuencia del desequilibrio al no respetarlo) e invasiones exteriores (interpretadas como castigo y oportunidad de reiniciar el ciclo), que continuó hasta el siglo XX. El acueducto de Segovia, una construcción utilitaria romana de finales del siglo I, sigue determinando la personalidad de una ciudad contemporánea, junto con otros hitos de su historia como las murallas o la catedral. Otras muestras de la pervivencia de la romanización en la actualidad son la lengua, el derecho, la religión, etc. Historia. Desarrollo de la escritura como consecuencia de la aparición de los primeros estados. iv milenio a. C. en Sumeria. Protohistoria. Período de solapamiento: las civilizaciones que desarrollan escritura dejan constancia escrita no solo de sí mismas, sino de otros pueblos que no lo han hecho. Habitualmente los pueblos colonizadores son los que dejan testimonio histórico de su relación los pueblos indígenas (por ejemplo, de los pueblos prerromanos). Edad Antigua Nacimiento de la civilización en el Antiguo Oriente Próximo (a veces denominado Antigüedad temprana). Primeros estados (templos, ciudades-estado, imperios hidráulicos) en Mesopotamia (Sumeria, Acad, Babilonia, Asiria), Antiguo Egipto, Levante Mediterráneo (Fenicia, Antiguo Israel) y el resto del Mediterráneo Oriental (civilizaciones anatólicas ,hititas,, y egeas ,minoica y micénica,); con muy poca relación con esos núcleos en India (cultura del valle del Indo), China; y de forma endógena en la América precolombina y en algunas culturas del África Subsahariana. Antigüedad clásica: Entre el siglo VIII a. C. y el siglo II. De validez restringida a las civilizaciones griega y romana, caracterizadas por la cultura clásica (término de gran ambigüedad, que en su aspecto espacial y temporal puede considerarse ampliado a todo el Próximo Oriente por el helenismo posterior al Imperio de Alejandro Magno y al Mediterráneo occidental por el helenizado Imperio romano; o restringido al periodo clásico del arte griego ,siglo V a. C. y siglo IV a. C.,; o de forma aún más estricta reducido al siglo de Pericles ,la Atenas de mediados del siglo V,), y unos precoces conceptos de libertad, democracia y ciudadanía que se basaban paradójicamente en la sumisión de otros pueblos y la utilización intensiva de la fuerza de trabajo esclava. Ambas civilizaciones contaban sus eras desde fechas del siglo VIII a. C. (la primera olimpiada o la fundación de Roma, respectivamente). Simultáneamente se desarrolló el Imperio persa, que ocupa el espacio intermedio y pone en contacto las civilizaciones mediterráneas con las civilizaciones asiáticas, especialmente la hindú, mientras que las civilizaciones de Extremo Oriente, como la china, se desarrollan de forma prácticamente independiente, y las americanas en total desconexión. Antigüedad tardía: De validez restringida a Occidente, es un periodo de transición, desde la crisis del siglo III hasta Carlomagno o la llegada del islam a Europa (siglo VIII), en que el Imperio romano entra en decadencia y sufre el impacto de las invasiones germánicas, nuevas religiones monoteístas (cristianismo e islam) se imponen como religiones dominantes y el modo de producción esclavista se sustituye por el modo de producción feudal. En Oriente sobrevive el Imperio bizantino rehelenizado. Cara de la guerra en el Estandarte de Ur, iii milenio a. C. Aparecen tropas uniformadas y en formación, carros de guerra y la figura destacada de un líder. Los enemigos vencidos son pisoteados por los caballos o sometidos. Cara de la guerra en el Estandarte de Ur, iii milenio a. C. Aparecen tropas uniformadas y en formación, carros de guerra y la figura destacada de un líder. Los enemigos vencidos son pisoteados por los caballos o sometidos. Dos guerreros griegos en combate singular. Tras ellos hay carros de guerra. Fragmento de una crátera ática de figuras negras, Selinunte, siglo VI a. C. (contemporánea a las reformas de Clístenes). El equipamiento militar para el combate cuerpo a cuerpo (casco, lanza) es similar al que usarán los hoplitas, pero ellos luchan agrupados en falanges, y el escudo estará diseñado para proteger tanto al compañero de filas como al que lo lleva. Dos guerreros griegos en combate singular. Tras ellos hay carros de guerra. Fragmento de una crátera ática de figuras negras, Selinunte, siglo VI a. C. (contemporánea a las reformas de Clístenes). El equipamiento militar para el combate cuerpo a cuerpo (casco, lanza) es similar al que usarán los hoplitas, pero ellos luchan agrupados en falanges, y el escudo estará diseñado para proteger tanto al compañero de filas como al que lo lleva. Sarcófago Ludovisi, hacia 250. Las legiones romanas luchan contra los godos, que en los siglos siguientes (periodo de las invasiones bárbaras) contribuirán decisivamente tanto a la continuidad como a la Caída del Imperio romano, tras la que instaurarán algunos de los más importantes reinos germánicos de la Alta Edad Media. Sarcófago Ludovisi, hacia 250. Las legiones romanas luchan contra los godos, que en los siglos siguientes (periodo de las invasiones bárbaras) contribuirán decisivamente tanto a la continuidad como a la Caída del Imperio romano, tras la que instaurarán algunos de los más importantes reinos germánicos de la Alta Edad Media. Chac Mool (Chichén Itzá, ciudad maya fundada en el siglo VI). Las civilizaciones mesoamericanas desarrollaron una cultura peculiar ligada a la guerra ritualizada entre ciudades-estado rivales, que incluía el sacrificio de los prisioneros para garantizar el orden cosmológico, además de una antropofagia de debatida consideración. Chac Mool (Chichén Itzá, ciudad maya fundada en el siglo VI). Las civilizaciones mesoamericanas desarrollaron una cultura peculiar ligada a la guerra ritualizada entre ciudades-estado rivales, que incluía el sacrificio de los prisioneros para garantizar el orden cosmológico, además de una antropofagia de debatida consideración. Un caballero, un clérigo y un campesino (los tres órdenes feudales) ilustran la miniatura de una letra capitular en un manuscrito medieval. Edad Media: De validez restringida a Occidente, desde la caída del Imperio romano de Occidente (siglo V) hasta la caída del Imperio romano de Oriente (siglo XV). En un periodo tan prolongado se produjeron dinámicas muy complejas, que poco tienen que ver con los tópicos de aislamiento, inmovilismo y oscurantismo con que se la definía desde la perspectiva de la modernidad, que la infravaloraba como un paréntesis de atraso y discontinuidad entre una mitificada Edad Antigua y su renacimiento en la moderna. Alta Edad Media: siglo V al siglo X. Una época en la que destaca el retroceso de la vida urbana y de la descomposición del poder político central que caracterizan al feudalismo. La Iglesia, sobre todo a través del monacato, se convierte en la única continuidad de la tradición intelectual. La nobleza y el clero, vinculados familiarmente, son los señores que ejercen el poder político, social y económico sobre los campesinos sometidos a servidumbre. Castillos y monasterios se imponen en un paisaje de bosques, baldíos y pequeñas aldeas casi incomunicadas. Baja Edad Media: Del siglo XI al siglo XV. A veces se restringe al siglo XIV y al siglo XV, como Crisis de la Edad Media o Crisis del siglo XIV; denominándose el periodo del siglo XI al siglo XIII como Plenitud de la Edad Media. Se produce una revolución urbana y un aumento de la actividad comercial y artesanal de una incipiente burguesía, al tiempo que se fortalece el poder de las monarquías feudales. Los poderes universales (Pontificado e Imperio) se enfrentan y entran en crisis. Las Cruzadas demuestran la capacidad de expansión europea hacia el oriente del Mediterráneo, mientras en la península ibérica se terminan imponiendo los reinos cristianos a Al-Ándalus (España musulmana) en un proceso denominado La Reconquista. La universidad medieval reelaboró el saber antiguo a través de la escolástica (revolución del siglo XII). En los siglos finales se conforman los rasgos que caracterizarán todo el periodo del Antiguo Régimen: una economía en transición del feudalismo al capitalismo, una sociedad estamental y una monarquía autoritaria en transición a la monarquía absoluta. El David de Miguel Ángel (1504), obra cumbre del Renacimiento italiano, y ejemplo de la confianza en el ser humano propia del antropocentrismo humanista. Edad Moderna: De mediados o finales del siglo XV a mediados o finales del siglo XVIII. (Para los anglohablantes, Early Modern Times, es decir, «Primera Edad Moderna» o «Edad Moderna Temprana»). Se toma como hitos que marcan su comienzo la Imprenta, la toma de Constantinopla por los turcos o el descubrimiento de América; como final, la Revolución francesa, la Independencia de los Estados Unidos de América o la Revolución industrial. Es por primera vez, un periodo de validez casi mundial, puesto que para la mayor parte del mundo (con la excepción solo parcial de China o Japón ,que tras unos primeros contactos optan por cerrarse a la influencia exterior en mayor o menor medida, o de espacios recónditos de América, África y Oceanía ,colonizados en el siglo XIX,), significó la imposición de la civilización occidental y la denominada economía-mundo. Se inició con la era de los descubrimientos y la expansión del imperio español y el portugués, mientras el mundo de las ideas experimentaba las innovaciones del Renacimiento, la Reforma Protestante y la Revolución científica; contrapesadas por la Contrarreforma y el Barroco. Mientras en la Francia de Luis XIV triunfaba el absolutismo, en otras partes de Europa noroccidental lo hacían las primeras revoluciones burguesas que desafiaban al Antiguo Régimen (revolución neerlandesa, revolución inglesa) y en el sur y este del continente se observaba un proceso de refeudalización. El eje de la civilización se desplazó de la cuenca del Mediterráneo al océano Atlántico. La crisis del siglo XVII y los tratados de Westfalia reedificaron un nuevo equilibrio europeo que imposibilitaba las hegemonías española o francesa, y que se mantuvo durante el siglo XVIII, caracterizado intelectualmente por la Ilustración. A lo largo de todo el periodo se van gestando los modernos conceptos de nación y estado. Prueba nuclear en el atolón de Bikini, 26 de marzo de 1954, en plena Guerra fría. La era nuclear se inauguró en 1945, cuando los Estados Unidos lanzaron en Hiroshima y Nagasaki las primeras bombas atómicas. La Unión Soviética la siguió en lo que se denominó carrera nuclear o carrera de armamentos (simultánea a la carrera espacial), así como las otras tres potencias con derecho a veto en el Consejo de Seguridad de Naciones Unidas: Reino Unido, Francia y China. Otros países no firmantes del tratado de no proliferación nuclear han desarrollado este armamento: abiertamente India y Pakistán; sin reconocerlo Israel, Sudáfrica ,lo desmanteló al caer el régimen de apartheid, y quizá otros. Edad Contemporánea. Desde mediados o finales del siglo XVIII hasta el presente. (Para los anglohablantes Later Modern Times, es decir, «Segunda Edad Moderna» o «Edad Moderna Tardía»). Una inicial era de las revoluciones (revolución industrial, revolución burguesa y revolución liberal) acabó con el Antiguo Régimen y dio paso en la segunda mitad del siglo XIX al triunfo del capitalismo que se extiende con el imperialismo a todo el mundo, al tiempo que se veía contestado por el movimiento obrero. Las guerras napoleónicas dieron paso a un periodo de hegemonía británica durante la era Victoriana. El comienzo de la transición demográfica (primero en Inglaterra, poco después en el continente europeo y posteriormente en el resto del mundo) produce una verdadera explosión demográfica que altera de forma radical el equilibrio social y el del hombre con la naturaleza, sobre todo a partir de la segunda revolución industrial (paso de la era del carbón y de la máquina de vapor a la era del petróleo y el motor de explosión y la era de la electricidad). La primera mitad del siglo XX se marcó por dos guerras mundiales y un período de entreguerras en el que las democracias liberales enfrentadas a la crisis de 1929 se ven desafiadas por los totalitarismos soviético y fascista. La segunda mitad del siglo XX se caracterizó por el equilibrio del terror entre las dos superpotencias (Estados Unidos y la Unión Soviética), y la descolonización del Tercer Mundo, en medio de conflictos regionales de gran violencia (como el árabe-israelí) y una aceleración de la innovación tecnológica (tercera revolución industrial o revolución científico-técnica). Desde 1989, la caída del muro de Berlín y la desaparición del bloque socialista condujeron al mundo actual del siglo XXI presidido por la globalización tanto de la economía como de la presencia política, militar e ideológica (poder blando) de la única superpotencia, así como de sus aliados (potencias clásicas ,Unión Europea, Japón,), socios o posibles rivales (potencias emergentes ,China,) y opositores (potencias menores, como algunos países islámicos, y movimientos a veces expresados en terrorismo ,11-S,)."
ksampletext_wikipedia_hist_guerra: str = "Guerra. La guerra o conflicto bélico, estrictamente hablando, es aquella lucha sociopolítica en la que dos o más grupos humanos relativamente masivos ,principalmente tribus, sociedades o naciones, se enfrentan de manera violenta, generalmente mediante el uso de armas de toda índole, a menudo con resultado de muerte ,individual o colectiva, y daños materiales de una entidad considerable. La guerra es la forma de conflicto sociopolítico más grave entre dos o más grupos humanos. Se da tanto en sociedades tribales como en civilizadas, pero es más grave entre estas últimas ya que son más complejas, masificadas y tecnificadas. Es quizás la más antigua de las relaciones internacionales y ya en el comienzo de las civilizaciones se constata el enfrentamiento organizado de grupos humanos armados con el propósito de controlar recursos naturales o humanos (conflictos entre cazadores nómadas y recolectores sedentarios que sí desarrollaron el concepto de propiedad), exigir un desarme o imponer algún tipo de tributo, ideología, nacionalidad o religión, sometiendo, despojando y, en su caso, destruyendo al enemigo. Es más, este tipo de conducta gregaria es extensible a la mayor parte de los homínidos y se encuentra estrechamente relacionado con el concepto etológico de territorialidad. Las guerras tienen como origen múltiples causas, entre las que suelen estar el mantenimiento o el cambio de relaciones de poder, dirimir disputas económicas, ideológicas, territoriales (por cuestiones históricas y estratégicas), religiosas, etc. (muchas veces una combinación de causas). En ciencia política y relaciones internacionales, la guerra es un instrumento político, al servicio de un Estado u otra organización con fines eminentemente políticos, ya que en caso contrario constituiría una forma más desorganizada aunque igualmente violenta: el bandolerismo por tierra o la piratería por mar. En las sociedades primitivas tribales su origen aparece más claro; deriva de dos elementos: la presión demográfica y la escasez de recursos.[cita requerida] Según Richard Holmes, la guerra es una experiencia universal que comparten todos los países y todas las culturas. Según Sun Tzu, «La guerra es el mayor conflicto de Estado, la base de la vida y la muerte, el Tao de la supervivencia y la extinción. Por lo tanto, es imperativo estudiarla profundamente». Por demás, la forma más astuta de ejercerla sería soslayarla de manera que no hubiera necesidad de llegar a ella. Según Carl von Clausewitz, la guerra es «la continuación de la política por otros medios». Las reglas de la guerra, y la existencia misma de reglas, han variado mucho a lo largo de la historia. El concepto de quiénes son los combatientes también varía con el grado de organización de las sociedades enfrentadas. Las dos posibilidades más frecuentes son civiles sacados de la población general, generalmente varones jóvenes, en caso de conflicto, o soldados profesionales formando ejércitos permanentes. También puede haber voluntarios y mercenarios. Las combinaciones de varios o de todos estos tipos de militares son asimismo frecuentes. Las formas de hacer una guerra dependen de los propósitos de los combatientes. Por ejemplo, en las guerras romanas, cuyo objetivo era expandir el imperio, el objetivo militar principal era, una vez sometido, incorporar al pueblo ajeno al imperio y a las leyes y costumbres de Roma. En la actualidad, a veces se hace distinción entre conflictos armados y guerras. De acuerdo con este punto de vista, un conflicto solo sería una guerra si los beligerantes han hecho una declaración formal de la misma. En una concepción de la doctrina militar de Estados Unidos no se hace distinción alguna, refiriéndose a los conflictos armados como guerras de cuarta generación. Batalla de Normandía. Entre el final de la Segunda Guerra Mundial y 2010 hubo 246 enfrentamientos armados en 151 lugares del mundo. Definiciones y conceptos La ciudad de Wesel en la cuenca del Ruhr, destruida por los bombarderos aliados. Platón no habla de guerreros, sino de «guardianes» de la polis, y distingue además entre la discordia (que se da entre los griegos) y la guerra (que se da entre griegos y bárbaros). Aristóteles afirmó que la guerra solo sería un medio en vista de la paz, como lo es el trabajo en vista del ocio y la acción en vista del pensamiento, pues considera que la guerra es tan natural en la sociedad humana como la paz, ya que también es legítima la esclavitud en la naturaleza para mantener la jerarquía de lo mejor sobre lo peor, el orden social: El ejercicio de la guerra no debe perseguirse con el fin de esclavizar a los que no lo merecen, sino, en primer lugar, para no ser esclavizados por otros; en segundo lugar, para procurar la hegemonía por el bien de los gobernados, no por deseo de dominar a todos; y en tercer lugar, para enseñorearse de los que merecen la esclavitud. La guerra, afirma el marqués de Olivart, es el litigio entre las naciones que defienden sus derechos, en el cual es el juez la fuerza y sirve de sentencia la victoria. Hugo Grocio la definió como status per vincertatium qua tales sunt. Por su parte, Alberico Gentilis afirmó que Bellum est armorum publicorum ensta contentio. Funk - Bretano y Alberto Sorel escribieron: La guerra es un acto político por el cual varios Estados, no pudiendo conciliar lo que creen son sus deberes, sus derechos o sus intereses, recurren a la fuerza armada para que esta decida cuál de entre ellos, siendo más fuerte, podrá en razón de la fuerza, imponer su voluntad a los demás.. Joseph de Maistre (1821) dijo, en sus Soirees de Saint Petesburg: «La guerra es divina en la gloria misteriosa que le rodea y en el atractivo no menos explicable que nos lleva hacia ella. La guerra es divina por la manera como se produce independientemente de la voluntad de los que luchan. La guerra es divina en sus resultados que escapan absolutamente a la razón». G.W.F Hegel escribió: «La guerra es bella, buena, santa y fecunda; crea la moralidad de los pueblos y es indispensable para el mantenimiento de su salud moral. Es en la guerra donde el Estado se acerca más a su ideal porque es entonces cuando la vida y los bienes de los ciudadanos están más estrechamente subordinados a la conservación de la entidad común». El instituto de investigación de la paz internacional de Suecia, define la guerra como todo aquel conflicto armado que cumple dos requisitos: enfrentar al menos una fuerza militar, ya sea contra otro u otros ejércitos o contra una fuerza insurgente y haber muerto diez mil o más personas. Johan Huizinga establece que la guerra obtiene un carácter lúdico cuando se cumple con la condición agonal; el elemento agonal empieza a actuar en el momento en el que los adversarios se consideran enemigos que luchan por una cosa a la que pretenden tener derecho. Los fines del derecho son la paz y la justicia, vocablos polisémicos; la paz incluye la seguridad; por eso la guerra supone la suspensión del derecho. El jurista Rudolf von Ihering en su Der Kampf ums Recht o La lucha por el Derecho (1872) sostuvo que la fuerza es la base del derecho y que el derecho sin la fuerza es una utopía. Pero el derecho es la lucha contra la injusticia: Todo derecho en el mundo debió ser adquirido por la lucha; esos principios de derecho que están hoy en vigor ha sido indispensable imponerlos por la lucha a los que no lo aceptaban, por lo que todo derecho, tanto el derecho de un pueblo, como el de un individuo, supone que están el individuo y el pueblo dispuestos a defenderlo. El derecho no es una idea lógica, sino una idea fuerza; he ahí porque la justicia, que sostiene en una mano la balanza donde pesa el derecho, sostiene en la otra la espada que sirve para hacerle efectivo. La espada, sin la balanza, es la fuerza bruta, y la balanza sin la espada, es el derecho en su impotencia; se completan recíprocamente: y el derecho no reina verdaderamente, más que en el caso en que la fuerza desplegada por la justicia para sostener la espada, iguale a la habilidad que emplea en manejar la balanza. Causas de la guerra La cosecha de la batalla (1918), óleo de Christopher Nevinson sobre la I Guerra Mundial Buscar una o varias causas a las guerras ha sido una constante para muchos historiadores y políticos con el fin de evitar posibles conflictos futuros o encontrar culpables. Pero el jurista Papiniano afirmaba que es más fácil cometer un crimen que justificarlo y el senador Hiram Johnson escribió ya en 1917 que «la primera víctima cuando llega la guerra es la verdad». Autores como Brian Hayes señalan, sin embargo, que hay consenso en tener como ciertas algunas causas. Causas tradicionales Una de las causas de la guerra es que dos naciones tengan diferencias profundas en diversos temas, que solo pueden resolverse con la vía armada. El historiador griego Tucídides afirma en su Diálogo de los melios, incluido en su Historia de la guerra del Peloponeso que no es vergonzoso someterse a un enemigo más fuerte, especialmente uno que está ofreciendo términos razonables... La justicia solo se tiene en cuenta en el razonamiento de los hombres si las fuerzas son iguales en ambos lados; en el caso contrario, los fuertes ejercen su poder y los débiles deben ceder ante ellos, pero de hecho muchos inferiores no se someten a la razón sino a la guerra. Desde el punto de vista sociofilosófico, se han avanzado muchas teorías sobre el origen y causa de la guerra. La primera, más contundente, resumida, filosófica, racional (en cuanto a explicar el origen de un fenómeno) es la que propone Platón en La República (tras afirmar que una ciudad es feliz si se ocupa de disponer de lo necesario y nada más): Si queremos tener bastantes pastos y tierras de labor, ¿tendremos necesidad de usurpar algo a nuestros vecinos y nuestros vecinos harán otro tanto con nosotros si, traspasando los límites de lo necesario, se entregan como nosotros al deseo insaciable de enriquecerse? [...] ¿Haremos, pues, la guerra en pos de esto? [...] Hemos descubierto nosotros el origen de este azote, que cuando descarga, acarrea funestos males a los estados y a los particulares. Sócrates Además, parece posible tratar de clasificar, muy en general, las teorías en dos grandes divisiones: la que ve la guerra como producto racional de ciertas condiciones, primariamente condiciones políticas (Carl von Clausewitz argumentó que la guerra es la continuación de la política por otros medios) y otra irracionalista, que ve la guerra como producto de una tendencia, últimamente irracional, de los seres humanos. Las teorías irracionalistas pueden aproximarse desde dos puntos de vista: A raíz de la aparición de las armas nucleares se cambió el concepto de guerra: por primera vez se podía dar por resultado la aniquilación total de los dos bandos. 1. Aquellas que ven el origen de la guerra en causas no atribuible a fundamento racional, por ejemplo, sentimientos religiosos o emociones. El extremo lógico de esta visión ,que el hombre es un animal inherentemente agresivo sujeto a tendencias tanto de competición como cooperación que se observan en animales sociales, situación que demanda la expresión ocasional de tales tendencias, se encuentra en algunas explicaciones ya sea biológicas, psicológicas o de la psicología social del origen de conflictos (ver, por ejemplo: Experimento de Robbers Cave). 2. La visión alternativa dentro de esta posición ve la guerra como originándose, en algunos casos, debido a equivocaciones o percepciones erróneas. Así, por ejemplo, Lindley y Schildkraut argumentan, a partir de un análisis estadístico, que la cantidad de guerras que se podría aducir tuvieron un origen racional ha disminuido dramáticamente en tiempos recientes (Lindley y Schildkraut ofrecen como ejemplos de tales equivocaciones la guerra de las Malvinas aunque se dice que la causa fue en verdad subir la popularidad de Margaret Thatcher de Inglaterra declarando ella la guerra ya que Argentina no había matado a nadie y ellos hundieron al Belgrano que estaba yendo al continente matando a la mitad de todos los argentinos que murieron, y la guerra de Irak) que otros aluden al deseo de petróleo, riquezas y dominio a la causa. La visión alternativa, de la guerra como actividad racional, se basa en dos percepciones. La original de von Clausewitz acerca de la guerra constituyendo la persecución de (objetivos de) la política por otros medios, y una percepción posterior (implícita en von Clausewitz) que indica que se recurriría a la guerra cuando se estima que las ganancias superan a las pérdidas potenciales (es decir, a través de un análisis de costo-beneficio). A su vez, se pueden distinguir dos posiciones: 1. La teoría de la primacía de las políticas domésticas: se encuentra, por ejemplo, en las obras de Eckart Kehr y Hans-Ulrich Wehler (op. cit). Para esta posición, la guerra es el producto de condiciones domésticas. Así, por ejemplo, la Primera Guerra Mundial no fue producto de disputas internacionales, tratados secretos o consideraciones estratégicas, sino el resultado de condiciones sociopolíticas, incluyendo económicas, que, a pesar de ser comunes a varias sociedades, hacían sentir tensiones a cada una de ellas en forma interna, tensiones que solo se pudieron resolver a través de la guerra. 2. La teoría de la primacía de la política internacional, que se encuentra, por ejemplo, en la concepción de Leopold von Ranke, de acuerdo a quien son las decisiones de estadistas motivados por consideraciones geopolíticas las que conducen a la guerra. Pedro Luis Lorenzo Cadarso sintetiza en tres grupos de teorías el origen de las guerras: Teoría psicogenética, que considera que la guerra es una forma de canalizar la agresividad humana, que existe bien por razones genéticas ,instintivas, por tanto,, bien por determinada configuración psicológica de nuestro carácter. El evolucionista Richard Dawkins la vincula a lo que llama el gen egoísta. Freud y el psicoanálisis la vinculan con el complejo de Edipo, generador de la frustración-competencia que se halla en los orígenes de la agresividad y la competitividad. Anthony Storr considera que la agresividad humana puede ser controlada y encauzada, pero no suprimida y la especie humana es la más despiadada dentro del reino animal. Teoría socioeconómica o infraestructural, que vincula el recurso a la guerra a la existencia de desequilibrios entre población y recursos o bien a la competencia entre grupos por la posesión o ampliación de los recursos disponibles. Un exponente de estas teorías es el antropólogo Robert Ardrey. Teoría política. Los partidarios de esta teoría tienden a analizar la guerra sin apriorismos morales ni de otro tipo: la guerra existe porque en un hipotético balance de costes y beneficios resulta rentable políticamente. Es la teoría de Clausewitz; no es sino una estrategia más en el eterno enfrentamiento por el reparto del mundo entre las naciones y el reparto del poder y la riqueza entre los grupos sociales. Paradójicamente, la guerra es útil socialmente porque saber que puede estallar obliga a los hombres a ser más tolerantes y recurrir a la negociación y a la política para evitarla. Decepción estadística Este deseo de conocer las causas para poder predecir cuando estallará el próximo conflicto ha sido abordado en varias ocasiones. Uno de los investigadores del fenómeno bélico fue Lewis Fry Richardson. Este autor investigó todos los conflictos desde el siglo XIX hasta la década de los 1950; considerando conflicto aquel enfrentamiento donde han muerto personas por causa intencionada de otra persona; de este modo juntaba los conflictos bélicos con las muertes por asesinato y homicidio, la mezcla fue intencionada por sus experiencias en la Segunda Guerra Mundial por las cuales pudo comprobar el efecto de muchas de las órdenes que vio dar y la suerte corrida por muchos soldados, enviados a la muerte a causa de esas órdenes. Richardson tuvo la idea de catalogar las guerras según el número de muertos de una forma similar a cómo se catalogan los terremotos: según su intensidad. Así, una guerra de magnitud 6 sería en la que morirían de 1 000 000 a 1 999 999 personas; pero por todas las dificultades que halló para saber el número de muertos en una contienda (llegó a decir que resultaba más fácil saber el número de estrellas de una galaxia o de neutrinos en el universo) Richardson aplicó un índice de error de 0,5 (más menos); con este índice de error una guerra de magnitud 3 sería aquella en la que perecieron entre 316 228 y 3 162 278. Aunque Richardson no fue el primero en recopilar conflictos bélicos su trabajo es uno de los más exhaustivos, pues comenzó en 1940 y siguió hasta el año de su muerte en 1953. Según sus estudios entre 1820 y 1950 hubo 315 conflictos de magnitud 2,5 o superior (al menos 300 muertos). Pese a reconocer que resulta muy difícil saber cuando comienza un conflicto y cuando termina, si es uno o varios al tiempo o el ya citado número de muertos; los resultados fueron decepcionantes en cierto modo: La frecuencia con la que estallan las guerras es muy similar a la de cualquier suceso aleatorio, lo que parece indicar que las guerras son imprevisibles. La frecuencia con la que estallan las confrontaciones sigue la distribución de Poisson, lo que parece indicar que las guerras son un suceso aleatorio. Así pues el autor concluyó que la principal causa de la guerra es la casualidad. En segundo lugar, colocó los conflictos cronológicamente y según su magnitud, para saber si algún tipo de conflicto se repetía o si un tipo de guerra iba en aumento o en detrimento respecto a las demás. Los resultados tampoco fueron concluyentes, volviendo a mostrar una distribución muy similar al suceso aleatorio. De esta forma la conclusión es que de las guerras no se aprende a evitarlas y que la probabilidad de que estalle un nuevo conflicto es la misma para cualquier día, no importa si antes ha sucedido otro ni el tamaño de este otro. Profundizando en su trabajo realizó un estudio de países vecinos que entraban en guerra. Midiendo las fronteras llegó a la conclusión de que un país linda con otras 6 naciones por término medio; por lo que la probabilidad de que una nación entrara en guerra con un vecino era casi del 10 %, si fuera un proceso aleatorio; sin embargo la estadística indicaba que la probabilidad era del 87,33 % (de 94 guerras estudiadas solo 12 no tenían frontera común). Por lo tanto, según el matemático, otra causa de la guerra es la vecindad. Richardson también relacionó las guerras con otros factores comúnmente indicados por los historiadores, como crisis económica o religión, llegando a otras tantas decepcionantes conclusiones: La carrera de armamento no tiene porqué desembocar en un conflicto armado: de 315 conflictos solo en 13 había una carrera de armamento preparatoria. Un idioma común no evita las guerras. Una crisis económica no tiene por qué desembocar en guerras civiles, ni tampoco entre estados. Solo pueblos de distintas religiones tienen más probabilidad de entablar guerras entre ellos. Así mismo, parece que los pueblos cristianos muestran más belicosidad que los de otros credos, al haber intervenido en una proporción mucho mayor de conflictos que el resto. No obstante Richardson concluyó que ni siquiera la religión es una causa de gran importancia. El siguiente en investigar en este ámbito es H. van Velzen y W. Wetering, quienes, en un análisis comparativo sobre residencia y conflicto, llegaban a la conclusión de que los grupos fraternos locales y la patrilinealidad constituyen las variables más significativas en relación con la frecuencia de la guerra. Algunos años más tarde, esta idea sería retomada por K. Otterbein, quien, en una nueva investigación transcultural, señalaría otra variable importante: la poliginia. En síntesis, Otterbein sostiene que las sociedades patrilocales y poligínicas y con grupos locales fraternos recurren más fácilmente a la violencia que las no patrilocales y poligínicas y sin dichos grupos. Es más, según este autor, las sociedades con mayor número de conflictos armados son aquellas que poseen comunidades políticas similares. Orrerbein denomina guerra interna al conflicto entre éstas, para distinguirlo del que se origina entre comunidades culturalmente distintas o guerra externa. Así, tomando como base su propia tabulación estadística resulta que, sobre una muestra de veintiocho sociedades patrilocales, un 71 % se caracteriza por guerra interna frecuente y un 19 % por guerra interna esporádica, mientras que en catorce sociedades no patrilocales, solo un 55 % presenta conflictos internos frecuentes. Concepto filosófico La Praefatio de Thomas Hobbes, De cive, donde la frase «Bellum omnium contra omnes / Guerra de todos contra todos» aparece por vez primera. Tomada de la edición revisada impresa en 1647 en Ámsterdam (apud L. Elzevirium). Para Maquiavelo y Thomas Hobbes está en la naturaleza humana el deseo y la ambición, lo que induce a la inseguridad colectiva y a la guerra de todos contra todos, y, por tanto, esa inseguridad es el fundamento de la ley y del Estado, que debe cuando menos reducirla. Hobbes determina que las causas principales de la guerra entre los hombres son: La primera es la competencia; en segundo lugar, la desconfianza; y en tercer lugar, la gloria. La primera hace que los hombres invadan el terreno de otros para adquirir ganancia; la segunda, para lograr seguridad; y la tercera, para adquirir reputación. La primera hace uso de la violencia, para que así los hombres se hagan dueños de otros hombres, de sus esposas, de sus hijos y de su ganado. La segunda usa la violencia con un fin defensivo. Y la tercera, para reparar pequeñas ofensas, como una palabra, una sonrisa, una opinión diferente, o cualquier otra señal de desprecio dirigido hacia la propia persona o, indirectamente, a los parientes, a los amigos, a la patria, a la profesión o al prestigio personal. Se ha sugerido si, desde un punto de vista moral o filosófico, sería posible hablar de una guerra justa o lícita. Si ese es el caso, hay que distinguir: Si la guerra en general puede ser lícita. Si se cumplen o garantizan las condiciones requeridas. A primera vista parece posible proponer que la guerra no es necesariamente ilícita. Existe el derecho natural de autodefensa o de legítima defensa contra el enemigo exterior cuando este ataca injustamente a un pueblo. Si se niega este derecho de legítima defensa, se robustece al agresor y se pone en peligro la paz de los pueblos. Sin embargo, se ha sugerido desde una perspectiva ética que, para que una guerra pueda tener una licitud ética, existen una serie de condicionantes adicionales: Que haya una injusticia real, verdadera y de gravedad. Inviabilidad de defenderse por vía pacífica. Perspectiva y esperanza de éxito final. Que se pueda evitar un perjuicio a terceros inocentes. La defensa del bien público prevalece sobre cualquier derecho del agresor e incluso sobre los riesgos que puedan tener los propios agredidos. Pero se considera ilícita la matanza injusta. Desde ese mismo punto de vista filosófico, se considera que el movimiento a favor de la paz se hace acreedor del más alto reconocimiento. Dicho movimiento es difusor de un espíritu de entendimiento y comprensión entre los pueblos. Su fin ético y moral es conseguir la paz y los acuerdos sin derramamiento de sangre. Clasificación de los tipos de guerras La clasificación de las guerras propuesta por el materialismo filosófico, toma como referencia fundamental la existencia o no de Estado en los contendientes. De este modo, las guerras se diferencian en función de si enfrentan a sociedades sin Estado, a un Estado frente a pueblos preestatales, a tribus contra un Estado ya constituido, a Estados entre sí, o bien a facciones dentro de un mismo Estado. El elemento decisivo es, por tanto, el estatus político-jurídico de los beligerantes. En este marco, las llamadas guerras civiles o de quinto género no se reducen a enfrentamientos internos, sino que se caracterizan por dirimirse en ellas la cuestión de la Legitimidad (política) del poder, puesto que la soberanía solo puede ser una. Además hay un sexto género, el de las guerras mixtas, con el que alude a aquellos conflictos históricos que no se dejan encasillar en un solo tipo y que presentan simultáneamente rasgos de varios de los anteriores. Un ejemplo paradigmático de estas guerras mixtas son las Guerras de independencia hispanoamericanas, en las que confluyeron componentes de guerra civil (entre facciones locales enfrentadas por la legitimidad), de secesión (ruptura con el Imperio español) y de guerra internacional (debido a la intervención de potencias extranjeras como el Imperio napolónico, Reino Unido o, posteriormente, Estados Unidos). Clasificación de las guerras según el Materialismo filosófico Género Descripción Ejemplo histórico 1. Entre sociedades preestatales Conflictos entre grupos humanos sin Estado organizado (tribus, clanes, aldeas). Guerras entre clanes o tribus antes de la formación de los estados. 2. De un Estado contra sociedades preestatales próximas Un Estado se enfrenta a pueblos sin organización estatal para dominarlos o someterlos. Conquista española contra pueblos mapuches o Guerra de Arauco. 3. De tribus contra un Estado Sociedades preestatales se rebelan contra un Estado ya constituido. Rebelión de Viriato contra Roma. 4. Guerras entre Estados Conflictos entre Estados soberanos reconocidos como tales. Guerra franco-prusiana (1870–1871). 5. Guerras civiles (dentro de un Estado) Enfrentamientos internos por el poder y la legitimidad. Guerra Civil Española(1936–39). 6. Guerras mixtas Conflictos que combinan varios géneros a la vez (civiles, estatales e imperios). Guerras de independencia hispanoamericanas. Tratadistas El general chino Sun Tzu, en su célebre obra El arte de la guerra, afirmó que la guerra había que ganarla antes de declararla o de que existiera en sí misma. En este aspecto, el célebre general expondría en una sucinta frase su concepción sobre el carácter de la guerra: «La guerra, es el Tao del engaño»; así, pretendería establecer que el estratega virtuoso debía basar todas sus decisiones militares, buscando primeramente distraer la atención del enemigo en los elementos más sobresalientes de su posición, y de no tenerlos, inventarlos. El pensamiento de Sun Tzu, dejaría una profunda impronta en el pensamiento militar moderno, no solo en reconocidos pensadores, sino también en eximios estrategas como Napoleón Bonaparte, quien en su renombrada victoria en la Batalla de Austerlitz, aplicara aquellos preceptos del engaño. El concepto de guerra justa fue presentado sistemáticamente por Tomás de Aquino en Summa Theologiae. Erasmo de Róterdam, el reconocido humanista renacentista, calificaba a la guerra con la frase Dulce bellum inexpertis est, cuya traducción al castellano es La guerra es dulce para los inexpertos. El historiador árabe Ibn Jaldún descubrió por primera vez las causas materiales de la guerra. Carl von Clausewitz, en su clásica obra De la guerra, pensaba que la guerra moderna es «La continuación de la política por otros medios» y que el fin de la misma era «desarmar al enemigo», no exterminarlo; de aquí nació el concepto de desarme mutuo, que imposibilita toda guerra y da paso a la política. La guerra sería pues un «acto político» y esta manifestación ponía en juego lo que él consideraba el único elemento racional de la guerra. Guerras e historia Según la Enciclopedia mundial de las relaciones internacionales y Naciones Unidas, en los últimos 5500 años se han producido 14513 guerras que han costado 1240 millones de vidas y no han dejado sino 292 años de paz. Y únicamente entre 1960 y 1982, dicha enciclopedia calcula 65 conflictos armados (solo los que hayan producido al menos mil muertos) en 49 países, con un total de 11 millones de víctimas. Estela de los Buitres, que relata la victoria de Eannatum de Lagash sobre Umma, hacia 2450 a. C. El primer conflicto bélico del que se tiene constancia es el que enfrentó a las ciudades-estado sumerias de Lagash y Umma, hacia el año 2450 a. C. La disputa se produjo por unas tierras de regadío. El rey de Lagash, Eannatum, comandó el ejército, que resultó victorioso, y convirtió a Umma en un estado vasallo. Guerras contemporáneas Los conflictos bélicos en la siguiente lista representan guerras por control de un estado, en las cuales un mínimo de 1000 personas habrían perdido sus vidas en 2011 o 2012. Las estadísticas son del Programa de Datos sobre Conflictos de Upsala en Suecia. Guerras con más de 1000 muertos en 2010, 2011 o 2012 Inicio Guerra/conflicto País Muertos en 2010 Muertos en 2011 Muertos en 2012 2001 Guerra de Afganistán Bandera de Afganistán Afganistán 6377 7418 7396 1991 Guerra civil somalí Bandera de Somalia Somalia 2076 1938 2620 2004 Guerra en el noroeste de Pakistán Bandera de PakistánPakistán 4858 2599 2705 2004 Conflicto de Sadah Bandera de Yemen Yemen y Bandera de Arabia Saudita Arabia Saudita 175 1140 2321 2011 Conflicto de Sudán (2011) Bandera de SudánSudán 931 1248 1119 2011 Guerra civil siria Bandera de Siria Siria - 842 55 000 Guerra moderna Equipo de ametralladora finlandesa durante la guerra de Invierno en 1939–1940, durante la Segunda Guerra Mundial. Tanques estadounidenses en formación durante la guerra del Golfo. La guerra moderna , aunque está presente en cada período histórico de la historia militar, se utiliza generalmente para describir los conceptos, métodos y tecnologías que estaban en uso durante y después de la Segunda Guerra Mundial. Aunque la Primera Guerra Mundial fue una guerra moderna, ya que en ella se introdujeron masivamente elementos de guerra muy conocidos en el presente como los tanques, ametralladoras, granadas, cascos y aviones, etc.; por lo tanto marcó un antes y después en la historia de las guerras. Con el advenimiento de las armas nucleares, el concepto de guerra total, tiene la posibilidad de la aniquilación global, y que los conflictos de este tipo desde la Segunda Guerra Mundial fueron, por definición, de baja intensidad. Las guerras modernas tienen como propósito el ganar control sobre el tejido social como una manera de destruir al enemigo, separando usualmente a los agresores de la verdad de sus propios actos. Esto, debido a que los asesinatos, masacres o los desplazamientos masivos eliminan a las víctimas y regalan a los vencedores una verdad indiscutible. La victoria encierra al vencedor en el olvido que libra del remordimiento, sentimientos imprescindible para encontrarse con la verdad. Lista de guerras modernas 1904-1905: guerra ruso-japonesa 1912-1913: guerras de los Balcanes 1914-1918: Primera Guerra Mundial 1917-1923: guerra civil rusa 1919-1921: guerra polaco-soviética 1921-1927: guerra del Rif 1932-1933: guerra colombo-peruana 1932-1935: guerra del Chaco 1936-1939: guerra civil española 1939-1945: Segunda Guerra Mundial 1941-1942: guerra peruano-ecuatoriana 1946-1954: guerra de Indochina 1947-1991: Guerra Fría 1947-1998: conflicto entre India y Pakistán 1948-1949: guerra árabe-israelí de 1948 1950-1953: guerra de Corea 1954-1962: guerra de Independencia de Argelia 1955-1975: guerra de Vietnam 1960-1966: crisis del Congo 1960-1996: guerra civil de Guatemala 1960: conflicto armado interno en Colombia 1961: invasión de Bahía de Cochinos 1961-1974: guerra colonial portuguesa 1961-1990: Revolución Sandinista 1962: guerra sino-india 1962-1966: confrontación indonesio-malaya 1963: guerra de las Arenas 1965: guerra indo-pakistaní de 1965 1966-1990: guerra de la frontera de Sudáfrica 1967: guerra de los Seis Días 1967-1970: guerra de Desgaste 1968-1998: conflicto de Irlanda del Norte 1969: guerra del Fútbol 1971: guerra indo-pakistaní de 1971 1973: guerra de Yom Kipur 1974-1991: guerra civil etíope 1975-1979: Operativo Independencia 1975-2002: guerra civil angoleña 1977-1978: guerra de Ogaden 1978-1989: guerra de Afganistán 1980-1988: guerra entre Irán e Irak 1980-2000: época del terrorismo en Perú 1980-1992: guerra civil de El Salvador 1982: guerra del Líbano de 1982 1982: guerra de las Malvinas 1983: invasión de Granada 1985: guerra subsidiaria irano-israelí 1983-2009: guerra civil de Sri Lanka 1987-1993: primera Intifada 1988-1994: guerra del Alto Karabaj 1989-1990: invasión estadounidense a Panamá de 1989 1990-1991: guerra del Golfo 1991-1993: guerra civil georgiana 1991-2002: guerra civil argelina 1991-actualidad: guerra civil somalí 1991-1999: guerras Yugoslavas 1992-1993: guerra de Abjasia 1992-1995: guerra de Bosnia 1992-1997: guerra civil tayika 1994-1996: primera guerra chechena 1995: guerra del Cenepa 1996-1997: primera guerra del Congo 1998-1999: guerra de Kosovo 1998-2003: segunda guerra del Congo 1998: Operación Zorro del Desierto 1999: guerra de Kargil 1999-2009: segunda guerra chechena 2000-2005: intifada de Al-Aqsa 2001: guerra contra el terrorismo 2001-2021: guerra de Afganistán 2001: insurgencia islamista en Nigeria 2001: insurgencia narcoterrorista en el Perú 2003-2011: guerra de Irak 2004-2015: conflicto de Sadah 2004: guerra en el noroeste de Pakistán 2004: guerra de Kivu 2006: guerra del Líbano 2006: guerra contra el narcotráfico en México 2008: guerra de Osetia del Sur 2008: incursión turca en el norte de Irak 2008-2009: conflicto de la Franja de Gaza 2009-2017: insurgencia en el Cáucaso Norte 2011-2013: insurgencia iraquí posterior al retiro de las tropas estadounidenses 2011: guerra de Libia 2011-2014: violencia miliciana en Libia 2011-actualidad: guerra civil siria 2011-2023: insurgencia en el Sinaí 2012: guerra civil de la República Centroafricana 2012-2015: enfrentamientos en los Altos del Golán 2012: Operación Pilar Defensivo 2013-2014: intervención militar en Malí 2014: guerra contra Estado Islámico 2014-2022: guerra civil en el este de Ucrania 2014-2020: segunda guerra civil libia 2014: conflicto entre la Franja de Gaza e Israel 2014-2022: Operación Barkhane 2014-2017: guerra civil iraquí 2014: insurgencia de Boko Haram 2015: intervención militar en Yemen 2016: guerra de los Cuatro Días 2016: guerra contra el narcotráfico en Filipinas 2017: insurgencia islamista en el norte de Mozambique 2018: guerra contra el narcotráfico en Bangladesh 2020: segunda guerra del Alto Karabaj 2020-2022: Guerra de Tigray 2021: conflicto entre la Franja de Gaza e Israel 2021: insurgencia republicana en Afganistán 2022: guerra de Ucrania 2022: guerra contra las pandillas en El Salvador 2023: tercera guerra civil sudanesa 2023: enfrentamientos entre Azerbaiyán y Artsaj 2023: Guerra de Gaza 2023: guerra libanesa-israelí 2024: invasión israelí del Líbano de 2024 2024: invasión israelí de Siria Operaciones basadas en efectos Operaciones basadas en efectos (en inglés aparece frecuentemente con las siglas EBO de Effects-Based Operations). Se trata de una forma de ver las operaciones militares que emplea recursos más allá de lo simplemente militar de tal forma que se maximice la eficiencia y se reduzca al mínimo el esfuerzo erróneo de perseguir objetivos colaterales, algunos autores lo definen como: El resultado físico, funcional o psicológico, así como un evento o consecuencia que se obtiene de una acción específica que puede ser o no militar. Otros como un proceso para obtener un resultado estratégico deseado o un efecto sobre el enemigo a través de la aplicación sinérgica y acumulada de un completo rango de capacidades tanto militares como no-militares a todos los niveles de un conflicto. Las Operaciones Basadas en Efectos se emplean no solo en el momento puntual del periodo bélico sino que van más allá y tratan los momentos de paz, tensión, conflicto y posconflicto. La idea de operaciones con EBO se conciben en sistemas de planificación donde se tiene en cuenta todo el rango de efectos en cascada, tanto directos e indirectos. Algunas de las nuevas ideas de la doctrina militar actual provienen de conceptos de Operaciones basadas en Efectos, se puede decir que otras han sufrido un cierto refinamiento gracias a la introducción de este nuevo concepto. Uno de los conceptos refinados es el de efecto que puede entenderse de dos formas diferentes: El estado físico o de comportamiento que resulta tras una acción dada. Un cambio en la condición comportamiento o grado de libertad. De la misma forma se ha revisado el concepto de enemigo comparado a veces como un sistema de sistemas y su estudio ha dado lugar a conceptos como el Análisis de sistemas de sistemas (en inglés: System-of-Systems Analysis - SoSA). El proceso SoSA incluye en sus inicios categorías muy simples como puede ser un sistema-azul (fuerzas amigas), rojo (fuerzas adversarias) y verdes (neutrales o no-alineadas). La necesidad de poner en funcionamiento Operaciones Basadas en Efectos hizo que esta clasificación se expandiera a nuevas dimensiones Políticas, Militar, Económicas, Social, Infraestructura, Informacional. Todas estas dimensiones se denominan en EBO con sus siglas PMESII. Guerra subsidiaria La guerra subsidiaria es un tipo de guerra que se produce cuando dos o más potencias utilizan a terceros como sustitutos, en vez de enfrentarse directamente. Aunque las superpotencias han utilizado a veces países enteros como subsidiarios, normalmente se prefiere utilizar a guerrillas, mercenarios, grupos terroristas, saboteadores o espías para golpear al oponente indirectamente. El objetivo es dañar, dislocar o debilitar a la otra potencia sin entrar en un conflicto abierto. Con frecuencia, las guerras subsidiarias se libran en el contexto de conflictos violentos o soterrados a gran escala. Rara vez es posible librar una guerra subsidiaria pura, pues los bandos utilizados tienen sus propios intereses, algunos de los cuales divergen de los intereses de los patrones. Entre las guerras que se considera que han tenido un componente de subsidiariedad importante se hallan la guerra civil española, la guerra civil griega, las guerras de Corea, Vietnam o Afganistán, la Guerra Civil del Líbano, la Guerra de Angola, la guerra indo-pakistaní, la guerra de Irak, guerra de Osetia del Sur, la guerra civil de El Salvador, actualmente la guerra en Siria y en general, los conflictos derivados de la Guerra Fría entre las que encontramos las guerras revolucionarias de América Latina impulsadas desde Cuba. Guerra de baja intensidad La guerra de baja intensidad (GBI) es una confrontación político militar entre Estados o grupos, por debajo de la guerra convencional y por encima de la competencia pacífica entre naciones. La GBI involucra a menudo luchas prolongadas de principios e ideologías y se desarrolla a través de una combinación de medios políticos, económicos, de información y militares. Este tipo de confrontación se ubica generalmente en el tercer mundo, pero contiene implicaciones de seguridad regional y global. Varios elementos militares y políticos se combinan para asegurar que la GBI sea la forma más común de confrontación que los ejércitos tendrán que enfrentar en el futuro inmediato. Entre ellas destacan los profundos problemas sociales, económicos y políticos de las naciones del tercer mundo que crean un terreno fértil para el desarrollo de la insurgencia y otros conflictos con un impacto adverso a los intereses de los gobiernos establecidos o las potencias extranjeras. Existen cinco imperativos para llevar a cabo las operaciones de GBI: Dominio político Unidad de acción Adaptabilidad Legitimidad Perseverancia Guerra híbrida Guerra híbrida es una teoría de la estrategia militar en el que se utilizan toda clase de medios y procedimientos ya sea la fuerza convencional o cualquier otro medio irregular como la insurgencia, el terrorismo, la migración, los recursos naturales e incluso otros más sofisticados mediante el empleo de las últimas tecnologías (guerra cibernética) con otros métodos de influencia como las noticias falsas, diplomacia, guerra jurídica e intervención electoral del extranjero y en las que la influencia sobre la población resulta vital. Es un nuevo tipo de guerra que viene a dar por superada la guerra asimétrica (ejército convencional contra fuerza insurgente). Una ventaja de esta estrategia es que el agresor puede evitar que le atribuyan el ataque (una idea en cierto modo similar a la negación plausible). Los conflictos híbridos implican esfuerzos a diferentes niveles con el objetivo de desestabilizar un estado funcional y provocar una polarización de su sociedad. A diferencia de lo que ocurre en la guerra convencional, el “centro de gravedad” de la guerra híbrida es un sector determinado de la población. El enemigo trata de influenciar a los estrategas políticos más destacados y a los principales responsables de la toma de decisiones combinando el uso de la presión con operaciones subversivas. El agresor a menudo recurre a actuaciones clandestinas para no asumir la responsabilidad o las posibles represalias. Guerra de cuarta generación La llamada Guerra de cuarta generación es una denominación dentro de la doctrina militar estadounidense que comprende a la Guerra de guerrillas, la Guerra asimétrica, la Guerra de baja intensidad, la Guerra Sucia, el terrorismo de Estado u operaciones similares y encubiertas, la Guerra popular, la Guerra civil, el Terrorismo y el Contraterrorismo, además de la Propaganda, en combinación con estrategias no convencionales de combate que incluyen la Cibernética, la Población civil y la Política. En este tipo de guerras no hay enfrentamiento entre ejércitos regulares ni necesariamente entre Estados, sino entre un estado y grupos violentos o mayormente entre grupos violentos de naturaleza política, económica, religiosa o étnica. Guerras récord Cementerio militar de la II Guerra Mundial cerca de Colleville-sur-mer en Normandía, Francia. La Segunda Guerra Mundial ha sido una de las más sangrientas de todas, así como la guerra de Vietnam. Niño sufriendo kwashiorkor, en un campo de refugiados nigerianos durante la guerra entre Nigeria y Biafra, década de los 60. La guerra del Golfo Pérsico (1990-1991) supuso la implicación de más de 30 países de todo el mundo Según el Libro Guinness de los Récords los siguientes conflictos están cada uno en un extremo La guerra más breve que se conoce fue la Guerra anglo-zanzibariana que se declaró entre Reino Unido y Zanzíbar el 27 de agosto de 1896, según los registros, duró solo 38 minutos. La guerra más larga habría sido la guerra de los Cien Años que duró 116 años. Otro conflicto bélico también de larga duración fue las Cruzadas, una serie de batallas que duró cerca de 200 años. No obstante, la llamada guerra de Arauco entre españoles e indígenas del Pueblo mapuche, una serie interrumpida de batallas, duró unos 300 años, con largos periodos de tregua. Si se considera como una guerra continua, la guerra de la Reconquista en la península ibérica es la más larga de la historia, con casi 800 años, si no consideramos los frecuentes tratados de paz, alianzas y batallas esporádicas muy localizadas. Eran las típicas guerras de tipo feudal, que ocasionaron la proliferación de castillos defensivos, los cuales le dieron el nombre al Reino de Castilla, aunque también proliferaron en el resto de la península. La guerra más sangrienta por el número de muertos fue la Segunda Guerra Mundial, con sus más de 60 millones de muertos por una u otra causa. Sin embargo, la guerra de la Triple Alianza lo sería en relación con la aniquilación de una población nacional organizada (la población paraguaya), descendiendo los habitantes de Paraguay de 500 000 a 120 000; sobreviviendo solo el 25 % de la población paraguaya, de los cuales solo el 10 % eran hombres. La Segunda Guerra Mundial ostenta el récord de ser la más costosa económicamente. La guerra civil más sangrienta, entendida como la que produjo mayor número de muertos, se produjo en la China de la dinastía Qing y es conocida como Rebelión Taiping (Gran Paz traducido del chino). Se libró entre la citada dinastía Qing y tropas del gobierno Manchú, también chino, desde 1851 a 1864 donde los cálculos más ajustados indican que las muertes pudieron oscilar entre los 20 y los 30 millones de personas, incluidos 100 000 asesinatos por las fuerzas gubernamentales en el saqueo de Nankín, entre el 19 y el 21 de julio de 1864. Las guerras que más continentes, territorio y países abarcaron en todo el mundo fueron la Primera y la Segunda Guerra Mundial. Sin embargo a lo largo de la historia hubo varios conflictos que abarcaron gran cantidad de territorio y países de todo el mundo, esos son: Cruzadas Guerra de los Siete Años Guerra de sucesión española Guerra de sucesión austríaca Guerra de los Treinta Años Guerras napoleónicas Guerras de independencia hispanoamericanas Guerra Grande Guerra Fría: Guerra de Vietnam Guerra de Corea Guerra civil griega Guerra de la frontera de Sudáfrica Guerra civil angoleña Guerra de Afganistán (1978-1992) Guerra de Granada Conflicto árabe-israelí Guerra del Golfo Guerra de Kosovo: Bombardeo de la OTAN sobre Yugoslavia Primavera Árabe (2010-2012): Guerra de Libia de 2011 Guerra civil siria Guerra contra el terrorismo Guerra y violencia sexual La violación (y las graves consecuencias que supone) no ha sido solo excluida tradicionalmente del listado de los horrores de la guerra, sino que tampoco estaba, hasta hace poco, reconocida jurídicamente. Se consideraba un efecto colateral inevitable, no como una transgresión de los derechos humanos, mucho menos como estrategias o herramientas para la guerra. Según afirma George Rodrigue «la base legal para encausar a los responsables de la prostitución forzada y la esclavitud sexual ha existido desde tiempo inmemorial, aunque los procesos no se hayan realizado de manera sólida». La violación no estaba reconocida como crimen de guerra en la Convención de Ginebra de 1949 ni en el juicio de Núremberg de 1946 y este reconocimiento no le llegó hasta los tribunales ad hoc creados para la ex Yugoslavia (1993) y Ruanda (1994), así como en el Estatuto de Roma del Tribunal Penal Internacional (TPI). En aquellos, se define la violación como crimen contra la humanidad en el caso de que estas violaciones sean generalizadas y sistemáticas (la sistematización puede ser utilizada para demostrar la intencionalidad que precisa el crimen de genocidio, mientras que en el TPI específica que, cuando la violación se comete como parte de un ataque contra civiles, puede ser considerada tanto un crimen de guerra como un crimen contra la humanidad. Puede ser esta una característica de las nuevas guerras, el reconociendo de la gravedad de las violaciones a las mujeres. Pero no se trata en absoluto de un fenómeno nuevo, sino una consecuencia de la guerra en Europa (Bosnia y Herzegovina) y la visibilización de sus horrores, entre los que sin duda destacó, como antes en numerosos conflictos armados, la violencia contra las mujeres. ¿Qué se consigue con la violación? A menudo, humillar, a través de la mujer, al colectivo. Con la violación no solo se destruye a la mujer sino también a los familiares que observan o son conscientes de la agresión. Muchas veces las violaciones son públicas, en grupo, en presencia del marido u otros allegados. Sin embargo, aunque los parientes cercanos también sufren las consecuencias, son las mujeres directamente violadas las que soportan en numerosas ocasiones el rechazo de la comunidad, incluso cuando se las pueda reconocer como víctimas y sean objeto de lástima. Afirma Carlos Martín Beristain que «mientras a los hombres y las mujeres que son heridos o asesinados se les considera héroes o mártires, el dolor de la violación se mantiene en silencio o se convierte en un «estigma». La violación es tanto un arma como una expresión de la guerra. En línea con esta afirmación, el Human Security Centre afirma que el riesgo de violencia sexual en contextos de guerra era mayor cuando las normas sobre violencia sexual anteriores al conflicto armado eran más débiles. Pero la violación no es solo instrumento de humillación, sino que también es utilizada para aterrar a las sociedades (en ocasiones para forzar su desplazamiento) o para castigar o controlar. De hecho, cabe no incluir la violación en el ámbito de la sexualidad, sino en el de la tortura. El empleo de la violación sexual como arma de guerra ha estado probado en al menos 13 países entre 2001 y 2004, aunque probablemente la cifra se quede corta. Además de expresión e instrumento, la violación también puede ser una consecuencia, porque se cree que «la guerra exacerba la violencia de género ejercida contra las mujeres en tiempo de paz». Tipos de guerras Lienzo pintado al óleo por Auguste Mayer en 1836. El navío británico HMS Sandwich (a la derecha) dispara al buque francés Redoutable (completamente desarbolado) durante la Batalla de Trafalgar (1805). El Redoutable también combate al HMS Victory (detrás de él) y al HMS Temeraire (en el lado izquierdo de la imagen). En realidad, el HMS Sandwich nunca combatió en Trafalgar; se trata de un error del pintor. Guerra absoluta Guerra acorazada Guerra aérea Guerra ártica Guerra asimétrica Guerra de independencia Guerra civil Guerra biológica o bacteriológica Guerra comercial Guerra convencional Guerra no convencional Guerra de agresión Guerra de baja intensidad Guerra de cuarta generación Guerra de desgaste Guerra de guerrillas Guerra de la información Guerra preventiva Guerra urbana Guerra psicológica Guerra de trincheras Guerra electrónica Guerra en red Guerra financiera Guerra fría Guerra híbrida Guerra irrestricta Guerra justa Guerra mundial Guerra naval Guerra nuclear Guerra química y bacteriológica Guerra relámpago Guerra ritualizada, tribal o endémica Guerra santa Guerra subsidiaria, proxy o por poderes Guerra sucia Guerra terrestre Guerra total Prisioneros de guerra Soldados del Imperio austrohúngaro hechos prisioneros de guerra en Rusia durante la Primera Guerra Mundial; una fotografía en color de 1915 tomada por Serguéi Prokudin-Gorski. Prisioneros de guerra alemanes capturados tras la caída de Aquisgrán, en 1944, durante la Segunda Guerra Mundial. Un prisionero de guerra, también conocido como enemigo prisionero de guerra, es un soldado, piloto o marino que es capturado por el enemigo durante o inmediatamente después de un conflicto armado. Existen leyes para asegurarse de que los prisioneros de guerra serán tratados humana y diplomáticamente. El grado de cumplimiento de tales leyes difiere notablemente entre unas naciones y otras. El artículo 4 del Tercer Convenio de Ginebra protege al personal militar apresado, algunos guerrilleros y ciertos civiles. Esto se aplica desde el momento de la captura hasta cuando es liberado o repatriado. Uno de los principales puntos de la convención condenan la tortura, y al prisionero solo le pueden pedir su nombre, fecha de nacimiento, rango y número de servicio (si es aplicable). En principio, para ser considerado como tal, el prisionero de guerra debe reunir determinadas condiciones: ser parte de un regimiento, vestir un uniforme, banderas e insignias y mostrar sus armas de forma fehaciente. Así, francotiradores, terroristas y espías pueden quedar fuera de esta calificación. En la práctica, esto no siempre se cumple estrictamente. Los miembros de las guerrillas, por ejemplo, pueden no vestir un uniforme o llevar armas abiertamente, pero ahora se les da estatus de prisionero de guerra si son capturados; sin embargo, las guerrillas o cualquier otro combatiente puede que no se le confiera el estatus de PDG si intentan pasar por dos tipos, por civil o por militar."
ksampletext_wikipedia_hist_mesopotamia: str = "Mesopotamia. Mesopotamia es el nombre por el cual se conoce a la región histórica del Oriente Próximo ubicada entre los ríos Tigris y Éufrates, si bien se extiende a las zonas fértiles contiguas a la franja entre ambos ríos, y que coincide aproximadamente con las áreas no desérticas del actual Irak y la zona limítrofe del norte y este de Siria. El término alude inicialmente a esta región durante la Edad Antigua, que se dividía en «Asiria» (al norte) y «Babilonia» (al sur). Babilonia (también conocida como Caldea), a su vez, se dividía en Acadia (parte alta) y Caldea (parte baja). Sus gobernantes eran llamados patesi. Los nombres de ciudades como Ur o Nippur, de héroes legendarios como Gilgameš, del Código Hammurabi, de los asombrosos edificios conocidos como zigurats, provienen de la Mesopotamia Antigua. Y episodios mencionados en la Biblia o en la Torá, como los del diluvio universal o la leyenda de la Torre de Babel, aluden a hechos ocurridos en esta zona. La historia de Mesopotamia está dividida en cinco etapas: periodo sumerio, Imperio acadio, Imperio babilónico, Imperio asirio e Imperio neobabilónico. El sistema social estaba ligado a la economía, por lo que no había castas ni estratificación, solo diferenciación en las posiciones económicas. La economía de Mesopotamia se basaba en la agricultura y la división de tierras de la siguiente forma: Sector estatal o público: propiedad del templo y el palacio, como propiedad del dios, y tenía como objetivo la producción para sustento del templo y el personal (escribas, sacerdotes y administrativos) y eran trabajadas por campesinos influenciados bajo coerción física o ideológica, los cuales eran remunerados con raciones de cebada, lana y aceite para iluminación e higiene en cantidades según edad y sexo. Sector privado: eran tierras de propiedad comunal y privadas, administradas por macrofamilias a cambio de tributo. También podían encontrarse las siguientes distinciones socio-económicas dentro de la población, lo cual estaba sujeto a su nivel de dependencia o independencia económica: Mezquinos: se trataba de ciertos sectores que podían vivir solamente de su trabajo corporal y el cultivo de sus parcelas. Pertenecen a los grupos sociales más débiles debido a que se encuentran socialmente desprotegidos y son súbditos del rey (responsable del templo). Hombres: son ciudadanos con posibilidad de acceder a la tierra. Están ligados a la actividad palaciega, propietarios de parcelas, escribas o funcionarios que han conseguido acumular el capital para la explotación de las tierras. Siervos: se trata de personas que tenían deudas con el palacio y eran siervos voluntarios para su pago. Esclavos: guerreros enemigos cautivos. Etimología Mapa que muestra el sistema fluvial Tigris-Éufrates, que rodea Mesopotamia. El topónimo regional Mesopotamia, griego antiguo: Μεσοποταμια «[la tierra] entre ríos»; árabe: Balad ٱ lrafdyn Bilad ar-Rafidayn o árabe: Internacional ٱ lnhryn AN-Nahrayn Bayn; persa: myanrvdan miyan Rudan; siríaco: ܒܝܬ ܢܗܪܝܢ Beth Nahrain «tierra de los dos ríos») proviene de las antiguas palabras griegas μέσος (mesos) «medio» y ποταμός (potamos) «río» y se traduce como «(tierra) entre ríos». Se utiliza en toda la Septuaginta griega (c. 250 a. C.) para traducir el hebreo y el arameo equivalente Naharaim. Un uso griego anterior del nombre Mesopotamia es evidente en La anabasis de Alejandro, que fue escrita a finales del siglo II d. C., pero se refiere específicamente a las fuentes de la época de Alejandro Magno. En la Anabasis, Mesopotamia se utilizó para designar la tierra al este del Éufrates, en el norte de Siria. El término arameo biritum / birit narim correspondía a un concepto geográfico similar. Más tarde el término Mesopotamia se aplicó de manera más general a todas las tierras entre el Éufrates y el Tigris, incorporando así no solo partes de Siria, sino también casi todo Irak y el sureste de Turquía. Las estepas vecinas al oeste del Éufrates y la parte occidental de las montañas Zagros también se incluyen a menudo bajo el término más amplio de Mesopotamia. Por lo general se hace una distinción adicional entre Mesopotamia norte o superior y Mesopotamia sur o inferior. La Alta Mesopotamia, también conocida como Jazira, es el área entre el Éufrates y el Tigris desde sus fuentes hasta Bagdad, mientras que la Baja Mesopotamia es el área desde Bagdad hasta el Golfo Pérsico e incluye Kuwait y partes del oeste de Irán. En el uso académico moderno, el término Mesopotamia a menudo también tiene una connotación cronológica. Por lo general, se usa para designar el área hasta las conquistas musulmanas, con nombres como Siria, Jazira e Irak para describir la región después de esa fecha. Se ha argumentado que estos eufemismos posteriores son términos eurocéntricos atribuidos a la región en medio de varias invasiones occidentales del siglo XIX. Geografía Artículo principal: Geografía de Mesopotamia Mundo conocido de las culturas mesopotámicas, babilónicas y asirias a partir de fuentes documentales. Mesopotamia abarca la tierra entre los ríos Éufrates y Tigris (en Oriente Próximo), los cuales tienen sus cabeceras en los montes Tauro. Ambos ríos son alimentados por numerosos afluentes y todo el sistema fluvial drena una vasta región montañosa. Las rutas terrestres en Mesopotamia generalmente siguen al Éufrates porque las orillas del Tigris son con frecuencia empinadas y difíciles. El clima de la región es semiárido, con una vasta extensión desértica en el norte que da paso a una región de pantanos, lagunas, marismas y bancos de cañas de 15 000 kilómetros cuadrados (5 800 millas cuadradas) en el sur. En el extremo sur, el Éufrates y el Tigris se unen y desembocan en el Golfo Pérsico. El ambiente árido que abarca desde las áreas del norte de la agricultura de secano hasta el sur, donde el riego de la agricultura es esencial para obtener un excedente de energía en la energía invertida (EROEI). Este riego es ayudado por una capa freática alta y por el deshielo de las altas cumbres de las montañas del norte de los montes Zagros y de las tierras altas armenias, la fuente de los ríos Tigris y Éufrates que dan nombre a la región. La utilidad del riego depende de la capacidad de movilizar mano de obra suficiente para la construcción y mantenimiento de canales, y esto, desde el primer período, ha ayudado al desarrollo de asentamientos urbanos y sistemas centralizados de autoridad política. La agricultura en toda la región se ha complementado con el pastoreo nómada, donde los nómadas que vivían en tiendas de campaña pastorearon ovejas y cabras (y luego camellos) desde los pastizales del río en los meses secos de verano hacia tierras de pastoreo estacionales en la franja del desierto en la estación húmeda de invierno. El área generalmente carece de piedra de construcción, metales preciosos y madera, por lo que históricamente se ha dedicado al comercio de productos agrícolas a larga distancia para obtener estos artículos de las áreas periféricas. En las marismas, al sur del área, ha existido una compleja cultura de pesca desde la prehistoria que se ha agregado a la mezcla cultural. Se han producido interrupciones periódicas en el sistema cultural por varias razones. De vez en cuando la demanda de mano de obra ha llevado a aumentos de población que superan los límites de la capacidad de carga ecológica, y en caso de que se produzca un período de inestabilidad climática, puede colapsar el gobierno central y disminuir las poblaciones. Alternativamente, la vulnerabilidad militar a la invasión de las tribus de las montañas marginales o los pastores nómadas ha llevado a períodos de colapso comercial y abandono de los sistemas de riego. Igualmente, las tendencias centrípetas entre las ciudades-estado han significado que la autoridad central sobre toda la región, cuando se impone, tiende a ser efímera, y el localismo ha fragmentado el poder en unidades tribales o unidades regionales más pequeñas. Estas tendencias han continuado hasta nuestros días en Irak. Historia Artículos principales: Historia de Mesopotamia y Creciente Fértil. Localización aproximada de las culturas Hassuna-Samarra y Halaf durante el «período 6». En el interior de Mesopotamia la agricultura y la ganadería se impusieron entre el 6000 y el 5000 a. C., suponiendo la entrada de lleno al Neolítico. Durante este período, las nuevas técnicas de producción que se habían desarrollado en el área neolítica inicial se expandieron por las regiones de desarrollo más tardío, entre ellas Mesopotamia interior. Este hecho conllevó el desarrollo de las ciudades, siendo algunas de las primeras Bouqras, Umm Dabaghiyah y Yarim y, más tardíamente, Tell es-Sawwan y Choga Mami, que formaron la llamada cultura Umm Dabaghiyah. Posteriormente esta fue sustituida por las culturas de Hassuna-Samarra, entre el 5000 y el 5600 a. C., y por la cultura Halaf entre el 5600 y el 4000 a. C. (Halaf tardío). Aproximadamente en el 3000 a. C., apareció la escritura, en aquella época utilizada solo para llevar las cuentas administrativas de la comunidad. Los primeros escritos que se han hallado están grabados sobre arcilla (muy frecuente en aquella zona) con unos dibujos formados por líneas (pictogramas). La civilización urbana siguió avanzando durante el período de El Obeid (5000 a. C.–3700 a. C.) con avances en las técnicas cerámicas y de regadío y la construcción de los primeros templos urbanos. Tras El Obeid, se sucede el Período de Uruk, en el cual la civilización urbana se asentó definitivamente con enormes avances técnicos como la rueda y el cálculo, realizado mediante anotaciones en tablillas de barro y que evolucionaría hacia las primeras formas de escritura. Sumerios y acadios Los sumerios Artículo principal: Sumeria La sumeria fue la primera civilización mesopotámica. Después del año 3000 a. C. los sumerios crearon en la baja Mesopotamia un conjunto de ciudades-estado: Uruk, Lagaš, Kiš, Uma, Ur, Eridu y Ea cuya economía se basaba en el regadío. En ellas gobernaba un rey absoluto, que se hacía llamar «vicario» del dios protector de la ciudad. Los sumerios fueron los primeros en utilizar la escritura (escritura cuneiforme) y también construyeron grandes templos (zigurats). El período dinástico arcaico Artículo principal: Período dinástico arcaico Situación de las principales ciudades sumerias y alcance de esta cultura durante el período dinástico arcaico. La difusión de los avances de la cultura de Uruk por el resto de Mesopotamia meridional dio lugar al nacimiento de la cultura sumeria. Estas técnicas permitieron la proliferación de las ciudades por nuevos territorios y regiones. Estas ciudades pronto se caracterizaron por la aparición de murallas, lo que parece indicar que las guerras entre ellas fueron frecuentes. También destaca la expansión de la escritura que saltó desde su papel administrativo y técnico hasta las primeras inscripciones dedicatorias en las estatuas consagradas de los templos. Pese a la existencia de las listas reales sumerias la historia de este período es relativamente desconocida, ya que gran parte de los reinados expuestos en ellas tienen fechas imposibles. En realidad estas listas se confeccionaron a partir del siglo XVII a. C., y su creación se debió probablemente al deseo de los monarcas de remontar su linaje hasta tiempos épicos. Algunos de los reyes son probablemente reales pero de muchos otros no hay constancia histórica y otros de los que se sabe su existencia no figuran en ellas. El Imperio acadio Artículo principal: Imperio acadio Mapa de la extensión del Imperio acadio con las conquistas de Sargón y las principales revueltas posteriores. Los 150 años de dominio acadio dejarán un profundo recuerdo en la mentalidad mesopotámica, que, en los siglos posteriores, será la cuna de grandes imperios sucesivos, para cuyos monarcas, Sargón y su nieto, Naram-Sim, se convertirán en los modelos arquetípicos de emperador. Sobre el primero se proyectarán las virtudes a seguir, convirtiéndole en mito; sobre el segundo, el anti-modelo del Imperio agotado en sofocar rebeliones. La prosperidad de los sumerios atrajo a diversos pueblos nómadas. Desde la península arábiga, las tribus semitas (árabes, hebreos y Asirios) invadieron constantemente la región mesopotámica a partir del 2500 a. C., hasta que establecieron su dominio definitivo. Hacia 3000 a. C. se extendieron hacia el norte, creando diferentes grupos como los amorreos, en los que se incluyen fenicios, israelitas y arameos. En Mesopotamia el pueblo semita que adquirió mayor relevancia fueron los acadios. Hacia 2350 a. C., Sargón, un usurpador de origen acadio, se hizo con el poder en la ciudad de Kiš. Fundó una nueva capital, Agadé y conquistó el resto de ciudades sumerias, venciendo al rey de Umma hasta entonces dominante, Lugalzagesi. Este fue el primer gran Imperio de la historia y sería continuado por los sucesores de Sargón, que tendrían que enfrentarse a constantes revueltas. Entre ellos destacó el nieto del conquistador, Naram-Sin. Esta etapa marcó el inicio de la decadencia de la cultura e idioma sumerios en favor de los acadios. El Imperio se deshizo hacia el 2220 a. C., debido a las constantes revueltas y las invasiones de los nómadas gutis y amorreos. Tras su caída, la región entera cayó bajo el dominio de esta tribu, que se impuso sobre las ciudades-estado de la región, especialmente en el entorno de la destruida Agadé. Las crónicas sumerias los describen constantemente de forma negativa, como «horda de bárbaros» o «dragones de montaña», pero es posible que la realidad no fuese tan negativa; en algunos centros se produjo un verdadero florecimiento de las artes, como la ciudad de Lagaš por ejemplo, especialmente durante el gobierno del patesi Gudea. Además de la calidad artística, en las obras de Lagaš se utilizaron materiales provenientes de regiones lejanas: madera de cedro del Líbano o diorita, oro y cornalina del valle del Indo; lo que parece indicar que el comercio no debió verse especialmente lastrado. Las ciudades meridionales, más alejadas del centro de poder guti, compraban su libertad a cambio de importantes tributos; Uruk y Ur prosperaron durante sus IV y II dinastías. Renacimiento sumerio Artículo principal: Renacimiento sumerio Según una tablilla conmemorativa fue Utu-hegal, rey de Uruk, quien, en torno a 2100 a. C., derrotó y expulsó a los gobernantes gutis de las tierras sumerias. Su éxito no le sería de mucho provecho ya que poco después fue vencido por Ur-Nammu, el rey de Ur, que pasó a ser la ciudad hegemónica en toda la región durante el período de la Tercera Dinastía de Ur (también se suele denominar a este período Renacimiento sumerio). El Imperio surgido a raíz de esta hegemonía sería tan extenso o más que el de Sargón, del que tomaría la idea de Imperio unificador, influencia que se aprecia incluso en la denominación de los monarcas, que a imitación de los acadios se harán llamar «reyes de Sumeria y Acad». A Ur-Nammu le sucederá su hijo Shulgi, quien combatió contra el reino oriental de Elam y las tribus nómadas de los Zagros. A este le sucedió su hijo Amar-Sin y a éste, primero un hermano suyo, Shu-Sin y después otro Ibbi-Sin. En el reinado de este último los ataques de los amorreos, provenientes de Arabia, se hicieron especialmente fuertes y en el 2003 a. C. cayó el último Imperio predominantemente sumerio. En adelante será la cultura acadia la que predomine y posteriormente Babilonia heredará el papel de los grandes imperios sumerios. Babilonios y asirios Véase también: Amorita Con la caída de la hegemonía de Ur no se repitió un período de oscuridad como el que había acontecido con la del Imperio acadio. Esta etapa estará marcada por el ascenso progresivo de dinastías amorritas en prácticamente todas las ciudades de la región. Durante los primeros 50 años parece que fue la ciudad de Isin la que trató sin éxito de imponerse en la región. Posteriormente, hacia 1930 a. C. serán los monarcas de Larsa los que se lancen a la conquista de las ciudades vecinas, atacando Elam y las ciudades del Diyala y conquistando Ur, pese a lo cual no consiguieron un dominio completo en la región, aunque conservaron su hegemonía hasta prácticamente el surgimiento del Imperio paleobabilónico de Hammurabi, salvo un período entre 1860 y 1803 a. C. en el que la vecina Uruk consiguió desafiar su liderazgo. En Elam la influencia acadia se hizo más fuerte y el reino pasó a inmiscuirse cada vez más en la política mesopotámica. En Mesopotamia septentrional empezaron a surgir los primeros Estados fuertes, posiblemente reformados por el comercio existente entre las áreas meridionales y Anatolia, destacando principalmente el nuevo reino de Asiria, el cual llegaría a expandirse hasta el Mediterráneo bajo el reinado de Šamši-Adad I. El Imperio paleobabilónico Artículo principal: Imperio paleobabilónico Mapa del Imperio paleobabilónico tras las conquistas de Hammurabi, hacia 1750 a. C. Los nómadas casitas, tal vez originarios del sureste de Irán, ya se habían establecido en los montes Zagros, en el límite oriental del Imperio. No tardarían en avanzar sobre él. En 1792 a. C. Hammurabi llega al trono de la hasta entonces poco importante ciudad de Babilonia, a partir de la cual comenzará una política de expansión. En primer lugar se liberó de la tutela de Ur para, en 1786, enfrentarse al vecino rey de Larsa, Rim-Sin I, arrebatándole Isin y Uruk; con la ayuda de Mari, en 1762 venció a una coalición de ciudades de la ribera del Tigris, para, un año después, conquistar la ciudad de Larsa. Tras esto se autoproclamó rey de Sumeria y Acad, título que había surgido en tiempos de Sargón de Acad, y que se había venido utilizando por los monarcas que conseguían el dominio de toda la región de Mesopotamia. Tras un nuevo enfrentamiento con una nueva coalición de ciudades conquistó Mari, tras lo cual, en 1753, completó su expansión con la anexión de Asiria y Ešnunna, al norte de Mesopotamia. Con el paso de los siglos la imagen del monarca fue mitificada, no solo debido a sus conquistas, sino también a su actividad constructora y de mantenimiento de los canales de riego, y a la elaboración de códigos de leyes, como el conocido código de Hammurabi. Hammurabi murió en 1750 a. C., siendo sucedido por su hijo Samsu-iluna, quien tuvo que enfrentarse a un ataque de los nómadas casitas. Esta situación se repetiría en 1708 a. C., durante el reinado de Abi-Eshuh. En efecto, desde la muerte del conquistador, los problemas con los casitas se habían multiplicado. Esta presión fue constante y en progreso durante el siglo XVII a. C., lo que fue desgastando el Imperio. Fue un ataque del rey hitita, Mursili I, lo que le dio el golpe de gracia a Babilonia, tras lo cual la región cayó bajo el poder de los casitas. Asirios Véase también: Asiria Hacia el 1250 a. C. se establecieron en el norte de Babilonia los asirios, quienes tomaron el control de todo el país. Sus ciudades más importantes fueron Assur y Nínive, y entre sus monarcas más ilustres destacaron: Asurnasirpal II, Tiglath-Pileser III, Asurbanipal, Salmanasar III, Sargón II y Senaquerib. Babilónicos y medos se aliaron y entraron a Asiria desde la meseta de Irán, y finalmente, en el año 612 a. C. tomaron e incendiaron Nínive. Los neobabilónicos Babilonia resurgió con los caldeos, otra tribu semita, cuando fue refundada por su rey Nabopolasar, a finales del siglo VII. Su hijo, Nabucodonosor II «el Grande», fue su sucesor y es considerado uno de los reyes babilónicos más importantes pues sus dominios llegaron desde Mesopotamia hasta Siria y la costa del Mediterráneo. Invasión persa En el año 539 a. C., el rey persa Ciro, el nuevo rey de Asia, ocupó Babilonia y estableció su poder en toda Mesopotamia. Historia arqueológica Los primeros sondeos en la región fueron realizados en 1786 por el vicario general de Bagdag, Joseph de Beauchamps, pero habría que esperar hasta 1842 para la primera excavación arqueológica real, promovida por el cónsul francés en Mosul, Paul Émile Botta, que se centró en el área de tell Kujunjik, cerca de Nínive. Los resultados no fueron interesantes pero, luego de trasladar la excavación por consejo de un aldeano, aparecieron unos bajorrelieves asirios que supusieron el primer hallazgo histórico de las civilizaciones mesopotámicas, de las que, hasta entonces, solo se sabía por las menciones en la Biblia. A partir de este momento la investigación estuvo marcada por la rivalidad entre ingleses y franceses. Los primeros, dirigidos por Austen Henry Layard, descubrieron la importantísima biblioteca de Asurbanipal; los segundos, el palacio de Sargón II en Khorsabad, cuyos hallazgos tuvieron un desgraciado fin al hundirse en el Tigris una embarcación con 235 cajas de material. En el área del sur, en la década de 1850, se descubrieron las ciudades de Uruk, Susa, Ur y Larsa, si bien no fue a partir de 1875 cuando se hallaron evidencias de la civilización sumeria. Hasta los primeros años del siglo XX aparecieron gran cantidad de restos, incluido un gran número de estatuas de Gudea. En esta etapa también comienzan a progresar las excavaciones de alemanes y estadounidenses. Una de las principales características de los yacimientos arqueológicos de la zona es que se han encontrado en gran abundancia textos escritos en cuneiforme, fundamentalmente sobre tablillas de arcilla cruda, que resistieron bien el paso del tiempo, lo que ha permitido conservar algunas de las primeras páginas de la historia de la humanidad. Cultura La cultura de Mesopotamia fue pionera en muchas de las ramas del conocimiento: desarrollaron la escritura que se denominó cuneiforme, en principio pictográfica, y más adelante la fonética; en el campo del derecho, crearon los primeros códigos de leyes; en arquitectura, desarrollaron importantes avances como la bóveda y la cúpula, crearon un calendario de 12 meses y 365 días e inventaron el sistema de numeración sexagesimal. Ciencias Muchos de ellos incursionaron en lo que hoy en día llamamos ciencias o matemáticas, legando también importantes conceptos como la Teoría atómica (Demócrito), diversos teoremas matemáticos (Tales de Mileto, Pitágoras, etc.), medicina (Hipócrates), la teoría de los cuatro humores (Empédocles), etc. Matemáticas La matemática y la ciencia mesopotámicas se basaron en un sistema de numeración sexagesimal (base 60). Esta es la fuente de la hora de 60 minutos, el día de 24 horas y el círculo de 360 grados. El calendario sumerio se basó en la semana de siete días. Esta forma de matemática fue instrumental en la creación temprana de mapas. Los babilonios también tenían teoremas sobre cómo medir el área de varias formas y sólidos. Midieron la circunferencia de un círculo como tres veces el diámetro y el área como una doceava parte del cuadrado de la circunferencia, lo que sería correcto si π se fijara en 3. El volumen de un cilindro se tomó como el producto del área de la base y la altura. El tronco de un cono o una pirámide cuadrada se tomó incorrectamente como el producto de la altura y la mitad de la suma de las bases. Además, hubo un descubrimiento reciente en el que una tableta usaba π como 25/8 (3.125 en lugar de 3.14159 ~). Los babilonios también son conocidos por la milla de Babilonia, que era una medida de distancia igual a unas siete millas modernas (11 km). Esta medida de distancias finalmente se convirtió en una milla de tiempo utilizada para medir el viaje del Sol, por lo tanto, representa el tiempo. Astronomía Desde la época sumeria, los sacerdotes del templo habían intentado asociar eventos actuales con ciertas posiciones de los planetas y las estrellas. Esto continuó hasta la época asiria, cuando las listas de Limmu se crearon como una asociación de eventos año tras año con posiciones planetarias, que, cuando han sobrevivido hasta nuestros días, permiten asociaciones precisas de relación relativa con datación absoluta para establecer la historia de Mesopotamia. Los astrónomos babilónicos eran muy expertos en matemáticas y podían predecir eclipses y solsticios. Los estudiosos pensaban que todo tenía algún propósito en astronomía. La mayoría de estos relacionados con la religión y los presagios. Los astrónomos mesopotámicos elaboraron un calendario de 12 meses basado en los ciclos de la luna. Dividieron el año en dos estaciones: verano e invierno. Los orígenes de la astronomía y la astrología datan de esta época. Durante el siglo VIII y el siglo VII a. C., los astrónomos de Babilonia desarrollaron un nuevo enfoque de la astronomía. Comenzaron a estudiar filosofía sobre la naturaleza ideal del universo primitivo y comenzaron a emplear una lógica interna dentro de sus sistemas planetarios predictivos. Esta fue una contribución importante a la astronomía y la filosofía de la ciencia y algunos estudiosos se han referido a este nuevo enfoque como la primera revolución científica. Este nuevo enfoque de la astronomía fue adoptado y desarrollado en astronomía griega y helenística. En los tiempos seléucida y parta, los informes astronómicos eran completamente científicos; cuánto antes se desarrollaron sus conocimientos y métodos avanzados es incierto. El desarrollo babilónico de métodos para predecir los movimientos de los planetas se considera un episodio importante en la historia de la astronomía. El único astrónomo greco-babilónico conocido que apoyó un modelo heliocéntrico de movimiento planetario fue Seleuco de Seleucia (n. 190 a. C.). Seleuco es conocido por los escritos de Plutarco. Apoyó la teoría heliocéntrica de Aristarco de Samos donde la Tierra giraba alrededor de su propio eje que a su vez giraba alrededor del Sol. Según Plutarco, Seleuco incluso probó el sistema heliocéntrico, pero no se sabe qué argumentos usó (excepto que teorizó correctamente sobre las mareas como resultado de la atracción lunar). La astronomía babilónica sirvió de base para gran parte de la astronomía griega, india clásica, sasánida, bizantina, siria, islámica medieval, asiática central y de Europa occidental. Medicina Los textos babilónicos más antiguos sobre medicina se remontan al período babilónico antiguo durante la primera mitad del segundo milenio antes de Cristo. Sin embargo, el texto médico babilónico más extenso es el Manual de diagnóstico escrito por el ummânū, o erudito principal, Esagil-kin-apli de Borsippa, durante el reinado del rey babilónico Adad-apla-iddina (1069-1046 a. C.) Junto con contemporánea medicina egipcia, los babilonios introdujeron los conceptos de diagnóstico, el pronóstico, el examen físico, los enemas y las recetas. Además, el Manual de diagnóstico introdujo los métodos de terapia y etiología y el uso del empirismo, la lógica y la racionalidad en el diagnóstico, el pronóstico y la terapia. El texto contiene una lista de síntomas médicos y observaciones empíricas a menudo detalladas junto con las reglas lógicas utilizadas para combinar los síntomas observados en el cuerpo de un paciente con su diagnóstico y pronóstico. Los síntomas y enfermedades de un paciente fueron tratados a través de medios terapéuticos como vendajes, cremas y píldoras. Si un paciente no podía curarse físicamente, los médicos de Babilonia a menudo confiaban en el exorcismo para limpiar al paciente de cualquier maldición. El Manual de diagnóstico de Esagil-kin-apli se basó en un conjunto lógico de axiomas y suposiciones, incluida la visión moderna de que a través del examen e inspección de los síntomas de un paciente, es posible determinar la enfermedad del paciente, su etiología, su desarrollo futuro, y las posibilidades de recuperación del paciente. Esagil-kin-apli descubrió una variedad de enfermedades y describió sus síntomas en su Manual de diagnóstico. Estos incluyen los síntomas de muchas variedades de epilepsia y enfermedades relacionadas, junto con su diagnóstico y pronóstico. Literatura Antes del desarrollo de la literatura, el lenguaje escrito se usaba para llevar las cuentas administrativas de la comunidad. Con el tiempo, se le empezó a dar otros usos, como explicar hechos, citas, leyendas o catástrofes. Himno a Iddin-Dagan, rey de Larsa. Inscripciones cuneiformes en arcilla en sumerio. Hacia 1950 a. C. La literatura sumeria comprende tres grandes temas: mitos, himnos y lamentaciones. Los mitos se componen de breves historias que tratan de perfilar la personalidad de los dioses mesopotámicos: Enlil, principal dios y progenitor de las divinidades menores; Inanna, diosa del amor y de la guerra; o Enki, dios del agua dulce, frecuentemente enfrentado a Ninhursag, diosa de las montañas. Los himnos son textos de alabanza a los dioses, reyes, ciudades o templos. Las lamentaciones relatan temas catastróficos como la destrucción de ciudades o palacios y el resultante abandono de los dioses. Algunas de estas historias es posible que se apoyasen en hechos históricos como guerras, inundaciones o la actividad constructora de un rey importante, magnificados y distorsionados con el tiempo. Una creación propia de la literatura sumeria fue un tipo de poemas dialogados basados en la oposición de conceptos contrarios. También los proverbios forman parte importante de los textos sumerios. Religión Artículo principal: Mitología mesopotámica El alivio de Burney, primera dinastía babilónica, alrededor de 1800 a. C. La religión era politeísta; en cada ciudad se adoraba a distintos dioses, aunque había algunos comunes. Entre estos figuran: Anu: dios del cielo y padre de los dioses. Enki: dios de la sabiduría. Tenía la misión de crear al hombre. Nannar: dios de la Luna. Utu: dios del Sol (hacia el 5100 a. C. se llamaba Ninurta). Inanna: diosa del amor y de la guerra; asociada posteriormente a la diosa Venus. Enlil: dios de la agricultura. En el siglo XVII a. C., el rey Hammurabi unificó el Estado, hizo de Babilonia la capital del imperio e impuso como dios principal a Marduk. Este dios fue el encargado de restablecer el orden celeste, de hacer surgir la tierra del mar y de esculpir el cuerpo del primer hombre antes de repartir los dominios del universo entre los demás dioses. Algo que caracterizaba a estos dioses era que estaban asociados a distintas actividades; es decir, existían dioses de la ganadería, escritura, confección, etc., lo que hizo que hubiera un panteón muy amplio Véase: Deidades por atributos. Filosofía Estatuilla de la diosa desnuda de pie, siglo I a. C. - siglo I d. C. Las numerosas civilizaciones del área influenciaron las religiones abrahámicas, especialmente la Biblia hebrea. Sus valores culturales y su influencia literaria son especialmente evidentes en el Libro del Génesis. Giorgio Buccellati cree que los orígenes de la filosofía se remontan a la sabiduría mesopotámica temprana, que encarnaba ciertas filosofías de la vida, particularmente la ética, en forma de dialéctica, diálogos, poesía épica, folclore, himnos, letras, obras en prosa y proverbios. La razón babilónica y la racionalidad se desarrollaron más allá de la observación empírica. La forma más temprana de lógica fue desarrollada por los babilonios, especialmente en la rigurosa naturaleza no ergódica de sus sistemas sociales. El pensamiento babilónico era axiomático y es comparable a la «lógica ordinaria» descrita por John Maynard Keynes. El pensamiento babilónico también se basaba en una ontología de sistemas abiertos que es compatible con los axiomas ergódicos. La lógica se empleó hasta cierto punto en la astronomía y medicina babilónicas. El pensamiento babilónico tuvo una influencia considerable en los principios de la antigua filosofía griega y helenística. En particular, el texto babilónico Diálogo del pesimismo contiene similitudes con el pensamiento agonista de los sofistas, la doctrina heracliteana de la dialéctica y los diálogos de Platón, así como un precursor del método socrático. El filósofo jónico Tales fue influenciado por las ideas cosmológicas de Babilonia. Lenguas El desarrollo temprano de la agricultura en la región pudo haber permitido que numerosos pequeños grupos humanos se expandieran independientemente por la región, causando que la diversidad lingüística de esta fuera inicialmente muy grande. Esta situación contrasta con la que se presenta cuando grupos humanos agrícolas con una tecnología superior penetran en un territorio menos densamente poblado por poblaciones seminómadas, lo que da lugar a una diversidad mucho menor, como lo acontecido en Europa con la entrada de los pueblos indoeuropeos. En Mesopotamia se reconocen dos grandes familias lingüísticas: la indoeuropea (cuya presencia se debe a varias olas, por lo que existen lenguas de diferentes ramas) y la semítica (de la que se testimonian dos ramas). Junto con estas existe un número importante de lenguas aisladas (sumerio, elamita) o cuasiaisladas (hurrita-uratiano), y un número de lenguas mal documentadas cuya filiación no puede precisarse adecuadamente (casita, hatti, kaskas). Muchas de las lenguas aisladas, cuasi-aisladas y no clasificadas parecen tener rasgos ergativos, lo cual las acerca tipológicamente a algunas lenguas caucásicas aunque esto no es prueba de parentesco, ya que dichos rasgos podrían ser muestra de que en el pasado habría existido un área lingüística. Festivales Alabastro con ojos de concha, adorador masculino de Eshnunna, 2750–2600 a. C. Los antiguos mesopotámicos tenían ceremonias cada mes. El tema de los rituales y festivales de cada mes estuvo determinado por al menos seis factores importantes: La fase lunar (una luna creciente significaba abundancia y crecimiento, mientras que una luna menguante se asociaba con el declive, la conservación y los festivales del inframundo) La fase del ciclo agrícola anual Equinoccios y solsticios El mito local y sus patrones divinos El éxito del monarca reinante The Akitu, o Festival de Año Nuevo (primera luna llena después del equinoccio de primavera) Conmemoración de eventos históricos específicos (fundación, victorias militares, festividades del templo, etc.) Música Artículo principal: Música de Mesopotamia Algunas canciones fueron escritas para los dioses, pero muchas fueron escritas para describir eventos importantes. Aunque la música y las canciones divertían a los reyes, también las disfrutaban personas comunes a quienes les gustaba cantar y bailar en sus hogares o en los mercados. Se cantaron canciones a los niños que las transmitieron a sus hijos. Así, las canciones se transmitieron a través de muchas generaciones como una tradición oral hasta que la escritura fue más universal. Estas canciones proporcionaron un medio para transmitir a través de los siglos información muy importante sobre eventos históricos. El Oud (en árabe: العود) es un pequeño instrumento musical de cuerda utilizado por los mesopotámicos. El registro pictórico más antiguo del Oud se remonta al período Uruk en el sur de Mesopotamia hace más de 5000 años. Está en un sello cilíndrico actualmente alojado en el Museo Británico y adquirido por el Dr. Dominique Collon. La imagen muestra a una mujer agachada con sus instrumentos en un bote, jugando con la mano derecha. Este instrumento aparece cientos de veces a lo largo de la historia mesopotámica y nuevamente durante el Egipto antiguo, a partir de la dinastía XVIII, en variedades de cuello largo y corto. El oud es considerado como un precursor de los europeos. laúd. Su nombre se deriva de la palabra árabe العود al-ūd the wood, que es probablemente el nombre del árbol del que se hizo el oud. (El nombre árabe, con el artículo definido, es la fuente de la palabra laúd). Juegos La caza era popular entre los reyes asirios. El boxeo y la lucha aparecen con frecuencia en el arte, y alguna forma de polo era probablemente popular, con hombres sentados sobre los hombros de otros hombres en lugar de sobre caballos. También jugaron majore, un juego similar al rugby deportivo, pero con una pelota de madera. También jugaron un juego de mesa similar al senet y al backgammon, ahora conocido como el «Juego Real de Ur». Vida Familiar El mercado matrimonial babilónico del pintor del siglo XIX Edwin Long Mesopotamia, como lo demuestran los sucesivos códigos legales, los de Urukagina, Lipit Ishtar y Hammurabi, a lo largo de su historia se convirtió cada vez más en una sociedad patriarcal, en la que los hombres eran mucho más poderosos que las mujeres. Por ejemplo, durante el primer período sumerio, el «en», o sumo sacerdote de dioses masculinos era originalmente una mujer, la de las diosas femeninas, un hombre. Thorkild Jacobsen, así como muchos otros, ha sugerido que la sociedad mesopotámica primitiva estaba gobernada por un «consejo de ancianos» en el que hombres y mujeres estaban igualmente representados, pero que con el tiempo, a medida que el estatus de las mujeres disminuía, el de los hombres aumentaba. En cuanto a la escolarización, solo los hijos reales y los hijos de los ricos y profesionales, como los escribas, los médicos y los administradores del templo, asistieron a la escuela. A la mayoría de los niños se les enseñó el oficio de su padre o aprendieron otro distinto, mientras que las niñas tuvieron que quedarse en casa con sus madres para aprender a limpiar, cocinar y cuidar a los niños más pequeños. Algunos niños ayudarían a triturar granos o limpiar pájaros. Inusualmente para ese período histórico, las mujeres de Mesopotamia tenían derechos. Podían poseer propiedades y, si tenían una buena razón, divorciarse. Entierros Cientos de tumbas han sido excavadas en distintas partes de Mesopotamia, revelando información sobre los hábitos de entierro mesopotámico. En la ciudad de Ur, la mayoría de las personas fueron enterradas en tumbas familiares debajo de sus casas, junto con algunas posesiones. Se han encontrado algunos envueltos en esteras y alfombras. Los niños fallecidos fueron puestos en grandes «frascos» que fueron colocados en la capilla de la familia. Se han encontrado otros restos enterrados en cementerios comunes de la ciudad. Se han encontrado 17 tumbas con objetos muy preciosos en ellas. Se supone que se trataba de tumbas reales. Rico de varios períodos, se ha descubierto que han buscado entierro en Baréin, identificado con Dumemun sumerio. Arte Artículo principal: Arte de Mesopotamia En la zona fértil de una y otra llanura, abundantemente regada en su parte inferior por los dos ríos que delimitan esta civilización, se produjo muy pronto la sedentarización de los pueblos nómadas que la atravesaban, convirtiéndose en agricultores y desarrollando una cultura y un arte con una sorprendente variedad de formas y estilos. Con todo, el arte en general mantiene bastante unidad en cuanto a su intencionalidad, que da como resultado un arte algo rígido, geométrico y cerrado, pues, ante todo, tiene una finalidad práctica y no estética y se desarrolla al servicio de la sociedad. Escultura La escultura representa tanto a dioses como a soberanos o funcionarios, pero siempre como personas individualizadas (a veces con su nombre grabado), y busca sustituir a la persona más que representarla. La cabeza y el rostro estaban desproporcionados respecto al cuerpo, por lo que se dice que desarrollaron el llamado realismo conceptual: simplificaban y regularizaban las formas naturales mediante la ley de la frontalidad (parte derecha e izquierda absolutamente simétricas) y el geometrismo (figura dentro de un esquema geométrico que solía ser el cilindro o el cono). Las representaciones humanas mostraban una total indiferencia por la realidad, aunque en los animales se presentaba un mayor realismo. Algunos temas recurrentes de la escultura mesopotámica son toros monumentales, muy estilizados y realistas (genios protectores, monstruosos y fantásticos como todo lo sobrenatural en Mesopotamia). Sus técnicas principales fueron el relieve monumental, la estela, el relieve parietal, el relieve de ladrillos esmaltados y el sello: otras formas de esculpir y desarrollar auténticos cómics o narraciones en ellos. Estatuilla de un hombre barbudo, probablemente un rey-sacerdote, en piedra caliza. Período de Uruk, año 3300 a. C., Museo del Louvre Estatuilla de un hombre barbudo, probablemente un rey-sacerdote, en piedra caliza. Período de Uruk, año 3300 a. C., Museo del Louvre Estela de los buitres. Conmemora la victoria del rey Eannatum de Lagaš sobre Umma durante el período dinástico arcaico, año 2450 a. C., Museo del Louvre Estela de los buitres. Conmemora la victoria del rey Eannatum de Lagaš sobre Umma durante el período dinástico arcaico, año 2450 a. C., Museo del Louvre Estatua del superintendente Ebih II (detalle de la cabeza), 52,5 cm de alto, procedente del templo de Ištar en Mari, período acadio, año 2400 a. C., Museo del Louvre Estatua del superintendente Ebih II (detalle de la cabeza), 52,5 cm de alto, procedente del templo de Ištar en Mari, período acadio, año 2400 a. C., Museo del Louvre Estatua sedente del príncipe Gudea, escultura en diorita, 46 centímetros de alto, excavado en Telloh (antigua Girsu), Irak, período neo-sumerio, año 2120 a. C., Museo del Louvre Estatua sedente del príncipe Gudea, escultura en diorita, 46 centímetros de alto, excavado en Telloh (antigua Girsu), Irak, período neo-sumerio, año 2120 a. C., Museo del Louvre Pintura Debido a las características del país, existen muy pocas muestras de pintura; sin embargo, el arte es muy parecido al arte del período magdaleniense de la prehistoria. La técnica era la misma que en el relieve parietal, sin perspectiva. Al igual que los mosaicos (más perdurables y característicos) tenía un fin más decorativo que las otras facetas del arte. En la pintura y el grabado, la jerarquía se mostraba de acuerdo al tamaño de las personas representadas en la obra: los de más alto rango se mostraban más grandes en comparación con el resto. La pintura fue estrictamente decorativa, pues se utilizó para embellecer la arquitectura. Carece de perspectiva, y es cromáticamente pobre: solo prevalecen el blanco, el azul y el rojo. Se usaba la técnica del temple, que se puede apreciar en los mosaicos decorativos o azulejos. La pintura se empleaba en la decoración doméstica. Los temas eran escenas de guerras y de sacrificios rituales con mucho realismo, y se representaban figuras geométricas, personas, animales y monstruos, sin representar las sombras. Arquitectura Artículo principal: Arquitectura de Mesopotamia Los mesopotámicos tenían una arquitectura muy particular debido a los recursos disponibles. Hicieron uso de los dos sistemas constructivos básicos: el abovedado y el adintelado. Construyeron mosaicos pintados en colores vivos, como el negro, verdes o bicolores, a manera de murales. Los edificios no tenían ventanas y la luz se obtenía del techo. Se preocupaban de la vida terrenal y no de la de los muertos, por tanto las edificaciones más representativas eran el templo y el palacio. El templo era el centro religioso, económico y político. Tenía tierras de cultivo y rebaños, almacenes (donde se guardaban las cosechas) y talleres (donde se hacían utensilios, estatuas de cobre y de cerámica). Los sacerdotes organizaban el comercio y empleaban a campesinos, pastores y artesanos, quienes recibían como pago parcelas de tierra para cultivo de cereales, dátiles o lana. Además, los zigurats tenían un amplio patio con habitaciones para alojar a las personas que habitaban en este pueblo. El urbanismo regulado estuvo presente en algunas ciudades, como la Babilonia de Nabucodonosor III, mayoritariamente con diseño en damero. En cuanto a las obras de ingeniería, destaca la muy extensa y antigua red de canales que unían los ríos Tigris, Éufrates y sus afluentes, propiciando la agricultura y la navegación. Tecnología El desarrollo de la tecnología en Mesopotamia estuvo condicionado en muchos aspectos a los avances en el dominio del fuego, conseguidos mediante la mejora de la capacidad térmica de los hornos, con los cuales era posible conseguir yeso (a partir de los 300 °C), y cal (a partir de los 800 °C). Con estos materiales se podían recubrir recipientes de madera lo que permitía ponerlos al fuego directo, una técnica predecesora de la cerámica a la que se ha llamado «vajilla blanca». Los inicios de esa técnica se han encontrado en Beidha, al sur de Canaán, y datan del IX milenio a. C. aproximadamente; desde los milenios posteriores se extiende hacia el norte y al resto del Próximo Oriente, cubriéndolo por completo entre 5600 y 3600 a. C. Cerámica En Mesopotamia, la cerámica comienza a desarrollarse ya empezado el Neolítico, por lo que se habla de un Neolítico Precerámico. Tras este, se da un período en el que la cerámica aparece de forma intermitente en los restos. Esto es debido, más que a una serie de descubrimientos y olvidos, a que la «vajilla blanca» era aún suficiente para la mayor parte de las aplicaciones. Hacia el IV milenio a. C. la cerámica alcanzó un desarrollo pleno, con hornos donde el fuego y la cámara de cocción estaban bien diferenciados. A partir de aquí y con el dominio de temperaturas aun superiores, surgió una nueva técnica: la vitrificación de la pasta. Hacia el III milenio a. C., durante el período Jemdet Nasr, se conseguía fabricar perlas de vidrio y un milenio después ya se dominaba la técnica del vidriado. Finalmente, durante el II milenio a. C., se logró la fabricación de objetos de vidrio. Metalurgia Regiones productoras de metales utilizados en Mesopotamia. Mesopotamia carecía de yacimientos de metales propios, por lo que podía haberse visto en desventaja frente a las vecinas regiones montañosas; no fue así, ya que en esas zonas el desarrolló político era muy inferior al mesopotámico, y no se creó ningún Estado poderoso a expensas de esta riqueza. A la larga, fueron los habitantes de Mesopotamia los que, mediante el comercio, controlaron este bien. La utilización de pequeños objetos metálicos tallados había sido una constante en la región desde el VI milenio a. C., sin embargo no fue hasta el desarrollo de hornos más potentes cuando se generalizó el uso de estos materiales mediante la aparición de la metalurgia. Este cambio puede situarse a mediados el III milenio a. C.; empieza a encontrarse mayor cantidad de objetos metálicos; por su composición, se aprecia que estos objetos son obtenidos mediante fundición, no por el tallado de metales en estado natural y se empieza a experimentar con aleaciones. Con el desarrollo de las aleaciones se produjo el nacimiento de la metalurgia del bronce, que se diferenció en dos vertientes según los metales con los que se obtenía la aleación, bien fuesen cobre y estaño o cobre y arsénico. El bronce arsenioso se desarrolló en las áreas del Cáucaso, este de Anatolia, sur de Mesopotamia y Levante mediterráneo, trazando un eje norte sur. El bronce de estaño predomina en Irán, toda Mesopotamia, el norte de Siria y en Cilicia, trazando un eje este-oeste. El punto de cruce de estos dos ejes es el sur de Mesopotamia, esto es, la cuna de la civilización sumeria. Esta situación se mantiene durante los milenios IV y III a. C., hasta que en el segundo el bronce arsenioso desaparece. Entre el 1200 y el 1000 a. C. se produce un nuevo avance: el hierro, que hasta entonces había sido escaso hasta el punto de costar igual que el oro, se populariza debido probablemente al descubrimiento de nuevas técnicas, conseguidas en el área del norte de Siria o en la tierra de los hititas. Gobierno La geografía de Mesopotamia tuvo un profundo impacto en el desarrollo político de la región. Entre los ríos y arroyos, el pueblo sumerio construyó las primeras ciudades junto con canales de riego que estaban separados por vastas extensiones de desierto abierto o pantano donde vagaban tribus nómadas. La comunicación entre las ciudades aisladas era difícil y, a veces, peligrosa. Así, cada ciudad sumeria se convirtió en una ciudad-Estado, independiente de las demás y protectora de su independencia. A veces una ciudad intentaba conquistar y unificar la región, pero tales esfuerzos fueron resistidos y fracasaron durante siglos. Como resultado, la historia política de Sumeria es una de guerra casi constante. Finalmente, Sumer fue unificado por Eannatum, pero la unificación fue tenue y no duró ya que los acadios conquistaron Sumeria en 2331 a. C. solo una generación después. El Imperio acadio fue el primer imperio exitoso que duró más de una generación y vio la sucesión pacífica de reyes. El imperio fue relativamente efímero, ya que los babilonios los conquistaron en unas pocas generaciones. Reyes Artículos principales: Lista de reyes sumerios y Lista de reyes de Babilonia Los mesopotámicos creían que sus reyes y reinas descendían de la Ciudad de los Dioses, pero a diferencia de los egipcios antiguos, nunca creyeron que sus reyes fueran dioses reales. La mayoría de los reyes se llamaban a sí mismos «rey del universo» o «gran rey». Otro nombre común era «pastor», ya que los reyes tenían que cuidar a su gente. Poder Cuando Asiria se convirtió en un imperio, se dividió en partes más pequeñas, llamadas provincias. Cada uno de estos lleva el nombre de sus principales ciudades, como Nínive, Samaria, Damasco y Arpad. Todos tenían su propio gobernador que tenía que asegurarse de que todos pagaran sus impuestos. Los gobernadores también tuvieron que convocar soldados para la guerra y suministrar trabajadores cuando se construyó un templo. También fue responsable de hacer cumplir las leyes. De esta manera, era más fácil mantener el control de un gran imperio. Aunque Babilonia era un Estado bastante pequeño en el sumerio, creció enormemente durante el gobierno de Hammurabi. Era conocido como «el legislador», y pronto Babilonia se convirtió en una de las principales ciudades de Mesopotamia. Más tarde se llamó Babilonia, que significaba «la puerta de entrada de los dioses». También se convirtió en uno de los mayores centros de aprendizaje de la historia. Guerra Fragmento de la estela de los buitres que muestra guerreros marchando, período dinástico temprano III, 2600–2350 a. C. Con el final de la fase Uruk, las ciudades amuralladas crecieron y muchas aldeas Ubaid aisladas fueron abandonadas, lo que indica un aumento de la violencia comunitaria. Se suponía que uno de los primeros reyes de Lugalbanda había construido los muros blancos alrededor de la ciudad. A medida que las ciudades-estado comenzaron a crecer, sus esferas de influencia se superpusieron, creando discusiones entre otras ciudades-estado, especialmente sobre la tierra y los canales. Estos argumentos se registraron en tabletas varios cientos de años antes de cualquier guerra importante: la primera grabación de una guerra ocurrió alrededor del 3200 a. C., pero no fue común hasta aproximadamente el 2500 a. C. Un rey dinástico temprano II (Ensi) de Uruk en Sumer, Gilgamesh (c. 2600 a. C.), fue elogiado por las hazañas militares contra Humbaba, guardián de la montaña del cedro, y más tarde se celebró en muchos poemas y canciones posteriores en los que se afirmaba que era un dios de dos tercios y solo un tercio humano. La estela de los buitres posterior al final del período dinástico temprano III (2600–2350 a. C.), que conmemora la victoria de Eannatum de Lagash sobre la vecina ciudad rival de Umma, es el monumento más antiguo del mundo que celebra una masacre. A partir de este momento, la guerra se incorporó al sistema político mesopotámico. A veces, una ciudad neutral puede actuar como árbitro para las dos ciudades rivales. Esto ayudó a formar sindicatos entre ciudades, lo que llevó a los Estados regionales. Cuando se crearon los imperios, fueron a la guerra más con países extranjeros. El rey Sargón, por ejemplo, conquistó todas las ciudades de Sumer, algunas ciudades en Mari, y luego fue a la guerra con el norte de Siria. Muchas paredes del palacio asirio y babilónico estaban decoradas con las imágenes de las luchas exitosas y el enemigo escapaba desesperadamente o se escondía entre los juncos. Leyes Una de las dos figuras del carnero en un matorral encontradas en el cementerio real de Ur, 2600–2400 a. C. Las ciudades-Estado de Mesopotamia crearon los primeros códigos legales, extraídos de la precedencia legal y las decisiones tomadas por los reyes. Se han encontrado los códigos de Urukagina y Lipit Ishtar. El más famoso de estos fue el de Hammurabi (creado hacia 1780 A. C.), debido a su conjunto de leyes, siendo uno de los primeros conjuntos de leyes encontrados y uno de los ejemplos mejor conservados de este tipo de documento de la antigua Mesopotamia. Codificó más de 200 leyes para Mesopotamia. El examen de las leyes muestra un debilitamiento progresivo de los derechos de la mujer y una gravedad cada vez mayor en el tratamiento de esclavos. Avances tecnológicos Algunas de las creaciones que les debemos a las civilizaciones que habitaron en Mesopotamia son: La escritura (escritura cuneiforme). La moneda. La rueda. Las primeras nociones de astrología y astronomía. El desarrollo del sistema sexagesimal y el primer código de leyes, escrito por el rey Hammurabi. El sistema postal o de correo. La irrigación artificial. El arado. El bote y la vela. Los arreos para los animales. La metalurgia del cobre y del bronce. Un calendario de 12 meses y 360 días."
ksampletext_wikipedia_hist_edadmedia: str = "Edad Media. La Edad Media o el Medievo es el período histórico de la civilización occidental comprendido entre los siglos v y xv, sucesor de la Edad Antigua y predecesor de la Edad Moderna. Convencionalmente, su inicio se sitúa en el año 476 con la caída del Imperio romano de Occidente y su fin en 1492 con el descubrimiento de América, o en 1453 con la caída de Constantinopla, fecha que tiene la singularidad de coincidir con la invención de la imprenta ,publicación de la Biblia de Gutenberg, y con el fin de la guerra de los Cien Años entre Francia e Inglaterra. Con esto dicho, considerando la caída del Imperio romano de Occidente hasta el descubrimiento de América, la Edad Media abarcó un periodo de 1016 años. No hay una fecha de finalización universalmente aceptada. Dependiendo del contexto, a veces se utilizan eventos como la conquista de Constantinopla por los turcos en 1453, el primer viaje de Cristóbal Colón a las Américas en 1492 o la Reforma en 1517. Al día de hoy, los historiadores del período prefieren matizar esta ruptura entre Edad Antigua y Edad Media, de manera que entre los siglos iii y viii se suele hablar de Antigüedad Tardía, que habría sido una gran etapa de transición en todos los ámbitos: en lo económico, el modo de producción esclavista da paso al modo de producción feudal; en lo social, el concepto de ciudadanía romana da paso a los estamentos medievales; en lo político, las estructuras centralizadas del Imperio romano dan paso a una dispersión del poder; y en lo ideológico y cultural, la cultura clásica da paso a las teocéntricas culturas cristiana o islámica (cada una en su espacio). La Edad Media suele dividirse en dos grandes períodos: Temprana o Alta Edad Media (ss. v-x, sin una clara diferenciación con la Antigüedad Tardía); y Baja Edad Media (ss. xi-xv). Esta última puede dividirse a su vez en un periodo de plenitud, la Plena Edad Media (ss. xi-xiii), y los dos últimos siglos que presenciaron la crisis del siglo xiv. Aunque hay algunos ejemplos de utilización previa,[Nota 1] el concepto de Edad Media nació como la segunda edad de la división tradicional del tiempo histórico debida a Cristóbal Cellarius (Historia Medii Aevi a temporibus Constantini Magni ad Constaninopolim a Turcis captam deducta, Jena, 1688) quien la consideraba un tiempo intermedio, sin apenas valor por sí mismo, entre la Edad Antigua identificada con el arte y la cultura de la civilización grecorromana de la Antigüedad clásica y la renovación cultural de la Edad Moderna ,en la que él se sitúa, que comienza con el Renacimiento y el Humanismo renacentista. La popularización de este esquema ha perpetuado un preconcepto erróneo: el de considerar a la Edad Media como una época de retroceso intelectual y cultural, y un aletargamiento social y económico secular (que a su vez se asocia con el feudalismo en sus rasgos más oscurantistas, tal como se definió por los revolucionarios que combatieron el Antiguo Régimen). Sería un periodo dominado por el aislamiento, la ignorancia, la teocracia, la superstición y el miedo milenarista alimentado por la inseguridad endémica, la violencia y la brutalidad de guerras e invasiones constantes y epidemias apocalípticas.[Nota 2] Sin embargo, en este largo período de mil años hubo todo tipo de hechos y procesos muy diferentes entre sí, diferenciados temporal y geográficamente, respondiendo tanto a influencias mutuas con otras civilizaciones y espacios como a dinámicas internas. Muchos de ellos tuvieron una gran proyección hacia el futuro, entre otros los que sentaron las bases del desarrollo de la posterior expansión europea, y el desarrollo de los agentes sociales que desarrollaron una sociedad estamental de base predominantemente rural pero que presenció el nacimiento de una incipiente vida urbana y una burguesía que con el tiempo desarrollarán el capitalismo. Lejos de ser una época inmovilista, la Edad Media, que había comenzado con migraciones de pueblos enteros, y continuado con grandes procesos repobladores (Repoblación en la península ibérica, Ostsiedlung en Europa Oriental) vio cómo en sus últimos siglos los antiguos caminos (muchos de ellos vías romanas decaídas) se reparaban y modernizaban con airosos puentes, y se llenaban de toda clase de viajeros (guerreros, peregrinos, mercaderes, estudiantes, goliardos, etc.) encarnando la metáfora espiritual de la vida como un viaje (homo viator). También surgieron en la Edad Media formas políticas nuevas, que van desde el califato islámico a los poderes universales de la cristiandad latina (Papado e Imperio) o el Imperio bizantino y los reinos eslavos integrados en la cristiandad oriental (aculturación y evangelización de Cirilo y Metodio); y en menor escala, todo tipo de ciudades estado, desde las pequeñas ciudades episcopales alemanas hasta repúblicas que mantuvieron imperios marítimos como Venecia; dejando en la mitad de la escala a la que tuvo mayor proyección futura: las monarquías feudales, que transformadas en monarquías autoritarias prefiguran el estado moderno. De hecho, todos los conceptos asociados a lo que se ha venido en llamar modernidad aparecen en la Edad Media, en sus aspectos intelectuales con la misma crisis de la escolástica. Ninguno de ellos sería entendible sin el propio feudalismo, se entienda este como modo de producción (basado en las relaciones sociales de producción en torno a la tierra del feudo) o como sistema político (basado en las relaciones personales de poder en torno a la institución del vasallaje), según las distintas interpretaciones historiográficas.[Nota 3] El choque de civilizaciones entre cristianismo e islamismo, manifestado en la ruptura de la unidad del Mediterráneo (hito fundamental de la época, según Henri Pirenne, en su clásico Mahoma y Carlomagno), la Reconquista española y las Cruzadas; tuvo también su parte de fértil intercambio cultural (escuela de Traductores de Toledo, Escuela Médica Salernitana) que amplió los horizontes intelectuales de Europa, hasta entonces limitada a los restos de la cultura clásica salvados por el monacato altomedieval y adaptados al cristianismo. La Edad Media realizó una curiosa combinación entre la diversidad y la unidad. La diversidad fue el nacimiento de las incipientes naciones... La unidad, o una determinada unidad, procedía de la religión cristiana, que se impuso en todas partes... esta religión reconocía la distinción entre clérigos y laicos, de manera que se puede decir que... señaló el nacimiento de una sociedad laica. ... Todo esto significa que la Edad Media fue el período en que apareció y se construyó Europa. Esa misma Europa Occidental produjo un arte medieval dinámico que mostraría una impresionante sucesión de estilos artísticos (prerrománico, románico y gótico), que en las zonas fronterizas se mestizaron también con el arte islámico (mudéjar, arte andalusí, arte árabe-normando) o con el arte bizantino. La ciencia medieval no respondía a una metodología moderna, pero tampoco lo había hecho la de los autores clásicos, que se ocuparon de la naturaleza desde su propia perspectiva; y en ambas edades sin conexión con el mundo de las técnicas, que estaba relegado al trabajo manual de artesanos y campesinos, responsables de un lento pero constante progreso en las herramientas y procesos productivos. La diferenciación entre oficios viles y mecánicos y profesiones liberales vinculadas al estudio intelectual convivió con una teórica puesta en valor espiritual del trabajo en el entorno de los monasterios benedictinos, cuestión que no pasó de ser un ejercicio piadoso, sobrepasado por la mucho más trascendente valoración de la pobreza, determinada por la estructura económica y social y que se expresó en el pensamiento económico medieval. Es impropio hablar de Edad Media en otras civilizaciones Mapa TO, con Jerusalén en el centro, y las tres partes simplificadas del mundo recordado, más que conocido en la Edad Media. Las grandes migraciones de la época de las invasiones significaron paradójicamente un cierre al contacto de Occidente con el resto del mundo. Muy pocas noticias tenían los europeos del milenio medieval (tanto los de la cristiandad latina como los de la cristiandad oriental) de que, aparte de la civilización islámica, que ejerció de puente pero también de obstáculo entre Europa y el resto del Viejo Mundo, se desarrollaban otras civilizaciones. Incluso un vasto reino cristiano como el de Etiopía, al quedar aislado, se convirtió en el imaginario cultural en el mítico reino del Preste Juan, apenas distinguible de las islas atlánticas de San Brandán y del resto de las maravillas dibujadas en los bestiarios y los escasos, rudimentarios e imaginativos mapas. El desarrollo marcadamente autónomo de China, la más desarrollada civilización de la época (aunque volcada hacia su propio interior y ensimismada en sus ciclos dinásticos: Sui, Tang, Song, Yuan y Ming), y la escasez de contactos con ella (el viaje de Marco Polo, o la mucho más importante expedición de Zheng He), que destacan justamente por lo inusuales y por su ausencia de continuidad, no permiten denominar a los siglos V al XV de su historia como historia medieval, aunque a veces se haga, incluso en publicaciones especializadas, más o menos impropiamente. La historia de Japón (que durante este periodo estaba en formación como civilización, adaptando las influencias chinas a la cultura autóctona y expandiéndose desde las islas meridionales a las septentrionales), a pesar de su mayor lejanía y aislamiento, suele ser paradójicamente más asociada al término medieval; aunque tal denominación es acotada por la historiografía, significativamente, a un periodo medieval que se localiza entre los años 1000 y 1868, para adecuarse al denominado feudalismo japonés anterior a la era Meiji (véase también shogunato, han y castillo japonés). La historia de la India o la del África negra a partir del siglo VII contaron con una mayor o menor influencia musulmana, pero se atuvieron a dinámicas propias bien diferentes (Sultanato de Delhi, Sultanato de Bahmani, Imperio Vijayanagara ,en la India,, Imperio de Malí, Imperio Songhay ,en África negra,). Incluso llegó a producirse una destacada intervención sahariana en el mundo mediterráneo occidental: el Imperio almorávide. De un modo todavía más claro, la historia de América (que atravesaba sus periodos clásico y postclásico) no tuvo ningún tipo de contacto con el Viejo Mundo, más allá de la llegada de la denominada Colonización vikinga en América que se limitó a una reducida y efímera presencia en Groenlandia y la enigmática Vinland, o las posibles posteriores expediciones de balleneros vascos en parecidas zonas del Atlántico Norte, aunque este hecho ha de entenderse en el contexto del gran desarrollo de la navegación de los últimos siglos de la Baja Edad Media, ya encaminada a la Era de los Descubrimientos. Lo que sí ocurrió, y puede considerarse como una constante del periodo medieval, fue la periódica repetición de puntuales interferencias centroasiáticas en Europa y el Próximo Oriente en forma de invasiones de pueblos del Asia Central, destacadamente los turcos (köktürks, jázaros, otomanos) y los mongoles (unificados por Gengis Kan) y cuya Horda de Oro estuvo presente en Europa Oriental y conformó la personalidad de los Estados cristianos que se crearon, a veces vasallos y a veces resistentes, en las estepas rusas y ucranianas. Incluso en una rara ocasión, la primitiva diplomacia de los reinos europeos bajomedievales vio la posibilidad de utilizar a los segundos como contrapeso a los primeros: la frustrada embajada de Ruy González de Clavijo a la corte de Tamerlán en Samarcanda, en el contexto del asedio mongol de Damasco, un momento muy delicado (1401-1406) en el que también intervino como diplomático Ibn Jaldún. Los mongoles ya habían saqueado Bagdad en una incursión de 1258. El inicio de la Edad Media Artículo principal: Antigüedad tardía Sueño de Constantino antes de la batalla del Puente Milvio. In hoc signo vinces (Con este signo vencerás). Ilustración de las Homilías de san Gregorio Nacianceno, siglo IX El papa Silvestre I bendice a Constantino, del que recibe con la tiara (símbolo del pontificado romano clásico, similar a otros tocados político-religiosos, como la doble corona de los faraones) el poder temporal sobre Roma. Fresco del siglo XIII, capilla de San Silvestre, monasterio de los Cuatro Santos Coronados. Encuentro de León Magno con Atila, fresco de Rafael Sanzio en las estancias del Vaticano (1514). Aunque se han propuesto varias fechas para el inicio de la Edad Media, de las cuales la más extendida es la del año 476, lo cierto es que no podemos ubicar el inicio de una manera tan exacta ya que la Edad Media no nace, sino que se hace a consecuencia de todo un largo y lento proceso que se extiende por espacio de cinco siglos y que provoca cambios enormes a todos los niveles de una forma muy profunda que incluso repercutirán hasta nuestros días. Podemos considerar que ese proceso empieza con la crisis del siglo III, vinculada a los problemas de reproducción inherentes al modo de producción esclavista, que necesitaba una expansión imperial continua que ya no se producía tras la fijación del limes romano. Posiblemente también confluyeran factores climáticos para la sucesión de malas cosechas y epidemias; y de un modo mucho más evidente las primeras invasiones germánicas y sublevaciones campesinas (bagaudas), en un periodo en que se suceden muchos breves y trágicos mandatos imperiales. Desde Caracalla la ciudadanía romana estaba extendida a todos los hombres libres del Imperio, muestra de que tal condición, antes tan codiciada, había dejado de ser atractiva. El Bajo Imperio adquiere un aspecto cada vez más medieval desde principios del siglo IV con las reformas de Diocleciano: difuminación de las diferencias entre los esclavos, cada vez más escasos, y los colonos, campesinos libres, pero sujetos a condiciones cada vez mayores de servidumbre, que pierden la libertad de cambiar de domicilio, teniendo que trabajar siempre la misma tierra; herencia obligatoria de cargos públicos ,antes disputados en reñidas elecciones, y oficios artesanales, sometidos a colegiación ,precedente de los gremios,, todo para evitar la evasión fiscal y la despoblación de las ciudades, cuyo papel de centro de consumo y de comercio y de articulación de las zonas rurales cada vez es menos importante. Al menos, las reformas consiguen mantener el edificio institucional romano, aunque no sin intensificar la ruralización y aristocratización (pasos claros hacia el feudalismo), sobre todo en Occidente, que queda desvinculado de Oriente con la partición del Imperio. Otro cambio decisivo fue la implantación del cristianismo como nueva religión oficial por el Edicto de Tesalónica de Teodosio I el Grande (380) precedido por el Edicto de Milán (313) con el que Constantino I el Grande recompensó a los hasta entonces subversivos por su providencialista ayuda en la batalla del Puente Milvio (312), junto con otras presuntas cesiones más temporales cuya fraudulenta reclamación (pseudodonación de Constantino) fue una constante de los Estados Pontificios durante toda la Edad Media, incluso tras la evidencia de su refutación por el humanista Lorenzo Valla (1440). División del Imperio romano, año 395. Ningún evento concreto ,a pesar de la abundancia y concatenación de hechos catastróficos, determinó por sí mismo el fin de la Edad Antigua y el inicio de la Edad Media: ni los sucesivos saqueos de Roma (por los godos de Alarico I en el 410, por los vándalos en el 455, por las propias tropas imperiales de Ricimero en 472, por los ostrogodos en 546), ni la pavorosa irrupción de los hunos de Atila (450-452, con la batalla de los Campos Cataláunicos y la extraña entrevista con el papa León I el Magno), ni el derrocamiento de Rómulo Augústulo (último emperador romano de Occidente, por Odoacro el jefe de los hérulos -476-); fueron sucesos que sus contemporáneos consideraran iniciadores de una nueva época. La culminación a finales del siglo V de una serie de procesos de larga duración, entre ellos la grave dislocación económica, las invasiones y el asentamiento de los pueblos germanos en el Imperio romano, hizo cambiar la faz de Europa. Durante los siguientes 300 años, la Europa Occidental mantuvo un período de unidad cultural, inusual para este continente, instalada sobre la compleja y elaborada cultura del Imperio romano, que nunca llegó a perderse por completo, y el asentamiento del cristianismo. Nunca llegó a olvidarse la herencia clásica grecorromana, y la lengua latina, sometida a transformación (latín medieval), continuó siendo la lengua de cultura en toda Europa occidental, incluso más allá de la Edad Media. El derecho romano y múltiples instituciones continuaron vivas, adaptándose de uno u otro modo. Lo que se operó durante ese amplio periodo de transición (que puede darse por culminado para el año 800, con la coronación de Carlomagno) fue una suerte de fusión con las aportaciones de otras civilizaciones y formaciones sociales, en especial la germánica y la religión cristiana. En los siglos siguientes, aún en la Alta Edad Media, serán otras aportaciones las que se añadan, destacadamente el islam. Véanse también: Caída del Imperio romano de Occidente, Invasiones bárbaras y Pueblos germánicos. Alta Edad Media (siglos V al X) Artículo principal: Alta Edad Media Los reinos germanorromanos (siglos V al VIII) Artículo principal: Reinos germánicos Bárbaros Los bárbaros se desparraman furiosos... y el azote de la peste no causa menos estragos, el tiránico exactor roba y el soldado saquea las riquezas y las vituallas escondidas en las ciudades; reina un hambre tan espantosa, que obligado por ella, el género humano devora carne humana, y hasta las madres matan a sus hijos y cuecen sus cuerpos para alimentarse con ellos. Las fieras aficionadas a los cadáveres de los muertos por la espada, por el hambre y por la peste, destrozan hasta a los hombres más fuertes, y cebándose en sus miembros, se encarnizan cada vez más para destrucción del género humano. De esta suerte, exacerbadas en todo el orbe las cuatro plagas: el hierro, el hambre, la peste y las fieras, cúmplense las predicciones que hizo el Señor por boca de sus Profetas. Asoladas las provincias... por el referido encruelecimiento de las plagas, los bárbaros, resueltos por la misericordia del Señor a hacer la paz, se reparten a suertes las regiones de las provincias para establecerse en ellas. Hidacio, Chronicon (hacia 468). El texto se refiere concretamente a Hispania y sus provincias, y los bárbaros citados son específicamente los suevos, vándalos y alanos, que en el 406 habían cruzado el limes del Rin (inhabitualmente helado) a la altura de Maguncia y en torno al 409 habían llegado a la península ibérica; pero la imagen es equivalente en otros momentos y lugares que el mismo autor narra, del periodo entre 379 y 468. Los pueblos germánicos procedentes de la Europa del Norte y del Este, se encontraban en un estadio de desarrollo económico, social y cultural obviamente inferior al del Imperio romano, al que ellos mismos percibían admirativamente. A su vez eran percibidos con una mezcla de desprecio, temor y esperanza (retrospectivamente plasmados en el influyente poema Esperando a los bárbaros de Constantino Cavafis), e incluso se les atribuyó un papel justiciero (aunque involuntario) desde un punto de vista providencialista por parte de los autores cristianos romanos (Orosio, Salviano de Marsella y San Agustín de Hipona). La denominación de bárbaros (βάρβαρος) proviene de la onomatopeya bar-bar con la que los griegos se burlaban de los extranjeros no helénicos, y que los romanos ,bárbaros ellos mismos, aunque helenizados, utilizaron desde su propia perspectiva. La denominación «invasiones bárbaras» fue rechazada por los historiadores alemanes del siglo XIX, momento en el que el término barbarie designaba para las nacientes ciencias sociales un estadio de desarrollo cultural inferior a la civilización y superior al salvajismo. Prefirieron acuñar un nuevo término: Völkerwanderung (Migración de Pueblos), menos violento que invasiones, al sugerir el desplazamiento completo de un pueblo con sus instituciones y cultura, y más general incluso que invasiones germánicas, al incluir a hunos, eslavos y otros. Los germanos, que disponían de instituciones políticas peculiares, en concreto la asamblea de guerreros libres (thing) y la figura del rey, recibieron la influencia de las tradiciones institucionales del Imperio y la civilización grecorromana, así como la del cristianismo (aunque no siempre del cristianismo católico o atanasiano, sino del arriano); y se fueron adaptando a las circunstancias de su asentamiento en los nuevos territorios, sobre todo a la alternativa entre imponerse como minoría dirigente sobre una mayoría de población local o fusionarse con ella. Los nuevos reinos germánicos conformaron la personalidad de Europa Occidental durante la Edad Media, evolucionaron en monarquías feudales y monarquías autoritarias, y con el tiempo, dieron origen a los estados-nación que se fueron construyendo en torno a ellas. Socialmente, en algunos de estos países (España o Francia), el origen germánico (godo o franco) pasó a ser un rasgo de honor u orgullo de casta ostentado por la nobleza como distinción sobre el conjunto de la población. Las transformaciones del mundo romano Gala Placidia y sus hijos, Valentiniano III y Justa Grata Honoria. Véase también: Caída del Imperio romano de Occidente El Imperio romano había pasado por invasiones externas y guerras civiles terribles en el pasado, pero a finales del siglo IV aparentemente, la situación estaba bajo control. Hacía escaso tiempo que Teodosio había logrado nuevamente unificar bajo un solo centro ambas mitades del Imperio (392) y establecido una nueva religión de Estado, el cristianismo niceno (Edicto de Tesalónica -380), con la consiguiente persecución de los tradicionales cultos paganos y las heterodoxias cristianas. El clero cristiano, convertido en una jerarquía de poder, justificaba ideológicamente a un Imperium Romanum Christianum (Imperio Romano Cristiano) y a la dinastía Teodosiana como había comenzado a hacer ya con la Constantiniana desde el Edicto de Milán (313). Se habían encauzado los afanes de protagonismo político de los más ricos e influyentes senadores romanos y de las provincias occidentales. Además, la dinastía había sabido encauzar acuerdos con la poderosa aristocracia militar, en la que se enrolaban nobles germanos que acudían al servicio del Imperio al frente de soldados unidos por lazos de fidelidad hacia ellos. Al morir en 395, Teodosio confió el gobierno de Occidente y la protección de su joven heredero Honorio al general Estilicón, primogénito de un noble oficial vándalo que había contraído matrimonio con Flavia Serena, sobrina del propio Teodosio. Pero cuando en el 455 murió asesinado Valentiniano III, nieto de Teodosio, una buena parte de los descendientes de aquellos nobles occidentales (nobilissimus, clarissimus) que tanto habían confiado en los destinos del Imperio parecieron ya desconfiar del mismo, sobre todo cuando en el curso de dos decenios se habían podido dar cuenta de que el gobierno imperial recluido en Rávena era cada vez más presa de los exclusivos intereses e intrigas de un pequeño grupo de altos oficiales del ejército itálico. Muchos de estos eran de origen germánico y cada vez confiaban más en las fuerzas de sus séquitos armados de soldados convencionales y en los pactos y alianzas familiares que pudieran tener con otros jefes germánicos instalados en suelo imperial junto con sus propios pueblos, que desarrollaban cada vez más una política autónoma. La necesidad de acomodarse a la nueva situación quedó evidenciada con el destino de Gala Placidia, princesa imperial rehén de los propios saqueadores de Roma (el visigodo Alarico I y su primo Ataúlfo, con quien finalmente se casó); o con el de Honoria, hija de la anterior (en segundas nupcias con el emperador Constancio III) que optó por ofrecerse como esposa al propio Atila enfrentándose a su propio hermano Valentiniano. Alaricus rex gothorum, sello de Alarico II, rey visigodo. Necesitados de mantener una posición de predominio social y económico en sus regiones de origen, reducidos sus patrimonios fundiarios a dimensiones provinciales, y ambicionando un protagonismo político propio de su linaje y de su cultura, los honestiores (los más honestos u honrados, los que tienen honor), representantes de las aristocracias tardorromanas occidentales habrían acabado por aceptar las ventajas de admitir la legitimidad del gobierno de dichos reyes germánicos, ya muy romanizados, asentados en sus provincias. Al fin y al cabo, estos, al frente de sus soldados, podían ofrecerles bastante mayor seguridad que el ejército de los emperadores de Rávena. Además, el avituallamiento de dichas tropas resultaba bastante menos gravoso que el de las imperiales, por basarse en buena medida en séquitos armados dependientes de la nobleza germánica y alimentados con cargo al patrimonio fundiario provincial de la que esta ya hacía tiempo se había apropiado. Menos gravoso tanto para los aristócratas provinciales como también para los grupos de humiliores (los más humildes, los rebajados en tierra -humus-) que se agrupaban jerárquicamente en torno a dichos aristócratas, y que, en definitiva, eran los que habían venido soportando el máximo peso de la dura fiscalidad tardorromana. Las nuevas monarquías, más débiles y descentralizadas que el viejo poder imperial, estaban también más dispuestas a compartir el poder con las aristocracias provinciales, máxime cuando el poder de estos monarcas estaba muy limitado en el seno mismo de sus gentes por una nobleza basada en sus séquitos armados, desde su no muy lejano origen en las asambleas de guerreros libres, de los que no dejaban de ser primun inter pares. Pero esta metamorfosis del Occidente romano en romano-germano, no había sido consecuencia de una inevitabilidad claramente evidenciada desde un principio; por el contrario, el camino había sido duro, zigzagueante, con ensayos de otras soluciones, y con momentos en que parecía que todo podía volver a ser como antes. Así ocurrió durante todo el siglo V, y en algunas regiones también en el siglo VI como consecuencia, entre otras cosas, de la llamada Recuperatio Imperii o Reconquista de Justiniano. Los distintos reinos Batalla de Vouillé (507), entre francos y visigodos, representada en un manuscrito del siglo XIV Las invasiones bárbaras desde el siglo III habían demostrado la permeabilidad del limes romano en Europa, fijado en el Rin y el Danubio. La división del Imperio en Oriente y Occidente, y la mayor fortaleza del imperio oriental o bizantino, determinó que fuera únicamente en la mitad occidental donde se produjo el asentamiento de estos pueblos y su institucionalización política como reinos. Fueron los visigodos, primero como Reino de Tolosa y luego como Reino de Toledo, los primeros en efectuar esa institucionalización, valiéndose de su condición de federados, con la obtención de un foedus con el Imperio, que les encargó la pacificación de las provincias de Galia e Hispania, cuyo control estaba perdido en la práctica tras las invasiones del 410 por suevos, vándalos y alanos. De los tres, solo los suevos lograron el asentamiento definitivo en una zona: el Reino de Braga, mientras que los vándalos se establecieron en el norte de África y las islas del Mediterráneo Occidental, pero fueron al siglo siguiente eliminados por los bizantinos durante la gran expansión territorial de Justiniano I (campañas de los generales Belisario, del 533 al 544, y Narsés, hasta el 554). Simultáneamente los ostrogodos consiguieron instalarse en Italia expulsando a los hérulos, que habían expulsado a su vez de Roma al último emperador de Occidente. El Reino Ostrogodo desapareció también frente a la presión bizantina de Justiniano I. Un segundo grupo de pueblos germánicos se instala en Europa Occidental en el siglo VI, de entre los que destaca el Reino franco de Clodoveo I y sus sucesores merovingios, que desplaza a los visigodos de las Galias, forzándolos a trasladar su capital de Tolosa (Toulouse) a Toledo. También derrotaron a burgundios y alamanes, absorbiendo sus reinos. Algo más tarde los lombardos se establecen en Italia (568-9), pero serán derrotados a finales del siglo VIII por los mismos francos, que reinstaurarán el Imperio con Carlomagno (año 800). En Gran Bretaña se instalarán los anglos, sajones y jutos, que crearán una serie de reinos rivales que serán unificados por los daneses (un pueblo nórdico) en lo que terminará por ser el reino de Inglaterra. Las instituciones Breviario de Alarico, en un manuscrito del siglo X La monarquía germánica era en origen una institución estrictamente temporal, vinculada estrechamente al prestigio personal del rey, que no pasaba de ser un primus inter pares (primero entre iguales), que la asamblea de guerreros libres elegía (monarquía electiva), normalmente para una expedición militar concreta o para una misión específica. Las migraciones a que se vieron sometidos los pueblos germánicos desde el siglo III hasta el siglo V (encajonados entre la presión de los hunos al este y la resistencia del limes romano al sur y oeste) fue fortaleciendo la figura del rey, al tiempo que se entraba en contacto cada vez mayor con las instituciones políticas romanas, que acostumbraban a la idea de un poder político mucho más centralizado y concentrado en la persona del Emperador romano. La monarquía se vinculó a las personas de los reyes de forma vitalicia, y la tendencia era a hacerse monarquía hereditaria, dado que los reyes (al igual que habían hecho los emperadores romanos) procuraban asegurarse la elección de su sucesor, la mayor parte de las veces aún en vida y asociándolos al trono. El que el candidato fuera el primogénito varón no era una necesidad, pero se terminó imponiendo como una consecuencia obvia, lo que también era imitado por las demás familias de guerreros, enriquecidos por la posesión de tierras y convertidos en linajes nobiliarios que se emparentaban con la antigua nobleza romana, en un proceso que puede denominarse feudalización. Con el tiempo, la monarquía se patrimonializó, permitiendo incluso la división del reino entre los hijos del rey. El respeto a la figura del rey se reforzó mediante la sacralización de su toma de posesión (unción con los sagrados óleos por parte de las autoridades religiosas y uso de elementos distintivos como orbe, cetro y corona, en el transcurso de una elaborada ceremonia: la coronación) y la adición de funciones religiosas (presidencia de concilios nacionales, como los Concilios de Toledo) y taumatúrgicas (toque real de los reyes de Francia para la cura de la escrófula). El problema se suscitaba cuando llegaba el momento de justificar la deposición de un rey y su sustitución por otro que no fuera su sucesor natural. Los últimos merovingios no gobernaban por sí mismos, sino mediante los cargos de su corte, entre los que destacaba el mayordomo de palacio. Únicamente tras la victoria contra los invasores musulmanes en la batalla de Poitiers el mayordomo Carlos Martel se vio justificado para argumentar que la legitimidad de ejercicio le daba méritos suficientes para fundar él mismo su propia dinastía: la carolingia. En otras ocasiones se recurría a soluciones más imaginativas (como forzar la tonsura ,corte eclesiástico del pelo, del rey visigodo Wamba para incapacitarle). Los problemas de convivencia entre las minorías germanas y las mayorías locales (hispanorromanas, galo-romanas, etc.) fueron solucionados con más eficacia por los reinos con más proyección en el tiempo (visigodos y francos) a través de la fusión, permitiendo los matrimonios mixtos, unificando la legislación y realizando la conversión al catolicismo frente a la religión originaria, que en muchos casos ya no era el paganismo tradicional germánico, sino el cristianismo arriano adquirido en su paso por el Imperio Oriental. Algunas características propias de las instituciones germanas se conservaron: una de ellas el predominio del derecho consuetudinario sobre el derecho escrito propio del Derecho romano. No obstante los reinos germánicos realizaron algunas codificaciones legislativas, con mayor o menor influencia del derecho romano o de las tradiciones germánicas, redactadas en latín a partir del siglo V (leyes teodoricianas, edicto de Teodorico, Código de Eurico, Breviario de Alarico). El primer código escrito en lengua germánica fue el del rey Ethelberto de Kent, el primero de los anglosajones en convertirse al cristianismo (comienzos del siglo VI). El visigótico Liber Iudicorum (Recesvinto, 654) y la franca Ley Sálica (Clodoveo, 507-511) mantuvieron una vigencia muy prolongada por su consideración como fuentes del derecho en las monarquías medievales y del Antiguo Régimen. Véanse también: Derecho germánico y Derecho visigodo. La cristiandad latina y los bárbaros Libro de Kells o Evangeliario de San Columba, arte hiberno-sajón o irlando-sajón. La expansión del cristianismo entre los bárbaros, el asentamiento de la autoridad episcopal en las ciudades y del monacato en los ámbitos rurales (sobre todo desde la regla de San Benito de Nursia ,monasterio de Montecassino, 529,), constituyeron una poderosa fuerza fusionadora de culturas y ayudó a asegurar que muchos rasgos de la civilización clásica, como el derecho romano y el latín, pervivieran en la mitad occidental del Imperio, e incluso se expandiera por Europa Central y septentrional. Los francos se convirtieron al catolicismo durante el reinado de Clodoveo I (496 o 499) y, a partir de entonces, expandieron el cristianismo entre los germanos del otro lado del Rin. Los suevos, que se habían hecho cristianos arrianos con Remismundo (459-469), se convirtieron al catolicismo con Teodomiro (559-570) por las predicaciones de San Martín de Dumio. En ese proceso se habían adelantado a los propios visigodos, que habían sido cristianizados previamente en Oriente en la versión arriana (en el siglo IV), y mantuvieron durante siglo y medio la diferencia religiosa con los católicos hispanorromanos incluso con luchas internas dentro de la clase dominante goda, como demostró la rebelión y muerte de San Hermenegildo (581-585), hijo del rey Leovigildo). La conversión al catolicismo de Recaredo (589) marcó el comienzo de la fusión de ambas sociedades, y de la protección regia al clero católico, visualizada en los Concilios de Toledo (presididos por el propio rey). Los años siguientes vieron un verdadero renacimiento visigodo con figuras de la influencia de san Isidoro de Sevilla (y sus hermanos Leandro, Fulgencio y Florentina, los cuatro santos de Cartagena), Braulio de Zaragoza o Ildefonso de Toledo, de gran repercusión en el resto de Europa y en los futuros reinos cristianos de la Reconquista (véase cristianismo en España, monasterio en España, monasterio hispano y liturgia hispánica). Los ostrogodos, en cambio, no dispusieron de tiempo suficiente para realizar la misma evolución en Italia. No obstante, del grado de convivencia con el papado y los intelectuales católicos fue muestra que los reyes ostrogodos los elevaban a los cargos de mayor confianza (Boecio y Casiodoro, ambos magister officiorum con Teodorico el Grande), aunque también de lo vulnerable de su situación (ejecutado el primero -523- y apartado por los bizantinos el segundo -538-). Sus sucesores en el dominio de Italia, los también arrianos lombardos, tampoco llegaron a experimentar la integración con la población católica sometida, y su divisiones internas hicieron que la conversión al catolicismo del rey Agilulfo (603) no llegara a tener mayores consecuencias. El cristianismo fue llevado a Irlanda por San Patricio a principios del siglo V y desde allí se extendió a Escocia, desde donde un siglo más tarde regresó por la zona norte a una Inglaterra abandonada por los cristianos britones a los paganos pictos y escotos (procedentes del norte de Gran Bretaña) y a los también paganos germanos procedentes del continente (anglos, sajones y jutos). A finales del siglo VI, con el papa Gregorio Magno, también Roma envió misioneros a Inglaterra desde el sur, con lo que se consiguió que en el transcurso de un siglo Inglaterra volviera a ser cristiana. A su vez, los britones habían iniciado una emigración por vía marítima hacia la península de Bretaña, llegando incluso hasta lugares tan lejanos como la costa cantábrica entre Galicia y Asturias, donde fundaron la diócesis de Britonia. Esta tradición cristiana se distinguía por el uso de la tonsura céltica o escocesa, que rapaba la parte frontal del pelo en vez de la coronilla. La supervivencia en Irlanda de una comunidad cristiana aislada de Europa por la barrera pagana de los anglosajones, provocó una evolución diferente al cristianismo continental, lo que se ha denominado cristianismo celta. Conservaron mucho de la antigua tradición latina, que estuvieron en condiciones de compartir con Europa continental apenas la oleada invasora se hubo calmado temporalmente. Tras su extensión a Inglaterra en el siglo VI los irlandeses fundaron en el siglo VII monasterios en Francia, en Suiza (Saint Gall), e incluso en Italia, destacándose particularmente los nombres de Columba y Columbano. Las islas británicas fueron durante unos tres siglos el vivero de importantes nombres para la cultura: el historiador Beda el Venerable, el misionero Bonifacio de Alemania, el educador Alcuino de York, o el teólogo Juan Escoto Erígena, entre otros. Tal influencia llega hasta la atribución de leyendas como la de Santa Úrsula y las Once Mil Vírgenes, bretona que habría efectuado un extraordinario viaje entre Britania y Roma para acabar martirizada en Colonia. Otras cristianizaciones medievales Cirilo y Metodio, los apóstoles de los eslavos, con el alfabeto cirílico en un icono ruso del siglo XVIII o XIX. Por su parte, la extensión del cristianismo entre los búlgaros y la mayor parte de los pueblos eslavos (serbios, moravos y los pueblos de Crimea y estepas ucranianas y rusas ,Vladimiro I de Kiev, año 988,) fue muy posterior, y a cargo del Imperio bizantino, con lo que se hizo con el credo ortodoxo (predicaciones de Cirilo y Metodio, siglo IX); mientras que la evangelización de otros pueblos de Europa Oriental (el resto de los eslavos ,polacos, eslovenos y croatas,, bálticos y húngaros ,San Esteban I de Hungría, hacia el año 1000,) y de los pueblos nórdicos (vikingos escandinavos) se hizo por el cristianismo latino partiendo de Europa Central, en un periodo todavía más tardío (hasta los siglos XI y XII); permitiendo (especialmente la conversión de Hungría) las primeras peregrinaciones por vía terrestre a Tierra Santa. Es una locura creer en los dioses. Saga de Hrafnkell, sacerdote de Frey (Islandia, compuesta a finales del siglo XIII pero ambientada en época precristiana). Jázaros Artículo principal: Jázaros Los jázaros eran un pueblo turco procedente del Asia central (donde se había formado desde el siglo VI el imperio de los Köktürks) que en su parte occidental había dado origen a un importante estado que dominaba el Cáucaso y las estepas rusas y ucranianas hasta Crimea en el siglo VII Su clase dirigente se convirtió mayoritariamente al judaísmo, peculiaridad religiosa que lo convertía en un vecino excepcional entre el califato islámico de Damasco y el imperio cristiano de Bizancio. El Imperio bizantino (siglos IV al XV) Corte del emperador bizantino Justiniano I, mosaico de San Vital de Rávena. Artículo principal: Imperio bizantino La división entre Oriente y Occidente fue, además de una estrategia política (inicialmente de Diocleciano ,286, y hecha definitiva con Teodosio I ,395,), un reconocimiento de la diferencia esencial entre ambas mitades del Imperio. Oriente, en sí mismo muy diverso (península balcánica, Mezzogiorno, Anatolia, Cáucaso, Siria, Palestina, Egipto y la frontera mesopotámica con los persas), era la parte más urbanizada y con economía más dinámica y comercial, frente a un Occidente en vías de feudalización, ruralizado, con una vida urbana en decadencia, mano de obra esclava cada vez más escasa y la aristocracia cada vez más ajena a las estructuras del poder imperial y recluida en sus lujosas villae autosuficientes, cultivadas por colonos en régimen similar a la servidumbre. La lengua franca en Oriente era el griego, frente al latín de Occidente. En la implantación de la jerarquía cristiana, Oriente disponía de todos los patriarcados de la Pentarquía menos el de Roma (Alejandría, Antioquía y Constantinopla, a los que se añadió Jerusalén tras el concilio de Calcedonia de 451); incluso la primacía romana (sede pontificia de San Pedro) era un hecho discutido porque el Estado bizantino se operaba según el cesaropapismo (empezado por Constantino I y fundado teológicamente por Eusebio de Cesarea). Mosaico bizantino con el tema de la Theotokos (María como Madre de Dios). Los nimbos representan la santidad (el del Niño Jesús, cruciforme, la divinidad y el sacrificio de la Cruz). El fondo dorado representa la eternidad celeste, además de cumplir con el horror vacui propio del estilo. Todos sus rasgos: el cromatismo, la frontalidad y la linealidad (bordes nítidos, marcado de los pliegues), además de influir grandemente en el románico de Europa Occidental, se reprodujeron y continuaron, estereotipados, en los iconos religiosos de épocas posteriores en toda Europa Oriental. La supervivencia de Bizancio no dependía de la suerte de Occidente, mientras que lo contrario sí: de hecho, los emperadores orientales optaron por sacrificar Roma ,que ya ni siquiera era la capital occidental, cuando lo consideraron conveniente, abandonándola a su suerte o incluso desplazando hacia ella a los germanos (hérulos, ostrogodos y lombardos), lo que precipitó su caída. Sin embargo, la Ciudad Eterna, que tenía un valor simbólico, fue reconquistada e incluida en el efímero Exarcado de Rávena. Véase también: Constantinopla La restauración imperial de Justiniano Artículo principal: Recuperatio Imperii Justiniano I consolidó la frontera del Danubio y, desde 532 logró un equilibrio en la frontera con la Persia sasánida, lo que le permitió desplazar los esfuerzos bizantinos hacia el Mediterráneo, reconstruyendo la unidad del Mare Nostrum: En 533, una expedición del general Belisario aniquila a los vándalos (batallas de Ad Decimum y de Tricamerón) incorporando la provincia de África y las islas del Mediterráneo Occidental (Cerdeña, Córcega y las Baleares). En 535 Mundus ocupó Dalmacia y Belisario Sicilia. Narsés elimina a los ostrogodos de Italia en 554-555. Rávena volvió a ser una ciudad imperial, donde se conservarán los fastuosos mosaicos de San Vital. Liberio solo consiguió desplazar a los visigodos de la costa sureste de la península ibérica y de la provincia Bética. En Constantinopla se iniciaron dos programas ambiciosos y de prestigio con el fin de asentar la autoridad imperial: uno de recopilación legislativa: el Corpus iuris civilis, dirigido por Triboniano (promulgado entre 529 y 534), y otro constructivo: la iglesia de Santa Sofía, de los arquitectos Antemio de Tralles e Isidoro de Mileto (levantada entre el 532 y el 537). Un símbolo de la civilización clásica fue clausurado: la Academia de Atenas (529).[Nota 4] Otro, las carreras de cuadrigas siguieron siendo una diversión popular que levantaba pasiones. De hecho, eran utilizadas políticamente, expresando el color de cada equipo divergencias religiosas (un precoz ejemplo de movilizaciones populares utilizando colores políticos). La revuelta de Niká (534) estuvo a punto de provocar la huida del emperador, que evitó la emperatriz Teodora con su famosa frase la púrpura es un glorioso sudario.[Nota 5] Crisis, supervivencia y helenización del Imperio Salterio Jlúdov, uno de los tres únicos manuscritos ilustrados iconódulos que sobrevivieron al siglo IX. Esta página ilustra un pasaje evangélico en que un soldado ofrece a Cristo vinagre en una esponja atada a una lanza. En el plano inferior se caricaturiza al último Patriarca de Constantinopla iconoclasta, Juan el Gramático, borrando un icono de Cristo con una esponja similar. Los siglos VII y VIII representaron para Bizancio una edad oscura similar a la de occidente, que incluyó también una fuerte ruralización y feudalización en lo social y económico y una pérdida de prestigio y control efectivo del poder central. A las causas internas se sumó la renovación de la guerra con los persas, nada decisiva pero especialmente extenuante, a la que siguió la invasión musulmana, que privó al Imperio de las provincias más ricas: Egipto y Siria. No obstante, en el caso bizantino, la disminución de la producción intelectual y artística respondía además a los efectos particulares de la querella iconoclasta, que no fue un simple debate teológico entre iconoclastas e iconódulos, sino un enfrentamiento interno desatado por el patriarcado de Constantinopla, apoyado por el emperador León III, que pretendía acabar con la concentración de poder e influencia política y religiosa de los poderosos monasterios y sus apoyos territoriales (puede imaginarse su importancia viendo cómo ha sobrevivido hasta la actualidad el Monte Athos, fundado más de un siglo después, en 963). Basilio II Bulgaróctono Βασίλειος Β΄ Βουλγαροκτόνος, que quiere decir: «matador de búlgaros»; el nombre Basilio, Basileus significa rey en griego, y era el título que se daba al emperador. La recuperación de la autoridad imperial y la mayor estabilidad de los siglos siguientes trajo consigo también un proceso de helenización, es decir, de recuperación de la identidad griega frente a la oficial entidad romana de las instituciones, cosa más posible entonces, dada la limitación y homogeneización geográfica producida por la pérdida de las provincias, y que permitía una organización territorial militarizada y más fácilmente gestionable: los temas (themata) con la adscripción a la tierra de los militares en ellos establecidos, lo que produjo formas similares al feudalismo occidental. El periodo entre 867 y 1056, bajo la dinastía macedonia, se conoce con el nombre de Renacimiento macedónico, en que Bizancio vuelve a ser una potencia mediterránea y se proyecta hacia los pueblos eslavos de los Balcanes y hacia el norte del mar Negro. Basilio II Bulgaróctono que ocupó el trono en el período 976-1025 llevó al Imperio a su máxima extensión territorial desde la invasión musulmana, ocupando parte de Siria, Crimea y los Balcanes hasta el Danubio. La evangelización de Cirilo y Metodio obtendrá una esfera de influencia bizantina en Europa Oriental que cultural y religiosamente tendrá una gran proyección futura mediante la difusión del alfabeto cirílico (adaptación del alfabeto griego para la representación de los fonemas eslavos, que se sigue utilizando en la actualidad); así como la del cristianismo ortodoxo (predominante desde Serbia hasta Rusia). Sin embargo, la segunda mitad del siglo XI presenciará un nuevo desafío islámico, esta vez protagonizado por los turcos selyúcidas y la intervención del Papado y de los europeos occidentales, mediante la intervención militar de las Cruzadas, la actividad comercial de los mercaderes italianos (genoveses, amalfitanos, pisanos y sobre todo venecianos) y las polémicas teológicas del denominado Cisma de Oriente o Gran Cisma de Oriente y Occidente, con lo que la teórica ayuda cristiana se demostró tan negativa o más para el Imperio Oriental que la amenaza musulmana. El proceso de feudalización se acentuó al verse forzados los emperadores Comneno a realizar cesiones territoriales (denominadas pronoia) a la aristocracia y a miembros su propia familia. La expansión del islam (desde el siglo VII) Expansión árabe en el siglo VII: califa Abu Bakr en la zona I, Omar en la II, Uthman en la III y Ali en la IV. Artículo principal: Expansión musulmana En el siglo VII, tras las predicaciones de Mahoma y las conquistas de los primeros califas (a la vez líderes políticos y religiosos, en una religión ,el islamismo, que no reconoce distinciones entre laicos y clérigos), se había producido la unificación de Arabia y la conquista del Imperio persa y de buena parte del Imperio bizantino. En el siglo VIII se llegó a la península ibérica, la India y el Asia Central (batalla del Talas ,751, victoria islámica ante China tras la que no se profundizó en ese Imperio, pero que permitió un mayor contacto con su civilización, aprovechando los conocimientos de los prisioneros). En el occidente la expansión musulmana se frenó desde la batalla de Poitiers (732) ante los francos y la mitificada batalla de Covadonga ante los asturianos (722). La presencia de los musulmanes como una civilización rival alternativa asentada en la mitad sur de la cuenca del Mediterráneo, cuyo tráfico marítimo pasan a controlar, obligó al cierre en sí misma de Europa Occidental por varios siglos, y para algunos historiadores significó el verdadero comienzo de la Edad Media. Manuscrito árabe ilustrado del siglo XIII La representación de figuras solo se consiente en algunas interpretaciones del islamismo, pero se prohíbe mayoritariamente. Esta prohibición incentivó otras artes, como la caligrafía. Esta ilustración representa a Sócrates (Sughrat). La recuperación y difusión de la cultura clásica grecorromana fue una de las principales aportaciones del islam medieval a la civilización. Desde el siglo VIII se produjo una difusión más lenta de la civilización islámica por sitios tan lejanos como Indonesia y el continente africano, y desde el siglo XIV por Anatolia y los Balcanes. Las relaciones con la India fueron también muy estrechas durante el resto de la Edad Media (aunque la imposición del imperio mogol no se produjo hasta el siglo XVI), mientras que el océano Índico se convirtió casi en un Mare Nostrum árabe, donde se ambientaron las aventuras de Simbad el marino (uno de los cuentos de Las mil y una noches de la época de Harún al-Rashid). El tráfico comercial de las rutas marítimas y caravaneras unían el Índico con el Mediterráneo a través del mar Rojo o el golfo Pérsico y las caravanas del desierto. Esa llamada ruta de las especias (prefigurada por la ruta del incienso en la Edad Antigua) fue esencial para que llegaran a occidente retazos de la ciencia y la cultura de Extremo Oriente. Por el norte, la Ruta de la Seda cumplió la misma función atravesando los desiertos y las cordilleras del Turquestán. El ajedrez, la numeración indoarábiga y el concepto de cero, así como algunas obras literarias (Calila e Dimna) estuvieron entre los aportes hindúes y persas. El papel, el grabado o la pólvora, entre las chinas. La función de los árabes, y de los persas, sirios, egipcios y españoles arabizados (no solo islámicos, pues hubo muchos que mantuvieron su religión cristiana o judía ,no tanto la zoroastriana,) distó mucho de ser mera transmisión, como testimonia la influencia de la reinterpretación de la filosofía clásica que llegó a través de los textos árabes a Europa Occidental a partir de las traducciones latinas desde el siglo XII, y la difusión de cultivos y técnicas agrícolas por la región mediterránea. En un momento en que estaban prácticamente ausentes de la economía europea, destacaron las prácticas comerciales y la circulación monetaria en el mundo islámico, animadas por la explotación de minas de oro tan lejanas como las del África subsahariana, junto con otro tipo de actividades, como el tráfico de esclavos. La Kaaba en la Mezquita de La Meca o mezquita sagrada (Masyid al-Haram). La unidad inicial del mundo islámico, que se había cuestionado ya en el aspecto religioso con la separación de suníes y chiíes, se rompió también en lo político con la sustitución de los Omeyas por los Abbasíes al frente del califato en el 749, que además sustituyeron Damasco por Bagdad como capital. Abderramán I, el último superviviente Omeya, consiguió fundar en Córdoba un emirato independiente para al-Ándalus (nombre árabe de la península ibérica), que su descendiente Abderramán III convirtió en un califato alternativo en el 929. Poco antes, en el 909 los Fatimíes habían hecho lo propio en Egipto. A partir del siglo XI se producen cambios muy importantes: el desafío a la hegemonía árabe como etnia dominante dentro del islam a cargo de los islamizados turcos, que pasarán a controlar distintas zonas del Medio Oriente (mamelucos, otomanos), o de kurdos como Saladino; la irrupción de los cristianos latinos en tres puntos clave del Mediterráneo (reinos cristianos de la Reconquista en al-Ándalus, normandos en el sur de Italia y cruzados en Siria y Palestina); y la de los mongoles desde el centro de Asia. Los eruditos como al-Biruni, al-Jahiz, al-Kindi, Abu Bakr Muhammad al-Razi, Ibn Sina, al-Idrisi, Ibn Bayya, Omar al-Jayyam, Ibn Zuhr, Ibn Tufail, Ibn Rushd, al-Suyuti, y miles de otros académicos no fueron una excepción, sino la norma general en la civilización musulmana. La civilización musulmana del periodo clásico fue destacable por el elevado número de eruditos polifacéticos que produjo. Es una muestra de la homogeneidad de la filosofía islámica sobre la ciencia, y su énfasis sobre la síntesis, las investigaciones interdisciplinares y la multiplicidad de métodos. Ziauddin Sardar Véanse también: Mahoma, Islam, Corán, Califa y Califato perfecto. Véanse también: Dinastía de los omeyas, Califato abasí, Califato fatimí y Umma (islam). Véanse también: Historia del islam, Edad de Oro del islam, Cultura musulmana, Filosofía islámica y Filosofía islámica antigua. Véanse también: Sufismo, Averroísmo, Ciencia medieval, Matemática en el islam medieval y Ciencia islámica. Véanse también: Literatura árabe y Poesía árabe. Véanse también: Omar Jayam, Alhacén y Algazel. Véanse también: Al Juarismi, Avicena, Averroes, Maimónides e Ibn Jaldún. Al-Ándalus (siglo VIII al XV) Interior de la Mezquita de Córdoba. Durante algo más de un siglo Córdoba fue la capital de un califato. Artículo principal: Historia de Al-Andalus Véanse también: Conquista omeya de Hispania, Emirato de Córdoba y Califato de Córdoba. Véanse también: Abderramán I, Abderramán II, Abderramán III, Alhakén II e Hisham II. Véanse también: Tudmir, Banu Qasi, Omar ibn Hafsún y Almanor. Véanse también: Ziryab, Moaxaja, Jarchas, Eulogio de Córdoba y Ciencia en Al-Ándalus. Véanse también: Arte de al-Ándalus, Arte emiral y califal, Gastronomía de al-Ándalus, Azaque y Parias (tributo). Véanse también: Muladí, Maulas, Dhimmi, Mozárabe y Yizia. Véanse también: Medina, Arrabal, Zoco, Alcazaba y Aceifa. Imperio carolingio (siglos VIII y IX) Artículo principal: Imperio carolingio Surgimiento y ascenso Coronación de Carlomagno por el papa León III, el día de Navidad del año 800. Hacia el siglo VIII, la situación política europea se había estabilizado. En oriente, el Imperio bizantino era fuerte otra vez, gracias a una serie de emperadores competentes. En occidente, algunos reinos aseguraban relativa estabilidad a varias regiones: Northumbria a Inglaterra, el Reino visigodo a España, el Reino lombardo a Italia y el Reino franco a Galia y Alemania. En realidad, el Reino franco era un compuesto de tres reinos: Austrasia, Neustria y Aquitania. El Imperio carolingio surge de las bases creadas por los predecesores de Carlomagno desde principios del siglo VIII (Carlos Martel y Pipino el Breve). La proyección de sus fronteras a través de una gran parte de la Europa Occidental permitió a Carlos la aspiración de reconstruir la extensión del antiguo Imperio romano occidental, siendo la primera entidad política de la Edad Media que estuvo en condiciones de convertirse en una potencia continental. Aquisgrán fue elegida como capital, en una situación central y suficientemente alejada de Italia, que a pesar de ser liberada del dominio de los longobardos y de las teóricas reivindicaciones bizantinas, conservó una gran autonomía que llegaba a la soberanía temporal con la cesión de unos incipientes Estados Pontificios (el Patrimonium Petri o Patrimonio de San Pedro, que incluía Roma y buena parte del centro de Italia). Como resultado de la estrecha vinculación entre el pontificado y la dinastía carolingia, que se legitimaban y defendían mutuamente ya por tres generaciones, el papa León III reconoció las pretensiones imperiales de Carlomagno con una coronación en extrañas circunstancias, el día de Navidad del año 800. KAROLUS. Monograma de Carlomagno, quien lo utilizaba como firma. Carlomagno, a pesar de sus esfuerzos, nunca aprendió a escribir con soltura. Se crearon las marcas para fijar las fronteras ante los enemigos exteriores (árabes en la Marca Hispánica, sajones en la Marca Sajona, bretones en la Marca Bretona, lombardos ,hasta su derrota, en la Marca Lombarda y ávaros en la Marca Ávara; posteriormente también se creó una para los húngaros: la Marca del Friuli). El territorio interior fue organizado en condados y ducados (unión de varios condados o marcas). Los funcionarios que los dirigían (condes, marqueses y duques) eran vigilados por inspectores temporales (los missi dominici ,enviados del señor,), y se procuraba que no se heredaran para evitar que quedaran patrimonializados en una familia (cosa, que con el tiempo, no pudo evitarse). La consignación de tierras junto con los cargos, pretendía sobre todo el mantenimiento de la costosa caballería pesada y los nuevos caballos de batalla (destreros, introducidos desde Asia en el siglo VII que se empleaban de una manera completamente distinta a la caballería antigua, con estribos, aparatosas sillas y que podían sostener armaduras). Tal proceso estuvo en el origen del nacimiento de los feudos que había que ceder a cada militar de acuerdo con su rango, hasta la unidad básica: el caballero que ejercía de señor sobre un territorio, se quedaba para su mantenimiento con una reserva señorial y dejaba los mansos para sus siervos, que estaban obligados a cultivar la reserva con prestaciones gratuitas de trabajo a cambio de la protección militar y el mantenimiento del orden y la justicia, que eran las funciones del señor. Lógicamente, los feudos en sus distintos niveles sufrieron la misma transformación patrimonial que marcas y condados, estableciendo una red piramidal de fidelidades que es el origen del vasallaje feudal. Carlomagno negoció de igual a igual con otras grandes potencias de la época, como el Imperio bizantino, el Emirato de Córdoba, y el Califato Abasida. Aunque él mismo, ya en edad adulta, no sabía escribir (cosa habitual en la época, en que únicamente algunos clérigos lo hacían), Carlomagno siguió una política de prestigio cultural y un notable programa artístico. Pretendió rodearse de una corte de sabios e iniciar un programa educativo basado en el trivium y el quadrivium, para lo que mandó llamar a la intelectualidad de su tiempo a sus dominios impulsando, con la colaboración de Alcuino de York, el llamado Renacimiento carolingio. Dentro de este empeño educativo ordenó a sus nobles aprender a escribir, cosa que él mismo intentó, aunque nunca consiguió hacerlo con soltura. División y hundimiento Ludovico Pío, hijo y heredero de Carlomagno. Muerto Carlomagno en 814, toma el poder su hijo Ludovico Pío. Los hijos de este: Carlos el Calvo (Francia occidental), Luis el Germánico (Francia oriental) y Lotario I (primogénito y heredero del título imperial), se enfrentaron militarmente disputándose los diferentes territorios del imperio, que, más allá de las alianzas aristocráticas, manifestaban distintas personalidades, interpretables desde una perspectiva protonacional (idiomas diferentes: hacia el sur y oeste se imponían las lenguas romances que se comenzaban a diferenciar del latín vulgar, hacia el norte y este las lenguas germánicas, como testimoniaban los previos Juramentos de Estrasburgo; costumbres, tradiciones e instituciones propias ,romanas hacia el sur, germanas hacia el norte,). Esta situación no concluyó ni siquiera en el 843 tras el Tratado de Verdún, puesto que la posterior división del reino de Lotario entre sus hijos (la Lotaringia, franja central desde los Países Bajos hasta Italia, pasando por la región del Rin, Borgoña y Provenza) llevó a los tíos de estos (Carlos y Luis), a otro reparto (el Tratado de Mersen del 870) que simplificaba las fronteras (dejando únicamente Italia y Provenza en manos de su sobrino el emperador Luis II el Joven ,cuyo cargo no suponía más primacía que la honorífica,, pero no condujo a una mayor concentración de poder en manos de esos monarcas, débiles y en manos de la nobleza territorial. En algunas regiones, el pacto no era más que una entelequia, puesto que la costa del mar del Norte estaba ocupada por los vikingos. Incluso en las zonas teóricamente controladas, las posteriores herencias y luchas internas entre los sucesivos reyes y emperadores carolingios subdividieron y reunificaron los territorios de manera casi aleatoria. La división, sumada al proceso institucional de descentralización inherente al sistema feudal, en ausencia de fuertes poderes centrales, y al debilitamiento preexistente de las estructuras sociales y económicas, hizo que la siguiente oleada de invasiones bárbaras, sobre todo las protagonizadas por húngaros y vikingos, sumieran de nuevo a Europa Occidental en el caos de una nueva edad oscura. Carlos el Calvo, rey de Francia Occidental. Carlos el Calvo, rey de Francia Occidental. Apogeo del Imperio carolingio hacia 814. Apogeo del Imperio carolingio hacia 814. Divisiones del Imperio en los tratados de Verdún (año 843, línea punteada) y Meersen (870). Divisiones del Imperio en los tratados de Verdún (año 843, línea punteada) y Meersen (870). Europa en torno al 998. Europa en torno al 998. El sistema feudal Artículo principal: Feudalismo Uso del término «feudalismo» El fracaso del proyecto político centralizador de Carlomagno llevó, en ausencia de ese contrapeso, a la formación de un sistema político, económico y social que los historiadores han convenido en llamar feudalismo, aunque en realidad el nombre nació como un peyorativo para designar del Antiguo Régimen por parte de sus críticos ilustrados. La Revolución francesa suprimió solemnemente todos los derechos feudales en la noche del 4 de agosto de 1789 y definitivamente el régimen feudal, con el decreto del 11 de agosto. La generalización del término permite a muchos historiadores aplicarlo a las formaciones sociales de todo el territorio europeo occidental, pertenecieran o no al Imperio carolingio. Los partidarios de un uso restringido, argumentando la necesidad de no confundir conceptos como feudo, villae, tenure, o señorío lo limitan tanto en espacio (Francia, Oeste de Alemania y Norte de Italia) como en el tiempo: un «primer feudalismo» o «feudalismo carolingio» desde el siglo VIII hasta el año 1000 y un «feudalismo clásico» desde el año 1000 hasta el 1240, a su vez dividido en dos épocas, la primera, hasta el 1160 (la más descentralizada, en que cada señor de castillo podía considerarse independiente, y se produce el proceso denominado incastellamento); y la segunda, la propia de la monarquía feudal). Habría incluso feudalismos de importación: la Inglaterra normanda desde 1066 y los estados latinos de oriente creados durante las Cruzadas (siglos XII y XIII). Otros prefieren hablar de régimen o sistema feudal, para diferenciarlo sutilmente del feudalismo estricto, o de síntesis feudal, para marcar el hecho de que sobreviven en ella rasgos de la Antigüedad clásica mezclados con contribuciones germánicas, implicando tanto a instituciones como a elementos productivos, y significó la especificidad del feudalismo europeo occidental como formación económico social frente a otras también feudales, con consecuencias trascendentales en el futuro devenir histórico.[Nota 6] Más dificultades hay para el uso del término cuando nos alejamos más: Europa Oriental experimenta un proceso de feudalización desde finales de la Edad Media, justo cuando en muchas zonas de Europa Occidental los campesinos se liberan de las formas jurídicas de la servidumbre, de modo que suele hablarse del feudalismo polaco o ruso. El Antiguo Régimen en Europa, el islam medieval o el Imperio bizantino fueron sociedades urbanas y comerciales, y con un grado de centralización política variable, aunque la explotación del campo se realizaba con relaciones sociales de producción muy similares al feudalismo medieval. Los historiadores que aplican la metodología del materialismo histórico (Marx definió el modo de producción feudal como el estadio intermedio entre el esclavista y el capitalista) no dudan en hablar de «economía feudal» para referirse a ella, aunque también reconocen la necesidad de no aplicar el término a cualquier formación social preindustrial no esclavista, puesto que a lo largo de la historia y de la geografía han existido otros modos de producción también previstos en la modelización marxista, como el modo de producción primitivo de las sociedades poco evolucionadas, homogéneas y con escasa división social ,como las de los mismos pueblos germánicos previamente a las invasiones, y el modo de producción asiático o despotismo hidráulico ,Egipto faraónico, reinos de la India o Imperio chino, caracterizado por la tributación de las aldeas campesinas a un estado muy centralizado. En lugares aún más lejanos se ha llegado a utilizar el término feudalismo para describir una época. Es el caso de Japón y el denominado feudalismo japonés, dadas las innegables similitudes y paralelismos que la nobleza feudal europea y su mundo tiene con los samuráis y el suyo. También se ha llegado a aplicarlo a la situación histórica de los periodos intermedios de la historia de Egipto, en los que, siguiendo un ritmo cíclico milenario, decae el poder central y la vida en las ciudades, la anarquía militar rompe la unidad de las tierras del Nilo, y los templos y señores locales que alcanzan a controlar un espacio de poder gobiernan en él de manera independiente sobre los campesinos obligados al trabajo. El vasallaje y el feudo Un vasallo arrodillado realiza la inmixtio manum durante el homenaje a su señor, sentado. Un escribiente toma nota. Todos están sonrientes. Dos instituciones eran claves para el feudalismo: por un lado el vasallaje como relación jurídico-política entre señor y vasallo, un contrato sinalagmático (es decir, entre iguales, con requisitos por ambas partes) entre señores y vasallos (ambos hombres libres, ambos guerreros, ambos nobles), consistente en el intercambio de apoyos y fidelidades mutuas (dotación de cargos, honores y tierras ,el feudo, por el señor al vasallo y compromiso de auxilium et consilium ,auxilio o apoyo militar y consejo o apoyo político,), que si no se cumplía o se rompía por cualquiera de las dos partes daba lugar a la felonía, y cuya jerarquía se complicaba de forma piramidal (el vasallo era a su vez señor de vasallos); y por otro lado el feudo como unidad económica y de relaciones sociales de producción, entre el señor del feudo y sus siervos, no un contrato igualitario, sino una imposición violenta justificada ideológicamente como un do ut des de protección a cambio de trabajo y sumisión. Por tanto, la realidad que se enuncia como relaciones feudo-vasalláticas es realmente un término que incluye dos tipos de relación social de naturaleza completamente distinta, aunque los términos que las designan se empleaban en la época (y se siguen empleando) de manera equívoca y con gran confusión terminológica entre ellos: El vasallaje era un pacto entre dos miembros de la nobleza de distinta categoría. El caballero de menor rango se convertía en vasallo (vassus) del noble más poderoso, que se convertía en su señor (dominus) por medio del Homenaje e Investidura, en una ceremonia ritualizada que tenía lugar en la torre del homenaje del castillo del señor. El homenaje (homage) ,del vasallo al señor, consistía en la postración o humillación ,habitualmente de rodillas,, el osculum (beso), la inmixtio manum ,las manos del vasallo, unidas en posición orante, eran acogidas entre las del señor,, y alguna frase que reconociera haberse convertido en su hombre. Tras el homenaje se producía la investidura ,del señor al vasallo,, que representaba la entrega de un feudo (dependiendo de la categoría de vasallo y señor, podía ser un condado, un ducado, una marca, un castillo, una población, o un simple sueldo; o incluso un monasterio si el vasallaje era eclesiástico) a través de un símbolo del territorio o de la alimentación que el señor debe al vasallo ,un poco de tierra, de hierba o de grano, y del espaldarazo, en el que el vasallo recibe una espada (y unos golpes con ella en los hombros), o bien un báculo si era religioso. La encomienda, encomendación o patrocinio (patrocinium, commendatio, aunque era habitual utilizar el término commendatio para el acto del homenaje o incluso para toda la institución del vasallaje) eran pactos teóricos entre los campesinos y el señor feudal, que podían también ritualizarse en una ceremonia o ,más raramente, dar lugar a un documento. El señor acogía a los campesinos en su feudo, que se organizaba en una reserva señorial que los siervos debían trabajar obligatoriamente (sernas o corveas) y en el conjunto de las pequeñas explotaciones familiares (mansos) que se atribuían a los campesinos para que pudieran subsistir. Obligación del señor era protegerles si eran atacados, y mantener el orden y la justicia en el feudo. A cambio, el campesino se convertía en su siervo y pasaba a la doble jurisdicción del señor feudal: en los términos utilizados en la península ibérica en la Baja Edad Media, el señorío territorial, que obligaba al campesino a pagar rentas al noble por el uso de la tierra; y el señorío jurisdiccional, que convertía al señor feudal en gobernante y juez del territorio en el que vivía el campesino, por lo que obtenía rentas feudales de muy distinto origen (impuestos, multas, monopolios, etc.). La distinción entre propiedad y jurisdicción no era en el feudalismo algo claro, pues de hecho el mismo concepto de propiedad era confuso, y la jurisdicción, otorgada por el rey como merced, ponía al señor en disposición de obtener sus rentas. No existieron señoríos jurisdiccionales en los que la totalidad de las parcelas pertenecieran como propiedad al señor, siendo muy generalizadas distintas formas de alodio en los campesinos. En momentos posteriores de despoblamiento y refeudalización, como la crisis del siglo XVII, algunos nobles intentaban que se considerase despoblado completamente de campesinos un señorío para liberarse de todo tipo de cortapisas y convertirlo en coto redondo reconvertible para otro uso, como el ganadero. Junto con el feudo, el vasallo recibe los siervos que hay en él, no como propiedad esclavista, pero tampoco en régimen de libertad; puesto que su condición servil les impide abandonarlo y les obliga a trabajar. Las obligaciones del señor del feudo incluyen el mantenimiento del orden, o sea, la jurisdicción civil y criminal (mero e mixto imperio en la terminología jurídica reintroducida con el Derecho Romano en la Baja Edad Media), lo que daba aún mayores oportunidades para obtener el excedente productivo que los campesinos pudieran obtener después de las obligaciones de trabajo ,corveas o sernas en la reserva señorial, o del pago de renta ,en especie o en dinero, de circulación muy escasa en la Alta Edad Media, pero más generalizada en los últimos siglos medievales, según fue dinamizándose la economía,. Como monopolio señorial solían quedar la explotación de los bosques y la caza, los caminos y puentes, los molinos, las tabernas y tiendas. Todo ello eran más oportunidades de obtener más renta feudal, incluidos derechos tradicionales, como el ius prime noctis o derecho de pernada, que se convirtió en un impuesto por matrimonios, buena muestra de que es en el excedente de donde se extrae la renta feudal de manera extraeconómica (en este caso en la demostración de que una comunidad campesina crece y prospera). Los órdenes feudales Artículo principal: Estamento Orator, bellator et laborator (clérigo, guerrero y labrador); o sea, los tres órdenes medievales. Letra capitular de un manuscrito. Con el tiempo, siguiendo la tendencia marcada desde el Bajo Imperio romano, que se consolidó en la época clásica del feudalismo y que pervivió durante todo el Antiguo Régimen, se fue conformando una sociedad organizada de manera estamental, en los llamados estamentos u ordines (órdenes): nobleza, clero y pueblo llano (o tercer estado): bellatores, oratores y laboratores los hombres que guerrean, los que rezan y los que trabajan, según el vocabulario de la época. Los dos primeros son privilegiados, es decir, no se les aplica la ley común, sino un fuero propio (por ejemplo, tienen distintas penas para el mismo delito, y su forma de ejecución es diferente) y no pueden trabajar (les están prohibidos los oficios viles y mecánicos), puesto que esa es la condición de no privilegiados. En época medieval, los órdenes feudales no eran estamentos cerrados y bloqueados, sino que mantenían una permeabilidad que permitía en casos extraordinarios el ascenso social debido al mérito (por ejemplo, a la demostración de un excepcional valor), que eran tan escasos que no se vivían como una amenaza, cosa que sí ocurrió a partir de las grandes convulsiones sociales de los siglos finales de la Baja Edad Media, en que los privilegiados se vieron obligados a institucionalizar su posición procurando cerrar el acceso a sus estamentos de los no privilegiados (en lo que tampoco tuvieron una eficacia total). Completamente impropia sería la comparación con la sociedad de castas de la India, en que guerreros, sacerdotes, comerciantes, campesinos y parias pertenecían a castas diferentes entendidas como linajes desconectados cuya mezcla se prohibía. Las funciones de los órdenes feudales estaban fijadas ideológicamente por el agustinismo político (Civitate Dei -426-), en búsqueda de una sociedad que, aunque como terrena no podía dejar de ser corrupta e imperfecta, podía aspirar a ser al menos una sombra de la imagen de una Ciudad de Dios perfecta de raíces platónicas[Nota 7] en que todos tuvieran un papel en su protección, su salvación y su mantenimiento. Esta idea fue reformulada y perfilada a lo largo de la Edad Media, sucesivamente por autores como Isidoro de Sevilla (630), la escuela de Auxerre (Haimón de Auxerre -865- en la abadía borgoñona en la que trabajaban Erico de Auxerre y su discípulo Remigio de Auxerre, que seguían la tradición de Escoto Eriúgena), Boecio (892), Wulfstan de York (1010), Gerardo de Cambrai (1024) o Adalberón de Laon; y utilizada en textos legislativos como la llamada Compilación de Huesca de los Fueros de Aragón (Jaime I), y las Siete Partidas (Alfonso X el Sabio, 1265). Los bellatores o guerreros eran la nobleza, cuya función era la protección física, la defensa de todos ante las agresiones e injusticias. Estaba organizada piramidalmente desde el emperador, pasando por los reyes y descendiendo sin solución de continuidad hasta el último escudero, aunque atendiendo a su rango, poder y riqueza puede clasificarse en dos partes diferenciadas: alta nobleza (marqueses, condes y duques) cuyos feudos tienen el tamaño de regiones y provincias (aunque la mayor parte de las veces no en continuidad territorial, sino repartido y difuso, lleno de enclaves y exclaves); y la baja nobleza o caballeros (barones, infanzones), cuyos feudos son del tamaño de pequeñas comarcas (a escala municipal o inferior a la municipal), o directamente no poseen feudos territoriales, viviendo en los castillos de señores más importantes, o en ciudades o poblaciones en las que no ejercen jurisdicción (aunque sí pueden ejercer su regimiento, es decir, participar en su gobierno municipal en representación del estado noble). A finales de la Edad Media y en la Edad Moderna, cuando la nobleza ya no ejercía su función militar, como era el caso de los hidalgos españoles, que aducían sus privilegios estamentales para evitar el pago de impuestos y obtener alguna ventaja social, alardeando de ejecutoria o de blasón y casa solariega, pero que al no disponer de rentas feudales suficientes para mantener la manera de vida nobiliaria, corrían el peligro de perder su condición por contraer un matrimonio desigual o ganarse la vida trabajando: Pues la sangre de los godos, y el linaje e la nobleza tan crescida, ¡por cuántas vías e modos se pierde su grand alteza en esta vida! Unos, por poco valer, por cuán baxos e abatidos que los tienen; otros que, por non tener, con oficios non debidos se mantienen. Copla X de las Coplas a la muerte de su padre de Jorge Manrique Además de la legitimación religiosa, a través de la cultura y el arte laicos (la épica de los cantares de gesta y la lírica del amor cortés de los trovadores provenzales) se difundía socialmente la legitimación ideológica de la forma de vida, la función social y los valores de la nobleza. Asesinato de Santo Tomás Becket (1170), provocado por el rey de Inglaterra, anteriormente su aliado. Vidriera de la catedral de Canterbury (siglo XIII). Excomunión de Roberto II de Francia (998), en una recreación de pintura histórica por Jean-Paul Laurens (1875). Los oratores o clérigos eran el clero, cuya función era facilitar la salvación espiritual de las almas inmortales: algunos formaban una élite poderosa llamada alto clero (abades, obispos), y otros más humildes, el bajo clero (curas de pueblo o los hermanos legos de un monasterio). La extensión y organización del monacato benedictino a través de la Orden de Cluny, estrechamente vinculado a la organización de la red episcopal centralizada y jerarquizada, con cúspide en el papa de Roma, estableció la doble pirámide feudal del clero secular, destinado a la administración los de sacramentos (que controlaban toda la trayectoria vital de la población, desde el nacimiento hasta muerte); y el clero regular, apartado del mundo y sometido a una regla monástica (habitualmente la regla benedictina). Los tres votos monásticos del clero regular: pobreza, obediencia y castidad; así como el celibato eclesiástico que se fue imponiendo al clero secular, funcionaron como un eficaz mecanismo de vinculación de los dos estamentos privilegiados: los hijos segundones de la nobleza ingresaban en el clero, donde eran mantenidos sin estrecheces gracias a las numerosas fundaciones, donaciones, dotes y mandas testamentarias; pero no disputaban las herencias a sus hermanos, que podían mantener concentrado el patrimonio familiar. Las tierras de la Iglesia quedaban como manos muertas, cuya función era la de garantizar las misas y oraciones previstas por los donadores, de modo que los hijos rezaban por las almas de los padres. Todo el sistema garantizaba el mantenimiento del prestigio social de los privilegiados, asistiendo a misa en lugares destacados mientras vivían y enterrados en lugares principales de iglesias y catedrales cuando morían.[Nota 8] No faltaron los enfrentamientos: la evidencia de simonía y nicolaísmo (nombramientos de cargos eclesiásticos interferidos por las autoridades civiles o su pura compraventa) y la utilización de la principal amenaza religiosa al poder temporal, equivalente a una muerte civil: la excomunión. El Papa se atribuía incluso la autoridad de eximir al vasallo de la fidelidad debida a su señor y reivindicarla para sí mismo, lo que fue utilizado en varias ocasiones para la fundación de reinos que pasaban a ser vasallos del Papa (por ejemplo, la independencia que Afonso Henriques obtuvo para el condado convertido en reino de Portugal frente al reino de León). Los laboratores o trabajadores, eran el pueblo llano, cuya función era el mantenimiento de los cuerpos, la función ideológicamente más baja y humilde ,humiliores eran los cercanos al humus, la tierra, mientras que sus superiores eran honestiores, los que podían mantener la honra u honor,.[Nota 9] Necesariamente los más numerosos, y la inmensa mayoría de ellos dedicados a tareas agrícolas, dado la bajísima productividad y rendimiento agrícola, propios de la época preindustrial y del muy escaso nivel técnico (de ahí la identificación en castellano de laborator con labrador). Por lo común estaban sometidos a los otros estamentos. El pueblo llano estaba compuesto en su gran mayoría por campesinos, siervos de los señores feudales o campesinos libres (villanos), y por artesanos, que eran escasos y vivían, bien en las aldeas (aquellos de menor especialización, que solían compartir las tareas agrícolas: herreros, talabarteros, alfareros, sastres) o en las pocas y pequeñas ciudades (los de mayor especialización y de productos de necesidad menos apremiante o de demandada de las clases altas: joyeros, orfebres, cereros, toneleros, tejedores, tintoreros). La autosuficiencia de los feudos y los monasterios limitaba su mercado y capacidad de crecer. Los oficios de la construcción (cantería, albañilería, carpintería) y la misma profesión de maestro de obras o arquitecto son una notable excepción: obligados por la naturaleza de su trabajo al desplazamiento al lugar donde se construye el edificio, se transformaron en un gremio nómada que se desplazaba por los caminos europeos comunicándose novedades técnicas u ornamentales transformadas en secretos de oficio, lo que está en el origen de su lejana y mitificada vinculación con la sociedad secreta de la masonería, que desde su origen los consideró como los primitivos masones.[Nota 10] Las zonas sin dependencia intermedia de señores nobles o eclesiásticos se denominaban realengo y solían prosperar más, o al menos solían considerar como una desgracia el pasar a depender de un señor, hasta el punto de que en algunas ocasiones conseguían evitarlo con pagos al rey, o se incentivaba la repoblación de zonas fronterizas o despobladas (como ocurrió en el reino astur-leonés con la despoblada Meseta del Duero) donde podían aparecer figuras mixtas, como el caballero villano (que podía mantener con su propia explotación al menos un caballo de guerra y armarse y defenderse a sí mismo) o las behetrías, que elegían a su propio señor y podían cambiar de uno u a otro si les convenía, o con la oferta de un fuero o carta puebla que otorgaba a un población su propio señorío colectivo. Los privilegios iniciales no fueron suficientes para impedir que con el tiempo la mayor parte de ellos cayeran en la feudalización. Los tres órdenes feudales no eran en la Edad Media aún unos estamentos cerrados: eran consecuencia básica de la estructura social que se había ido creando lenta pero inexorablemente con la transición del esclavismo al feudalismo desde la crisis del siglo III (ruralización y formación de latifundios y villae, reformas de Diocleciano, descomposición del Imperio romano, las invasiones, el establecimiento de los reinos germánicos, instituciones del Imperio carolingio, descomposición de este y nueva oleada de invasiones). Los señores feudales eran continuación de las líneas clientelares de los condes carolingios, y algunos pueden remontarse a los latifundistas romanos o los séquitos germanos, mientras que el campesinado provenía de los antiguos esclavos o colonos, o de campesinos libres que se vieron forzados a encomendarse, recibiendo a veces una parte de sus antiguas tierras propias en forma de manso concedido por el señor. El campesino heredaba su condición servil y su sujeción a la tierra, y rara vez tenía oportunidad de ascender de nivel como no fuera por su fuga a una ciudad o por un hecho todavía más extraordinario: su ennoblecimiento por un destacado hecho de armas o servicio al rey, que en condiciones normales le estaban completamente vedados. Lo mismo puede decirse del artesano o el mercader (que en algunos casos podía acumular fortuna, pero no alterar su origen humilde). El noble lo era generalmente por herencia, aunque en ocasiones podía alguien ennoblecerse como soldado de fortuna, después de una victoriosa carrera de armas (como fue el caso, por ejemplo, de Roberto Guiscardo). El clero, por su parte, era reclutado por cooptación, con un acceso distinto según el origen social: asegurado para los segundones de las casas nobles y restringido a los niveles inferiores del bajo clero para los del pueblo llano; pero en casos particulares o destacados, el ascenso en la jerarquía eclesiástica estaba abierto al mérito intelectual. Todo esto le daba al sistema feudal una extraordinaria estabilidad, en donde había un lugar para cada hombre, y cada hombre en su lugar, al tiempo que una extraordinaria flexibilidad, porque permitía al poder político y económico atomizarse a través de toda Europa, desde España hasta Polonia. El año mil El legendario año mil, final del primer milenio, que se utiliza convencionalmente para el paso de la Alta a la Baja Edad Media, en realidad tan solo es una cifra redonda para el cómputo de la era cristiana, que no era de universal utilización: los musulmanes utilizaban su propio calendario islámico lunar que comienza en la Hégira (622); en algunas partes de la Cristiandad se utilizaban eras locales (como la era hispánica, que cuenta desde el 38 a. C.). Pero ciertamente, el milenarismo y los pronósticos del final de los tiempos estaban presentes; incluso el propio papa durante el cambio de milenio Silvestre II, el francés Gerberto de Aurillac, interesado en todo tipo de conocimientos, se ganó una reputación esotérica. La astrología siempre pudo encontrar fenómenos celestes extraordinarios en los que apoyar su prestigio (como los eclipses), pero ciertamente otros eventos de la época estuvieron entre los más espectaculares de la historia: el cometa Halley, que se acerca a la Tierra periódicamente cada ocho décadas, alcanzó su brillo máximo en la visita de 837, despidió el primer milenio en 989 y llegó a tiempo de la batalla de Hastings en 1066; mucho más visibles aún, las supernovas SN 1006 y SN 1054, que reciben el número del año en que se registraron, fueron más detalladamente reflejadas en fuentes chinas, árabes e incluso indoamericanas que en las escasas europeas (a pesar de que la de 1054 coincidió con la batalla de Atapuerca). Todo el siglo X, más bien por las condiciones reales que por las imaginarias, puede considerarse parte de una época oscura, pesimista, insegura y presidida por el miedo a todo tipo de peligros, reales e imaginarios, naturales y sobrenaturales: miedo al mar, miedo al bosque, miedo a las brujas y los demonios y a todo lo que, sin entrar dentro de lo sobrenatural cristiano, quedaba relegado a lo inexplicable y al concepto de lo maravilloso, atribuido a seres de dudosa o quizá posible existencia (dragones, duendes, hadas, unicornios). El hecho no tenía nada de único: mil años más tarde, el siglo XX hizo nacer miedos comparables: al holocausto nuclear, al cambio climático (versiones contemporáneas del fin del mundo); al comunismo (la caza de brujas con la que se identificó al macarthismo), a la libertad (Miedo a la Libertad es la base del fascismo en la interpretación de Erich Fromm), comparación que ha sido puesta de manifiesto por los historiadores e interpretada por los sociólogos (Sociedad del riesgo de Ulrich Beck). La Edad Media cree firmemente que todas las cosas en el universo tienen un significado sobrenatural, y que el mundo es como un libro escrito por la mano de Dios. Todos los animales tienen un significado moral o místico, al igual que todas las piedras y todas las hierbas (y esto es lo que explican los bestiarios, los lapidarios y los herbarios). Se llega así a atribuir significados positivos o negativos también a los colores... Para el simbolismo medieval una cosa puede tener incluso dos significados opuestos según el contexto en el que se contempla (de ahí que el león a veces simbolice a Jesucristo y a veces al demonio). Umberto Eco La coyuntura del año mil En la coyuntura histórica del año mil, las estructuras políticas más fuertes del periodo anterior se estaban demostrando muy débiles: el islam se descompuso en califatos (Bagdad, El Cairo y Córdoba), que para el año 1000 se estaban demostrando incapaces de contener a los reinos cristianos, especialmente al Reino de León, en la península ibérica (fracaso final de Almanzor) y al Imperio bizantino en el Mediterráneo Oriental. También sufre la expansión bizantina el Imperio búlgaro, que queda destruido. Los particularismos nacionales francés, polaco y húngaro dibujan fronteras protonacionales que, curiosamente, son muy similares a las del año 2000. En cambio, el Imperio carolingio se había disuelto en principados feudales ingobernables, que los Otónidas se proponían incluir en una segunda Restauratio Imperii (Otón I, en el 962), esta vez sobre bases germanas. La persistencia del miedo y la función de la risa Nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura chè la diritta via era smarrita. En el medio del camino de nuestra vida me encontraba en un bosque oscuro porque el recto camino había extraviado. Dante, Divina Comedia Disciplinantes o flagelantes en un grabado del siglo XV Penitenciagite (haced penitencia) Hay que castigar el cuerpo para salvar el alma. El ascetismo ve en la mortificación un camino para superar las tentaciones de la carne y obtener méritos en vida para la redención de la culpa por los pecados. Los miedos y la inseguridad no acabaron con el año mil, ni tampoco hubo que esperar para volver a encontrarlos a la terrible peste negra y a los flagelantes del siglo XIV Incluso en el óptimo medieval del expansivo siglo XIII lo más habitual era encontrar textos como el de Dante, o como los siguientes: Este himno de autor desconocido, atribuido a muy diversos personajes (el papa Gregorio ,que pudiera ser Gregorio Magno, a quien también se atribuye el canto gregoriano, u otro de los de ese nombre,, al fundador del Cister San Bernardo de Claraval, a los monjes dominicos Umbertus y Frangipani y al franciscano Tomás de Celano) e incorporado a la liturgia de la misa: Dies iræ, dies illa, Solvet sæclum in favilla, Teste David cum Sibylla ! Quantus tremor est futurus, quando judex est venturus, cuncta stricte discussurus ! ... Confutatis maledictis, flammis acribus addictis, voca me cum benedictis. Oro supplex et acclinis, cor contritum quasi cinis, gere curam mei finis. Lacrimosa dies illa, qua resurget ex favilla judicandus homo reus. Huic ergo parce, Deus. Día de la ira; día aquel en que los siglos se reduzcan a cenizas; como testigos el rey David y la Sibila. ¡Cuánto terror habrá en el futuro cuando el juez haya de venir a juzgar todo estrictamente! ... Tras confundir a los malditos arrojados a las llamas voraces hazme llamar entre los benditos Te lo ruego, suplicante y de rodillas, el corazón acongojado, casi hecho cenizas: hazte cargo de mi destino. Día de lágrimas será aquel día en que resucitará, del polvo para el juicio, el hombre culpable. A ese, pues, perdónalo, oh Dios. Un monstruoso demonio arranca la lengua con una tenaza a un condenado (posiblemente un castigo por haber pecado de palabra), mientras otro demonio le arrastra tirándole del pelo. Capitel románico de la iglesia de Bois-Sainte-Marie, Brionnais, Francia. Pero también participa de la misma concepción pesimista del mundo este otro, proveniente de un ambiente totalmente opuesto, recogido en una colección de poemas goliardos (monjes y estudiantes de vida desordenada): O Fortuna velut luna statu variabilis, semper crescis aut decrescis; vita detestabilis nunc obdurat et tunc curat ludo mentis aciem egestatem, potestatem dissolvit ut glaciem. Sors immanis et inanis, rota tu volubilis, status malus, vana salus semper dissolubilis, obumbrata et velata O Fortuna, como la Luna variable creces sin cesar o desapareces. ¡Vida detestable! primero embota y después estimula, como juego, la agudeza de la mente. la pobreza y el poder se derriten como el hielo. Destino monstruoso y vacío, una rueda girando es lo que eres, si está mal colocada la salud es vana, siempre puede ser disuelta, eclipsada y velada Fortuna imperatrix mundi: Fortuna emperatriz del mundo (Carmina Burana) Lo sobrenatural estaba presente en la vida cotidiana de todos como un constante recordatorio de la brevedad de la vida y la inminencia de la muerte, cuyo radical igualitarismo se aplicaba, en contrapunto con la desigualdad de las condiciones, como un cohesionador social, al igual que la promesa de la vida eterna. La imaginación se excitaba con las imágenes más morbosas de lo que ocurriría en el juicio final, los tormentos del infierno y de los méritos que los santos habían obtenido con su vida ascética y sus martirios (que bien administrados por la Iglesia podían ahorrar las penas temporales del purgatorio). Esto no solo operaba en los amedrentados iletrados que únicamente disponían del evangelio en piedra de las iglesias; la mayor parte de los lectores cultos daban todo crédito a las escenas truculentas que llenaban los martirologios y a las inverosímiles historias de la Leyenda Áurea de Jacopo da Vorágine. El miedo era inherente a la violencia estructural permanente del feudalismo, que aunque se encauzara por mecanismos aceptables socialmente y estableciera un orden estamental teóricamente perfecto, era un permanente recuerdo de la posibilidad de subversión del orden, periódicamente renovado con guerras, invasiones y sublevaciones internas. En particular, las sátiras contra el rústico eran manifestaciones de la mezcla de desprecio y desconfianza con que clérigos y nobles veían al siervo, reducido a un monstruo deforme, ignorante y violento, capaz de las mayores atrocidades, sobre todo cuando se agrupaba. A furia rusticorum libera nos, Domine De la furia de los campesinos, líbranos Señor. Adición a la liturgia eclesiástica de la Letanía de los Santos. Pero al mismo tiempo, se sostenía, como parte esencial del edificio ideológico (era la justificación de la elección papal) que la voz del pueblo era la voz de Dios (Vox populi, vox Dei). El espíritu medieval debía asumir la contradicción de impulsar manifestaciones públicas de piedad y devoción y al tiempo permitir generosas concesiones al pecado. Los carnavales y otras parodias grotescas (la fiesta del asno o el charivari) permitían todo tipo de licencias, incluso la blasfemia y la burla a lo sagrado, invirtiendo las jerarquías (se elegían reyes de los tontos obispillos u obispos de la fiesta) haciendo triunfar todo lo que el resto del año estaba prohibido, era considerado feo, desagradable o daba miedo, como reacción saludable al terror cotidiano al más allá y garantía de que, pasados los excesos de la fiesta, se volvería dócilmente al trabajo y la obediencia. Seriedad y tristeza eran prerrogativas de quien practicaba un sagrado optimismo (hay que sufrir pues luego nos aguarda la vida eterna), mientras que la risa era la medicina del que vivía con pesimismo una vida miserable y difícil. Frente al mayor rigorismo del cristianismo primitivo, los teólogos medievales especulaban sobre si Cristo río o no (la Epístola de Léntulo, uno de los evangelios apócrifos sostenía que no; mientras que algunos padres de la iglesia defendían el derecho a una santa alegría), lo que justificaba textos cómicos eclesiásticos, como la Coena Cypriani y la Joca monachorum. Plena Edad Media (siglos XI al XIII) Faenas agrícolas del mes de junio, ilustración de Las muy ricas horas del Duque de Berry (1411-1416). Fenómenos tradicionales y de larga duración, como la necesidad de murallas, lo rudimentario de las técnicas y la explotación de los campesinos se contraponen a fenómenos nuevos y dinámicos, como el crecimiento de la ciudad y su atrevida arquitectura, que no obstante se siguen basando en la extracción y distribución del excedente productivo del campo. Aún queda mucho para culminar la transición del feudalismo al capitalismo. Artículo principal: Plena Edad Media Se asigna el nombre de Plenitud de la Edad Media al periodo de la Historia de Europa que ocupa los siglos XI al XIII. Esa Plena Edad Media o Plenitud del Medievo terminaría en la crisis del XIV o crisis de la Edad Media, en la que se pueden apreciar procesos «decadentes», y es habitual calificarla de ocaso u otoño. No obstante, los últimos siglos medievales están llenos de hechos y procesos dinámicos, con enormes repercusiones y proyecciones en el futuro, aunque lógicamente son los hechos y procesos que pueden entenderse como nuevos, que prefiguran los nuevos tiempos de la modernidad. Al mismo tiempo, los hechos, procesos, agentes sociales, instituciones y valores caracterizados como medievales han entrado claramente en decadencia; sobreviven, y sobrevivirán por siglos, en buena medida gracias a su institucionalización (por ejemplo, el cierre de los estamentos privilegiados o la adopción del mayorazgo), lo que no deja de ser un síntoma de que es entonces, y no antes, que se consideró necesario defenderlos tanto. Puente del Diablo en Borgo a Mozzano, ejemplo de ingeniería medieval, probablemente encargado por la condesa Matilda de Toscana en el siglo XI La justificación de esa denominación es lo excepcional del desarrollo económico, demográfico, social y cultural de Europa que tiene lugar en ese período, coincidente con un clima muy favorable (se ha hablado del óptimo medieval) que permitía cultivar vides en Inglaterra. También se ha hablado, en concreto para el siglo XII, de la revolución del siglo XII o renacimiento del siglo XII El simbólico año mil (cuyos terrores milenaristas son un mito historiográfico frecuentemente exagerado) no significa nada por sí mismo, pero a partir de entonces se da por terminada la Edad Oscura de las invasiones de la Alta Edad Media: húngaros y normandos están ya asentados e integrados en la cristiandad latina. La Europa de la Plena Edad Media es expansiva también en el terreno militar: las cruzadas en el Próximo Oriente, la dominación angevina de Sicilia y el avance de los reinos cristianos en la península ibérica (desaparecido el Califato de Córdoba) amenazan con reducir el espacio islámico a la ribera sur de la cuenca del Mediterráneo y el interior de Asia. El modo de producción feudal se desarrolla sin encontrar de momento límites a su extensión (como ocurrirá con la crisis del siglo XIV). La renta feudal se distribuye por los señores fuera del campo, donde se origina: las ciudades y la burguesía crecen con el aumento de la demanda de productos artesanales y del comercio a larga distancia, nacen y se desarrollan las ferias, las rutas comerciales terrestres y marítimas e instituciones como la Hansa. Europa Central y Septentrional entran en el corazón de la civilización Occidental. El Imperio bizantino se mantiene entre el islam y los cruzados, extendida su influencia cultural por los Balcanes y las estepas rusas donde se resiste el empuje mongol. El arte románico y el primer gótico son protegidos por las órdenes religiosas y el clero secular. Cluny y el Císter llenan Europa de monasterios. El Camino de Santiago articula la península ibérica con Europa. Nacen las Universidades (Bolonia, Sorbona, Oxford, Cambridge, Salamanca, Coímbra). La escolástica llega a su cumbre con Tomás de Aquino, tras recibir la influencia de las traducciones del árabe (averroísmo). El redescubrimiento del derecho romano (Bártolo de Sassoferrato, Baldo degli Ubaldi) empieza a influir en los reyes que se ven a sí mismos como emperadores en su reino. Los conflictos crecen a la par que la sociedad: herejías, revueltas campesinas y urbanas, la salvaje represión de todas ellas y las no menos salvajes guerras feudales son constantes. La expansión del sistema feudal Dinamismo interno: económico, social, tecnológico e intelectual Un campesino ordeña una oveja, mientras en la cabaña un niño come ante una mesa (los muebles no eran muy habituales en las casas de los pobres). Ilustración del siglo XIV de Tacuinum sanitatis, un tratado médico árabe de Ibn Butlan que se tradujo al latín y tuvo una gran difusión por Europa Occidental en la Baja Edad Media, como otras obras de origen similar. Lejos de ser un sistema social anquilosado (el cierre del acceso a los estamentos es un proceso que se produce como reacción conservadora de los privilegiados, tras la crisis final de la Edad Media, ya en el Antiguo Régimen), el feudalismo medieval demostró suficiente flexibilidad como para permitir el desarrollo de dos procesos, que se retroalimentaron mutuamente favoreciendo una rápida expansión. Por una parte, el asignar un lugar a cada persona dentro del sistema, permitió la expulsión de todos aquellos para quienes no había lugar, enviándolos como colonos y aventureros militares a tierras no ganadas para la Cristiandad Occidental, expandiendo así brutalmente sus límites. Por la otra, el asegurar un cierto orden y estabilidad social para el mundo agrario tras el fin del periodo de las invasiones; aunque ni mucho menos se acabaron las guerras ,consustanciales al sistema feudal, el nivel habitual de violencia en periodos bélicos tendía a controlarse por las propias instituciones ,código de honor, tregua de Dios, acogimiento a sagrado, y en periodos normales tendía a ritualizarse , desafíos, duelos, rieptos, justas, torneos, paso honroso,, aunque no desaparecía ni en las relaciones internacionales ni dentro de los reinos, con unas ciudades que basaban su seguridad y pax urbana en sus fuertes murallas, sus toques de queda y su expeditiva justicia, y unos inseguros campos en los que señores de horca y cuchillo imponían sus prerrogativas e incluso abusaban de ellas (malhechores feudales), no sin encontrar la resistencia antiseñorial de los siervos, a veces mitificada (Robin Hood). A diferencia del modo de producción esclavista, el modo de producción feudal ponía en el productor ,campesino, la responsabilidad en el aumento de la producción: sea buena o mala la cosecha, debe pagar unas mismas rentas. Es por ello que el sistema por sí solo estimula el trabajo y la incorporación de lo que la experiencia demuestre como buenas prácticas agrícolas, incluso la incorporación de nuevas técnicas que mejoren el rendimiento de la tierra. Si el aumento de la producción es permanente y no coyuntural (una sola buena cosecha por causas climáticas), quien empezará a recibir estímulos será el señor feudal, que detectará ese aumento de los excedentes cuya extracción es la base de su renta feudal (mayor uso del molino, mayor circulación por los caminos y puentes, mayor consumo en tiendas y tabernas; de todos los cuales cobra impuestos o aspirará a hacerlo), incluso se verá impulsado a subir la renta. Cuando lo que ocurre es que los campesinos, empujados por el aumento de sus familias, presionan los límites de los mansos roturando tierras antes incultas (eriales, pastos, bosques, humedales desecables), el señor podrá imponer nuevas condiciones, e incluso impedirlo, porque forman parte de su reserva o de sus usos monopolísticos (caza, alimento de sus caballos). Caballos de tiro equipados con colleras para permitir el aprovechamiento eficaz de su fuerza. La fotografía es actual, pero la tecnología empleada es similar a la mejorada en la Edad Media. Esa dinámica lucha de clases entre siervos y señores dinamizaba la economía y hacía posible el inicio de una concentración de riquezas acumuladas a partir de las rentas agrícolas; pero nunca de manera comparable a la acumulación de capital propia del capitalismo, pues no se hacía con ellas inversión productiva (como hubiera ocurrido de disponer los campesinos del uso del excedente), sino atesoramiento en manos de nobleza y clero. Tal cosa, en última instancia, a través de los programas de construcción (castillos, monasterios, iglesias, catedrales, palacios) y el gasto suntuario en productos de lujo ,caballos, armas sofisticadas, joyas, obras de arte, telas de calidad, tintes, sedas, tapices, especias, no pudo dejar de estimular el rudimentario comercio a larga distancia, la circulación monetaria y la vida urbana; en definitiva, el resurgimiento económico de Europa Occidental. Irónicamente, ambos procesos terminarían por minar las bases del feudalismo, y llevarlo hacia su destrucción.[Nota 11] No obstante, no hay que imaginar que se produjo nada parecido a la revolución agrícola previa a la revolución industrial: el hecho de que ni campesinos ni señores pudieran convertir en capital el excedente (unos porque se lo extraían y otros porque su posición social era incompatible con las actividades económicas) hacía lenta y costosa cualquier innovación, además del hecho de que cualquier innovación chocaba con prejuicios ideológicos y una mentalidad fuertemente tradicionalista, ambas cosas propias de la sociedad preindustrial. Solo en el transcurso de siglos, y debido al ensayo y error del buen hacer artesanal de anónimos herreros y talabarteros sin ningún tipo de conexión con la investigación científica, se produjo la incorporación de escasas pero decisivas mejoras técnicas como la collera (que posibilita el aprovechamiento eficaz de la fuerza de los caballos de tiro, que empiezan a sustituir a los bueyes) o el arado de vertedera (que sustituye al arado romano en las tierras húmedas y pesadas del norte de Europa, no así en las secas y ligeras del sur). El barbecho de año y vez siguió siendo el método de cultivo más utilizado; la rotación de cultivos era desconocida, el abonado era un recurso excepcional, dada la escasez de animales, cuyo estiércol era el único abono disponible; el regadío estaba limitado a algunas de las zonas mediterráneas de cultura islámica; se escatimaba la utilización de hierro en herramientas y aperos de labranza, dado su coste inasumible por los campesinos; el nivel técnico, en general, era precario. El molino de viento fue una transferencia tecnológica que, como tantas otras en otros campos (pólvora, papel, brújula, grabado), provenía de Asia. Aun con su alcance limitado, el conjunto de innovaciones y cambios se concentró especialmente en un periodo que algunos historiadores han venido en llamar el Renacimiento del siglo XII o la Revolución del XII, momento en el que el dinamismo económico y social, a partir del motor principal, que es el campo, produce el despertar de un mundo urbano hasta entonces marginal en Europa Occidental, y el surgimiento de fenómenos intelectuales como la universidad medieval y la escolástica. Artículo principal: Revolución del siglo XII La universidad Artículo principal: Universidad medieval Aula universitaria. Laurentius de Voltolina, segunda mitad del siglo XIV Siguiendo el precedente de la organización carolingia de las escuelas palatinas, catedralicias y monásticas (debida a Alcuino de York -787-), más que el de otras instituciones semejantes existentes en el mundo islámico,[Nota 12] las primeras universidades de la Europa cristiana fueron fundadas para el estudio del derecho, la medicina y la teología. La parte central de la enseñanza envolvía el estudio de las artes preparatorias (denominadas artes liberales por cuanto eran mentales o espirituales y liberaban del trabajo manual propio de las artesanías, consideradas oficios viles y mecánicos); estas artes liberales eran el trivium (gramática, retórica y lógica) y el quadrivium (aritmética, geometría, música y astronomía). Después, el alumno entraba en contacto con estudios más específicos. Además de centros de enseñanza, eran también el lugar de investigación y producción del saber, y foco de vigorosos debates y polémicas, lo que a veces requirió incluso las intervenciones del poder civil y eclesiástico, a pesar de los fueros de los que estaban dotadas y que las convertían en instituciones independientes, bien dotadas económicamente con una base patrimonial de tierras y edificios. La transformación cultural generada por las universidades ha sido resumida de este modo: En 1100, la escuela seguía al maestro; en 1200, el maestro seguía a la escuela. Las más prestigiosas recibían el nombre de Studium Generale, y su fama se extendía por toda Europa, requiriendo la presencia de sus maestros, o al menos la comunicación epistolar, lo que inició un fecundo intercambio intelectual facilitado por el uso común de la lengua culta, el latín. Entre 1200 y 1400 fueron fundadas en Europa 52 universidades; 29 de ellas de fundación papal, las demás de fundación imperial o real. La primera fue posiblemente Bolonia (especializada en Derecho, 1088), a la que siguió Oxford (antes de 1096), de la que se escindió su rival Cambridge (1209), París, de mediados del siglo XII (uno de cuyos colegios fue La Sorbona, 1275), Salamanca (1218, precedida por el Estudio General de Palencia de 1208), Padua (1222), Nápoles (1224), Coímbra (1308, trasladada desde el Estudio General de Lisboa de 1290), Alcalá de Henares (1293, refundada por el Cardenal Cisneros en 1499), La Sapienza (Roma, 1303), Valladolid (1346), la Universidad Carolina (Praga, 1348), la Universidad Jagellónica (Cracovia, 1363), Viena (1365), Heidelberg (1386), Colonia (1368) y, ya al final del periodo medieval, Lovaina (1425), Barcelona (1450), Basilea (1460) y Upsala (1477). En medicina gozaba de un gran prestigio la Escuela Médica Salernitana, con raíces árabes, que provenía del siglo IX; y en 1220 empezó a rivalizar con ella la Facultad de Medicina de Montpellier. Véase también: Universidad La escolástica Artículo principal: Escolástica Los intelectuales medievales buscaban entender los principios geométricos y armónicos con los que Dios habría creado el Universo. El compás en esta ilustración de un manuscrito del siglo XIII es un símbolo del acto de creación de Dios. La escolástica fue la corriente teológico-filosófica dominante del pensamiento medieval, tras la patrística de la Antigüedad tardía, y se basó en la coordinación de fe y razón, que en cualquier caso siempre suponía la clara sumisión de la razón a la fe (Philosophia ancilla theologiae -la filosofía es esclava de la teología-). Pero también es un método de trabajo intelectual: todo pensamiento debía someterse al principio de autoridad (Magister dixit ,lo dijo el Maestro,), y la enseñanza se podía limitar en principio a la repetición o glosa de los textos antiguos, y sobre todo de la Biblia, la principal fuente de conocimiento, pues representa la Revelación divina; a pesar de todo ello, la escolástica incentivó la especulación y el razonamiento, pues suponía someterse a un rígido armazón lógico y una estructura esquemática del discurso que debía exponerse a refutaciones y preparar defensas. Desde el comienzo del siglo IX al fin del XII los debates se centraron en la cuestión de los universales, que opone a los realistas encabezados por Guillermo de Champeaux, a los nominalistas representados por Roscelino y a los conceptualistas (Pedro Abelardo). En el siglo XII tiene lugar la recepción de textos de Aristóteles antes desconocidos en Occidente, primero indirectamente a través de los filósofos judíos y musulmanes, especialmente Avicena y Averroes, pero en seguida directamente traducido del griego al latín por san Alberto Magno y por Guillermo de Moerbeke, secretario de santo Tomás de Aquino, verdadera cumbre del pensamiento medieval y elevado al rango de Doctor de la Iglesia. El apogeo de la escolástica coincide con el siglo XIII en que se fundan las universidades y surgen las órdenes mendicantes: dominicos (que siguieron una tendencia aristotélica -los anteriormente citados-) y franciscanos (caracterizados por el platonismo y la tradición patrística -Alejandro de Hales o san Buenaventura-). Ambas órdenes coparán las cátedras y la vida de los colegios universitarios, y de ellas procederán la mayoría de los teólogos y filósofos de la época. El siglo XIV representará la crisis de la escolástica a través de dos franciscanos británicos: el doctor subtilis Juan Duns Escoto y Guillermo de Occam. Precedente de ambos sería la Escuela de Oxford (Robert Grosseteste y Roger Bacon) centrada en el estudio de la naturaleza, defendiendo la posibilidad de una ciencia experimental apoyada en la matemática, contra el tomismo dominante. La polémica de los universales se terminó decantando por los nominalistas, lo que dejaba un espacio a la filosofía más allá de la teología. Ergo Domine, qui das fidei intellectum, da mihi, ut, quantum scis expedire, intelligam, quia es sicut credimus, et hoc es quod credimus. Et quidem credimus te esse aliquid quo nihil maius cogitari possit. An ergo non est aliqua talis natura, quia dixit insipiens in corde suo: non est Deus ? [...] Si enim vel in solo intellectu est, potest cogitari esse et in re; quod maius est. Si ergo id quo maius cogitari non potest, est in solo intellectu: id ipsum quo maius cogitari non potest, est quo maius cogitari potest. Sed certe hoc esse non potest. Existit ergo procul dubio aliquid quo maius cogitari non valet, et in intellectu et in re. Luego Señor, tú que das el entendimiento a la fe, dame de entender, tanto como consideres bueno, que tú eres como creemos y lo que creemos. Y bien, creemos que tú eres algo mayor que lo cual no puede pensarse cosa alguna. Ahora, ¿acaso no existe esta naturaleza, porque dijo el necio en su corazón: no hay Dios ? [...] Si existe sólo en la mente, no se cree que exista en la realidad; El más grande. Por lo tanto, si aquello de lo que no se puede concebir un mayor existe sólo en el entendimiento, eso mismo de lo que no se puede concebir un mayor es aquello que no se puede concebir nada mayor. Pero obviamente esto no es posible. Existe, por tanto, más allá de toda duda, algo que no se puede pensar más grande que existe tanto en el entendimiento como en la realidad. Anselmo de Canterbury, inicio del argumento ontológico para probar la existencia de Dios. Proslogio, capítulo II (1078). La frase entrecomillada es una cita bíblica (Salmos 13:1). Dicitur Exodi III, ex persona Dei, ego sum qui sum. Respondeo dicendum quod Deum esse quinque viis probari potest. Prima autem et manifestior via est, quae sumitur ex parte motus. Certum est enim, et sensu constat, aliqua moveri in hoc mundo. [...] Impossibile est ergo quod, secundum idem et eodem modo, aliquid sit movens et motum, vel quod moveat seipsum. Omne ergo quod movetur, oportet ab alio moveri. Si ergo id a quo movetur, moveatur, oportet et ipsum ab alio moveri et illud ab alio. Hic autem non est procedere in infinitum, quia sic non esset aliquod primum movens; et per consequens nec aliquod aliud movens, quia moventia secunda non movent nisi per hoc quod sunt mota a primo movente. [...] Quinta via sumitur ex gubernatione rerum. Videmus enim quod aliqua quae cognitione carent, scilicet corpora naturalia, operantur propter finem, quod apparet ex hoc quod semper aut frequentius eodem modo operantur, ut consequantur id quod est optimum; unde patet quod non a casu, sed ex intentione perveniunt ad finem. Ea autem quae non habent cognitionem, non tendunt in finem nisi directa ab aliquo cognoscente et intelligente, sicut sagitta a sagittante. Ergo est aliquid intelligens, a quo omnes res naturales ordinantur ad finem, et hoc dicimus Deum. Se dice en Éxodo 3,14 de la persona de Dios: Yo soy el que soy. La existencia de Dios puede ser probada de cinco maneras distintas. La primera y más clara es la que se deduce del movimiento. Pues es cierto, y lo perciben los sentidos, que en este mundo hay movimiento. [...] Igualmente, es imposible que algo mueva y sea movido al mismo tiempo, o que se mueva a sí mismo. Todo lo que se mueve necesita ser movido por otro. Pero si lo que es movido por otro se mueve, necesita ser movido por otro, y éste por otro. Este proceder no se puede llevar indefinidamente, porque no se llegaría al primero que mueve, y así no habría motor alguno pues los motores intermedios no mueven más que por ser movidos por el primer motor. Ejemplo: Un bastón no mueve nada si no es movido por la mano. Por lo tanto, es necesario llegar a aquel primer motor al que nadie mueve. En éste, todos reconocen a Dios. [...] La quinta se deduce a partir del ordenamiento de las cosas. Pues vemos que hay cosas que no tienen conocimiento, como son los cuerpos naturales, y que obran por un fin. Esto se puede comprobar observando cómo siempre o a menudo obran igual para conseguir lo mejor. De donde se deduce que, para alcanzar su objetivo, no obran al azar, sino intencionadamente. Las cosas que no tienen conocimiento no tienden al fin sin ser dirigidas por alguien con conocimiento e inteligencia, como la flecha por el arquero. Por lo tanto, hay alguien inteligente por el que todas las cosas son dirigidas al fin. Le llamamos Dios. Tomás de Aquino, quinta de las Cinco Vías (Quinquae viae) para probar la existencia de Dios. Summa Theologiae (Suma Teológica, 1274), Quaestio 2, Articulus 3. Véase los argumentos cosmológicos y teleológicos actuales. El surgimiento de la burguesía Signoria de Florencia, una institución municipal que ejerce el poder soberano en esta ciudad estado italiana, dominada por una potente burguesía artesanal y comercial que se va ennobleciendo y convirtiendo en patriciado urbano. La burguesía es el nuevo agente social formado por los artesanos y mercaderes que surgen en el entorno de las ciudades, bien en las antiguas ciudades romanas que habían decaído, bien en nuevos núcleos creados en torno a castillos o cruces de caminos -los propiamente llamados burgos-. Muchas de estas ciudades incorporaron ese nombre - Hamburgo, Magdeburgo, Friburgo, Estrasburgo; en España Burgo de Osma o Burgos-. La burguesía estaba interesada en presionar al poder político (imperio, papado, las diferentes monarquías, la nobleza feudal local o instituciones eclesiásticas -diócesis o monasterios- de las que dependieran sus ciudades) para que se facilitara la apertura económica de los espacios cerrados de las urbes, se redujeran los tributos de portazgo y se garantizaran formas de comercio seguro y una centralización de la administración de justicia e igualdad de las normas en amplios territorios que les permitieran desarrollar su trabajo, al tiempo que garantías de que los que vulnerasen dichas normas serían castigados con igual dureza en los distintos territorios. Aquellas ciudades que abrían las puertas al comercio y a una mayor libertad de circulación, veían incrementar la riqueza y prosperidad de sus habitantes y las del señor, por lo que con reticencias pero de manera firme se fue difundiendo el modelo. Las alianzas entre señores eran más comunes, no ya tanto para la guerra, como para permitir el desarrollo económico de sus respectivos territorios, y el rey fue el elemento aglutinador de esas alianzas. Los burgueses pueden considerarse como hombres libres en cuanto estaban parcialmente fuera del sistema feudal, que literalmente los asediaba -se ha comparado a las ciudades con islas en un océano feudal-, porque no participaban directamente de las relaciones feudo-vasalláticas: ni eran señores feudales, ni campesinos sometidos a servidumbre, ni hombres de iglesia. La sujeción como súbdito del poder político era semejante a un lazo de vasallaje, pero más bien como señorío colectivo que hacía que la ciudad respondiera como un todo a las demandas de apoyo militar y político del rey o del gobernante a la que estuviera vinculada, y que a su vez participara en la explotación feudal del campo circundante (alfoz en España). La expresión alemana Stadtluft macht frei Los aires de la ciudad dan libertad, o te hacen libre[Nota 13] (paráfrasis de la frase evangélica la verdad os hará libres), indicaba que quienes podían radicarse en las ciudades, a veces huyendo literalmente de la sujeción de la servidumbre. El siervo huido se consideraba libre de retornar con su señor si conseguía domiciliarse en una corporación urbana por un año y un día. tenían todo un nuevo mundo de oportunidades que explotar, aunque no en régimen de libertad, entendida esta en su forma contemporánea. La sujeción a las normas gremiales y a las leyes urbanas podía ser más dura incluso que las del campo: la pax urbana significaba la rigidez en la aplicación de la justicia, que mantenía los caminos y las puertas de entrada flanqueados con cadáveres de ajusticiados y un severo toque de queda, con cierre de puertas al anochecer y rondas de vigilancia. Eso sí: concedía a los burgueses la oportunidad de ejercer parcela de poder, incluyendo el uso de las armas en la milicia urbana (como las hermandades castellanas que se unificaron en la Santa Hermandad ya en el siglo XV), que en no pocas ocasiones se utilizaron en contra de las huestes feudales, con el beneplácito de las emergentes monarquías autoritarias. En el caso más precoz y espectacular fueron las comunas italianas, que se independizaron de hecho del Sacro Imperio Romano Germánico a partir de la batalla de Legnano (1176). Eva hilando ante la cuna de uno de sus hijos. Ilustración del folio 8 del Salterio Hunter. La introducción de la rueca para hilar fue una de las innovaciones introducidas desde Asia en la Plena Edad Media. La de la ilustración es una hilandera primitiva, sin rueda. Ambas eran utilizadas tanto en la artesanía urbana como en las labores domésticas de las mujeres en campo y ciudad. Como todos los trabajos, dio origen a tensiones sociales: When Adam delved, and Eve span / Who was then a gentleman? (Cuando Adán cavaba y Eva hilaba, ¿quién era entonces caballero?) era una rima popular con la que el clérigo John Ball movilizó a los campesinos ingleses de la revuelta de 1381. En los burgos surgieron muchas instituciones sociales nuevas. El desarrollo del comercio llevó aparejado consigo el del sistema financiero y la contabilidad. Los artesanos se unieron en asociaciones llamadas gremios, ligas, corporaciones, cofradías, o artes, según el lugar geográfico. El funcionamiento interno de los talleres gremiales implicaba un aprendizaje de varios años del aprendiz a cargo de un maestro (el dueño del taller), que implicaba el paso de aquel a la condición de oficial cuando demostrara conocer el oficio, lo que implicaba su consideración como trabajador asalariado, una condición de por sí ajena al mundo feudal que incluso se trasladó al campo (en principio de manera marginal) con los jornaleros que no disponían de tierras propias ni concedidas por el señor. La asociación de los talleres en los gremios, funcionaba de manera completamente contraria al mercado libre capitalista: se procuraba evitar todo rasgo posible de competencia fijando los precios, las calidades, los horarios y condiciones de trabajo, e incluso las calles donde podían radicarse. La apertura de nuevos talleres y el paso del rango de oficial al de maestro estaban muy restringidos, de modo que en la práctica se incentivaban las herencias y los enlaces matrimoniales endogámicos dentro del gremio. El objetivo era conseguir la supervivencia de todos, no el éxito del mejor. Más apertura demostró el comercio. Los buhoneros que iban de aldea en aldea, y los escasos aventureros que se atrevían a hacer viajes más largos eran los mercaderes más habituales de la Alta Edad Media, antes del año 1000. En tres siglos, para comienzos del siglo XIV las ferias de Champaña y de Medina habían creado rutas terrestres estables y más o menos seguras que (a lomos de mulas o con carretas en el mejor de los casos) recorrían Europa de norte a sur (en el caso castellano siguiendo las cañadas trashumantes de la Mesta, en el caso francés enlazando los emporios flamenco y norte-italiano a través de las prósperas regiones borgoñonas y renanas, todas ellas salpicadas de ciudades). La Hansa o liga hanseática estableció a su vez rutas marítimas de una estabilidad y seguridad similar (con mayor capacidad de carga, en barcos de tecnología innovadora) que unían el Báltico y el mar del Norte a través de los estrechos escandinavos, conectando territorios tan lejanos como Rusia y Flandes y rutas fluviales que conectaban todo el norte de Europa (ríos como el Rin y el Vístula), permitiendo el desarrollo de ciudades como Hamburgo, Lübeck y Danzing, y estableciendo consulados comerciales denominados kontor. En el Mediterráneo se llamaron Consulado del Mar: el primero en Trani en 1063 y luego Pisa, Mesina, Chipre, Constantinopla, Venecia, Montpellier, Valencia (1283), Mallorca (1343) y Barcelona (1347). Cuando el estrecho de Gibraltar fue seguro, se pudieron conectar marítimamente ambas Europas, con rutas entre las ciudades italianas (sobre todo Génova), Marsella, Barcelona, Valencia, Sevilla, Lisboa, los puertos del Cantábrico (Santander, Laredo, Bilbao), los del Atlántico francés y los del canal de la Mancha (ingleses y flamencos, sobre todo Brujas y Amberes). El contacto cada vez más fluido de gentes de distintas naciones (como comenzaron a llamarse a las agrupaciones de comerciantes de cercano origen geográfico que se entendían en la misma lengua vulgar, al igual que ocurría en las secciones de las órdenes militares) terminó produciendo que ambas instituciones funcionaran de hecho, como primitivas organizaciones internacionales. Todo ello desarrolló un incipiente capitalismo comercial (véase también Historia del capitalismo) con el incremento o surgimiento ex novo de la economía monetaria, la banca (crédito, préstamos, seguros, letras de cambio), actividades que mantuvieron siempre recelos morales (pecado de usura para todas las que significara lucro indebido, y en que únicamente podían incurrir los judíos cuando prestaban a otros que no fueran de su religión, oficio prohibido tanto a los cristianos como a los musulmanes). La aparición de burgueses ricos y de una plebe urbana pobre originó un nuevo tipo de tensiones sociales, que produjeron revueltas urbanas. En cuanto a los aspectos ideológicos, la expresión del inconformismo burgués con su puesto marginal en la sociedad feudal está en el origen de las herejías a lo largo de toda la Baja Edad Media (cátaros, valdenses, albigenses, dulcinianos, husitas, wycliffianos). Los intentos de responder a esas demandas del mundo urbano por parte de la Iglesia, así como de controlarlas y en su caso reprimirlas, produjeron la aparición de las órdenes mendicantes (franciscanos y dominicos) y de la Inquisición. A veces, la imposibilidad de conseguir el control hizo optar por el exterminio, como ocurrió en Beziers en 1209, siguiendo la respuesta del legado pontificio Arnaud Amaury: - ¿Cómo distinguiremos a los herejes de los católicos? - Matadlos a todos, que Dios reconocerá a los suyos Las catedrales y la búsqueda de la altura Catedral de Siena Santa María del Fiore En la Edad Media, la oposición entre lo alto y lo bajo se proyecta en el espacio: se construyen torres y murallas muy elevadas, muy visibles, para manifestar que se quiere escapar de lo bajo... lo alto y la altura designan lo que es grande y hermoso... se expresa en la construcción de los castillos y las catedrales... Esa oposición es el correlato de la que existe entre el cielo y la tierra. (...) Luego, se buscó la luz, e incluso se acabó por identificar a Dios con la luz. Los progresos técnicos, la búsqueda de espacios abiertos y el uso cada vez más sofisticado del hierro y los diversos metales dieron nacimiento, entre los siglos XI y XIII a las grandes catedrales. La rivalidad entre castillos señoriales tuvo su correlato urbano en la rivalidad entre casas fortificadas, con torres desafiantes, que han sobrevivido en los espectaculares conjuntos de San Gimignano o de Cáceres. Mucho más extendida estuvo la rivalidad de las catedrales, cuya construcción se demoraba por siglos, desarrollándose de un modo orgánico, sin que los planes originarios se terminaran, haciendo que el resultado final fuera habitualmente la suma de estilos muy diferentes. Se llegaron a producir verdaderas carreras de prestigio, como la que se prolongó por cientos de años entre las de Siena y Florencia. Las dimensiones extraordinarias de ambas hicieron imposible que se terminaran antes de la crisis bajomedieval, lo que determinó que los sieneses (izquierda: catedral de Siena Duomo di Santa María) optaran por conformarse con lo construido hasta entonces (para que pudiera utilizarse desde sus inicios, siempre se comenzaban las obras por el ábside, permitiendo consagrar el altar y dar culto mientras continuaban las obras). Lo que se pretendía era convertir el actual brazo mayor en el menor, y construir un brazo mayor verdaderamente descomunal (proyecto de 1339 que tuvo que abandonarse; el diseño inicial era de 1215-1263). Mientras tanto, los florentinos (derecha: catedral de Florencia Duomo di Santa María dei Fiori), humillados por no ser capaces de cubrir el gigantesco espacio central del crucero (un desproporcionado tambor octogonal sobreelevado), tuvieron que esperar a que Filippo Brunelleschi consiguiera resolver el desafío técnico con una impresionante cúpula que abre la época del Renacimiento (concurso de 1419 y construcción entre 1420 y 1436). Véase también catedrales de España. Nuevas entidades políticas Poderes universales, monarquías feudales y ciudades-Estado En la Plena Edad Media se observó una gran disparidad en la escala a que se ejercía el poder político: los poderes universales (Pontificado e Imperio) seguían reivindicando su primacía frente a las Monarquías feudales, que en la práctica funcionaban como estados independientes. Al mismo tiempo, entidades mucho más pequeñas en extensión demostraban ser muy dinámicas en las relaciones internacionales (las ciudades-estado italianas y las ciudades libres del Imperio Germánico), y el municipalismo demostró ser una fuerza muy a tener en cuenta en todos los territorios de Europa. El redescubrimiento del Digesto justinianeo (Digestum Vetus) permitió el estudio autónomo del Derecho (Pepo e Irnerio) y el surgimiento de la Escuela de los Glosadores y de la Universidad de Bolonia (1088). Ese suceso, que permitirá el redescubrimiento paulatino del Derecho romano, llevará a la formación del llamado Corpus Iuris Civilis y a la posibilidad de plantear un Ius commune (Derecho común), y justificar la concentración de poder y capacidad reglamentaria en la institución imperial, o en los monarcas, cada uno de los cuales empezará a considerarse como imperator in regno suo (emperador en su reino, definiciones de Bártolo de Sassoferrato y Baldo degli Ubaldi). Rex superiorem non recognoscens in regno suo est Imperator: El rey no reconoce superiores, en su reino es emperador. Decretal Per Venerabilem de Inocencio III, 1202. La difícil convivencia de Pontificado e Imperio (regnum et sacerdocium) a lo largo de los siglos dio origen entre 1073 y 1122 a la querella de las investiduras. Distintas formulaciones ideológicas (teoría de las dos espadas, Plenitudo potestatis, Dictatus papae, condenas de la simonía y el nicolaísmo) constituían un edificio levantado durante siglos por el que el papa pretendía marcar la supremacía de la autoridad religiosa sobre el poder civil (lo que se ha venido denominando agustinismo político), mientras que el Emperador pretendía hacer valer la legitimidad de su cargo, que pretendía derivar del antiguo Imperio romano (Translatio imperii), así como el hecho material de su capacidad militar para imponer su poder territorial e incluso tutelar la vida religiosa (tanto en los aspectos institucionales como los dogmáticos), a semejanza de su equivalente en Oriente. El acceso de distintas dinastías a la dignidad imperial debilitó el poder de los emperadores, sujetos a un sistema de elección que les hacía dependientes de un delicado juego de alianzas entre los dignatarios que alcanzaron el título de príncipe elector, unos laicos (príncipes territoriales, independientes en la práctica) y otros eclesiásticos (obispos de ciudades libres). No obstante, periódicamente se asistía a intentos de recuperar el poder imperial (Otón III y Enrique II entre los últimos otónidas), que en ocasiones llegaban a enfrentamientos espectaculares (Enrique IV, de la dinastía salia, o Federico I Barbarroja y Federico II de la dinastía Hohenstaufen). La oposición entre güelfos y gibelinos, cada uno asociado a uno de los poderes en liza (papa y emperador), presidió la vida política de Alemania e Italia desde el siglo XII hasta bien entrada la Baja Edad Media. Ambas pretensiones distaron mucho de hacerse efectivas, agotadas en su propio debate y superadas por la mayor eficacia política de las entidades urbanas y los reinos del resto de Europa. Artículo principal: Dominium mundi Véase también: Derecho penal Parlamentarismo Apareció el parlamentarismo, una forma de representación política que con el tiempo se convirtió en el precedente de la división de poderes consustancial a la democracia de la Edad Contemporánea. La primacía en el tiempo la tiene el Alþingi islandés (930), que seguía el modelo de los thing o asambleas de guerreros germanos; pero desde finales del siglo XI se fue gestando un nuevo modelo institucional, derivado de la obligación feudal de consilium, que implicaba a los tres órdenes feudales, y se generalizó por Europa occidental: las Cortes de León (1188), el Parlamento inglés (1258) -previamente las relaciones de poder entre rey y nobleza habían sido reguladas en la Carta EMagna, 1215, o las Provisiones de Oxford, 1258- y los Estados Generales franceses (1302). La Reforma Gregoriana y las reformas monásticas Abadía de Cluny. Artículo principal: Reforma gregoriana Hildebrando de Toscana, ya desde su posición bajo los pontificados de León IX y Nicolás II, y más tarde como papa Gregorio VII (con lo que cubre toda la segunda mitad del siglo XI), emprendió un programa de centralización de la Iglesia, con la ayuda de los benedictinos de Cluny, que se extendieron por toda Europa Occidental implicando a las monarquías feudales (||sdestacadamente en los reinos cristianos peninsulares, a través del Camino de Santiago). Las siguientes reformas monásticas, como la cartuja (San Bruno) y sobre todo la cisterciense (San Bernardo de Claraval) significarán nuevos fortalecimientos de la jerarquía eclesiástica y su implantación dispersa en todo el territorio europeo como una impresionante fuerza social y económica ligada a las estructuras feudales, vinculada a las familias nobles y a las dinastías regias y con una base de riqueza territorial e inmobiliaria, a la que se añadía el cobro de los derechos propios de la Iglesia (diezmos, primicias, derechos de estola, y otras cargas locales, como el voto de Santiago en el noroeste de España). El fortalecimiento del poder papal intensificó las tensiones políticas e ideológicas con el Imperio Germánico y con la Iglesia oriental, que en este caso terminarán llevando al Cisma de Oriente. Las Cruzadas trajeron como consecuencia la creación de un tipo especial de órdenes religiosas, que, además de someterse a una regla monástica (habitualmente la cisterciense, incluyendo el cumplimiento teórico de los votos monásticos) exigían a sus componentes una vida castrense más que ascética: fueron las órdenes militares, fundadas tras la toma de Jerusalén en 1099 (caballeros del Santo Sepulcro, templarios -1104- y hospitalarios -1118-). También se constituyeron en otros contextos geográficos (órdenes militares españolas y caballeros teutónicos). La adaptación a la pujante vida urbana de los siglos XII y XIII será misión de un nuevo ciclo de fundaciones en el clero regular: las órdenes mendicantes, cuyos miembros no eran monjes, sino frailes (franciscanos de San Francisco de Asís y dominicos de Santo Domingo de Guzmán, a las que siguieron otras, como los agustinos); y de nuevas instituciones: las Universidades y la Inquisición. Innovaciones dogmáticas y devocionales Anunciación por Conrad von Soest, 1403. La Virgen, modelo de virtudes femeninas, cuya inocencia es simbolizada por el lirio, escucha el mensaje divino traído por el arcángel San Gabriel y acepta su destino (concebir a Cristo por obra y gracia del Espíritu Santo -la paloma-) con humildad y obediencia: Ecce ancilla Domini; fiat mihi secundum verbum tuum: He aquí la esclava del Señor; hágase en mí según tu palabra (Lucas 1:38). A partir del siglo XI y el siglo XII se introdujeron en el cristianismo latino innovaciones dogmáticas y devocionales de gran trascendencia: La imposición del rito romano frente a la anterior multiplicidad de liturgias (rito hispánico, rito bracarense, rito ambrosiano, etc.) La imposición del celibato sacerdotal en el Concilio de Letrán (1123). El hallazgo del papel del purgatorio como estadio intermedio de las almas entre cielo e infierno, que intensificará la función intermediadora de la Iglesia a través de las oraciones y misas y los méritos de la Comunión de los Santos por ella administrados. Mariología La intensificación del papel de la Virgen María, que pasa a ser una corredentora con atributos investigados por la mariología y aún no dogmatizados (Inmaculada Concepción, Asunción de la Virgen), con nuevas devociones y oraciones (Avemaría, yuxtaposición de textos evangélicos que se introduce en occidente en el; Salve, adoptada por Cluny en 1135; y Rosario, introducido por Santo Domingo contra los albigenses), una fiebre de fundaciones de iglesias en su nombre, y con un amplísimo tratamiento artístico. En la época del amor cortés la devoción a la Virgen apenas podía distinguirse, al menos en las formas, de la que el caballero sentía por su dama.[Nota 14] La mariología había nacido en la Antigüedad tardía con la patrística, y el culto popular de la virgen fue uno de los factores clave de la suave transición del paganismo al cristianismo, que suele interpretarse como una adaptación del patriarcal monoteísmo del judaísmo al matriarcal panteón de las diosas-vírgenes-madre del Mediterráneo clásico: la cananea Astarté, la babilonia Istar, las griegas Rea y Gaia, la frigia Cibeles, la Artemisa de Éfeso, la Deméter de Eleusis, la egipcia Isis, etc., si bien hay dos diferencias fundamentales entre el culto cristiano a María y los cultos paganos: la clara conciencia de la absoluta trascendencia de Dios, que opera como factor que elimina cualquier tendencia idolátrica y la oposición por parte del cristianismo a una divinización de la vida que ponga en peligro el carácter absolutamente libre de la decisión creadora de Dios.[Nota 15] La controversia Cristotokos-Theotokos (María como Madre de Cristo o Madre de Dios), y el amplio tratamiento de esta en el arte bizantino habían caracterizado a la iglesia oriental. El protagonismo de la Virgen quedaba ampliamente compensado con la misoginia del tratamiento de otras figuras femeninas, destacadamente Eva, la Magdalena y Santa María Egipcíaca. La renuncia al cuerpo (la carne enemiga del alma) y a las riquezas, que da oportunidad al arrepentimiento y la redención (y confía su gestión a la Madre Iglesia) solía ser el aspecto más destacable también en las vidas de otras santas y mártires. Sacramentos y cohesión social. Minorías religiosas El pecado original, por Bertram von Minden, 1375. El tema de Adán y Eva daba la ocasión más habitual de representación de desnudos durante la Edad Media. Por último, la institucionalización de los sacramentos, especialmente la penitencia y la comunión pascual que se plantean como trámites anuales que el fiel ha de cumplir ante su párroco y confesor. La vivencia comunitaria de los sacramentos, sobre todo los que significan cambios vitales (bautismo, matrimonio, extrema unción), y los rituales funerarios, cohesionaban fuertemente a las sociedades locales tanto aldeanas como urbanas, sobre todo cuando se enfrentaban a la convivencia con otras comunidades religiosas ,judíos en toda Europa y musulmanes en España,. La celebración de las festividades en días distintos (viernes los musulmanes, sábados los judíos, domingos los cristianos), los distintos tabúes alimentarios (cerdo, alcohol, rituales de matanza que obligan a separar las carnicerías) y la separación física de las comunidades -guetos, aljamas o juderías y morerías- planteaban una situación que, incluso con tolerancia religiosa, distaba mucho de ser un trato igualitario. Los judíos cumplieron una función social de chivo expiatorio que dio salida a las tensiones sociales en determinados momentos, con el estallido de pogromos (revueltas antijudías, que tras la conversiones masivas dieron paso a revueltas anticonversas) o con las políticas de expulsión (Inglaterra -1290-, Francia -1394- y España -1492- y Portugal en 1496). La existencia de minorías religiosas dentro del cristianismo, en cambio, no podía ser aceptada, puesto que la comunidad política se identificaba con la unidad en la fe. Los definidos como herejes, por tanto, eran perseguidos por todos los medios. Delito, pecado y sexo En cuanto a las desviaciones del comportamiento que no supusieran desafíos de opinión sino delitos o pecados (conceptos identificables y de imposible deslindamiento), su tratamiento era objeto de las jurisdicciones civil (que aplicaba el fuero correspondiente, la legislación del reino o el derecho común) y religiosa (que aplicaba el Derecho Canónico en cuestiones ordinarias, o el procedimiento inquisitorial en caso necesario), cuya coordinación era a veces compleja, como ocurría con las desviaciones de la conducta sexual considerada correcta (masturbación, homosexualidad, incesto, estupro, amancebamiento, adulterio y otros asuntos matrimoniales). En cualquier caso, la vivencia de la sexualidad y la desnudez del cuerpo tuvo tratamientos muy distintos en cada época y lugar; y diferentes expectativas para cada nivel social (se consideraba que era propio de los campesinos un comportamiento animal, es decir, natural, y se pretendía que los nobles y clérigos tuvieran más voluntad para controlar sus instintos). También costumbres como los baños (conocidos desde las termas romanas y reintroducidos por los árabes) y prácticas como la prostitución fueron objeto de críticas morales y reglamentaciones más o menos permisivas, llegando en el caso de los baños progresivamente hasta la prohibición (se les acusaba de inmorales y de producir el afeminamiento de los guerreros), y en el de la prostitución al confinamiento en determinados barrios, la obligación de llevar determinadas prendas y la detención de sus actividades en determinadas fechas (Semana Santa). La erradicación de la prostitución no se concebía posible, dado lo inevitable del pecado, y su papel de mal menor que evitaba que el deseo irrefrenable de los varones fuera en contra del honor de las doncellas y las mujeres respetables. Por lo general, los historiadores suelen coincidir que el periodo de la Plena Edad Media fue una etapa de mayor libertad de costumbres que no tuvo que esperar a El Decamerón (1348), y que en algunas cuestiones, como la condición femenina, significó una verdadera promoción, tanto frente a la Alta Edad Media como frente a la Edad Moderna; aunque el extendido mito de que se llegara a dudar si la mujer tenía alma es un error filológico. Expansión geográfica de la Europa feudal Willelm Dux, el Duque Guillermo de Normandía dirige sus tropas a la batalla de Hastings que le convertirá en rey de Inglaterra (1066). Tapiz de Bayeux, bordado pocos años después. La expansión geográfica se llevó a cabo, o se intentó llevar a cabo, al menos, en varias direcciones, siguiendo no tanto un propósito determinado por concepciones nacionalistas inexistentes en la época, sino la dinámica propia de las casas feudales. Los normandos, vikingos asentados en Normandía, dieron origen a una de las casas feudales más expansivas de Europa, que se extendió por Francia, Inglaterra e Italia, enlazada con las de Anjou-Plantagenet y Aquitania. Las casas de Navarra y Castilla (dinastía Jimena), Francia, Borgoña y Flandes (Capetos, casa de Borgoña ,extendida por la península ibérica,, Valois) y Austria (casa de Habsburgo) son otros buenos ejemplos, y todas ellas se vieron vinculadas por alianzas, enlaces matrimoniales y enfrentamientos sucesorios o territoriales, consustanciales a las relaciones feudo-vasalláticas y expresión de la violencia inherente al feudalismo. En el contexto espacial de la Europa nórdica y centro-oriental tuvieron un desarrollo similar la casa de Sweyn Estridsson danesa, la Bjälbo noruega y los Sverker y Erik suecos; y más tarde la Dinastía Jogalia o Jagellón (Hungría, Bohemia, Polonia y Lituania). En España, simultáneamente a la disolución del Califato de Córdoba (en guerra civil desde el 1010 y extinguido el 1031), se creó un vacío de poder que los reinos feudales cristianohispánicos de Castilla, León, Navarra, Portugal y Aragón (fusionado dinásticamente con el condado de Barcelona) intentaron aprovechar, expandiéndose frente a los reinos de taifas musulmanes en la llamada Reconquista. En las islas británicas, el reino de Inglaterra intentó repetidas veces invadir a Gales, Escocia e Irlanda, con mayor o menor éxito. Reconstrucción de un drakkar, embarcación usada habitualmente por los vikingos. En Europa del Norte, acabadas las invasiones de los vikingos, las riquezas saqueadas por estos sirvieron para adquirir productos y servicios occidentales, creando en el mar Báltico una próspera red comercial que atrajo a los escandinavos a la civilización occidental, mientras su expansión hacia el oeste por el Atlántico (Islandia y Groenlandia) no pasó de la mítica Vinlandia (asentamiento fracasado en América del Norte, en torno al año 1000). Los vikingos orientales (varegos) fundaron numerosos reinos en la Rusia europea y llegaron hasta Constantinopla. Los vikingos occidentales (normandos) se instalaron en Normandía, Inglaterra, Sicilia y el sur de la actual Italia, creando reinos centralizados y eficientes (Rolón, Guillermo el Conquistador y Roger I de Sicilia). En el este, en el año 955, Otón el Grande batió a los húngaros en la batalla del Río Lech y reincorporó Hungría a Occidente, al tiempo que comenzaba la germanización de Polonia, hasta entonces pagana. Posteriormente, desde tiempos de Enrique el León (siglo XII), los alemanes se fueron abriendo paso a través de las tierras de los vendos, hasta el mar Báltico, en un proceso de colonización conocido como Ostsiedlung (que será mitificado posteriormente con el romántico nombre de Drang nach Osten, o Afán de ir hacia el Este, lo que sirvió para justificar la teoría nazi del espacio vital alemán Lebensraum). Pero sin lugar a dudas, el movimiento de expansión más espectacular, aunque finalmente fallido, fueron las Cruzadas, en donde selectos miembros de la nobleza guerrera occidental cruzaron el mar Mediterráneo e invadieron el Medio Oriente, creando reinos de efímera duración. Luis IX de Francia (San Luis) dirigió a sus caballeros a un desembarco naval contra el fuerte egipcio de Damietta en la Quinta Cruzada (1217-1221). Las Cruzadas Artículo principal: Cruzadas Las Cruzadas fueron expediciones emprendidas, en cumplimiento de un solemne voto, para liberar Tierra Santa de la dominación musulmana. El origen de la palabra remonta a la cruz hecha de tela y usada como insignia en la ropa exterior de los que tomaron parte en esas iniciativas, a partir de la petición del papa Urbano II y las predicaciones de Pedro el Ermitaño. Las sucesivas cruzadas tuvieron lugar entre los siglos XI y XIII. Fueron motivadas por los intereses expansionistas de la nobleza feudal, el control del comercio con Asia y el afán hegemónico del papado sobre las iglesias de Oriente. Balance de la expansión geográfica Espada, cetro, orbe y corona (con su característica cruz inclinada) de San Esteban de Hungría, rey húngaro convertido al cristianismo y coronado en diciembre del año 1000 por el papa Silvestre II, en un acto similar al que protagonizó Carlomagno exactamente doscientos años antes, significando en este caso la expansión del cristianismo occidental y las instituciones feudales por la Europa centro-oriental. El balance de esta expansión fue espectacular, por comparación a la vulnerabilidad de la oscura época anterior: Tras medio siglo de instituciones carolingias, hacia 843 (Tratado de Verdún), los territorios que podían identificarse más o menos próximamente con ellas (lo que podría denominarse una formación social cristiano occidental) se extendían por Francia, el oeste y sur de Alemania, el sur de Gran Bretaña, las montañas septentrionales de España y el norte de Italia. Un siglo después, en la época de la batalla del Río Lech (955), no había región de Europa Occidental a salvo de las nuevas oleadas de invasores bárbaros, que parecían conducir a una nueva crisis de civilización.[Nota 16] Sin embargo, en los dos siglos siguientes al fatídico año mil el panorama había cambiado completamente: para la época de la batalla de Navas de Tolosa (1212), habían sido incorporadas a la civilización europea toda Italia hasta Sicilia, la Gran Bretaña no inglesa (Escocia y Gales), Escandinavia (que se expandía por el Atlántico Norte hasta Groenlandia), buena parte de Europa Oriental (Polonia, Bohemia, Moravia y Hungría, quedando los pueblos eslavos de los Balcanes y Rusia en la órbita del cristianismo oriental e institucionalizando sus propios reinos) y media península ibérica (en el transcurso del siglo XIII lo sería toda excepto el tributario reino nazarí de Granada, quedando marcado definitivamente el predominio cristiano sobre el estrecho de Gibraltar con la batalla del Salado -1340-). Otros territorios periféricos (como Lituania o Irlanda) estaban sometidos a una presión militar cada vez mayor por parte de los reinos centrales de la cristiandad latina. Más allá de los límites de Europa Occidental, las incursiones militares de huestes latinas de muy variada composición habían puesto en sus manos lugares tan lejanos como Constantinopla y los ducados Atenas y de Neopatria o Jerusalén y los Estados Cruzados. Europa en 1328. Europa en 1328. Europa en la década de 1430. Europa en la década de 1430. Europa en la década de 1470. Europa en la década de 1470. Baja Edad Media (siglos XIV y XV) Artículo principal: Baja Edad Media Muerte de Wat Tyler, líder de la revuelta campesina de 1381 en Inglaterra. La Baja Edad Media es un término que a veces produce confusión, pues procede de un equívoco etimológico entre alemán y castellano: baja no significa decadente, sino reciente; por oposición al alta de la Alta Edad Media, que significa antigua (en alemán alt: viejo, antiguo). No obstante, es cierto que desde alguna perspectiva historiográfica puede verse al conjunto del periodo medieval como el ciclo de nacimiento, desarrollo, auge e inevitable caída de una civilización, modelo interpretativo que inició Gibbon para el Imperio romano (donde es más obvia la oposición entre Alto Imperio y Bajo Imperio) y que se ha aplicado con mayor o menor fortuna a otros contextos históricos y artísticos.[Nota 17] El símil astronómico de ocaso, que Johan Huizinga convierte en otoño, es utilizado con mucha frecuencia en la historiografía, con un valor analógico que más que una decadencia en lo económico o lo intelectual refleja un claro agotamiento de los rasgos específicamente medievales frente a sus sustitutos modernos. La crisis del siglo XIV Artículo principal: Crisis del siglo XIV El final de la Edad Media llega con el comienzo de la transición del feudalismo al capitalismo, otro periodo secular de transición entre modos de producción que no finalizará hasta el final del Antiguo Régimen y el comienzo de la Edad Contemporánea, con lo que tanto este último periodo medieval como la Edad Moderna entera cumplen un papel similar y cubren una similar extensión temporal (500 años) a lo que significó la Antigüedad Tardía para el comienzo de la Edad Media. La ley de rendimientos decrecientes empezó a mostrar sus efectos a medida que el dinamismo de los campesinos forzó la roturación de tierras marginales y las lentas mejoras técnicas no podían sucederse a un ritmo semejante. La coyuntura climática cambió, acabando con el denominado óptimo medieval que permitió la colonización de Groenlandia y el cultivo de vides en Inglaterra. Las malas cosechas condujeron a hambrunas que debilitaron físicamente a las poblaciones, preparando el terreno para que la Peste negra de 1348 fuera una catástrofe demográfica en Europa. La repetición sucesiva de epidemias caracterizó un ciclo secular. Consecuencias de la crisis El matrimonio Arnolfini, por Jan van Eyck (1430), representa el interior de una acomodada casa burguesa, que ambientan bien algunos de los nuevos valores de esa emergente clase social: la propiedad privada ganada con el trabajo, la familia nuclear, la moderación, la discreción y la privacidad. La escena transcurre en Flandes, un emporio comercial y artesanal, que suscitó el florecimiento de una nueva forma de pintura, la de los primitivos flamencos que entre otras innovaciones, iniciaron la pintura al óleo, lo que permitía detalles sutilísimos para hacer cada vez más fieles los retratos, un género que siglos antes no tenía ninguna demanda social. Las consecuencias no fueron negativas para todos. Los supervivientes acumularon inesperadamente capital en forma de herencias, que pudo en algunos casos invertirse en empresas comerciales, o acumularon inesperadamente patrimonios nobiliarios. Las alteraciones de los precios de mercado de los productos, sometidos a tensiones nunca vistas de oferta y demanda cambió la forma de percibir las relaciones económicas: los salarios (un concepto, como el de circulación monetaria ya de por sí disolvente de la economía tradicional) crecían al tiempo que las rentas feudales pasaron a ser inseguras, obligando a los señores a decisiones difíciles. Alternativamente primero tendieron a ser más comprensivos con sus siervos, que a veces estuvieron en situación de imponer una nueva relación, liberados de la servidumbre; mientras que en un segundo momento, sobre todo tras algunas rebeliones campesinas fracasadas y duramente reprimidas, impusieron en algunas zonas una nueva refeudalización, o cambios de estrategia productiva como el paso de la agricultura a la ganadería (expansión de la Mesta). El negocio lanero produjo curiosas alianzas internacionales e interestamentales (señores ganaderos, mercaderes de la lana, artesanos de paños) que suscitaron verdaderas guerras comerciales (en ese sentido se ha podido interpretar las cambiantes alianzas y divisiones internas Inglaterra-Francia-Flandes durante la guerra de los Cien Años, en la que Castilla se implicó en su propia guerra civil). Únicamente los nobles con más capacidad (demostrada la mayor parte de las veces por el despojo de nobles con menos capacidad) pudieron convertirse en una gran nobleza o aristocracia de grandes casas nobiliarias, mientras que la pequeña nobleza se empobrecía, reducida a la mera supervivencia o a la búsqueda de nuevos tipos de ingresos en la creciente administración de las monarquías, o a los tradicionales de la Iglesia. En las instituciones del clero también se va abriendo un abismo entre el alto clero de obispos, canónigos y abades y los curas de parroquias pobres; y el bajo clero de frailes o clérigos vagabundos, de opiniones teológicas difusas, o bien supervivientes materialistas en la práctica, goliardos o estudiantes sin oficio ni beneficio. En las ciudades, la alta burguesía y la baja burguesía viven un similar proceso de separación de fortunas, que hace imposible mantener que un aprendiz o incluso un oficial o un maestro de taller pobre tenga algo que ver con un mercader enriquecido por el comercio a larga distancia de la Hansa o las ferias de Champaña y de Medina, o un médico o un letrado salidos de la universidad para entrar en la alta sociedad. Se va abriendo paso la posibilidad (antes inaudita) de que la condición social dependa más de la capacidad económica (no necesariamente ligada siempre a la tierra) que del origen familiar. Frente al mundo medieval de los tres órdenes, basado en una economía agraria y firmemente ligada a la posesión de la tierra, emerge un mundo de ciudades basado en una economía comercial. Los centros de poder se desplazan hacia los nuevos burgos. Estos reequilibrios se vieron reflejados en los campos de batalla, ya que los caballeros feudales empezaron a ser superados por el desarrollo de técnicas militares como el arco de tiro largo, arma que los ingleses usaron para barrer a los franceses en la batalla de Agincourt, en 1415, o la pica, usada por la infantería de mercenarios suizos. Es en esta época cuando aparecen los primeros ejércitos profesionales, compuestos por soldados a los que no les une un pacto de vasallaje con su señor sino la paga. A partir del siglo XIII se registran en Occidente los primeros usos de la de pólvora, invención china extendida desde la India por los árabes, pero de forma muy discontinua. Roger Bacon la describe en 1216) y hay relatos del uso de armas de fuego en la defensa musulmana de Sevilla (1248) y Niebla (1262, véase El cañón en la Edad Media). Con el tiempo, el oficio militar se envilece, devaluando las funciones de la nobleza con las de la caballería y los castillos, que quedan obsoletos. El aumento de los costes y las tácticas de batallas y asedios traerá como consecuencia el aumento del poder del rey frente a la aristocracia. La guerra pasa a depender no de las huestes feudales, sino de los crecientes impuestos, pagados por los no privilegiados. Díptico de Melun, de Jean Fouquet (1450). Panel izquierdo: Étienne Chevalier, el donante, con San Esteban, su santo patronímico. En otra época, la perspectiva jerárquica hubiera distanciado a un simple mortal, por muy poderoso que fuera, de personajes celestiales. Mismo díptico, Panel derecho: La Virgen con el Niño. La modelo fue Agnès Sorel, amante del rey Carlos VII de Francia, lo que aumenta el atrevimiento de la representación, que aun así resultaba asumible por la sensibilidad de la época. Nuevas ideas Las nuevas ideas religiosas -que se adaptan mejor a la forma de vida de la burguesía que a la de los privilegiados- ya estuvieron en el fermento de las herejías que se habían producido previamente, a partir del siglo XII (cátaros, valdenses), y que habían encontrado eficaz respuesta en las nuevas órdenes religiosas mendicantes, insertas en el entorno urbano; pero en los últimos siglos medievales el husismo o el wycliffismo tienen una mayor proyección hacia lo que será la Reforma protestante del siglo XVI El milenarismo de los flagelantes convivía con el misticismo de Tomás de Kempis y con los desórdenes y corrupción de costumbres en la Iglesia que culminaron en el Cisma de Occidente. Fue devastador el impacto que tuvo en la cristiandad occidental el espectáculo de dos (y hasta tres) papas excomulgándose mutuamente (y a emperadores, reyes y obispos, y con ellos a todos sus sacerdotes y fieles), uno en la llamada cautividad de Aviñón a la que le sometía el rey de Francia (fille ainée de lEglise, hija mayor de la Iglesia), otro en Roma y un tercero elegido por el Concilio de Pisa (1409). La situación no se recondujo totalmente ni siquiera con el Concilio de Constanza (1413), que si hubieran prosperado las tesis conciliaristas se habría convertido en una especie de parlamento europeo supranacional, cuasi-soberano y competente en toda clase de temas. Hasta la humilde Peñíscola se llegó a convertir por algún tiempo en el centro del mundo cristiano -para los escasos seguidores del papa Luna-. Los intentos de imprimir mayor racionalidad al catolicismo ya venían estando presentes desde la cumbre de la escolástica de los siglos XII y XIII con Pedro Abelardo, Tomás de Aquino o Roger Bacon; pero ahora esa escolástica se enfrenta a su propia crisis y cuestionamiento interno, con Guillermo de Ockham o Juan Duns Escoto. La mentalidad teocéntrica iba lentamente dando paso a una nueva antropocéntrica, en un proceso que culminará con el humanismo del siglo XV en lo que ya puede denominarse Edad Moderna. Ese cambio no se limitó únicamente a las élites intelectuales: personalidades extravagantes, como Juana de Arco, se convierten en héroes populares (con el contrapunto de otras terribles, como Gilles de Rais -Barba Azul-); la mentalidad social va alejándose del conformismo temeroso para acoger otras concepciones que implican una nueva forma de afrontar el futuro y las novedades: Hoy comamos y bebamos y cantemos y holguemos, que mañana ayunaremos. Villancico de Juan del Encina El anonimato conscientemente buscado en el que vivieron silenciosamente generaciones durante siglos Non nobis, Domine, non nobis, sed nomini tuo da gloriam ¡No a nosotros, Señor, no a nosotros, sino a tu nombre da la gloria! Salmos 115:1, musicalizado y utilizado muy frecuentemente para uso litúrgico. Se adoptó como lema de los templarios y aparece en la obra Enrique V de Shakespeare. y que seguirá siendo la situación de los humildes durante los siglos siguientes, da paso a la búsqueda de la fama y de la gloria personal, no solo entre los nobles, sino en todos los ámbitos sociales: los artesanos comienzan a firmar sus productos (desde las obras de arte a las marcas artesanas), y cada vez es menos excepcional que cualquier acto de la vida deje su huella documental (libros parroquiales, registros mercantiles, escribanos, protocolos notariales, actos jurídicos). El desafío al monopolio económico, social, político e intelectual de los privilegiados, creaba lentamente nuevos espacios de poder en beneficio de los reyes, así como un lugar cada vez más amplio para la burguesía. Aunque la mayor parte de la población siguió siendo campesina, lo cierto es que el impulso y las novedades ya no provenían del castillo o el monasterio, sino de la Corte y la ciudad. Entretanto, el amor cortés (procedente de la Provenza del siglo XI) y el ideal caballeresco se revitalizaron y pasaron a convertirse en una ideología justificativa del modo de vida nobiliario justo cuando este empezaba a estar en cuestión, viviendo una época dorada, obviamente decadente, localizada en el período de esplendor del ducado de Borgoña, que reflejó Johan Huizinga en su magistral El otoño de la Edad Media. Véanse también: Gótico tardío, Gótico flamígero, Gótico internacional, Primitivos flamencos y Trecento. Véanse también: Quattrocento, Dante, Petrarca, Bocaccio y Chaucer. El fin de la Edad Media en la península ibérica Artículo principal: Crisis de la Edad Media en España Mientras que para el Mediterráneo Oriental el fin de la Edad Media supuso el avance imparable del islámico Imperio otomano, en el extremo occidental, los expansivos reinos cristianos de la península ibérica, tras un periodo de crisis y ralentización del avance secular hacia el sur, simplificaron el mapa político con la unión matrimonial de los Reyes Católicos (Fernando II de Aragón e Isabel I de Castilla), los acuerdos de estos con el de Portugal (Tratado de Alcáçovas, que suponían el reparto de influencias sobre el Atlántico) y la conquista de Granada. Navarra, dividida en una guerra civil entre bandos orientados e intervenidos por franceses y aragoneses, sería anexionada en su mayor parte a la creciente Monarquía Católica en 1512. Véanse también: Reino nazarí de Granada, Primera guerra civil castellana, Casa de Trastámara, Almogávar y Compromiso de Caspe. Véanse también: Guerra de sucesión castellana, Conquista de las Islas Canarias, Guerra Civil de Navarra y Guerra civil catalana. Véanse también: La Biga y la Busca, Guerra de los Remensas, Sentencia arbitral de Guadalupe, Revuelta Irmandiña y Revuelta de Pedro Sarmiento. Véanse también: Cristiano nuevo, Inquisición española, Expulsión de los judíos de España y Revuelta antijudía de 1391. Véanse también: Gótico isabelino, Plateresco y Manuelino. Capilla del Condestable en la Catedral de Burgos, gótico final (1482). Capilla del Condestable en la Catedral de Burgos, gótico final (1482). La Virgen de los Reyes Católicos, Maestro de la Virgen de los Reyes Católicos (anónimo hispano flamenco), 1491-1493, Museo del Prado. La Virgen de los Reyes Católicos, Maestro de la Virgen de los Reyes Católicos (anónimo hispano flamenco), 1491-1493, Museo del Prado. Portada manuelina de la iglesia de Golega. El retorcimiento de las columnas imita el de las gruesas maromas de los barcos, en una nación marinera volcada en la Era de los descubrimientos. Portada manuelina de la iglesia de Golega. El retorcimiento de las columnas imita el de las gruesas maromas de los barcos, en una nación marinera volcada en la Era de los descubrimientos. Decreto de la Alhambra por el que se expulsa a los judíos de España, el mismo año que se conquista Granada, se descubre América y Nebrija pública su Gramática Castellana: 1492. Es el final de la Edad Media y el comienzo de la Edad Moderna, con una unidad religiosa que acompañó a la unión de los reinos de la Monarquía Católica. Decreto de la Alhambra por el que se expulsa a los judíos de España, el mismo año que se conquista Granada, se descubre América y Nebrija pública su Gramática Castellana: 1492. Es el final de la Edad Media y el comienzo de la Edad Moderna, con una unidad religiosa que acompañó a la unión de los reinos de la Monarquía Católica. Medievalismo: el estudio de la Edad Media Artículo principal: Medievalismo La ciudad medieval francesa de Carcasona. Ciudades amuralladas, puentes bien guarnecidos y castillos son parte de la imagen bélica de la Edad Media. El aspecto actual es fruto de una recreación historicista del siglo XIX, cuando las murallas ya no eran funcionales, y la mayor parte de las ciudades europeas las derribaba. El deseo de recuperarlas es una muestra de medievalismo. Medievalismo es tanto la cualidad o carácter de medieval, como el interés por la época y los temas medievales y su estudio; y medievalista el especialista en estas materias.[Nota 18] El descrédito de la Edad Media fue una constante durante la Edad Moderna, en la que Humanismo, Renacimiento, Racionalismo, Clasicismo e Ilustración se afirman como reacciones contra ella, o más bien contra lo que entienden que significaba, o contra los rasgos de su propio presente que intentan descalificar como pervivencias medievales. No obstante desde fines del siglo XVI se producen interesantes recopilaciones de fuentes documentales medievales que buscan un método crítico para la ciencia histórica. El Romanticismo y el Nacionalismo del siglo XIX revalorizaron la Edad Media como parte de su programa estético y como reacción antiacadémica (poesía y drama románticos, novela histórica, nacionalismo musical, ópera), además de como única posibilidad de encontrar base histórica a las emergentes naciones (pintura de historia, arquitectura historicista, sobre todo el neogótico ,labor restauradora y recreadora de Eugène Viollet-le-Duc, y el neomudéjar). Los abusos románticos de la ambientación medieval (exotismo), produjeron ya a mediados del siglo XIX la reacción del realismo. Otro tipo de abusos son los que dan lugar a una abundante literatura pseudohistórica que llega hasta el presente, y que ha encontrado la fórmula del éxito mediático entremezclando temas esotéricos sacados de partes más o menos oscuras de la Edad Media (Archivo Secreto Vaticano, templarios, rosacruces, masones y el mismísimo Santo Grial).[Nota 19] Algunos de ellos se vincularon al nazismo, como el alemán Otto Rahn. Por otro lado, hay abundancia de otros tipos de producciones artísticas de ficción de diversa calidad y orientación inspiradas en la Edad Media (literatura, cine, cómic). También se han desarrollado en el siglo XX otros movimientos medievalistas: un medievalismo historiográfico serio, centrado en la renovación metodológica (fundamentalmente por la incorporación de la perspectiva económica y social aportada por el materialismo histórico y la Escuela de los Annales) y un medievalismo popular (espectáculos medievales, más o menos genuinos, como actualización del pasado en el que la comunidad se identifica, lo que se ha venido en llamar memoria histórica)."

ksampletext_wikipedia_geog_pais: str = "País. Un país (del francés pays) es un territorio con características geográficas y culturales propias, que puede o no constituir un Estado soberano o una entidad política dentro de un Estado. También es utilizado como sinónimo de Estado, conjunto de instituciones políticas dotadas de territorio, población y soberanía. A veces, partes de un Estado con una historia o cultura características son llamados países, especialmente por los oriundos del lugar, como por ejemplo Escocia, Gales, Inglaterra o Irlanda del Norte, países que conforman el Reino Unido. Puede referirse también a regiones o incluso comarcas sin grandes diferencias culturales con las de los alrededores. Este uso se da especialmente al hablar de las regiones naturales de Francia ,el País de Auge, País de Buch, País de Caux, País de Sault, Países del Loira, el País Vasco francés, etc., ya que la palabra «país» proviene del francés pays, idioma en el que el término también tiene esta polisemia. A su vez, el término francés procede del latín pagus. Nación Artículo principal: Nación Nación tiene dos acepciones: la nación política, en la escena jurídico-política, es el sujeto político en el que reside el poder constituyente de un Estado; la nación cultural, concepto socio-ideológico más subjetivo y ambiguo que el anterior, puede definirse como una comunidad humana con ciertas características culturales comunes a las que se les dota de un sentido ético-político. La palabra nación se emplea en la vida cotidiana con múltiples significados: estado, país, territorio o habitantes de ellos, etnia y otros. En España se da el caso de las nacionalidades históricas, realidad nacional, carácter nacional o, simplemente, nacionalidad; son términos acuñados ad hoc para la política de este país, usados para designar a aquellas comunidades autónomas con una identidad colectiva, lingüística y/o cultural diferenciada, según sus estatutos autonómicos, del resto del Estado. Además, también se utiliza la palabra país en el propio nombre de la comunidad autónoma del País Vasco. Por otro lado, las regiones catalanoparlantes (incluida Andorra) son a menudo denominadas Países Catalanes. Finalmente, tanto en el discurso político actual como en el habla popular en Galicia, es de uso extendido el término país para referirse a esta comunidad histórica. Un ejemplo de este uso se encuentra en el etiquetado de ciertos productos de origen gallego (e.g. queso del país, vino del país). Estado Artículo principal: Estado Un Estado es un conjunto de instituciones que poseen la autoridad para establecer las normas que regulan una sociedad, teniendo soberanía interna y externa sobre un territorio definido. El Estado incluye el control de instituciones tales como las Fuerzas armadas, administración pública, los tribunales y la policía. Estado nación Artículo principal: Estado nación Según algunas escuelas de ciencia política, un estado nación se caracteriza por tener un territorio claramente definido, una población constante, si bien no fija, y un gobierno. Otros atributos menores son un ejército permanente y un cuerpo de representación diplomática, es decir, una política exterior. El Estado Nación se crea, históricamente, mediante el tratado de Westfalia, al finalizar la guerra de los 30 años (1648). Mediante este tratado finaliza el antiguo orden feudal y se da paso a organizaciones territoriales y poblacionales definidas en torno a un gobierno que reconoce sus límites espaciales, y por lo tanto, de poder. Forma de gobierno Artículo principal: Forma de gobierno La forma de gobierno es un término que se refiere al conjunto de las instituciones políticas mediante las cuales un estado se organiza para ejercer sus poderes sobre una comunidad política. Existen diferentes formas de gobernar un estado, como pueden ser una monarquía, república u otros como un sistema unipartidista o una dictadura militar. Organizaciones internacionales Artículo principal: Organización internacional Diversas organizaciones internacionales, es decir, agrupaciones formadas por diversos estados, se han ido creando a lo largo del siglo XX. Las organizaciones internacionales pueden ser de diversos tipos: Científicas, como la Agencia Espacial Europea Económicas, como la APEC o la NAFTA y el Mercosur Lingüísticas, como la Comunidad de Países de Lengua Portuguesa Policíacas, como la Interpol Político-económicas, como la ONU, la Unión Europea o la Unión Africana País constituyente Artículo principal: País constituyente País constituyente es un término a veces usado, normalmente por instituciones oficiales, en contextos en los cuales un número de países compone una larga entidad o agrupación (forma parte o constituye parte de); así la OCDE ha usado el término refiriéndose a la antigua Yugoslavia, El Consejo de Europa organización establecida en 1949, de la que forman parte 49 Estados y centrada en la protección de los derechos humanos en Europa, no usa esta denominación en referencia a los 27 Estados miembros de la Unión Europea, sino que la Unión Europea es considerada una organización internacional. Los Tratados impulsados por el Consejo de Europa son firmados por los Estados miembros del Consejo de Europa y, en el caso de los 27 Estados miembros de la Unión Europea, la Unión Europea los ha firmado también, especificando la distribución de competencias en los casos en que la Unión Europea las tiene, como se puede ver en la lista de los tratados del Consejo de Europa firmados por la Unión Europea junto con estos aspectos (protocolos adicionales, declaraciones, etc...). Territorio dependiente Artículo principal: Territorio dependiente Los territorios dependientes son territorios que por diferentes razones no poseen privilegios de total independencia o soberanía y, por lo tanto, son gobernadas por otros estados, llamados metrópoli. Muchos de estos territorios pueden ser considerados como colonias. Los territorios dependientes cuentan con un sistema administrativo diferente al de la metrópoli o a las unidades que conforman la metrópoli. Por lo general gozan de menores derechos administrativos y políticos que una subunidad nacional. Este tipo de administración varía según el nivel de dependencia del territorio. Así existen territorios completamente deshabitados. La mayoría de estos estados dependientes corresponden a islas de baja población que no pueden sostener un gobierno autónomo. Frontera Artículo principal: Frontera Las fronteras son líneas invisibles que marcan el territorio de un país y que lo separan del o de los países colindantes. De esta forma se delimita el espacio en el que un país ejerce su soberanía. A pesar de ser líneas invisibles el control de fronteras evita que puedan pasar sin control personas y mercancías a través de las fronteras. Territorio disputado Artículo principal: Territorios disputados Reclamaciones territoriales que caen sobre la Antártida. Un territorio disputado es aquel territorio cuya soberanía es ambicionada por dos o más países. Normalmente la administración del territorio la lleva a cabo uno de los países que reclama la soberanía, mientras que el otro país no reconoce la soberanía sobre el territorio del otro país. Esto no suele ocurrir en áreas terrestres o marítimas sobre las que ninguno posee el control efectivo, como por ejemplo la Antártida, o solo lo tiene parcialmente. También se puede considerar como un territorio disputado a aquellas zonas que están administradas por dos gobiernos distintos, y por lo tanto están divididas; un ejemplo es la República Turca del Norte de Chipre y Chipre. Patriotismo Artículo principal: Patriotismo Véase también: Nacionalismo cultural El patriotismo es una conexión emocional positiva con el país al que pertenece una persona. El patriotismo es un sentimiento de amor, devoción y apego a la patria. Este apego puede ser una combinación de muchos sentimientos y lenguaje diferentes relacionados con la patria, incluidos aspectos étnicos, culturales, políticos o históricos. Abarca un conjunto de conceptos estrechamente relacionados con el nacionalismo, principalmente el nacionalismo cívico y, a veces, el nacionalismo cultural. Economía Producto interno bruto per cápita de 213 países (2020) (paridad de poder adquisitivo - dólares internacionales. Varias organizaciones buscan identificar tendencias para producir clasificaciones de países según la economía. Los países a menudo se distinguen como países en desarrollo o países desarrollados. El Departamento de Asuntos Económicos y Sociales de las Naciones Unidas produce anualmente el Informe sobre la situación y las perspectivas económicas mundiales, que clasifica a los estados como países desarrollados, economías en transición o países en desarrollo. El informe clasifica el desarrollo de los países en función del Producto nacional bruto (PNB) per cápita. La ONU identifica subgrupos dentro de categorías amplias en función de la ubicación geográfica o criterios ad hoc. La ONU describe las regiones geográficas para las economías en desarrollo como África, Asia Oriental, Asia Meridional, Asia Occidental, América Latina y el Caribe. El informe de 2019 reconoce solo a los países desarrollados de América del Norte, Europa, Asia y el Pacífico. La mayoría de las economías en transición y los países en desarrollo se encuentran en África, Asia, América Latina y el Caribe. El Banco Mundial también clasifica a los países según el PNB per cápita. El Atlas del Banco Mundial clasifica a los países como economías de bajos ingresos, economías de ingresos medianos bajos, economías de ingresos medianos altos o economías de ingresos altos. Para el año fiscal 2020, el Banco Mundial define las economías de bajos ingresos como países con un INB per cápita de $1,025 o menos en 2018; economías de ingresos medianos bajos como países con un PNB per cápita entre $1,026 y $3,995; economías de ingresos medianos altos como países con un PNB per cápita entre $3,996 y $12,375; Las economías de altos ingresos son aquellas con un ingreso nacional bruto (PNB) per cápita de 12.376 dólares o más. El Banco Mundial también identifica tendencias regionales. El Banco Mundial define sus regiones como Asia oriental y el Pacífico, Europa y Asia central, América Latina y el Caribe, Oriente Medio y el norte de África, América del Norte, Asia meridional y África subsahariana. Por último, el Banco Mundial distingue a los países en función de sus políticas operativas. Las tres categorías incluyen a los países de la Asociación Internacional de Fomento (AIF), los países del Banco Internacional de Reconstrucción y Fomento (BIRF) y los países de la categoría Blend."
ksampletext_wikipedia_geog_clima: str = "Clima. El clima se define como las condiciones meteorológicas medias que caracterizan a un lugar determinado. Es una síntesis del tiempo atmosférico, obtenida a partir de estadísticas a largo plazo. Los elementos meteorológicos a tomar en cuenta son la temperatura, la presión, el viento, la humedad y la precipitación. El clima difiere del tiempo meteorológico en que la meteorología solo describe las condiciones de corto plazo de estas variables en una región dada. Así, el clima de un lugar o una región está constituido por los datos estadísticos de la meteorología de dicho lugar o región analizados a lo largo de un plazo relativamente largo, de 30 años o más, como señala F. J. Monkhouse (1978). Los componentes meteorológicos que suelen tomarse como elementos climáticos son cinco, como ya se ha indicado arriba. El clima de una ubicación está afectado por su latitud, altitud, orientación del relieve, distancia al mar y corrientes marinas, que constituyen lo que se conoce como factores modificadores del clima. Los climas pueden clasificarse según la media y las gamas típicas de los cinco elementos señalados, principalmente temperatura y precipitación. El esquema de clasificación más utilizado la clasificación climática de Köppen, originalmente desarrollada por Wladimir Köppen, es la utilizada en el mapa que acompaña esta introducción con datos actuales de 1980 a 2016. El sistema Thornthwaite, en uso desde 1948, incorpora la evapotranspiración junto con la información de temperatura y precipitación y se utiliza en el estudio de la diversidad biológica y los efectos potenciales de cambios de clima sobre ella. Los sistemas de clasificación de Bergeron y Spatial Synoptic se centran en el origen de las masas de aire que definen el clima de una región. La paleoclimatología es el estudio de los climas antiguos. Ya que no se dispone de observaciones directas del tiempo antes del siglo XIX, los paleoclimas se infieren a partir de variables (indicadores paleoclimáticos) que incluye pruebas no bióticas como los sedimentos encontrados en lechos lacustres y núcleos de hielo, y prueba biótica como los anillos de árbol y coral. Los modelos climáticos son modelos matemáticos de climas del pasado, presente y futuro. Un cambio climático puede ocurrir durante periodos largos y cortos a partir de una variedad de factores; el calentamiento reciente se trata en Calentamiento global. El calentamiento global produce redistribuciones. Por ejemplo, «un cambio de 3 °C en la temperatura media anual corresponde a un cambio en las isotermas en aproximadamente 300-400 km en latitud (en la zona térmica) o 500 m en elevación. Por lo tanto, se prevé que las especies se muevan hacia arriba en elevación o hacia los polos en latitud en respuesta a los cambios de las zonas climáticas». Parámetros climáticos Para el estudio del clima hay que analizar los elementos del tiempo meteorológico: la temperatura, la humedad, la presión, los vientos, la precipitación, nubosidad, brillo y visibilidad. De ellos, las temperaturas medias mensuales y los montos pluviométricos mensuales a lo largo de una serie bastante larga de años son los datos más importantes que normalmente aparecen en los gráficos climáticos. Hay una serie de factores que pueden influir sobre estos elementos: la latitud geográfica, la altitud del lugar, la orientación del relieve con respecto a la incidencia de los rayos solares (vertientes o laderas de solana y umbría) o a la de los vientos predominantes (Sotavento y barlovento), las corrientes oceánicas y la continentalidad (que es la mayor o menor lejanía de una región respecto del océano o del mar). Véase también: Efecto del ángulo solar sobre el clima Estudio del tiempo atmosférico Artículo principal: Meteorología Hay muchas clases de tiempo: cálido o frío, húmedo o seco, despejado o tormentoso, las cuales resultan de diferentes combinaciones de las variables atmosféricas de temperatura, presión, viento, humedad y precipitación. El tiempo siempre ejerció poderosa influencia sobre las actividades humanas, y durante siglos el hombre ha estudiado la atmósfera, tratando de comprender su comportamiento. La meteorología es la rama de la ciencia que estudia esta envoltura de aire en torno de nuestro planeta. Las variaciones a corto plazo de la atmósfera (que llamamos tiempo meteorológico), se relacionan con nuestra vida cotidiana. La lluvia que riega nuestras cosechas y llena nuestros embalses es parte del tiempo, lo mismo que los huracanes y tornados que dañan nuestras ciudades y el rayo que puede fulminarnos en un segundo. En un principio, los hombres simplemente observaban el tiempo; luego trataron de emplear sus observaciones como base para la predicción y anticipación de las condiciones meteorológicas; finalmente aprendieron que no podían pronosticarlas con mucho éxito sin comprender su funcionamiento. Y cuando finalmente se consiguió cierto conocimiento de los procesos atmosféricos, se comenzó a pensar en el intento de alterarlos. Estos son los tópicos que consideramos aquí: los esfuerzos humanos para observar, predecir, entender y aminorar los efectos negativos del tiempo atmosférico. Elementos del clima Una nube cumulonimbus bastante desarrollada vista hacia el este en el sureste de Caracas, Venezuela. Un buen ejemplo del flujo de energía (térmica, eléctrica, físico-química, etc.) en el seno de la atmósfera. Los elementos constituyentes del clima son: temperatura, presión, viento, humedad y precipitación. Tener un registro durante muchos años de los valores correspondientes a dichos elementos en un lugar determinado, nos sirve para poder definir cómo es el clima de ese lugar. De estos cinco elementos, los más importantes son la temperatura y la precipitación, porque en gran parte, los otros tres elementos o rasgos del clima están estrechamente relacionados con los dos que se han citado. Ello significa que una mayor o menor temperatura da origen a una menor o mayor presión atmosférica, respectivamente, ya que el aire caliente tiene menor densidad y por ello se eleva (ciclón o zona de baja presión), mientras que el aire frío tiene mayor densidad y tiene tendencia a descender (zona de alta presión o anticiclón). A su vez, estas diferencias de presión dan origen a los vientos (de los anticiclones a los ciclones), los cuales transportan la humedad y las nubes y, por lo tanto, dan origen a la repartición de las lluvias sobre la superficie terrestre. Temperatura atmosférica Artículo principal: Temperatura atmosférica Se refiere al grado de calor específico del aire en un lugar y momento determinados. El clima depende de la manera en que la energía de la radiación solar se reparte entre la atmósfera y la superficie terrestre. El clima es más cálido donde llega más energía a la superficie, y más frío donde menos. Esta insolación depende de dos tipos de factores: Factores planetarios: el movimiento de rotación terrestre (que origina el día y la noche, con las diferencias térmicas que ello conlleva) y el movimiento de traslación de la Tierra alrededor del Sol, que da origen a las estaciones (épocas de mayor o menor exposición de la radiación solar debido a la inclinación del eje terrestre con respecto a la eclíptica u órbita terrestre). Véase también: Efecto del ángulo solar sobre el clima Factores geográficos. Son aquellos que dependen de las condiciones específicas del lugar con respecto a las características térmicas del aire en dicho lugar. Son: la latitud (que explica la mayor o menor radiación solar en función de la inclinación del eje terrestre a lo largo del año); la altitud, que da origen a la diferenciación térmica de la atmósfera dando origen a lo que se conoce como pisos térmicos, aspecto fundamental en el estudio del clima; la mayor o menor distancia al mar que afecta la mayor o menor oscilación o amplitud térmica del aire, respectivamente; la orientación del relieve de acuerdo a la insolación (vertientes o laderas de solana, más cálidas, y de umbría, más frías, ambas consideradas a una altitud y latitud equivalentes) y las corrientes marinas, que proporcionan una forma muy importante de trasladar calor de la zona intertropical a las zonas templadas y polares, haciendo más suave el clima en estas últimas zonas geoastronómicas. Estos cinco factores no afectan solamente a la temperatura atmosférica, sino también al resto de los elementos del clima: la presión atmosférica, los vientos, la humedad y la precipitación. Presión atmosférica Artículo principal: Presión atmosférica Es la presión que ejerce el peso de las masas de aire en todas direcciones, y varía inversamente con la altitud y directamente con la temperatura, es decir, en condiciones normales, a mayor altitud, menor temperatura, menor presión. Viento Artículo principal: Viento Es el movimiento de masas de aire de acuerdo con las diferencias de presión atmosférica. En sentido general, el viento es el vehículo por el medio del cual se realiza el transporte de energía en el seno de la atmósfera y, por lo tanto, ayuda a distribuir más equitativamente esa energía. El viento constituye un elemento fundamental en el ciclo hidrológico que, a su vez, resulta imprescindible para sustentar la vida en la Tierra. Humedad Artículo principal: Humedad Se denomina humedad al agua que impregna un cuerpo o al vapor presente en la atmósfera. El agua está presente en todos los cuerpos vivos, ya sean animales o vegetales, y esa presencia es de gran importancia para la vida. Precipitación Artículo principal: Precipitación Es cualquier forma de hidrometeoro procedente del agua atmosférica en forma de nubes que cae a la superficie terrestre por medio de las diferentes formas de precipitación (lluvia, nieve, granizo, etc.). Factores que determinan el clima Partes constitutivas del sistema climático terrestre. Latitud Altitud Distancia al mar Corrientes oceánicas Orientación del relieve Dirección de los vientos planetarios y estacionales Latitud geográfica Artículo principal: Zonas térmicas Efectos sobre la temperatura atmosférica: La latitud determina la inclinación con la que caen los rayos del Sol y la diferencia de la duración del día y la noche. Cuanto más directamente incide la radiación solar, más calor aporta a la Tierra. Las variaciones de la insolación que recibe la superficie terrestre se deben a los movimientos de rotación (variaciones diarias) y de traslación (variaciones estacionales). Las variaciones en latitud son causadas, de hecho, por la inclinación del eje de rotación de la Tierra. El ángulo de incidencia de los rayos del Sol no es el mismo en verano que en invierno siendo la causa principal de las diferencias estacionales. Cuando los rayos solares inciden con mayor inclinación calientan mucho menos porque el calor atmosférico tiene que repartirse en un espesor mucho mayor de atmósfera, con lo que se filtra y dispersa parte de ese calor. Fácilmente se puede comprobar este hecho cuando comparamos la insolación producida en horas de la mañana y de la tarde (radiación con mayor inclinación) con la que recibimos en horas próximas al mediodía (insolación más efectiva por tener menor inclinación). Es decir, una mayor inclinación en los rayos solares provoca que estos tengan que atravesar mayor cantidad de atmósfera, atenuándose más que si incidieran más perpendicularmente. Por otra parte, a mayor inclinación, mayor será la componente horizontal de la intensidad de radiación. Mediante sencillos cálculos trigonométricos puede verse que: I (incidente) = I (total) • cosθ. Es así que los rayos solares inciden con mayor inclinación durante el invierno por lo que calientan menos en esta estación. También podemos referirnos a la variación diaria de la inclinación de los rayos solares: las temperaturas atmosféricas más frías se dan al amanecer y las más elevadas, en horas de la tarde. Véase también: Efecto del ángulo solar sobre el clima Efectos sobre las precipitaciones: La latitud determina la localización de los centros de acción que dan origen a los vientos: anticiclones (centros de altas presiones) y ciclones (áreas de baja presión o depresiones). Los anticiclones son áreas de alta presión, donde el aire desciende de cierta altura por ser frío y seco (el aire frío y seco es más pesado que el cálido y húmedo), mientras que los ciclones son áreas de baja presión donde el aire se eleva por su menor densidad. La ubicación de los mayores centros de acción determina la dirección y mecánica de los vientos planetarios o constantes y por consiguiente, las zonas de mayor o menor cantidad de precipitación. Los cuatro paralelos notables (Trópicos y círculos polares) generan la existencia de grandes zonas anticiclónicas y depresiones de origen dinámico, es decir, originadas por el movimiento de rotación terrestre y de origen térmico (originadas por la desigual repartición del calentamiento de la atmósfera). Altitud del relieve Artículo principal: Pisos térmicos La altura del relieve modifica sustancialmente el clima, en especial en la zona intertropical, donde se convierte en el factor modificador del clima de mayor importancia. Este hecho ha determinado un criterio para la conceptualización de los pisos térmicos, que son fajas climáticas delimitadas por curvas de nivel que generan también curvas de temperatura (isotermas) que se han establecido tomando en cuenta tipos de vegetación, temperaturas y orientación del relieve. Se considera la existencia de cuatro o cinco pisos térmicos en la zona intertropical: Macrotérmico (menos de 1 km de altura), con una temperatura que varía entre los 27 °C al nivel del mar y los 20°. Mesotérmico (1 a 3 km): presenta una temperatura entre los 10 y 20 °C, su clima es templado de montaña. Microtérmico (3 a 4,7 km): su temperatura varía entre los 0 y 10 °C. Presenta un tipo de clima de páramo o frío. Gélido (más de 4,7 km): su temperatura es menor de 0 °C y le corresponde un clima de nieves perpetuas. Algunos autores subdividen el piso mesotérmico en dos para lograr una mayor precisión debido a que la diferencia de altitud y temperatura entre 1 y 3 km es demasiado grande como para incluir un solo piso climático. Quedaría así un piso intermedio entre 1000 y 1500 m que se le ha denominado piso subtropical, aunque se trata de un nombre poco apropiado ya que este término se refiere a una latitud determinada y no a un piso térmico determinado por la temperatura. Y el piso ubicado entre los 1500 y 3000 m constituiría el piso templado, al que le seguiría el piso de páramo hasta los 4700 m s. n. m. El cálculo aproximado que se realiza, es que al elevarse 160 m, la temperatura baja 1 °C. Como se puede ver en el artículo principal sobre los pisos térmicos, la disminución de la temperatura con la altitud varía según las zonas geoastronómica en la que nos encontremos. Si es en la zona intertropical, en la que el espesor de la atmósfera es bastante mayor, la temperatura desciende 1 °C, no a los 160 m de ascenso, sino a los 180 m aproximadamente. Orientación del relieve La disposición de las cordilleras con respecto a la incidencia de los rayos solares determina dos tipos de vertientes o laderas montañosas: de solana y de umbría. Al norte del trópico de Cáncer, las vertientes de solana son las que se encuentran orientadas hacia el sur, mientras que al sur del trópico de Capricornio las vertientes de solana son, obviamente, las que están orientadas hacia el norte. En la zona intertropical, las consecuencias de la orientación del relieve con respecto a la incidencia de los rayos solares no resultan tan marcadas, ya que una parte del año el sol se encuentra incidiendo de norte a sur y el resto del año en sentido inverso. La orientación del relieve con respecto a la incidencia de los vientos dominantes (los vientos planetarios) también determina la existencia de dos tipos de vertientes: de barlovento y de sotavento. Llueve mucho más en las vertientes de barlovento porque el relieve da origen a las lluvias orográficas, al forzar el ascenso de las masas de aire húmedo. Continentalidad Artículo principal: Continentalidad La proximidad del mar modera las temperaturas extremas y suele proporcionar más humedad en los casos en que los vientos procedan del mar hacia el continente. Las brisas marinas atenúan el calor durante el día y las terrestres limitan la irradiación nocturna. En la zona intertropical, este mecanismo de las brisas atempera el calor en las zonas costeras ya que son más fuertes y refrescantes, precisamente, cuanto más calor hace (en las primeras horas de la tarde). Una alta continentalidad, en cambio, acentúa la amplitud térmica. Provocará inviernos fríos y veranos calurosos. El ejemplo más notable de la continentalidad climática lo tenemos en Rusia, especialmente, en la parte central y oriental de Siberia: Verjoyansk y Oimyakon rivalizan entre sí como los polos del frío durante los largos inviernos boreales (menos de 70 °C bajo cero). Ambas poblaciones se encuentran relativamente cerca del océano Glacial Ártico y del océano Pacífico en su parte más septentrional (al norte del círculo polar ártico), pero muy lejos de las zonas occidentales del ecuador térmico tanto del Atlántico como del océano Pacífico, que es de donde proceden los vientos dominantes cargados de humedad (vientos del Oeste). La continentalidad es el resultado del alto calor específico del agua, que le permite mantenerse a temperaturas más frías en verano y más cálidas en invierno. Es lo mismo que decir que el agua no es diatérmana ya que se calienta directamente con los rayos solares aunque posee una gran inercia térmica: tarda mucho en calentarse, pero también tarda más en enfriarse por irradiación, en comparación con las áreas terrestres o continentales. Las masas de agua son, pues, el más importante agente moderador del clima. Corrientes oceánicas Artículo principal: Corriente marina Las corrientes marinas o, con mayor propiedad, las corrientes oceánicas, se encargan de trasladar una enorme cantidad de agua y, por consiguiente, de energía térmica (calor). La influencia muy poderosa de la corriente del Golfo, que trae aguas cálidas desde las latitudes intertropicales hace más templada la costa atlántica de Europa que lo que le correspondería según su latitud. En cambio, otras zonas de la costa este de América del Norte, situadas a la misma latitud que las de Europa presentan unas temperaturas mucho más bajas, especialmente en invierno. El caso de Washington D. C., por ejemplo, puede compararse con Sevilla, que está a la misma latitud, pero que tiene unos inviernos mucho más cálidos. Y esta diferencia se acentúa más hacia el norte, porque al alejamiento de la corriente del Golfo hay que sumar la influencia de las aguas frías de la corriente del Labrador: Oslo, Estocolmo, Helsinki y San Petersburgo, capitales de países europeos, se encuentran a la misma latitud que la península del Labrador y la bahía de Hudson, territorios prácticamente deshabitados por el clima extremadamente frío. Otro interesante ejemplo de que las temperaturas no guardan una correspondencia estricta con la latitud, cuando se tratan de corrientes oceánicas frías o cálidas se encuentra en el hecho de que las aguas oceánicas en España y Portugal son más cálidas que en las costas de Canarias y Mauritania, a pesar de la menor latitud de las costas africanas, por el hecho de que en ambos casos están incidiendo los efectos de dos corrientes distintas: la corriente del Golfo en las costas europeas y la de las Canarias en las costas africanas. Las corrientes frías también ejercen una poderosa influencia sobre el clima. En la zona intertropical producen un clima muy árido en las costas occidentales de África y de América, tanto del norte como del sur. Estas corrientes frías no se deben a un origen polar de las aguas (algo que se señala en algunos textos desde hace mucho tiempo), que no se explicaría en el caso de las corrientes frías de California y de Canarias ya que ambas están ubicadas entre corrientes cálidas a mayor y a menor latitud. La frialdad de las corrientes se debe al ascenso de aguas profundas en dichas costas occidentales de la Zona Intertropical. Ese ascenso de las aguas, lento pero constante, es muy evidente en el caso de la Corriente de Humboldt o del Perú, una zona muy rica en plancton y en pesca, precisamente, por el ascenso de aguas profundas, que traen a la superficie una gran cantidad de materia orgánica. Como las aguas frías producen alta presión atmosférica, como se explica en los artículos sobre la Guayana Venezolana y sobre la diatermancia, la humedad relativa en las áreas de aguas frías es muy baja y las lluvias son muy escasas o nulas: el desierto de Atacama es el más árido del mundo. Los motivos de la surgencia de las aguas frías se deben a dos razones relacionadas con el movimiento de rotación de la Tierra: En primer lugar, a la dirección de los vientos planetarios en la zona intertropical y a la propia dirección de las corrientes ecuatoriales. En ambos casos, es decir, en el caso de los vientos y de las corrientes marinas, el desplazamiento se produce de este a oeste (en sentido contrario a la rotación terrestre) y alejándose de la costa. A su vez, este alejamiento de la costa de los vientos y de las aguas superficiales, crea las condiciones que explican en parte el ascenso de las aguas más profundas, que vienen a reemplazar a las aguas superficiales que se alejan. Por último, en la zona intertropical, los vientos son de componente Este, debido al movimiento de rotación de la Tierra, por lo que en las costas occidentales de los continentes en la zona intertropical soplan del continente hacia el océano, por lo que su humedad es muy escasa. En otra escala mucho más reducida, este fenómeno puede comprobarse en las playas levantinas españolas: cuando sopla el viento de Poniente, el Mediterráneo se encuentra sin olas (rizado, cuando mucho) pero las aguas en la playa se notan mucho más frías de lo normal. Y en el caso de la isla de Margarita es mucho más evidente, porque en ella soplan los vientos del Este durante todo el año y a cualquier hora: la temperatura de la playa de La Galera en Juan Griego es mucho más fría, aunque sin ningún oleaje perceptible, que la de Playa El Agua o la Playa de El Tirano, en las costas orientales de la isla, ubicadas apenas a unos 15 km hacia el Este. En segundo lugar, el propio movimiento de rotación es el responsable directo del ascenso de las aguas frías en las costas occidentales de los continentes en las latitudes subtropicales. El proceso es relativamente sencillo: debido al movimiento de rotación terrestre, de oeste a este, las aguas del fondo oceánico, que se desplazan conjuntamente con la parte sólida de las cuencas oceánicas, se ven forzadas a ascender cuando el talud continental actúa como una especie de pala (inmóvil con relación al resto de la litosfera) que las obliga a subir. En realidad, este segundo motivo es el más importante, mientras que la dirección de los vientos de este a oeste con las corrientes marinas frías no es una causa de éstas sino que también se debe al movimiento de rotación terrestre. Más aún, en el caso de la Corriente de Humboldt, los vientos de este a oeste no coinciden con la corriente ya que ésta va de sur a norte y cuando se acerca al ecuador terrestre se desvía hacia el noroeste. Precisamente, la falta de coincidencia entre las direcciones de los vientos y de las corrientes es una demostración de que no se trata exactamente de las mismas causas. El artículo corrientes oceánicas frías explica estas ideas con mayor amplitud. Clasificaciones climáticas Artículo principal: Clasificación climática de Köppen Clasificación climática de Köppen en función de: temperatura, precipitación y vegetación. Las temperaturas mensuales promedio de la superficie de 1961-1990. Este es un ejemplo de cómo el clima varía con la localización y la temporada. Imágenes globales mensuales de la NASA Observatorio de la Tierra. (SVG interactiva) Manchas nubladas y soleadas del mundo. Mapa del Observatorio de la Tierra de la NASA utilizando los datos recogidos entre julio de 2002 y abril de 2015. La obra principal de Köppen (o Kœppen) con respecto a la Climatología se titula Die Klimate der Erde (El Clima de la Tierra) publicada en 1923, y en la que describe los climas del mundo en función de su régimen de temperaturas y de precipitaciones. Constituye la primera obra sistemática sobre Climatología y que marcó la pauta para introducir distintas mejoras que la convirtieron en la clasificación climática más conocida. Emplea un sistema de letras mayúsculas y minúsculas cuyo valor está establecido en torno a ciertos umbrales en cuanto a las temperaturas medias anuales para separar los climas tropicales (letra A) de los templados (letra C) y a estos de los continentales (letra D) y polares (letra E). La letra B la destina a los climas secos con dos tipos: BS, clima semidesértico o estepario y BW, o clima desértico propiamente dicho. Por último, la letra H la emplea para los climas indiferenciados de alta montaña, aspecto en el que, con el diseño de una clasificación de pisos térmicos, es decir, con la división de las fajas altitudinales empleando curvas de nivel de una altitud determinada, se introdujo una mejora sustancial y que ha venido a sustituir a esos climas indiferenciados de montaña. Resumiendo la clasificación climática de Köppen se puede señalar los siguientes tipos de clima: A – Climas Macrotérmicos (Cálidos, de la zona intertropical). B – Climas secos (localizados en las zonas subtropicales y en el interior de los continentes de la zona intertropical o de las zonas templadas). Se divide en dos tipos: Desértico (BW) y semidesértico o estepario (BS). C – Climas Mesotérmicos o templados. D – Climas fríos (localizados en latitudes altas, próximas a los círculos polares y donde la influencia del mar es muy escasa). E – Climas polares. Se localizan en las zonas polares, limitadas. hacia el ecuador por los Círculos polares. H – Climas indiferenciados de alta montaña. Para determinar los subgrupos o subtipos se añaden otras letras minúsculas: f – Lluvias todo el año sin estación seca definida, p. ej., en la zona intertropical Af = Clima de selva. w – Lluvias en verano, también en la zona intertropical, p. ej., Aw = Clima de sabana. m – Lluvias de monzón. Similar al Aw, pero con lluvias más intensas originadas por la diferencia acentuada de las presiones atmosféricas entre el océano y los continentes. Solo se presenta en el sur y sureste del continente asiático. Las lluvias suelen ser muy intensas y prolongadas durante la época de calor, cuando las bajas presiones continentales atraen a los vientos procedentes del océano Índico cargados de humedad, que se descargan en las vertientes meridionales del Himalaya y otras cordilleras provocando desbordamientos de los grandes ríos de la zona, como el Indo, el Ganges, el Bhramaputra, el Irawaddy, el Saluen y el Mekong, así como otros ríos del sur de China. s – Lluvias en invierno. Corresponde al clima subtropical seco o clima mediterráneo (Csa según Köppen), localizado en las latitudes subtropicales de las costas occidentales de los continentes. La clasificación por tipo de vegetación utiliza letras mayúsculas y son: S – Estepa, por ejemplo BS W – Desierto, por ejemplo BW T – Tundra F – Hielos perpetuos B – De alta montaña Entre las principales modificaciones al sistema ideado por Köppen pueden citarse las de Trewartha y la de Thornthwaite, que ha sido considerado por Strahler como un sistema aparte. En función exclusivamente de la temperatura Climas sin inviernos: el mes más frío tiene una temperatura media mayor de 18 °C. Corresponde a los climas isotermos de la zona intertropical en áreas inferiores a los 1000 m de altitud, aproximadamente. Climas de latitudes medias: con las cuatro estaciones. Climas sin verano: el mes más caluroso tiene una temperatura media menor a 10 °C. En función de la altitud Artículo principal: Pisos térmicos En la Zona Intertropical existen 4 pisos térmicos (también llamados pisos climáticos o pisos bióticos) ya que los cinco elementos o parámetros del clima que se han indicado varían con la altitud. Algunos autores añaden un piso intermedio (también llamado subtropical) entre el macrotérmico y el mesotérmico, ya que este último abarca una diferencia considerable de altura. Como se ha indicado, estos 4 pisos son: Macrotérmico, con las temperaturas siempre elevadas y constantes, ubicado entre el nivel del mar y los 800 a 1000 m s. n. m. (metros sobre el nivel del mar), según los criterios de distintos autores. Mesotérmico o piso templado, entre los 800 a 1000 m, hasta los 2500 a 3000 m de altitud. Microtérmico o piso frío (llamado en algunos países hispanoamericanos como «piso de páramo»), desde los 2500 o 3000 m s. n. m. hasta el nivel de las nieves perpetuas (aproximadamente, a los 4700 m s. n. m.) Gélido, helado o de nieves perpetuas, a partir de los 4700 m de altitud, cota donde se ubica, aproximadamente, la isoterma de los 0 °C. Y a medida que avanzamos en latitud, el número de pisos climáticos va disminuyendo porque la influencia de la altitud va siendo sustituida por la de la misma latitud. Esto significa que el primer piso que desaparece (ya en las zonas templadas) es el piso macrotérmico. Y la diferencia esencial entre los pisos térmicos o climáticos en la zona intertropical y en otras zonas geoastronómicas es que en aquella solo encontramos climas isotermos, es decir, con las temperaturas semejantes a lo largo de todo el año mientras que en las zonas templadas, las temperaturas varían considerablemente durante las estaciones debido a la distinta inclinación de los rayos solares durante el año y, por ende, a las distintas cantidades de energía solar que recibe la superficie terrestre a lo largo del año. En función de la precipitación Árido Semiárido Subhúmedo Húmedo Muy húmedo Con relación a los umbrales que separan unos climas de otros según las precipitaciones respectivas, existen diversas interpretaciones (según distintos autores), que deberían estar basadas, además de los montos pluviométricos de las estaciones ubicadas en un clima dado, en las temperaturas medias mensuales de esas mismas estaciones, tal como se indica en el artículo sobre el índice xerotérmico de Gaussen ya que no es lo mismo una pluviosidad de 40 mm para un mes determinado en una estación meteorológica de un clima cálido que si se trata de un clima frío. De hecho, una escasa precipitación en un mes de apenas un litro de agua por m² (es decir, 1 mm) no tendría ningún efecto cuando se trata de un clima cálido, ya que ese valor de la precipitación quedaría anulado rápidamente por la evaporación: pero si hablamos de un clima de tundra durante el invierno, en el que las temperaturas medias fueran inferiores a los 0 °C, ese litro de agua permanecería en el suelo en forma líquida o sólida, por la casi ausencia de evaporación que se presenta con esas temperaturas. En el caso de España, por ejemplo, la pluviosidad disminuye de noroeste a sureste, desde unos 1500 mm anuales en una gran parte de Galicia hasta los 300 mm o menos en las costas de Almería, con una aridez extrema en los valles internos de la provincia por el efecto de sotavento de las alineaciones montañosas, como sucede, por ejemplo, en el valle de Tabernas. Y el ejemplo de las laderas occidentales de la Sierra de Grazalema, en Cádiz, con una pluviosidad aún mayor que la de Galicia servirían para aclarar un poco la idea ya indicada de la influencia de la temperatura con respecto a la efectividad de las lluvias. Si no se toma en cuenta la Sierra de Grazalema en lugar de Galicia para definir la gradación progresiva de los climas según su mayor o menor aridez es porque esta Sierra, que fue declarada en 1977 Reserva de la biosfera por la UNESCO, representa un caso especial y muy localizado, e inverso al de Tabernas, en el sentido de que se trata de un área expuesta a los vientos del oeste, es decir, a barlovento, lo que incide en la ocurrencia de lluvias orográficas. En cambio, en el valle de Tabernas, con un clima desértico y ubicado en el extremo oriental de Andalucía, en Almería, se trata de una zona a sotavento de los vientos del oeste, por lo que la humedad es muy escasa. Clasificación genética Clasifica en función de las masas de aire que le dan origen: Clima I: vaguada ecuatorial y clima seco. Clima II: controlado por la zona de contacto de viento tropical y polar. Clima III: controlado por vientos polares y árticos y tundras Tiene el problema de ser excesivamente sintético y los detalles, es decir, la innumerable gama de variaciones continentales, regionales y locales, prácticamente se dejan de tener en cuenta. Diferentes tipos de clima por temperatura En el mundo los tipos de clima por su temperatura se clasifican en tres grupos (no confundir con los pisos térmicos arriba descritos). Cálidos Los climas cálidos se encuentran dentro de la zona intertropical, siendo climas tropicales en su mayoría. Su temperatura media es siempre superior a los 22 °C y no se presentan las estaciones, son isotérmicos y la temperatura disminuye con la altura. La continentalidad no tiene efecto aquí y la precipitación varía entre desde 400 mm en los áridos hasta superar los 8000 mm en el ecuatorial. Clima ecuatorial (región amazónica, parte oriental de Panamá, península de Yucatán, centro de África, occidente costero de Madagascar, sur de la península de Malaca e Insulindia). Clima tropical de sabana (Caribe, Llanos y costas de Colombia, Costa Rica y Venezuela, costa del Ecuador, costa norte del Perú, la mayor parte del este de Bolivia, noroeste de Argentina, norte de Paraguay, centro y sur de África, sudeste asiático, norte de Australia, sur y parte del centro de la India, la Polinesia, etc., y la costa surcentral del Pacífico de México). Clima subtropical árido (suroeste de América del Norte, norte y suroeste de África, oriente medio, costa central y sur del Perú, norte de Chile, centro de Australia). Se ubica entre los climas desérticos subtropicales y las franjas de clima mediterráneo, del cual se distingue por una pequeña diferencia en cuanto a la lluvia recibida. Clima desértico cálido y semidesértico cálido. La mayor parte del Sáhara y Medio Oriente, zonas del norte de México, la Guajira y el Zulia en el norte de Sudamérica y la costa peruana. Templados Los climas templados son los propios de latitudes medias, y se extienden entre los paralelos 30 grados y 70 grados aproximadamente (zona templada). Su carácter procede de los contrastes estacionales de las temperaturas y las precipitaciones, y de una dinámica atmosférica condicionada por los vientos del oeste. Excepcionalmente, aparecen en zonas altas intertropicales aunque de carácter isotérmico (sin oscilación térmica anual relevante). Las temperaturas medias anuales se sitúan alrededor de los 15 °C y la precipitación va desde 300 a más de 1000 mm anuales, dependiendo de factores como la exposición del relieve a los vientos y a la insolación, la distancia al mar o continentalidad y otros. Dentro de los climas templados distinguimos dos grandes conjuntos: los climas subtropicales, o templados-cálidos, y los climas templados propiamente dichos, o templados-fríos. A su vez, dentro de cada uno de esos grandes conjuntos se engloban varios subtipos climáticos. Clima desértico templado y frío y semidesértico templado y frío, este último también llamado clima estepario, se ubican en el interior de los continentes en la zona templada (Asia Central, centro-oeste de América del Norte, Mongolia, norte y oeste de China). Clima subtropical húmedo (sudeste de Estados Unidos y Australia, Noreste de México, sur de China, noreste de Argentina, sur de Brasil, sur de Paraguay y Uruguay, norte de la India y Pakistán, Japón y Corea del Sur). Clima mediterráneo (zona del Mediterráneo, California, norte de Baja California, centro de Chile, sur de Sudáfrica, suroeste de Australia). Clima oceánico o atlántico (zona atlántica europea, costas del Pacífico del noroeste de Estados Unidos y de Canadá, sureste de Australia, Nueva Zelanda, sur de Chile, costa de la provincia de Buenos Aires, Argentina, ). Clima templado de montaña o tropical de montaña (sitios de no más de 3000 m de altura de los Andes intertropicales (Colombia, Ecuador, Perú y Venezuela), zonas altas de Centroamérica y México, macizos de África Oriental e Insulindia). Clima continental (centro de Europa y China y la mayor parte de Estados Unidos, norte y este de Europa, sur y centro de Siberia, Canadá y Alaska). Fríos Climas polares (al norte del círculo polar ártico y al sur del círculo polar antártico y montañas nevadas). Clima de tundra (en zonas subpolares y montañas altas). Microclimas Clima urbano: Es un tipo de microclima originado por el calentamiento del aire por las actividades domésticas de tipo urbano, la industria, el transporte, la calefacción y otras causas. También produce un clima más seco y con mayores extremos meteorológicos. Incendios: ver tormenta ígnea. El lugar donde se producen incendios forestales suele tener unos efectos similares a los de los climas urbanos debido al calentamiento del aire en esos lugares. Erupciones: las erupciones volcánicas pueden producir lluvias torrenciales, nubes de polvo y agua, con tormentas eléctricas producidas por el ascenso violento de aire con gases y vapor muy calientes. Un microclima es un clima local de características distintas a las que están en la misma zona en que se encuentra. El microclima es un conjunto de valores meteorológicos que caracterizan un contorno o ámbito reducido y que se diferencian de los que existen en su entorno. Los factores que lo componen son la topografía, temperatura, humedad, altitud, latitud, insolación y la cobertura vegetal."
ksampletext_wikipedia_geog_geografiafisica: str = "Geografía física. La geografía física (conocida en un tiempo como fisiografía, término ahora en desuso) es la rama de la geografía que estudia en forma sistémica y espacial, la superficie terrestre considerada en su conjunto y específicamente, el espacio geográfico natural. Constituye uno de los tres grandes campos del conocimiento geográfico; los otros son la geografía humana cuyo objeto de estudio comprende el espacio geográfico humanizado y la geografía regional que ofrece un enfoque unificador, estudiando los sistemas geográficos en forma integrada. La geografía física se preocupa (según Strahler) de los procesos que son el resultado de dos grandes flujos de energía: el flujo de radiación solar que dirige las temperaturas de la superficie junto al movimiento de los fluidos, y el flujo de calor desde el interior de la Tierra que se manifiesta en los materiales de los estratos superiores de la corteza terrestre. Estos flujos interactúan en la superficie terrestre que es el campo de estudio del geógrafo físico. Son diversas las disciplinas geográficas que estudian en forma específica las relaciones de los componentes de la superficie terrestre. La geografía física enfatiza el estudio y la comprensión de los patrones y procesos geográficos del ambiente natural, haciendo abstracción por razones metodológicas del ambiente cultural que es el dominio de la Geografía humana. Ello significa que, aunque las relaciones entre estos dos campos de la Geografía existen y son muy importantes, cuando se estudia uno de dichos campos, es necesario excluir al otro de alguna manera, con el fin de poder profundizar el enfoque y los contenidos. La metodología geográfica tiende a relacionar estos campos al proporcionar un marco seguro para la localización, distribución y representación del espacio geográfico además de emplear herramientas tales como los sistemas de información geográfica o el desarrollo de mapas que sirven a ambas especialidades. Por otra parte, las ciencias con las que se relaciona y los métodos empleados suelen ser diferentes en los tres campos, aunque tienen en común el interés humano en conocer cada vez más y mejor el mundo en que vivimos. Mapamundi realizado sobre la base de una compilación de imágenes de satélite. Hay que tener en cuenta que las proyecciones cilíndricas, modificadas o no, dan una imagen exagerada de las regiones polares, como se ve aquí, que parecen ser las que mayor superficie cubierta de nubes tienen, lo cual no es cierto. Patrones y procesos El Gran Glaciar de Aletsch, en los Alpes suizos al sur del Macizo de la Jungfrau, es el más grande de Europa. Estos dos conceptos equivalen a los de estructuras y sistemas en la teoría de sistemas, siendo el de patrones un concepto similar al de estructuras y el de procesos uno similar al de sistemas. De nuevo solemos separar estos dos conceptos de manera individual por razones metodológicas ya que no suelen estar separados en la naturaleza. La diferencia entre procesos y patrones es que en el primer caso, resulta fundamental la escala temporal y en el segundo no es tan importante: cuando estamos estudiando los efectos de la erosión fluvial en los márgenes de un río consideramos a la erosión como un proceso, es decir, un fenómeno que ocurre a lo largo del tiempo. Por el contrario, cuando nos referimos a las características de la cuenca de un río, estamos haciendo un estudio de patrones espaciales, es decir, nos estamos refiriendo a un área determinada, con una extensión, relieve, clima, caudal, vegetación, etc., sin referirnos en detalle a cómo estos patrones han venido siendo modificados a lo largo del tiempo por los procesos geográficos. En el caso del glaciar de Aletsch pueden verse las lenguas de hielo, las morrenas intermedias y otras características estructurales del glaciar. Pero su lento movimiento y evolución constituyen la culminación actual de un proceso que es necesario analizar a través del tiempo. Estratos descubiertos por la erosión fluvial en los Cárpatos. Lo mismo sucede en el campo de la geología: la Geología histórica hace referencia a procesos que han ocurrido en el tiempo geológico, mientras que la Geología física hace referencia al presente:[cita requerida] patrones estratigráficos o geológicos de la época actual y en la forma como se localizan en la superficie terrestre. En el caso de los estratos descubiertos por la erosión fluvial en el margen izquierdo de un río en los Cárpatos podemos ver los patrones estratigráficos típicos de las rocas sedimentarias, por ejemplo, una disposición estratigráfica normal sería cuando los estratos más recientes son los más altos de una sucesión estratigráfica. Geoformas en Ischigualasto, provocadas tanto por la erosión hídrica como la eólica, especialmente por la primera, temas que tiene en cuenta la geografía física. La investigación científica progresa básicamente mediante el análisis: el estudio detallado de un fenómeno precede al de la comprensión general del mismo (síntesis), si bien no hay consenso total sobre esto. Así muchos geógrafos que consideran que la Geografía es una ciencia de síntesis han pretendido comenzar con la metodología holística apoyándose en investigaciones temáticas previas. De este modo se puede ver las definiciones. Concepto Humboldt consideraba a la geografía como una cosmología o cosmografía, es decir, como una geografía física. Es cierto que planteaba en sus obras principales, la acción y reacción del hombre a los fenómenos y procesos de la geografía física, pero siempre vinculados a esta ciencia, idea que aparece en su obra fundamental Cosmos (recordemos que no existía aún la Geografía Antrópica o Humana como disciplina independiente aunque algunos de sus temas ya estaban presentes en la tradición corográfica) El Diccionario Rioduero de Geografía se limita a enumerar los temas que se incluyen en el campo de la Geografía física (climatología, geomorfología, oceanografía, y la hidrografía continental, incluyendo a la glaciología.) El Diccionario de términos geográficos de F. J. Monkhouse se refiere a la Geografía física como la ciencia que estudia aquellos aspectos de la geografía que están entroncados con la forma y relieve de la superficie terrestre, la configuración, extensión y naturaleza de los mares y océanos, de la atmósfera que nos rodea y de los procesos correspondientes, la capa del suelo y la vegetación natural que la recubre, es decir, el medio ambiente físico del paisaje. Arthur Newell Strahler, autor de un texto universitario de Geografía Física de amplia difusión en muchos idiomas, puntualiza que esta ciencia se preocupa de los procesos que son el resultado de dos grandes flujos de energía: el flujo de radiación solar que dirige las temperaturas de la superficie junto al movimientos de los fluidos, y el flujo de calor desde el interior de la Tierra que se manifiesta en los materiales de los estratos superiores de la corteza terrestre. Estos flujos interactúan en la superficie terrestre que es el campo del geógrafo físico. El Diccionario de Geografía de Elsevier señala que la geografía física estudia los componentes del ambiente físico de la Tierra (litósfera, atmósfera, hidrósfera, biósfera), las relaciones entre sí, su distribución sobre la superficie de la Tierra y los cambios en el tiempo que experimentan por causas naturales o por el impacto humano. Menciona que las ramas de la geografía física son la geomorfología, la oceanografía, la climatología, la hidrología terrestre, la glaciología, la biogeografía, la paleogeografía, la edafogeografía, incluyendo también la geocriología y el estudio del paisaje. Sin embargo los autores reconocen que la oceanografía se ha desarrollado como una disciplina independiente. Los campos de estudio de la geografía física Las ciencias geográficas que estudian un componente específico del espacio natural en su relación con los demás son numerosas y entre las más importantes pueden citarse: Orografía. Parte de la geografía física que trata de la descripción y estudio de las montañas. Hidrografía e hidrología. El estudio de las aguas continentales (básicamente, ríos, lagos y aguas subterráneas): ríos y sus cuencas, (cauces, caudal, redes hidrográficas, curso superior, medio e inferior de los ríos, aprovechamiento hidráulico, régimen fluvial, dinámica fluvial, etc.) La glaciología, a diferencia de la hidrografía, se preocupa de los cuerpos de agua en estado sólido, tales como glaciares, casquetes polares, icebergs, plataformas de hielo, etc. La geocriología que se dedica al estudio del permahielo. Oceanografía. Estudio de los océanos: características hidrológicas, físicas, biológicas, económicas; movimientos de las aguas oceánicas como las olas, mareas y corrientes oceánicas, etc. Además, resulta muy importante la acción de los océanos y mares sobre el ciclo hidrológico y sobre la dinámica atmosférica, que constituyen la base, directamente, de la meteorología e indirectamente, de la climatología. Geografía litoral. Se dedica al estudio de las dinámicas de los paisajes costeros. Biogeografía, con sus ramas fitogeografía o geografía de las plantas, zoogeografía o geografía de los animales y ecología del paisaje. Pedología o edafogeografía o geografía de los suelos, que estudia los suelos desde el punto de vista geográfico. La climatología, ciencia que estudia el clima a partir de la información meteorológica. Un antiguo y valioso manual de esta ciencia es el de Austin Miller. El desarrollo y divulgación de programas de acceso libre con imágenes satelitales de la superficie terrestre, que incluyen los patrones y procesos atmosféricos (Centros de acción como los anticiclones y depresiones) han hecho crecer rápidamente esta ciencia y su interpretación aunque los resultados de las nuevas tecnologías todavía están por verse de una manera más completa y coherente. La geomorfología, es el estudio de las formas del relieve en la superficie terrestre (montañas, mesetas o altiplanos, llanuras y cuencas sedimentarias, volcanes, etc). Incluye también los procesos que originaron estas formas del relieve y los procesos geomorfológicos actuales: meteorización, erosión, sedimentación, deslizamientos en masa, etc. Fundada a principios del siglo XX por el geógrafo estadounidense William Morris Davis, en la actualidad numerosos geógrafos de muchas nacionalidades han venido desarrollando esta disciplina, como por ejemplo, los franceses Jean Tricart, Emmanuel de Martonne, Max Derruau y muchos otros. La paleogeografía, encargada de investigar y reconstruir la geografía de épocas pasadas y su evolución, de gran importancia para el resto de la geografía física ya que sirve para comprender mejor la dinámica actual de la geografía de nuestro planeta. El estudio de los riesgos naturales, ya que pese a que el número de desastres naturales no ha aumentado de manera significativa en los últimos años, sí que ha aumentado el número de personas a los que afectan. Es un tema del que también se ocupa la geografía humana. Ciencias relacionadas con la geografía física La cima del Roraima, el tepuy más elevado de la Guayana venezolana. Las curiosas formas han sido producidas por la erosión, tema estudiado por la geomorfología. Debido al campo de estudio tan amplio de la geografía física, existen numerosas ciencias que están relacionadas con ella, entre las cuales podemos citar a: Las ciencias de la Tierra o geociencias, que sirven para integrar el conocimiento que tenemos de nuestro planeta. La geología, tanto la histórica como la geología estructural. También la Estratigrafía o geología estratigráfica, la sedimentología y la vulcanología. La geografía matemática, encargada del estudio de la Tierra como planeta, especialmente en lo que se refiere a su forma, dimensiones, y características, conocimientos también enlazados con los tratados por la Geodesia. La física, especialmente en lo que se refiere a la meteorología o física de la atmósfera, también se utilizan conceptos de la física básica para el estudio de los océanos, de la litosfera, de los conceptos y procesos geomorfológicos (o geomórficos) y cada una de las ciencias físico - naturales de interés geográfico. La ecología, como ciencia que estudia las relaciones mutuas entre los seres vivos y la superficie terrestre (el hábitat y ecosistemas) en lo que respecta a los animales (ecología animal), a las plantas (ecología vegetal) e incluso a los seres humanos (ecología humana, ecología cultural). Las ciencias ambientales. Evolución histórica de la geografía física Mapa de Europa realizado por Estrabón. A partir del nacimiento de la geografía como ciencia durante la época clásica griega y hasta fines del siglo XIX con el nacimiento de la antropogeografía o geografía humana, la geografía fue en parte una ciencia natural: el estudio descriptivo de localización y la toponimia de todos los lugares del mundo conocido. Diversas obras entre las más conocidas durante ese largo período podrían citarse como ejemplo, desde las de Estrabón (Geografía), Eratóstenes (Geografía) o Dionisio Periegeta (Periegesis Oiceumene) en la Edad Antigua hasta las de Alejandro de Humboldt (Cosmos) en el siglo XIX, en las cuales se consideraba a la Geografía como una ciencia físico-natural; desde luego, pasando por la obra Suma de Geographia de Martín Fernández de Enciso () de comienzos del siglo XVI, donde aparece señalado por primera vez el Nuevo Mundo. Entre los siglos XVIII y XIX, una polémica tomada de la Geología, entre los partidarios de James Hutton (Tesis del uniformismo) y de Georges Cuvier (catastrofismo) influyó poderosamente en el campo de la Geografía. Dos procesos desarrollados durante el siglo XIX tuvieron una gran importancia en el desarrollo posterior de la geografía física: el primero se trata del imperialismo colonial europeo en Asia, África, Australia e incluso América, en busca de las materias primas exigidas por la Revolución industrial, que contribuyó a que se crearan y se invirtiera en los departamentos de geografía de las universidades de las potencias coloniales y al nacimiento y desarrollo de las sociedades geográficas nacionales, dando origen así al proceso identificado por Horacio Capel como la institucionalización de la Geografía (). Uno de los imperios más prolíficos en este sentido fue el Ruso. A mediados del siglo XVIII numerosos geógrafos son enviados por el almirantazgo ruso en diferentes oportunidades a realizar levantamientos geográficos en la zona del ártico siberiano. Entre estos se encuentra quien es considerado el patriarca de la geografía rusa: Mijaíl Lomonósov quien a mediados del decenio de 1750 comienza a trabajar en el Departamento de Geografía de la Academia de Ciencias para realizar investigaciones en Siberia; sus aportes en este sentido son notables: demuestra el origen orgánico de los suelos, desarrolla una ley general sobre el movimiento de los hielos que aún rige en lo básico, fundando así una nueva rama geográfica: la glaciología. En 1755 se funda, por iniciativa suya, la Universidad de Moscú, que ahora lleva su nombre, donde promueve el estudio de la geografía y la formación de geógrafos. En 1758 es nombrado director del Departamento de Geografía de la Academia de Ciencias, puesto desde el cual elaboraría una metodología de trabajo para el levantamiento geográfico, que guiaría por mucho tiempo las más importantes expediciones y estudios geográficos en el extenso territorio de Rusia. De esta manera la línea de Lomonosov siguió y los aportes de la escuela rusa se fueron multiplicando a través de sus discípulos; ya en el siglo XIX tenemos grandes geógrafos como Vasili Dokucháyev quien realizó obras de gran relevancia como “principio del análisis integral del territorio” y Los Chernozem rusos, esta última la más importante, donde introduce el concepto geográfico de suelo, distinguiéndolo de un simple estrato geológico, y fundando de esta manera una nueva área de estudio geográfico: la Pedología o Edafología. La climatología recibiría también un fuerte impulso desde la escuela rusa a través de Wladimir Peter Köppen cuyo principal aporte, su clasificación climática, sigue vigente hoy en día, aunque con algunas modificaciones y mejoras. Por otra parte, este gran geógrafo físico también contribuyó a la Paleogeografía a través de su trabajo Los climas del pasado geológico por el cual se le considera el padre de la Paleoclimatología. Otros geógrafos rusos que realizaron grandes aportes a la disciplina en este periodo fueron N.M. Sibirtsev, Piotr Semiónov, K. D. Glinka, Neustrayev, entre otros. El segundo proceso de importancia se trata de la teoría de la evolución formulada por Darwin a mediados de siglo (lo que influyó decisivamente en la obra de Ratzel, quien poseía una formación académica como zoólogo y era seguidor de las ideas de Darwin) lo que significó un importante impulso en el desarrollo de la Biogeografía. Otro importante suceso a finales del siglo XIX y principios del siglo XX dará un importante impulso al desarrollo de la geografía y tendrá lugar en Estados Unidos. Se trata del trabajo del afamado geógrafo William Morris Davis quien no solo realizó importantes aportes al establecimiento de la disciplina en su país, sino que revolucionó el campo al desarrollar la teoría del ciclo geográfico la cual propuso como paradigma para la geografía en general, aunque en realidad sirvió solo como paradigma para la geografía física. Su teoría explicaba que las montañas y demás accidentes geográficos están modelados por la influencia de una serie de factores que se manifiestan en el ciclo geográfico. Él explicó que el ciclo comienza con el levantamiento del relieve por procesos geológicos (fallas, vulcanismo, levantamiento tectónico, etc.). Factores geográficos tales como ríos y el escurrimiento superficial comienzan a crear los valles de forma de V entre las montañas (la etapa llamada juventud). Durante esta primera etapa, el relieve es más escarpado y más irregular. En un cierto plazo, las corrientes pueden tallar valles más anchos (madurez) y después comenzar a serpentear, sobresaliendo solamente suaves colinas (senectud). Finalmente, todo llega a lo que es un llano plano, llano en la elevación más baja posible (llamado el nivel de base). Este llano fue llamado por Davis peneplanicie que significa casi un llano. Entonces, el rejuvenecimiento ocurre y hay otro levantamiento de montañas y el ciclo continúa. Aunque la teoría de Davis no es enteramente exacta, era absolutamente revolucionaria y excepcional en su tiempo y ayudaba a modernizar la geografía y a crear el subcampo de la geomorfología. Sus implicaciones impulsaron un sinnúmero de investigaciones en distintas ramas de la geografía física. Para el caso de la paleogeografía esta teoría aportó un modelo para comprender la evolución del paisaje. Para la hidrología, la glaciología y la climatología fue un impulso en cuanto se investigó como los factores geográficos que estudian, modelan el paisaje e influyen en el ciclo. El grueso de los trabajos de William Morris Davis condujeron al desarrollo de una nueva rama de la geografía física: la geomorfología, cuyos contenidos hasta entonces no se diferenciaban del resto de la geografía. Al poco tiempo esta rama presentaría un gran desarrollo. Algunos de sus discípulos realizaron importantes aportes a diferentes ramas de la geografía física y humana tales como Curtis Marbut con su invaluable legado para la pedología, Mark Jefferson, Isaiah Bowman, entre otros. La geografía física y sus aplicaciones La geografía física integra el conocimiento que han desarrollado las ciencias geográficas más especializadas como la Geomorfología, climatología, hidrografía e hidrología, oceanografía, pedología y muchas otras. Todas estas ciencias suelen tener una perspectiva aplicada. Por otra parte, nuevas disciplinas aún más específicas han venido a desarrollar campos aplicados dentro de la geografía física. Unos ejemplos: la geocriología que se ha desarrollado en Rusia y Canadá se especializa en el estudio del permahielo. La geografía litoral se aboca al estudio específico de la dinámica costera, la geografía de los riesgos, enfoque resumido en un artículo de Francisco Calvo García-Tornel (), estudia las implicaciones de los riesgos naturales sobre los seres humanos. Por lo general, las mayores aplicaciones de la geografía física tienen lugar en el desarrollo de las materias específicas de esta disciplina como son la geomorfología, en especial, la geomorfología fluvial; la climatología, la geomorfología litoral e incluso la oceanografía entendida como una geografía del mar y no como una física o una geología del mar, y muchas otras ciencias más específicas. Geógrafos físicos notables Eratóstenes (276 a 194 a. C.), quien hizo la primera estimación fiable de la conocida tamaño de la Tierra, además de ser considerado el padre de la geografía. Ptolomeo (c.90 - c.168), quien compiló conocimiento griego y romano para producir el libro Geographia. Abu Rayhān Bīrūnī (973 - 1048 d. C.), considerado el padre de la geodesia. Ibn Sina (Avicena, 980-1037), quien formuló la ley de superposición y el concepto del uniformismo en El Libro de la curación Muhammad al-Idrisi (Dreses, 1100 – c.1165), quien realizó la Rogeriana Tabula, el mapa del mundo más preciso en la Edad Media. Piri Reis (1465 – c.1554), uno de sus mapas es el mapa del mundo más antiguo conservado que incluye América y la Antártida. Gerardus Mercator (1512–1594), innovó en el campo de la cartografía y fue el creador de la proyección de Mercator. Bernhardus Varenius (1622-1650), escribió su importante trabajo Geografía General (1650), primera visión general de la geografía, que significó la fundación de la geografía moderna. Mikhail Lomonosov (1711–1765), padre de la geografía rusa y fundador de la glaciología. Alexander Von Humboldt (1769–1859) publicó su libro Kosmos y fundó el estudio de la biogeografía. Arnold Henry Guyot (1807-1884), quien destacó la estructura de los glaciares y ha permitido conocer más del movimiento de glaciares, sobre todo en el flujo de hielo fijo. Louis Agassiz (1807-1873), autor de una teoría glacial que discute la noción de una Tierra estacionaria de enfriamiento. Alfred Russel Wallace (1823-1913), fundador de la biogeografía moderna y la línea de Wallace. Vasily Dokuchaev (1840-1903), patriarca de la geografía rusa y fundador de la Pedología. Wladimir Peter Köppen (1846-1940), creador de la clasificación climática más importante y fundador de la paleoclimatología. William Morris Davis (1850–1934), padre de la geografía estadounidense, fundador de la Asociación Americana de Geógrafos, cofundador de la National Geographic Society, fundador de la geomorfología y desarrollador de la teoría del ciclo geográfico. Sir Ernest Shackleton (1874-1922), explorador de la Antártida durante la edad heroica de la exploración antártica. Robert E. Horton (1875–1945), fundador de la hidrología moderna y creador de conceptos hidrológicos básicos como la capacidad de infiltración y la escorrentía. J. Harlen Bretz (1882-1981), pionero de la investigación en la formación de paisajes por las inundaciones catastróficas, especialmente por las inundaciones del Bretz (Missoula). Luis García Sainz (1894-1965), uno de los precursores de la geografía física en España. Willi Dansgaard (nacido en 1922), paleoclimatologo y cuaternarista, jugó un papel decisivo en el uso de isótopos de oxígeno para la datación y fue co-identificador de eventos Dansgaard-Oeschger. Hans Oeschger (1927-1998), paleoclimatólogo y pionero en la investigación de núcleos de hielo, co-identificador de eventos Dansgaard-Orschger. Richard Chorley (1927-2002), un elemento clave para la revolución cuantitativa y el uso de la teoría de sistemas en la geografía. Sir Nicholas Shackleton (1937–2006), quien planteó que las oscilaciones en el clima durante los últimos millones de años podría estar relacionada con variaciones en la relación orbital y de posición entre la Tierra y el Sol."
ksampletext_wikipedia_geog_geografiahumana: str = "Geografía humana. La geografía humana constituye la segunda gran división de la geografía general. Como disciplina se encarga de estudiar las sociedades desde una perspectiva espacial, la relación entre estas sociedades y el medio físico en el que habitan, así como los paisajes culturales y las regiones humanas que éstas construyen. Según esta idea, la geografía humana podría considerarse como una geografía regional de las sociedades humanas, un estudio de las actividades humanas desde un punto de vista espacial, una ecología humana y una ciencia de los paisajes culturales. Analiza la desigual distribución de la población sobre la superficie terrestre, las causas de dicha distribución y sus consecuencias políticas, sociales, económicas, demográficas y culturales en relación con los recursos existentes o potenciales del medio geográfico a distintas escalas. Parte de la premisa de que el ser humano siempre forma parte de agrupaciones sociales amplias. Estas sociedades crean un entorno social y físico mediante procesos de transformación de sus propias estructuras sociales y de la superficie terrestre en la que se asientan. Su accionar modifica ambos aspectos en función de las necesidades e intereses que los agentes sociales que las forman, especialmente de los agentes sociales dominantes. Estas transformaciones se deben a procesos económicos, políticos, culturales, demográficos, etc. El conocimiento de estos sistemas geográficos formados por la sociedad y su medio físico (regiones humanas, paisajes culturales, territorios, etc.), es el objeto de estudio de la geografía humana. Podemos considerar como iniciador de la geografía humana a Elisée Reclus en Francia, teniendo como antecedente la obra de Karl Ritter en Alemania. Fue Vidal de la Blache quien definió la geografía como una ciencia de síntesis que estudia la interacción entre el ser humano y su medio, definición que ha perdurado hasta nuestros días en la escuela francesa de Geografía. Concepciones acerca de la geografía humana Retrato de Friedrich Ratzel. Aunque la primera obra de Geografía humana apareció en Alemania en el siglo XIX con el nombre de Antropogeografía, obra de Friedrich Ratzel, fueron varios geógrafos franceses los que le dieron un gran impulso a esta rama de la geografía a fines de dicho siglo y en la primera mitad del siglo XX a nivel de investigación empírica. Más recientemente, la Geografía humana a nivel universitario ha venido siendo dividida en subdisciplinas más específicas y aplicadas. En algunas universidades, aparece con el nombre de Geografía simplemente al desaparecer la Geografía física como disciplina o pasar a otras escuelas y facultades, y lo mismo podemos decir de otras ramas geográficas como es el caso de la Geografía Regional en este caso por absorción o confluencia a un punto de vista común. Entre los geógrafos franceses que han desarrollado obras sobre Geografía humana podemos citar a Vidal de la Blache, Albert Demangeon y Max Derruau, además de Eliseo Reclus, cuya obra El hombre en la Tierra constituye la primera obra de Geografía Humana de orientación ecológica cuidadosa y exhaustivamente desarrollada y que constituye el punto de partida de la geografía francesa que se desarrolló posteriormente. Retrato de Paul Vidal de la Blache, geógrafo humano y fundador de la geografía regional Paul Vidal de la Blache fue el verdadero impulsor de la escuela francesa de geografía. Presenta una visión distinta de la Geografía humana a la desarrollada por Ratzel. Bastantes historiadores de la geografía coinciden en atribuir a Ratzel la visión determinista de la Geografía humana desarrollada con mayor intensidad por su discípula Ellen Churchill Semple en Estados Unidos Vidal de la Blache en cambio es conocido como el fundador del posibilismo geográfico. Sus aportes más importantes al campo de la geografía humana fueron los conceptos de género de vida o modos de vida y el desarrollo del enfoque regional de la geografía. Max Sorre fue uno de los discípulos de Vidal de la Blache que más contribuyó al desarrollo de la Geografía humana en Francia. En El hombre en la Tierra se presentan algunos enunciados que sirven para definir a la geografía humana francesa desde una óptica ecológica y paisajística: El primer problema de la Geografía humana consiste en dilucidar las relaciones entre el hombre y el medio consideradas desde un ángulo espacial. En buena parte, la Geografía humana se nos presenta como una Ecología del Hombre. La Geografía humana es la descripción científica de los paisajes humanos y de su distribución en el globo. La Geografía es también la disciplina de los espacios terrestres; entre todas las ciencias de la Naturaleza y del Hombre, ninguna otra sitúa en primer plano la localización de los fenómenos. Enfoque de la geografía humana Aunque el objetivo de la geografía humana no se centra en el conocimiento del medio físico, estudiado por la geografía física, es necesario cierto conocimiento del paisaje natural para adentrarnos en la geografía ambiental, un campo de estudio emergente dentro de la geografía humana. Los métodos de la geografía humana, lo mismo que sucede con la geografía física, son sumamente diversos, y podemos citar procedimientos tanto cuantitativos como cualitativos, incluyendo entre los primeros, los estudios de casos, las encuestas, el análisis estadístico, y la formulación de modelos, todo lo cual se ha venido agrupando como la geografía cuantitativa, desarrollada en la década de los 60 del siglo XX, con los trabajos iniciales de David Harvey y otros. Y entre los procedimientos de investigación cualitativos podemos señalar todos aquellos utilizados por las ciencias sociales en general, como los que se emplean en demografía, antropología, historia, sociología y muchas otras ciencias. En resumen, la metodología empleada en geografía humana es aproximadamente la misma que la que se emplea en la geografía general y en muchas otras ciencias (aunque con énfasis distinto en cuanto al empleo de dichos métodos), tal vez con la excepción del método regional aunque, en sentido estricto, este método siempre ha sido empleado por numerosas ciencias sistemáticas: no hay muchas limitaciones en el empleo de diferentes metodologías en cualquier ciencia. Y al referirnos a la metodología en las ciencias sociales no podemos olvidar las críticas de Paul Karl Feyerabend en su obra Contra el método (1975, edic. española), donde critica la simplicidad metodológica con que se venían abordando los estudios de historia y de otras ciencias sociales. Campos de estudio de la geografía humana Distribución espacial de la población mundial. Aunque en un principio, el objeto de la geografía humana era el estudio de las regiones humanas y de las relaciones mutuas entre el hombre y el medio natural, el desarrollo progresivo del conocimiento de los procesos sociales obligó a la sucesiva aparición de diversas ramas que enfatizaban algunos de ellos considerándolas como ciencias o ramas relativamente autónomas. Todo ello vino a sustituir el concepto original de la geografía humana por una integración de una serie de conocimientos sistemáticos estudiados con más detalle por ciencias como: Geografía de la población, que estudia los patrones de distribución de los seres humanos sobre la superficie terrestre y los procesos espaciales o históricos que los han originado o modificado. Geografía económica, que estudia los patrones y procesos económicos y su distribución en el tiempo y, esencialmente, en el espacio terrestre. En sentido estricto, la geografía económica estudia la distribución geográfica de los factores económicos y las implicaciones de la misma sobre los países, regiones y, en general, sobre las sociedades humanas. Está estrechamente relacionada con la Economía, pero enfatizando los temas referidos a la distribución geográfica de los factores económicos. Para Krugman es la rama de la economía acerca de la localización de la producción en el espacio. Geografía cultural, que más que un campo de estudio es un enfoque de la geografía humana que investiga las relaciones mutuas entre los seres humanos y el paisaje vistas desde un punto de vista posibilista. Aunque este enfoque fue introducido por Vidal de La Blache, fueron geógrafos como el estadounidense Carl Sauer (de la Escuela californiana de Geografía) y otros, los que lo desarrollaron hasta el punto de que llegaron a formar una escuela o concepción de la geografía en el siglo XX. Vino a surgir como una reacción en contra del determinismo geográfico. El parlamento de la República de Singapur, con rascacielos de la ciudad al fondo de la imagen. Un ícono del campo de estudio de la geografía urbana. Geografía urbana, ciencia que estudia las aglomeraciones humanas representadas por las ciudades, su población, características, evolución histórica, funciones e importancia relativa. Geografía rural, estudia el mundo rural, las estructuras y los sistemas agrarios, los espacios rurales, las actividades económicas que se llevan a cabo en éstos (agricultura, ganadería, turismo), los tipos de asentamiento y los problemas de estas áreas (despoblación, envejecimiento, problemas económicos, problemas ambientales, etc). Como ciencias afines pueden citarse a la agronomía, la sociología rural y la economía. Geografía política, se encarga del estudio de los espacios políticos. Como ciencias afines pueden citarse a la ciencia política, a la geopolítica y al campo multidisciplinario de los estudios internacionales. Geografía médica ciencia que se ocupa del estudio de los efectos del medio ambiente en la salud de las personas y de la distribución geográfica de las enfermedades incluyendo también el estudio de los factores ambientales que influyen en su propagación. Su ciencia auxiliar es la Medicina. Geografía del envejecimiento, también conocida como geografía gerontológica, analiza las implicaciones socioespaciales del envejecimiento de la población a partir de la comprensión de las relaciones entre el entorno físico-social y las personas mayores, a diferentes escalas, micro (vivienda), meso (barrio) y macro (ciudad, región, país), etc. La contribución de los geógrafos del envejecimiento, como Graham D. Rowles, están contribuyendo a la gerontología ambiental comprendiendo los aspectos ambientales de la gerontología en países desarrollados y en desarrollo."

ksampletext_wikipedia_poli_politica: str = "Política. La política es el conjunto de actividades que se asocian con la toma de decisiones en grupo, u otras formas de relaciones de poder entre individuos, como la distribución de recursos o el estatus. También es el arte, doctrina o práctica referente al gobierno de los Estados, promoviendo la participación ciudadana al poseer la capacidad de distribuir y ejecutar el poder según sea necesario para garantizar el bien común en la sociedad. El término puede usarse positivamente en el contexto de una «solución política» que sea comprometedora y no violenta, o descriptivamente como «el arte o la ciencia del gobierno», pero a menudo también tiene una connotación negativa. Por ejemplo, el abolicionista Wendell Phillips declaró que «no jugamos a la política; la lucha contra la esclavitud no es una broma para nosotros». El concepto se ha definido de diversas maneras, y los diferentes enfoques tienen puntos de vista fundamentalmente diferentes sobre si debe usarse de manera extensiva o limitada, empírica o normativa, y sobre si el conflicto o la cooperación son más esenciales para él. En la política se implementan una variedad de métodos, que incluyen promover las propias opiniones políticas entre las personas, negociar con otros sujetos políticos, hacer leyes y ejercer la fuerza, incluida la guerra contra los adversarios. La política se ejerce en una amplia gama de niveles sociales, desde clanes y tribus de sociedades tradicionales, pasando por gobiernos locales, empresas, instituciones modernas y estados soberanos, hasta el nivel internacional. En los estados nacionales modernos, la gente a menudo forma partidos políticos para representar sus ideas. Los miembros de un partido acuerdan adoptar la misma posición en muchos temas y aceptan apoyar proyectos de ley y a sus líderes. Una elección suele ser una competencia entre diferentes partidos. Un sistema político es el marco que define métodos políticos aceptables dentro de una sociedad. La ciencia política constituye una rama de las ciencias sociales que se ocupa de la actividad en virtud de la cual una sociedad, compuesta por seres humanos libres, resuelve los problemas que le plantea su convivencia colectiva. Etimología En español política tiene sus raíces en el nombre de la obra clásica de Aristóteles, Politiká, que introdujo el término del Griego (Πολιτικά, asuntos de las ciudades). A mediados del siglo XV, la composición de Aristóteles se traduciría en inglés moderno temprano como Polettiques [sic],[b] que se convertiría en Politics en inglés moderno. El vocablo política se atestigua en la primera traducción al castellano de 1584 por Simón Abril de Política de Aristóteles a su vez tomado de politicus, una Latinización del griego πολιτικός de πολίτης (polites, ciudadano) y πόλις (ninguna: polis, lit. ciudad). Definiciones Concepto amplio Una definición más amplia (acuñada de diversas lecturas) nos haría definir la política como toda actividad, arte, doctrina u opinión, cortesía o diplomacia, tendentes a la búsqueda, al ejercicio, a la modificación, al mantenimiento, a la preservación o a la desaparición del poder público. En esta amplia definición se puede observar claramente al objeto de la ciencia política, entendido como el poder público sustraído de la convivencia humana, ya sea de un Estado, ya sea de un grupo social: una empresa, un sindicato, una escuela, una iglesia, etcétera. Es por ello que cuando se utiliza la definición más amplia de «política», se suele aclarar que esta es una actividad de la que es muy difícil sustraerse, por encontrarse en casi todos los ámbitos de la vida humana. Concepto restringido Una definición más estricta, propondría que la política es únicamente el resultado expreso oficialmente en las leyes de convivencia en un determinado Estado. Definición que restringe a la vida de las agrupaciones y organizaciones no estatales, limitándolas únicamente a las disposiciones legales de sus Estados. Una perspectiva opuesta contempla la política un sentido ético, como una disposición a obrar en una sociedad utilizando el poder público organizado para lograr objetivos provechosos para el grupo. Así las definiciones posteriores del término han diferenciado poder como forma de acuerdo y decisión colectiva, de fuerza como uso de medidas coercitivas o la amenaza de su uso. Una definición intermedia, que abarque a las otras dos, debe incorporar ambos momentos: medio y fin, violencia e interés general o bien común. Podría ser entendida como la actividad de quienes procuran obtener el poder, retenerlo o ejercitarlo con vistas a un fin que se vincula al bien o con el interés de la generalidad o pueblo. Gramsci concebía la ciencia de la política tanto en su contenido concreto como en su formación lógica, como un organismo en desarrollo. Al comparar a Maquiavelo con Bodin afirma que este crea la ciencia política en Francia en un terreno mucho más avanzado y complejo que Maquiavelo y que a Bodin no le interesa el momento de la fuerza, sino el del consenso. En la misma página Gramsci opina que el primer elemento, el pilar de la política, es el que existen realmente gobernados y gobernantes, dirigentes y dirigidos. Toda la ciencia y el arte político se basa en este hecho primordial, irreductible (en ciertas condiciones generales). El ejercicio de la política permite gestionar los activos del estado nacional, también resuelve conflictos dentro de las sociedades adscritas a un estado específico lo que permite la coherencia social, las normas y leyes que determine la actividad política se vuelven obligatorias para todos los integrantes del estado nacional de donde proceden tales disposiciones. Frank Goodnow hace una especial acentuación sobre la función de la política que corresponde a la voluntad del Estado. Esta se complementa en su ejecución a través del gobierno. La política solo es funcional cuando permite poner reglas entre los gobernantes y los gobernados, los cuales son doblegados a la voluntad de las acciones que se desean orientar con el propósito de alcanzar un determinado fin. A su vez, en la era moderna, la política ha logrado agrandar el espectro de conflicto en la vida privada y publica de los ciudadanos, debido a la voluntad de promover intereses individuales y colectivos. Historia Esta sección es un extracto de Historia de las doctrinas políticas.[editar] Platón y Aristóteles en el fresco de Rafael La escuela de Atenas. Historia de las doctrinas políticas, historia de la teoría política, historia de las ideas políticas o historia del pensamiento político son expresiones utilizadas para denominar a una disciplina historiográfica confluyente con la parte de la historia de la filosofía que se refiere a la política (filosofía política). Se entiende, genéricamente, como parte de la historia de las ideas y, específicamente, como vertiente historiográfica de la politología o ciencia política, una de las ciencias sociales. La historia del pensamiento político se remonta a la antigüedad temprana, con obras seminales como la República de Platón, la Política de Aristóteles, Arthashastra de Chanakya, así como las obras de Confucio. Prehistoria Frans de Waal argumentó que ya los chimpancés se dedican a la política a través de la manipulación social para asegurar y mantener posiciones influyentes. Las primeras formas humanas de organización social -bandas y tribus- carecían de estructuras políticas centralizadas. A veces se les denomina Sociedades sin Estado. Estados primitivos En la historia antigua, las civilizaciones no tenían límites definidos como los estados actuales, y sus fronteras podrían describirse más exactamente como fronteras. Sumeria Dinástica Temprana, y Egipto Dinástico Temprano fueron las primeras civilizacioness en definir sus fronteras. Además, hasta el siglo XII, muchos pueblos vivían en sociedades no estatales. Éstas van desde la bandas y tribus relativamente igualitarias hasta las jefaturas complejas y altamente estratificadas. Formación del Estado Artículo principal: Formación del Estado Hay una serie de teorías e hipótesis diferentes sobre la formación temprana del Estado que buscan generalizaciones para explicar por qué el Estado se desarrolló en algunos lugares pero no en otros. Otros creen que las generalizaciones no son útiles y que cada caso de formación de un estado temprano debe tratarse por separado. Las teorías voluntarias sostienen que diversos grupos de personas se unieron para formar estados como resultado de algún interés racional compartido. Las teorías se centran en gran medida en el desarrollo de la agricultura, y en la presión demográfica y organizativa que siguió y dio lugar a la formación de estados. Una de las teorías más destacadas sobre la formación temprana y primaria del Estado es la hipótesis hidráulica, que sostiene que el Estado fue el resultado de la necesidad de construir y mantener proyectos de irrigación a gran escala. Las Teorías del conflicto de la formación de los estados consideran que el conflicto y el dominio de una población sobre otra son la clave de la formación de los estados. En contraste con las teorías voluntarias, estos argumentos creen que las personas no acuerdan voluntariamente crear un estado para maximizar los beneficios, sino que los estados se forman debido a alguna forma de opresión de un grupo sobre otros. A su vez, algunas teorías sostienen que la guerra fue fundamental para la formación del Estado. Historia antigua Los primeros estados de tipo fueron los de Sumer Dinástico Temprano y Egipto Dinástico Temprano, que surgieron del período Uruk y del Egipto Predinástico respectivamente alrededor del año 3000 a. C. El Egipto dinástico temprano tenía su base en torno al río Nilo en el noreste de África, las fronteras del reino se basaban en el Nilo y se extendían a las zonas donde existían oasis. La Sumer dinástica temprana estaba situada en el sur de Mesopotamia con sus fronteras extendidas desde el Golfo Pérsico hasta partes del Éufrates y Tigris. Aunque las formas de Estado existían antes del surgimiento del imperio de la Antigua Grecia, los griegos fueron el primer pueblo conocido que formuló explícitamente una filosofía política del Estado y que analizó racionalmente las instituciones políticas. Antes de esto, los estados se describían y justificaban en términos de mitos religiosos. Varias innovaciones políticas importantes de la antigüedad clásica vinieron de las ciudades-estado griegas (polis) y de la República Romana. Las ciudades-estado griegas anteriores al siglo IV concedían derechos de ciudadanía a su población libre; en Atenas estos derechos se combinaron con una forma de gobierno democrática directa que tendría una larga vida en el pensamiento político y en la historia. Estados modernos Presentación de mujeres votantes (1935) La Paz de Westfalia (1648) es considerada por científicos políticos como el inicio del sistema internacional moderno, en el que las potencias externas deben evitar interferir en los asuntos internos de otro país. El principio de no injerencia en los asuntos internos de otros países fue expuesto a mediados del siglo XVIII por el jurista suizo Emer de Vattel. Los Estados se convirtieron en los principales agentes institucionales de un sistema de relaciones interestatales. Se dice que la Paz de Westfalia puso fin a los intentos de imponer una autoridad supranacional a los Estados europeos. La doctrina westfaliana de los estados como agentes independientes se vio reforzada por el auge en el pensamiento del siglo XIX del nacionalismo, en virtud del cual se asumía que los estados legítimos Estados se correspondían con las naciones, es decir, con grupos de personas unidas por la lengua y la cultura. Ciencia política Artículo principal: Ciencia política La ciencia política es el estudio científico de la política. Es una ciencia social que se refieren a los sistemas de gobierno y el poder, y el análisis de actividades políticas, pensamientos políticos, el comportamiento político y asociados constituciones y leyes. La ciencia política moderna generalmente se puede dividir en tres subdisciplinas de política comparada, relaciones internacionales y teoría política. Otras subdisciplinas notables son las políticas públicas y la administración, la política interna y el gobierno (a menudo estudiados dentro de la política comparada), la economía política y la metodología política. Además, la ciencia política está relacionada y se basa en los campos de la economía, el derecho, la sociología, la historia, la filosofía, geografía humana, periodismo, antropología política, psicología y política social. La ciencia política es metodológicamente diversa y se apropia de muchos métodos que se originan en la psicología, la investigación social y la neurociencia cognitiva. Los enfoques incluyen el positivismo, el interpretivismo, la teoría de la elección racional, el conductismo, el estructuralismo, el postestructuralismo , el realismo, el institucionalismo y el pluralismo. La ciencia política, como una de las ciencias sociales, utiliza métodos y técnicas que se relacionan con los tipos de investigaciones buscadas: fuentes primarias , como documentos históricos y registros oficiales, fuentes secundarias, como artículos de revistas académicas, investigación de encuestas, análisis estadístico, estudios de casos, investigación experimental y construcción de modelos. Sistemas políticos Esta sección es un extracto de Sistema político.[editar] Un sistema político es la plasmación organizativa de un conjunto de interacciones que son estables a través de las cuales se ejerce la política en un contexto limitado por el Estado de Derecho. Este sistema está formado por agentes, instituciones, organizaciones, comportamientos, creencias, normas, actitudes, ideales, valores y sus respectivas interacciones, que mantienen o modifican el orden del que resulta una determinada distribución de utilidades, conllevando a distintos procesos de decisión de los factores, que modifican la utilización del poder por parte de lo político a fin de obtener el objetivo deseado. Trata de satisfacer las necesidades de la población. Formas de gobierno Esta sección es un extracto de Forma de gobierno.[editar] Sistemas de gobierno Repúblicas color #0068ad Presidenciales (58) color #f8a345 Parlamentarias (41) color #f7f171 Semipresidenciales (26) color #4bb16a Parlamentarias mixtas (9) Monarquías color #af77a1 Constitucionales (22) color #fd0011 Parlamentarias (17) color #893070 Absolutas (4) Otros color #d0e1b3 Juntas militares (6) color #731610 Unipartidistas (6) color #b9b9b9 Gobiernos de transición (4) Nota: varios Estados se declaran constitucionalmente a sí mismos como repúblicas multipartidistas, mientras que exteriormente se les considera Estados autoritarios. Este mapa representa el sistema político de iure y no su grado de madurez democrática de facto. La forma de gobierno, también denominada régimen de gobierno, sistema de gobierno, modelo de gobierno, régimen político o sistema político,[nota 1] es un concepto central de la ciencia política, la teoría del Estado y el derecho constitucional. No debe confundirse con la forma de Estado, que es la organización territorial del Estado. Hace referencia al modelo de organización del poder político que adopta un Estado, en función de la relación que se establece entre los distintos poderes públicos. La forma en que se estructura y ejerce la autoridad determina el modo de coordinación entre las instituciones estatales y los mecanismos de control y equilibrio que caracterizan a cada sistema político. Los distintos modelos de gobierno varían según el contexto histórico y las condiciones estructurales o coyunturales de cada país. Su configuración puede responder a factores estructurales ,como el territorio, la historia, la cultura, la religión o la idiosincrasia nacional, o a circunstancias específicas ,como períodos de crisis económica, guerras, catástrofes, vacíos de poder, ausencia de consenso o de liderazgo,, y en todo caso reflejan la plasmación política de un determinado proyecto ideológico. La denominación correspondiente a la forma o modelo de gobierno (además de referencias a la forma de Estado, que indica la estructura territorial) suele incluso incorporarse al nombre o denominación oficial del Estado, con términos de gran diversidad y que, aunque proporcionan cierta información sobre lo que proclaman, no responden a criterios comunes que permitan definir por sí solos su régimen político. Por ejemplo: Estado Plurinacional de Bolivia, Estados Unidos Mexicanos, República Bolivariana de Venezuela, Reino de España, Principado de Andorra, Gran Ducado de Luxemburgo, Federación Rusa, República Popular Democrática de Corea, Emiratos Árabes Unidos o República Islámica de Irán. Entre los 200 estados, solo hay 18 que no añaden ninguna palabra más a su nombre oficial, como por ejemplo: Jamaica; mientras que 11 sólo indican que son estados. La forma más común es la república con 134 países, seguida de la monarquía con 43; otras forma de gobierno son las juntas militares (6), los estados unipartidistas (6) y los gobiernos provisionales (4). Hay muy distintas nomenclaturas para denominar las distintas formas de gobierno, desde los teóricos de la Antigüedad hasta la Edad Contemporánea; en la actualidad suelen utilizarse de forma habitual tres tipos de clasificaciones: El carácter electivo o no de la jefatura de Estado define una clasificación, entre repúblicas (electiva) y monarquías (no electiva). El grado de libertad, pluralismo y participación política define otra clasificación, entre sistemas democráticos, autoritarios, y totalitarios, según permitan en mayor o menor grado el ejercicio de la discrepancia y la oposición política o bien niegan más o menos radicalmente la posibilidad de disidencia (estableciendo un régimen de partido único, o distintos tipos de regímenes excepcionales, como las dictaduras o las juntas militares); a su vez el sistema electoral por el que en los sistemas participativos se expresa la voluntad popular ha tenido muy diversas conformaciones históricas (democracia directa o asamblearia, democracia indirecta o representativa, sufragio censitario o restringido, sufragio universal masculino o de ambos sexos, diferentes determinaciones de la mayoría de edad, segregación racial, inclusión o no de los inmigrantes, y otros), así como muy distintas maneras de alterarlo o desvirtuarlo (burgo podrido, gerrymandering, fraude electoral, pucherazo). La relación existente entre la jefatura del Estado, el gobierno y el parlamento define otra clasificación más, entre presidencialismos y parlamentarismos (con muchos grados o formas mixtas entre uno y otro). Estas tres clasificaciones no son excluyentes, sino que se complementan, de modo que una república puede ser democrática (Estados Unidos o Sudáfrica) o no democrática (China o Corea del Norte); una democracia republicana puede ser parlamentaria (Alemania o India), semipresidencialista (Francia o Portugal) o presidencialista (Turquía o Corea del Sur); y una monarquía puede ser democrática y parlamentaria (España, Reino Unido o Japón), no democrática (Arabia Saudita o Ciudad del Vaticano) o situarse en posiciones intermedias (Marruecos o Bután), muy habitualmente calificadas de forma más o menos anacrónica con términos propios de las formas históricas de la monarquía (monarquía feudal, monarquía autoritaria, monarquía absoluta). Doctrinas e ideologías políticas Esquema bidimensional que muestra las ideologías principales dentro del espectro político. En rojo el totalitarismo o estatismo, en azul el capitalismo antiguo o conservadurismo tradicionalista, en amarillo el totalismo o socialismo, en verde el liberalismo o capitalismo después de las revoluciones burguesas. El eje vertical corresponde al eje moral (autoritarismo-libertarismo) y el eje horizontal al eje económico (izquierda-derecha). Esta sección es un extracto de Doctrina política.[editar] El concepto de doctrina política (tendencia política o corriente política) sirve para distinguir entre partidos o movimientos políticos, así como para identificar subdivisiones dentro de ellos. Cada corriente política se define por los principios y valores que defiende y por las figuras representativas que los encarnan. Por otro lado, los movimientos y partidos políticos son asociaciones de personas organizadas con fines políticos. Sin embargo, hay diferencias clave en su funcionamiento. Los movimientos políticos realizan acciones políticas que pueden ser de alcance general o específico, pero no siempre buscan representación electoral. Esto puede deberse a la falta de acceso a recursos electorales o simplemente a que no consideran necesario participar en el sistema institucional para cumplir sus objetivos. En contraste, los partidos políticos concentran su actividad en lograr representación institucional, ya sea mediante el parlamento u otros niveles de gobierno. Su acción política incluye intervenciones mediáticas, actos proselitistas y actividades públicas orientadas a promover su programa, ideales y principios. Además, los partidos suelen recibir regulación, privilegios e incluso financiamiento estatal, lo que depende de las leyes y libertades públicas vigentes en cada país, así como del sistema electoral establecido. Otra clasificación de las ideologías políticas según Luetich. Todas las ideologías políticas se agrupan en torno a dos dimensiones que son la económica y la social. La dimensión económica está integrada por dos ideologías opuestas, izquierda-derecha, que forman una línea horizontal y la dimensión social está integrada por otras dos ideologías opuestas, autoritarismo-libertarismo, que forman una línea vertical. Juntas estas dos dimensiones integran un mapa ideológico en el cual podemos encontrar cuatro grandes sistemas como el totalitarismo, conservadurismo, socialismo y el liberalismo, y el punto en donde se cruzan las dos líneas se considera como el centro político. Véanse también: Espectro político y Gráfico de Nolan. No obstante, ante la emergencia de problemáticas actuales como el problema ecológico y culturales como el multiculturalismo y la globalización diversos autores muy variados proponen otros sistemas de clasificación basados en premisas que Nolan no previó en su típica clasificación bidimensional. También existen clasificaciones alternativas a las de Nolan elaboradas por personas del ámbito cultural latino como Andrés Ariel Luetich. Clasificación tridimensional de la política según Florent Marcellesi. Eje rojo «autoritarismo-libertarismo»; eje azul «derecha-izquierda»; eje verde «productivista-antiproductivista». Otras clasificaciones sobre las interacciones culturales e históricas Progresistas versus románticas. Aculturación política versus rechazo de la aculturación (globalismo versus nacionalismos). Véase también: choque de civilizaciones Otras clasificaciones sobre la estructura social Colectivistas versus individualistas. Elitismo versus pluralismo (gobiernos no democráticos versus democracias). Otras clasificaciones sobre la propiedad y los medios de producción Rechazo versus aceptación de la propiedad privada. Productivistas versus anti-productivistas. Cultura política Artículo principal: Cultura política Por cultura política se entiende el conjunto de conocimientos, evaluaciones y actitudes que una población determinada manifiesta frente a diversos aspectos de la vida y el sistema político en el que se inserta. Abarca tanto los ideales políticos como las normas operativas de un gobierno, y es el producto tanto de la historia de un sistema político como de las historias de los miembros de este. La cultura política es un concepto profusamente utilizado en la ciencia política desde la década de 1960 a la actualidad, como un modelo alternativo a las premisas marxistas sobre la política. En las últimas décadas, la difusión de estudios efectuados a través de encuestas transnacionales y la multiplicación de estudios de caso, han permitido reunir información sistemática sobre la cultura política de sociedades de todos los niveles de desarrollo y tradiciones culturales. Niveles de política Macropolítica La macropolítica puede describir cuestiones políticas que afectan a todo un sistema político (por ejemplo, el estado nacional) o hacer referencia a interacciones entre sistemas políticos (por ejemplo, relaciones internacionales). La política global (o política mundial) cubre todos los aspectos de la política que afectan a múltiples sistemas políticos, es decir, en la práctica, cualquier fenómeno político que cruce las fronteras nacionales. Esto puede incluir ciudades, estados-nación, corporaciones multinacionales, organizaciones no gubernamentales y/u organizaciones internacionales. Un elemento importante son las relaciones internacionales: las relaciones entre los Estados-nación pueden ser pacíficas cuando se llevan a cabo a través de la diplomacia , o pueden ser violentas, lo que se describe como guerra. Los Estados que pueden ejercer una fuerte influencia internacional se denominan superpotencias , mientras que los menos poderosos pueden denominarse regionales.o potencias medias. El sistema internacional de poder se llama orden mundial , que se ve afectado por el equilibrio de poder que define el grado de polaridad en el sistema. Los poderes emergentes son potencialmente desestabilizadores para él, especialmente si muestran revanchismo o irredentismo. La política dentro de los límites de los sistemas políticos, que en el contexto contemporáneo corresponden a las fronteras nacionales , se conoce como política interna . Esto incluye la mayoría de las formas de política pública , como la política social , la política económica o la aplicación de la ley , que son ejecutadas por la burocracia estatal . Mesopolítica La mesopolítica describe la política de las estructuras intermedias dentro de un sistema político, como los partidos o movimientos políticos nacionales . Un partido político es una organización política que normalmente busca alcanzar y mantener el poder político dentro del gobierno , por lo general participando en campañas políticas , actividades de divulgación educativa o acciones de protesta . Los partidos suelen abrazar una ideología o visión expresada , reforzada por una plataforma escrita con objetivos específicos, formando una coalición entre intereses dispares. Los partidos políticos dentro de un sistema político particular forman juntos el sistema de partidos , que puede ser multipartidista, bipartidista, de partido dominante o de un solo partido, según el nivel de pluralismo. Esto se ve afectado por las características del sistema político, incluido su sistema electoral. De acuerdo con la ley de Duverger, es probable que los sistemas de “ primero después del poste” conduzcan a sistemas bipartidistas, mientras que los sistemas de representación proporcional tienen más probabilidades de crear un sistema multipartidista. Micropolítica La micropolítica describe las acciones de los actores individuales dentro del sistema político. Esto se describe a menudo como participación política. La participación política puede adoptar muchas formas, entre ellas: Activismo Boicotear Desobediencia civil Demostración Petición Piquetes Huelga Resistencia fiscal Votar (o su contrario, abstencionismo ) Valores políticos Democracia Esta sección es un extracto de Democracia.[editar] Asamblea popular en el cantón suizo de Glarus en 2006. Las asambleas en este cantón tienen por función decidir la elección del Consejo, de los tribunales y funcionarios principales, así como la revisión de la constitución, aprobación y discusión de proyectos de ley y el presupuesto, concesión de ciudadanía, creación y suspensión de cargos. El sufragio es una parte importante del proceso democrático. La democracia (del griego: δημοκρατία dēmokratía, dēmos, pueblo y kratos, poder) es una forma de organización social y política presentada en el platonismo y aristotelismo que atribuye la titularidad del poder al conjunto de la ciudadanía. En sentido estricto, la democracia es un tipo de organización del Estado en el cual las decisiones colectivas son adoptadas por el pueblo mediante herramientas de participación directa o indirecta que confieren legitimidad a sus representantes. En sentido amplio, democracia es una forma de convivencia social en la que los miembros son libres e iguales y las relaciones sociales se establecen conforme a mecanismos contractuales. La democracia se puede definir a partir de la clasificación de las formas de gobierno realizada por Platón, primero y Aristóteles, después, en tres tipos básicos: monarquía (gobierno de uno), aristocracia (gobierno «de los mejores» para Platón, «de los menos», para Aristóteles), democracia (gobierno «de la multitud» para Platón y «de los más», para Aristóteles). Hay democracia indirecta o representativa cuando las decisiones políticas son adoptadas por personas reconocidas por el pueblo como sus representantes. Hay democracia participativa cuando se aplica un modelo político que facilita a los ciudadanos su capacidad de asociarse y organizarse de tal modo que puedan ejercer una influencia directa en las decisiones públicas o cuando se facilita a la ciudadanía amplios mecanismos plebiscitarios consultivos. Finalmente hay democracia directa cuando las decisiones son adoptadas directamente por los miembros del pueblo, mediante plebiscitos y referéndums vinculantes, elecciones primarias, facilitación de la iniciativa legislativa popular y votación popular de leyes, concepto que incluye la democracia líquida. Estas tres formas no son excluyentes y suelen integrarse como mecanismos complementarios en algunos sistemas políticos, aunque siempre suele haber un mayor peso de una de las tres formas en un sistema político concreto. No hay que confundir a la república con la democracia, pues aluden a principios distintos. Según James Madison, uno de los padres fundadores de los Estados Unidos: Los dos grandes puntos de diferencia entre una democracia y una república son: primero, la delegación del gobierno, en esta última, a un pequeño número de ciudadanos elegidos por el resto; en segundo lugar, el mayor número de ciudadanos, y la mayor esfera del país, sobre el cual puede extenderse este último. Igualdad social Esta sección es un extracto de Igualdad social.[editar] Igualdad, en francés Égalité segundo principio del lema Liberté, égalité, fraternité de la Revolución francesa. Atenea, alegoría de la justicia, lleva en su mano izquierda la Declaración de los Derechos del Hombre y del Ciudadano de 1789 - grabado de 1793 de Jean-Guillaume Moitte (1746-1810) Igualdad social es la característica de aquellos estados en los que todos sus individuos o ciudadanos sin exclusión, alcanzan en la práctica la realización de todos los derechos humanos, fundamentalmente los derechos civiles y políticos, así como los derechos económicos, sociales y culturales necesarios para alcanzar una verdadera justicia social. La igualdad social supone el reconocimiento de la igualdad ante la ley, la igualdad de oportunidades así como la igualdad de resultados civiles, políticos, económicos y sociales. La igualdad social es lo opuesto a la desigualdad social -desigualdad económica, esclavitud, racismo, sexismo, sociedad de castas y estamentos-, así como cualquier otro tipo de discriminación por género, etnia, orientación sexual, recursos, religión, especie biológica, idioma, edad, por discapacidad -física o intelectual- o cualquier otra condición personal. Libertad Artículo principal: Libertad La libertad política (también conocida como autonomía política o agencia política) es un concepto central en la historia y el pensamiento político y una de las características más importantes de las sociedades democráticas. La libertad política se describió como la ausencia de opresión o coerción, la ausencia de condiciones incapacitantes para un individuo y el cumplimiento de las condiciones propicias, o la ausencia de condiciones de vida de coacción, por ejemplo, coacción económica, en una sociedad. Aunque la libertad política a menudo se interpreta negativamentecomo la ausencia de restricciones externas irrazonables a la acción, también puede referirse al ejercicio positivo de derechos, capacidades y posibilidades de acción y al ejercicio de derechos sociales o grupales. El concepto también puede incluir la libertad de restricciones internas sobre la acción política o el discurso (por ejemplo, conformidad social, coherencia o comportamiento no auténtico). El concepto de libertad política está estrechamente relacionado con los conceptos de libertades civiles y derechos humanos , que en las sociedades democráticas suelen gozar de protección legal por parte del Estado. Disfunción política Corrupción política Esta sección es un extracto de Corrupción política.[editar] Reformadores como el estadounidense Joseph Keppler describieron que el senado de los Estados Unidos del siglo XIX estaba controlado por las gigantescas bolsas de dineros, que representaban los fideicomisos y monopolios financieros que se mantuvieron vigentes mediante la corrupción política. La corrupción política es un fenómeno criminal que consiste en la actuación intencionalmente indebida de funcionarios y autoridades públicas, usualmente en connivencia, o por indicación, o presión de personas físicas ajenas al Estado, empresas privadas o extranjeras y grupos de poder, haciendo uso de los recursos del Estado a los que tienen acceso, para conseguir un beneficio ilegítimo, generalmente de forma secreta. Según Hernández Gómez (2018), la corrupción se define como «toda violación o acto desviado, de cualquier naturaleza, con fines económicos o no, ocasionada por la acción u omisión de los deberes institucionales, de quien debía procurar la realización de los fines de la administración pública y que en su lugar los impide, retarda o dificulta». Por esta razón se puede hablar del nivel de corrupción o de transparencia de un Estado legítimo. Las formas de corrupción varían, pero las más comunes son el uso ilegítimo de información privilegiada y el patrocinio; además de los sobornos, el tráfico de influencias, la evasión fiscal, las extorsiones, los fraudes, la malversación, la prevaricación, el caciquismo, el compadrazgo, la cooptación, el nepotismo, la impunidad y el despotismo. La corrupción facilita a menudo otro tipo de hechos criminales como el narcotráfico, el lavado de dinero, la prostitución ilegal y la trata de personas, aunque por cierto no se restringe a estos crímenes organizados y no siempre apoya o protege otros crímenes. Un antídoto conocido contra la corrupción política es la transparencia. Conflicto político Esta sección es un extracto de Conflicto.[editar] El conflicto es una situación en la cual dos o más personas con intereses distintos entran en confrontación, oposición o emprenden acciones mutuamente antagonistas, con el objetivo de dañar, eliminar a la parte rival o arrebatarle poder de algún tipo en favor de la propia persona o grupo."
ksampletext_wikipedia_poli_gobierno: str = "Gobierno. El Gobierno (del griego: kybernéin, «pilotar un barco», también «dirigir» o «gobernar») es el principal pilar del Estado; la autoridad que dirige, controla y administra sus instituciones, la cual consiste en la conducción política general o ejercicio del poder ejecutivo del Estado. En ese sentido, habitualmente se entiende por tal órgano (que puede estar formado por un presidente o primer ministro y un número variable de ministros) al que la Constitución o la norma fundamental de un Estado atribuye la función o poder ejecutivo, y que ejerce el poder político sobre una sociedad. También puede ser el órgano que dirige cualquier comunidad política. Más estrechamente, Gobierno significa el conjunto de los ministros, es decir, es sinónimo de «gabinete». Son las definiciones formales de lo que tangiblemente es un Gobierno; pero sustancial e intangiblemente el gobierno de un Estado comprende el conjunto de intereses vitales que ejercita y defiende a través de los objetivos nacionales permanentes, estos son las pautas o normas de conducta inalterables en el arte de gobernar, como la vigencia de la integridad territorial, o la división del poder en tres ramas, para lo cual por periodos que varían entre cuatro y seis años generalmente, se identifican cuales objetivos nacionales actuales, conducen a la vigencia de los intereses vitales, cualquiera que sea la orientación ideológica y filosófica del gobernante de turno. Definición La palabra gobierno deriva, en última instancia, del verbo griego κυβερνάω [kubernáo] (que significa dirigir con gubernaculum (timón), cuyo sentido metafórico se atestigua en la Nave del Estado de Platón). El término «Gobierno» se suele utilizar con dos significados diferentes: en un sentido amplio, como equivalente a régimen político, al conjunto de los poderes públicos, y, en un sentido restringido, identificándolo con el poder ejecutivo: El «Gobierno» entendido como conjunto de los poderes públicos. Este primer significado proviene de la tradición clásica. Aristóteles en su Política presentó tres formas de gobierno (monarquía, o gobierno de uno; aristocracia, o gobierno de pocos; y «politeia» o gobierno de los más) con sus respectivas formas degeneradas (tiranía, oligarquía y democracia). En la actualidad esta concepción se utiliza en los países anglosajones en los que el término government equivale a los poderes del Estado. Así entendido, el Gobierno es el conjunto de instituciones, estructuras administrativas y autoridades que ejercen las diversas actividades estatales, pero no debe ser confundido con el Estado. El Gobierno pasa, cambia y se transforma, mientras que el Estado permanece, aunque históricamente puede experimentar algunas transformaciones en algunos aspectos. Así pues, el Gobierno es el conjunto de los órganos directores de un Estado a través del cual se expresa el poder estatal, por medio del orden jurídico. La Enciclopedia Columbia define el gobierno como «un sistema de control social bajo el cual el derecho a hacer leyes, y el derecho a hacerlas cumplir, se confiere a un grupo particular de la sociedad». El «Gobierno» entendido como poder ejecutivo. Esta acepción más específica se refiere al equipo formado por el presidente del Gobierno y sus ministros que forman el gabinete (incluye también al jefe de Estado si ostenta poderes políticos efectivos). Esta concepción, predominante en la Europa continental, deriva fundamentalmente de la obra de Montesquieu De lesprit des lois (1748) en la que expuso su tesis de la división de poderes (poder legislativo, poder ejecutivo y poder judicial). Históricamente el gobierno, entendido como el poder ejecutivo del Estado, tiene su origen en el grupo de consejeros de los que se rodearon los reyes absolutos, que (con diversas denominaciones, según los casos, como «secretario del Despacho») actuaban en su ámbito de competencias (Estado, Gracia y Justicia, Guerra, entre otras) siguiendo las directrices políticas del monarca y solo eran responsables ante él. Al principio despachaban directamente con el rey pero más tarde formaron un órgano colegiado, por lo que los acuerdos se tomaban conjuntamente, pero siempre contando con la aprobación del rey. En las posteriores «Monarquías constitucionales» el «gabinete» o consejo de ministros con su presidente al frente fue también responsable ante el Parlamento, y en las monarquías parlamentarias exclusivamente, por lo que el soberano (cuando subsistió) dejó de formar parte del poder ejecutivo. Por otro lado, la palabra «Gobierno» también suele utilizarse para referirse a los órganos ejecutivos de entidades sociopolíticas subnacionales (estados federados, comunidades autónomas, regiones, territorios dependientes, etc.). Debates en la ciencia política En la ciencia política, desde hace mucho tiempo es un objetivo crear una tipología o taxonomía de polities, ya que las tipologías de los sistemas políticos no son obvias. Es especialmente importante en los campos de ciencia política de política comparada y relaciones internacionales. Como todas las categorías discernidas dentro de las formas de gobierno, los límites de las clasificaciones gubernamentales son fluidos o están mal definidos.[cita requerida] Superficialmente, todos los gobiernos tienen una forma oficial o ideal. Estados Unidos es una república constitucional, mientras que la antigua Unión Soviética era una república socialista. Sin embargo, la autoidentificación no es objetiva y, como sostienen Kopstein y Lichbach, definir los regímenes puede ser complicado. Por ejemplo, Voltaire sostenía que el Sacro Imperio Romano no es ni santo, ni romano, ni un imperio. Identificar una forma de gobierno también es difícil porque muchos sistemas políticos se originan como movimientos socioeconómicos y luego son llevados a los gobiernos por partidos que se autodenominan como esos movimientos; todos con ideologías políticas en competencia. La experiencia de esos movimientos en el poder, y los fuertes vínculos que pueden tener con determinadas formas de gobierno, pueden hacer que se consideren formas de gobierno en sí mismas.[cita requerida] Otras complicaciones incluyen la falta de consenso general o la distorsión o sesgo deliberada de las definiciones técnicas razonables de las ideologías políticas y las formas de gobierno asociadas, debido a la naturaleza de la política en la era moderna. Por ejemplo: El significado de conservadurismo en los Estados Unidos tiene poco que ver con la forma en que se utiliza la definición de la palabra en otros lugares. Como señala Ribuffo, lo que los estadounidenses llaman ahora conservadurismo gran parte del mundo lo llama liberalismo o neoliberalismo; un conservador en Finlandia sería etiquetado como socialista en los Estados Unidos. Desde la década de 1950 el conservadurismo en los Estados Unidos se ha asociado principalmente con el Partido Republicano. Sin embargo, durante la época de la segregación racial en Estados Unidos muchos demócratas sureños eran conservadores, y desempeñaron un papel clave en la «Coalición Conservadora» que controló el Congreso de 1937 a 1963. Las opiniones varían según los sujetos en cuanto a los tipos y propiedades de los gobiernos que existen. Los matices de gris son habituales en cualquier gobierno y su correspondiente clasificación. Incluso las democracias más liberales limitan la actividad política de sus rivales en una u otra medida, mientras que las dictaduras más tiránicas deben organizar una amplia base de apoyo, creando así dificultades para encasillar a los gobiernos en categorías estrechas. Ejemplos de ello son las reivindicaciones de la Estados Unidos como una plutocracia en lugar de una democracia ya que algunos votantes estadounidenses creen que las elecciones están siendo manipuladas por los ricos super PACs. El filósofo griego clásico Platón habla de cinco tipos de regímenes: aristocracia, timocracia, oligarquía, democracia y tiranía. Platón también asigna un hombre a cada uno de estos regímenes para ilustrar lo que representan. El hombre tirano representaría la tiranía, por ejemplo. Estos cinco regímenes degeneran progresivamente, empezando por la aristocracia en la cima y la tiranía en la base. Formas de gobierno Artículo principal: Forma de Gobierno Las formas de gobierno actuales se suelen agrupar en dos grandes tipos: democracias y autocracias. Según el politólogo español Jaume Colomer, las primeras se caracterizan por ser «sistemas de consenso» que se basan en «la existencia de una aceptación, o cuando menos, de una tolerancia social generalizada frente al poder establecido» (cuya «concreción más avanzada [sería] el Estado social y democrático de Derecho»), mientras que los segundos serían «sistema de coerción» (también llamados autoritarios) que se basan, «fundamentalmente, en el uso de la fuerza para la preservación de sus estructuras, quedando la creación de consenso en un plano subordinado, cuando no pura y simplemente inexistente». Colomer señala que la dictadura sería la forma moderna que habrían adoptado los «sistemas de coerción» que «han sido ampliamente dominantes a lo largo de la Historia en todas las áreas geográficas, con muy pocas excepciones». Otros autores han señalado que una autocracia (del griego «autokrateia») es una forma de gobierno que concentra el poder en una sola figura (a veces divinizada) cuyas acciones y decisiones no están sujetas ni a restricciones legales externas, ni a mecanismos regulativos de control popular (excepto quizás por la amenaza implícita de un golpe de Estado o de una insurrección en masa). Por el contrario, los sistemas democráticos incluyen la participación de la población general en la toma de decisiones, más notoria como en la democracia directa o más remota como sucede en la democracia representativa. En los estados modernos con millones de personas, se dan formas de democracia representativa, con la posibilidad de referenda y plebiscitos sobre cuestiones particulares, que usualmente obligan al gobierno a decidir entre dos o más alternativas según el voto mayoritario de la población. Por otro lado, las democracias han tenido mayor apoyo de la población que los regímenes no democráticos, por esa razón muchos sistemas autoritarios e incluso totalitarios han llegado a referirse a sí mismos como democracias, democracias populares o democracias orgánicas, cuando en realidad dichos regímenes no serían considerados propiamente democráticos por muchos analistas. Formas de gobierno autoritarias Josep Colomer ha propuesto diferenciar seis formas de gobierno autoritarias: Formas de gobierno tradicionalistas. Según Jaume Colomer, en las «formas de gobierno tradicionalistas» «el sistema político fundamenta toda su legitimidad en la preservación de estructuras tradicionales (más o menos reales o presentadas como tales), casi siempre bajo formas hereditarias (monarquías, emiratos, sultanatos, etc.)» y se caracteriza por la «concentración ejecutiva y legislativa (enmascarada o no esta última por la existencia de algún órgano consultivo) en manos del monarca o figuras asimilables, basado en el apoyo del clan dinástico»; la «difusión de una ideología de preservación de elementos supuestamente constitutivos del cuerpo social que hunden sus raíces en la historia (la defensa de la tradición religiosa suele constituir, en estos casos, un elemento no determinante pero sí relevante); y el «carácter elemental del sistema institucional, normalmente reducido a la existencia de asambleas de notables». Los ejemplos más claros de dictaduras monárquicas son Arabia Saudí y los Emiratos del Golfo Pérsico, mientras que Marruecos y Jordania «presentan aspectos diferenciados en la medida en que han adoptado formas pseudoparlamentarias y han abierto el juego político hasta los límites de una oposición básicamente colaboracionista o, al menos, bajo control». Formas de gobierno teocráticas. Según Colomer, las formas de gobierno teocráticas son aquellas en las que los poderes religiosos ostentan la supremacía institucional. Sin embargo, este politólogo advierte que «para poder hablar propiamente de poder teocrático, no basta con el hecho de que la defensa de las tradiciones religiosas llegue a constituir el referente ideológico fuerte del sistema», como sería el caso de Sudán o de Pakistán. Por esta razón es difícil encontrar en la actualidad ejemplos claros de teocracias. El caso más cercano podría ser la República Islámica de Irán instaurada en 1979, pero su estructura de poder es «bicéfala», «manteniendo lo político y lo religioso una delimitación relativamente clara». Una estructura similar presenta el Emirato Islámico de Afganistán. «En realidad, la única teocracia propiamente dicha que ha llegado hasta el siglo XX ha sido el régimen lamanista tibetano», hasta la anexión del Tíbet por la República Popular China en 1950-1951, puntualiza Colomer. Regímenes militares. La dictadura militar se considera el tipo de dictadura más simple y más abiertamente coercitivo ya que sus dirigentes proceden del ejército, la institución coercitiva por excelencia. Se caracterizan por una escasa o nula institucionalización, el uso brutal y sistemático de la represión, la presencia subordinada de civiles en cargos políticos y la apelación a las justificaciones ideológicas (anticomunismo, defensa de la «civilización occidental» o de los valores tradicionales «en peligro», etc.). Se suelen distinguir dos variables: la personalista (el poder lo detenta un único militar indiscutido, como la dictadura de Franco en España) o la colegial (en la que el poder es detentado por una «junta» o «directorio», en el que el liderazgo puede ser rotatorio, como las Juntas Militares de Argentina). Regímenes cívico-militares. Colomer los distingue de los regímenes militares propiamente dichos porque en los regímenes cívico-militares la dominación de las Fuerzas Armadas no se presenta en exclusiva sino que es compartida por elementos civiles. Según este politólogo, se pueden distinguir tres situaciones: Regímenes de junta cívico-militar. Los militares y los civiles comparten el vértice del poder conjuntamente. Puede existir un cierto juego político interno pero siempre bajo la indiscutible hegemonía militar. Ejemplo: la Dictadura cívico-militar de Uruguay que encabezó Juan María Bordaberry. Regímenes de junta civil con garantía militar. La institución militar, iniciadora del régimen generalmente mediante un golpe de Estado, regresa a los cuarteles para dar paso a personal civil, cuya actuación estará permanentemente bajo la influencia de aquella. A veces en la junta civil también participan militares e incluso pueden llegar a presidirla, como en los casos de Argelia o la Indonesia de Suharto. Regímenes de «democracia vigilada». «Pese al funcionamiento aparentemente normal de instituciones democrático-representativas, la institución militar constituye un poder de reserva que se pone de manifiesto, en ocasiones de crisis, con un intervencionismo político, directo o indirecto, determinante. Es el caso paradigmático de Turquía, Malasia o, con matices, Egipto». Regímenes de partido único. Como su nombre indica se caracterizan por la existencia de un único partido político que controla el poder del Estado. Colomer propone distinguir tres situaciones: El partido-Estado, que sería el caso paradigmático de los totalitarismos.[nota 1] El partido subordina al Estado hasta tal punto que se confunde con él, convirtiéndose de hecho en el Estado mismo. Tras los casos de la Alemania nazi, la Italia fascista y la Unión Soviética, en la actualidad solo se puede citar un único ejemplo: la Corea del Norte de la saga de Kim Il-sung. El partido de control de masas. Una situación muy próxima a la del «partido-Estado», en la que el partido único tiene como función preferente el control social. Según Colomer, «esta forma de Gobierno participa de elementos propios del totalitarismo y vendría a ser una forma menos acabada del mismo». Los casos más significativos son el Partido Comunista de China en la República Popular de China y el Partido Comunista de Cuba en la Cuba castrista y postcastrista. El partido fachada. Es una situación, frecuente en las dictaduras militares, en la que el partido único constituye solo una fachada del poder, sin un papel relevante en la toma de decisiones, como ocurrió con el Movimiento Nacional en la Dictadura de Francisco Franco. Autoritarismos de base étnica. Se trata de regímenes autoritarios en los que una etnia asegura su supremacía sobre otras recurriendo a la fuerza («etnocracia»). Los casos más frecuentes se dieron en los países descolonizados de África y Asia, en la década de 1960, durante la Guerra Fría, y generalmente adoptaron la forma de una dictadura militar. Como ha señalado Jaume Colomer, «en muchas de estas situaciones el Estado como tal no existe. Y el Ejército se asemeja más a una banda armada que a una institución». Como ejemplos más representativos se suelen citar los casos de la República Centroafricana, República Democrática del Congo, Liberia, Somalia, Ruanda o Burundi. Formas de gobierno democráticas El politólogo español Joaquim Lleixà ha señalado como «las tres formas de gobiernos más difundidas en los regímenes democráticos consolidados» las tres siguientes: Parlamentarismo. En esta variante de la democracia liberal el Parlamento es la institución que elige y revoca a los gobiernos, que son responsables ante él. En las elecciones los ciudadanos votan diputados y son éstos los que otorgan o deniegan la confianza al candidato a presidir el gobierno (voto de confianza, moción de censura). El parlamentarismo, cuyo origen hay que situarlo en la Revolución Gloriosa inglesa de 1688, se consolidó en el siglo XX cuando las monarquías constitucionales, que habían puesto fin a las absolutas en el siglo anterior, dieron paso a las repúblicas parlamentarias o a las monarquías parlamentarias (en aquellos países donde la monarquía pervivió). Es la variante democrática dominante en Europa, con la excepción de Francia. Presidencialismo. Esta variante de la democracia liberal nació en Estados Unidos y su funcionamiento institucional quedó establecido en la Constitución aprobada en 1787, y que en lo esencial sigue vigente en la actualidad (con sus veintisiete enmiendas). Partiendo de una rígida separación de poderes, el presidente, elegido directamente por los ciudadanos por un periodo de cuatro años, es a la vez jefe de Estado y jefe del gobierno, y no es responsable (ni los miembros de su Gabinete, nombrados por él) ante el Congreso, que ostenta el poder legislativo y que no puede ser disuelto por el presidente (dando lugar, según Lleixà, a una especie de «monarquía constitucional electiva, pero sin corona»: «al igual que en la vieja monarquía constitucional, también aquí Parlamento [el Congreso] y soberano se sitúan frente a frente, sin que un órgano como el Gobierno medie entre ambos»). Sin embargo, Estados Unidos es el único caso de presidencialismo puro, aunque ha influido en presidencialismos existentes en otros países, especialmente de Latinoamérica. Semipresidencialismo. Es una variante de la democracia liberal cuyo primer ejemplo fue la República de Weimar (1919-1933) ,a la que siguió la Segunda República española (1931-1939), y el más notorio en la actualidad es la Quinta República francesa, establecida por el general De Gaulle en 1958. Se trata de un régimen híbrido entre el presidencialismo y el parlamentarismo. Del primero toma la elección directa por los ciudadanos del Presidente de la República, que no puede ser destituido por el Parlamento y que es quien propone al jefe del Gobierno (además de ostentar amplias atribuciones, especialmente en política exterior y de defensa). Del segundo toma que el jefe de Gobierno para poder desempeñar sus funciones ejecutivas tiene que contar también con la confianza del Parlamento («doble confianza»), que puede destituirlo mediante una moción de censura, obligando al Presidente de la República a presentar un nuevo candidato acorde con las mayorías existentes en la Cámara o convocar elecciones legislativas. Además de en Francia, el sistema semipresidencialista es el que existe en Austria, Irlanda, Islandia, Portugal y Finlandia (aunque en este último país el presidente de la República no es elegido directamente por los ciudadanos). Según Joaquim Lleixà, «los semiparlamentarismos forman parte de un curso de experiencias aludidas en la idea de racionalización del parlamentarismo», con el objetivo de «fortalecer el ejecutivo frente al Parlamento liberal-democrático, crear un Gobierno fuerte», como en el ámbito del parlamentarismo lo constituye la moción de censura constructiva. Otros autores han señalado que históricamente se pueden diferenciar la democracia directa, la democracia representativa o alguna forma mixta (democracia semidirecta). Democracia directa. El Gobierno directo es aquel en el cual el pueblo ejerce directamente las funciones de Gobierno, actúa realizando actos de Gobierno sin representantes. Este régimen no existe actualmente y puede afirmarse que nunca se realizó, en Estado alguno. Solo ha sido posible en pequeñas circunscripciones (Municipios, Cantones suizos). Se ha dicho que en la Antigua Grecia se practicó la Democracia directa; lo que no es exacto, pues si bien el pueblo se reunía en el Ágora para discutir y resolver las cuestiones de Gobierno, era en realidad una aristocracia ya que estaban excluidos los extranjeros, esclavos y mujeres. En la época moderna todos los autores citan como ejemplo de Gobierno directo los cantones suizos. Pero en realidad esas reuniones eran esporádicas y en ellas se limitaban a votar por sí o por no a los proyectos sometidos a su consideración. El Gobierno directo es una forma teórica y actualmente imposible de practicar, por el aumento de población de los Estados y la complejidad de la tarea gubernativa, cada vez más técnica. Democracia representativa. El Gobierno representativo es aquel en el cual las funciones de Gobierno son realizadas por los representantes del pueblo. Actualmente la casi totalidad de los regímenes democráticos de Gobierno son representativos. Los gobernantes son considerados “representantes” de la ciudadanía y son ungidos en su calidad de tales mediante el sufragio. Este es el único contrato del elegido con el elector; el pueblo solamente tiene derecho de elección, la relación de representación se desarrolla a través del partido político. El representante no puede ser revocado, porque sus electores no tienen ningún contrato después del voto, salvo a través del partido político. Teóricamente el votante se inclina por un partido político por adhesión al programa de Gobierno que este propugna y vota por los candidatos de ese partido. Por esa razón el representante debiera cumplir con el programa y las autoridades del partido controlar su actuación. El mayor desarrollo de la democracia representativo se producido en Occidente, basándose en el reconocimiento a la eminente dignidad humana y en la organización estatal con el objeto de fomentar las múltiples posibilidades que derivan de dicha persona. Asimismo se basa en el predominio de la mayoría, pero con respeto a las minorías, lo que conduce al pluripartidismo. Y en cuanto al funcionamiento se han señalado como rasgos esenciales: la pluralidad de órganos constitucionales y la aceptación de la teoría de separación de poderes; la Constitución rígida y un control de constitucionalidad de las leyes ordinarias; el Parlamento electivo; una amplia tutela jurisdiccional de los derechos públicos subjetivos y particularmente los derechos de la libertad civil; una descentralización amplia; e ideas parlamentadas. Democracia semidirecta. El régimen semi representativo es aquel que participa de ambos sistemas; el Gobierno se realiza indirectamente por medio de representantes, pero el pueblo realiza directamente algunos actos de Gobierno, es decir que no limita su intervención al sufragio, sino que a veces utiliza formas de Gobierno directo: plebiscitos, referendos, iniciativa popular. Jefe de Estado: Monarquía o República En cuanto a quien desempeñe la Jefatura del Estado se distingue entre: Monarquía. El jefe de Estado es vitalicio y normalmente hereditario. En las monarquías parlamentarias ostenta funciones exclusivamente simbólicas y protocolarias y es irresponsable políticamente, ya que sus actuaciones siempre están refrendadas por uno u otro miembro del Gobierno, en cuyo caso es éste el responsable ante el Parlamento. República. El jefe de Estado es nombrado por un tiempo limitado y elegido directa o indirectamente por los ciudadanos. En el caso de las repúblicas parlamentarias ostenta únicamente funciones simbólicas y protocolarias y generalmente es elegido por el Parlamento y no directamente por los ciudadanos, quienes sí eligen directamente a los Presidentes de la República en los regímenes presidencialistas y semipresidencialistas, que ostentan amplios poderes políticos. El término República proviene de la idea de considerar el país como un «asunto público» (en latín: res publica), no una propiedad privada o propiedad de los gobernantes, y donde los cargos de los estados se eligen o nombran posteriormente directa o indirectamente en lugar de ser hereditarios. El pueblo, o una parte significativa de él, tiene el control supremo sobre el gobierno y sobre los lugares donde los cargos del estado son elegidos o elegidos por personas elegidas. Una definición simplificada común de república es un gobierno donde el jefe de Estado no es un monarca. Montesquieu incluía tanto las democracias, donde todo el pueblo participa en el gobierno, como las aristocracias u oligarquías, donde solo gobierna una parte del pueblo, como formas republicanas de gobierno. Organización del Gobierno En cuanto a la organización del Gobierno se distingue entre aquellos gobiernos que están formados por un único órgano (monismo) y los que están formados por dos órganos principales (dualismo). Estos órganos pueden ser unipersonales (monocrático) o pluripersonales (colegiado). La combinación de los dos criterios puede dar lugar a diversas variables, pero las principales son dos: Presidencialista (monista y monocrática). El ejemplo más acabado es el de Estados Unidos donde el gobierno está constituido por un único órgano unipersonal: la Presidencia (ciertamente existe un Gabinete pero sus miembros no son responsables ante el Congreso sino exclusivamente ante el presidente que es quien los nombra, con el consentimiento del Senado, y quien únicamente los puede destituir). Es la forma de organización del gobierno propia de los sistemas presidencialistas. Un caso específico lo constituye Suiza, cuyo gobierno también es monista, pero no monocrático, ya que está constituido por un órgano colegiado, el Consejo Federal. Gabinete encabezado por un primer ministro (dualista y colegiada). En los gobiernos organizados de forma dual existe un jefe de Estado (monárquico o republicano) y un Gobierno, que es un órgano colegiado formado por el presidente del Gobierno y los ministros, algunos de los cuales pueden ostentar el cargo de vicepresidentes. Todos ellos conjuntamente forman el consejo de ministros. Sin embargo, en algún caso como el de Reino Unido, no todos los ministros forman parte del Gabinete, al existir un segundo escalón de junior ministers con competencias más sectoriales (equivaldrían a lo que en otros países se denominan Secretarios de Estado). Esta segunda variante de la organización del gobierno es la que predomina en los sistemas parlamentarios y semipresidencialistas. No obstante, esta variable está evolucionado en los últimos decenios en los sistemas parlamentarios hacia una situación monocrática de facto conforme el presidente del Gobierno ha ido ganando peso y protagonismo políticos frente a sus ministros (de hecho las elecciones se focalizan en los candidatos a presidir el Gobierno, aunque lo que se votan son diputados, que son los que finalmente otorgarán o no el voto de confianza al candidato). Sistemas económicos Históricamente, la mayoría de los sistemas políticos se originaron como ideologías socioeconómicas. La experiencia con esos movimientos en el poder y los fuertes lazos que puedan tener con formas particulares de gobierno pueden hacer que se consideren formas de gobierno en sí mismas. Término Definición Capitalismo Sistema socioeconómico en el que los medios de producción (máquinas, herramientas, fábricas, etc.) son de propiedad privada. Comunismo Un sistema socioeconómico en el que los medios de producción son de propiedad común (ya sea por el pueblo directamente, a través de la comunidad o por la sociedad comunista), y la producción se lleva a cabo para el uso, en lugar de para el beneficio. Normalmente, las sociedades comunistas utilizan una economía planificada para dirigir la producción y distribución de bienes y servicios. Distributismo Un sistema socioeconómico en el que se generaliza la propiedad como derecho fundamental; los medios de producción se extienden lo más ampliamente posible en lugar de estar centralizados bajo el control del Estado (socialismo de Estado), de unos pocos individuos (plutocracia), o de corporaciones (corporatocracia). El distributismo se opone fundamentalmente al socialismo y al capitalismo, que los distributistas consideran igualmente defectuosos y explotadores. Por el contrario, el distributismo pretende subordinar la actividad económica a la vida humana en su conjunto, a nuestra vida espiritual, nuestra vida intelectual, nuestra vida familiar. Feudalismo Un sistema socioeconómico de propiedad de la tierra y de obligaciones. Bajo el feudalismo, toda la tierra de un reino era del rey. Sin embargo, el rey regalaba parte de la tierra a los señores o nobles que luchaban por él. Estos regalos de tierras se llamaban señoríos. Luego, los nobles daban parte de sus tierras a los vasallos. Los vasallos tenían que cumplir con las obligaciones de los nobles. Las tierras de los vasallos se llamaban feudos. Socialismo Un sistema socioeconómico en el que los trabajadores, democráticamente y poseen socialmente los medios de producción y el marco económico puede ser descentralizada, distribuida o centralizada planificada o autogestionada en unidades económicas autónomas. Servicios públicos serían de común, colectiva, o propiedad estatal, como atención sanitaria y educación. Estatismo Sistema socioeconómico que concentra el poder en el Estado a expensas de la libertad individual. Entre otras variantes, el término engloba la teocracia, la monarquía absoluta, el nazismo, el fascismo, el socialismo autoritario y la dictadura simple y sin adornos. Estas variantes difieren en cuestiones de forma, táctica e ideología. Estado de bienestar Sistema socioeconómico en el que el Estado desempeña un papel fundamental en la protección y promoción del bienestar económico y social de sus ciudadanos. Se basa en los principios de igualdad de oportunidades, de la distribución equitativa de la riqueza y de la responsabilidad pública por aquellos que no pueden disponer de las provisiones mínimas para una buena vida. Críticas a la noción de Gobierno Este artículo o sección necesita referencias que aparezcan en una publicación acreditada. Busca fuentes: «Gobierno» – noticias · libros · académico · imágenes Diversas ideologías históricas han hecho una crítica radical de la existencia del Estado en sí mismo, o las formas de Gobierno elegidas para dirigir el Estado. Así diversas formas de anarquismo han pugnado por la abolición de ciertas instituciones del Estado, mientras que en general el comunismo no ha abogado por la desaparición inmediata del Estado, sino por la forma que obligatoriamente debe estar constituida el Gobierno y la desaparición de ciertos tipos de Gobierno y los objetivos que debe perseguir dicho Gobierno. Igualmente, otras ideologías como el socialismo, la socialdemocracia, la democracia cristiana, el liberalismo o el fascismo apoyan decididamente la existencia de un Gobierno, y no hacen afirmaciones muy concretas sobre quien debe constituirlo, y más bien tienden a propugnar cuales son los objetivos ideales de un Gobierno. Desde el libertarismo y el anarcocapitalismo, algunos de sus partidarios han criticado la existencia del Gobierno político, no supeditado a la lógica del mercado y han difundido argumentos sugiriendo que el Gobierno es siempre una institución de autoprotección social, poco segura al largo plazo, que tal vez no sea capaz de asegurar los servicios de protección social a futuro, cuando la especie alargue la esperanza de vida por encima de los 100 años."
ksampletext_wikipedia_poli_ideologia: str = "Ideología. En las ciencias sociales, una ideología es un conjunto normativo de emociones, ideas y creencias colectivas que son compatibles entre sí y están especialmente referidas a la conducta social humana. Las ideologías describen y postulan modos de actuar sobre la realidad colectiva, ya sea sobre el sistema general de la sociedad o en uno o varios de sus sistemas específicos, como el económico, social, científico-tecnológico, político, cultural, moral, religioso, medioambiental u otros relacionados con el bien común. El historiador español José Luis Rodríguez Jiménez ha definido la ideología como «un universo de valores o conjunto de ideas que reflejan una concepción del mundo, codificados en un cuerpo doctrinal, con el objetivo de establecer canales de influencia y de justificación de sus intereses [del grupo social o político que la sostiene]». Las ideologías suelen constar de dos componentes: una representación del sistema, y un programa de acción. La representación proporciona un punto de vista propio y particular sobre la realidad vigente, observándola desde una determinada perspectiva compuesta por emociones, percepciones, creencias, ideas y razonamientos, a partir del cual se le analiza y compara con un sistema real o ideal alternativo, finalizando en un conjunto de juicios críticos y de valor que plantean un punto de vista superior a la realidad vigente. El programa de acción tiene como objetivo acercar en lo posible el sistema real existente al sistema ideal pretendido. Por su receptividad frente al cambio, hay ideologías que pretenden la conservación del sistema ,conservadoras,, su transformación radical y súbita ,revolucionarias,, el cambio gradual ,reformistas,, o la readopción de un sistema previamente existente ,restaurativas,. Por su origen, alcance y propósito, las ideologías pueden desarrollarse gradualmente a través de la observación, el diálogo, el ajuste mutuo y el consenso sobre lo que es considerado socialmente correcto, desviado o dañino, o bien ser impuestas (incluso por medio de la violencia) por un grupo dominante especialmente interesado en generar influencia, conducción o control colectivo, sin distinción si este es un grupo social, una institución, o un movimiento político, social, religioso o cultural o si su propósito se centra en promover el bien común o un interés particular. El concepto de ideología se diferencia del de cosmovisión (Weltanschauung) en que este se proyecta a una civilización o sociedad entera, en cuyo caso está relacionado con el concepto de ideología dominante, cuando esta abarca todos los sistemas específicos de la sociedad y es compartida por una amplia mayoría de la población. Por su naturaleza colectiva, el concepto rara vez se restringe al modo de pensar de un individuo aislado o particular. Origen del término El término ideología fue formulado por Antoine Destutt de Tracy (Mémoire sur la faculté de penser, 1796), y originalmente denominaba la ciencia que estudia las ideas, su carácter, origen y las leyes que las rigen, así como las relaciones con los signos que las expresan. Medio siglo más tarde, el concepto acoge su sentido actual al asociarse con una perspectiva epistemológica, fundada por Karl Marx y Friedrich Engels en su obra La ideología alemana (1845-1846), para quienes la ideología es el conjunto de principios que explican el mundo en cada sociedad en función de sus modos de producción, relacionando los conocimientos prácticos necesarios para la vida con el sistema de relaciones sociales. La relación con la realidad es muy importante para mantener esas relaciones sociales, y en los sistemas sociales en los que se da alguna clase de explotación, para evitar que los oprimidos perciban su estado de opresión. En su célebre prólogo a su libro Contribución a la crítica de la economía política Marx escribe: [...] El conjunto de estas relaciones de producción forma la estructura económica de la sociedad, la base real sobre la que se levanta la superestructura jurídica y política y a la que corresponden determinadas formas de conciencia social. El modo de producción de la vida material condiciona el proceso de la vida social política y espiritual en general. No es la conciencia del hombre la que determina su ser sino, por el contrario, el ser social es lo que determina su conciencia. Sociología e ideología Hablamos de ideología cuando una idea o conjunto de ideas determinadas interpretadoras de lo real son consideradas como verdaderas y son ampliamente compartidas conscientemente por un grupo social en una sociedad determinada. Tales ideas se convierten en un rasgo fuertemente identitario, de forma similar a la religión, la nación, la clase social, el sexo, partido político, club social, etc., y se forman tanto grupos pequeños y cerrados como las sectas o grupos mayores y abiertos como los partidarios de un equipo de fútbol. Exteriormente se ha asociado con mayor fuerza a la política, donde el clientelismo de los partidos impone unos intereses estrechos y cerrados. En su desarrollo lleva a que el comportamiento individual pueda derivar en una continuada falsa creencia, en un falso pensamiento y de ahí a una falsa práctica social. Además interiormente, los miembros del grupo ideológico admiten o no que determinado individuo pertenezca al grupo según comparta o no ciertos presupuestos comunes de pensamientos básicos. La ideología interviene y justifica dirigiendo los actos personales o colectivos de los grupos o clases sociales, a cuyos intereses sirve. Pretende explicar la realidad de una forma asumible y tranquilizadora, pero sin crítica, funcionando solo por consignas y lemas. Ahora bien lo que ocasiona son falsas creencias que mantienen la interpretación o justificación previa tal como estaba en el imaginario individual y colectivo, independientemente de las circunstancias reales. Por ello suelen acabar produciendo una separación entre las ideas y su práctica difícilmente asumible en la realidad. Del estudio de la ideología se encarga la sociología del conocimiento, cuyo presupuesto básico es la tendencia humana a falsear la realidad en función del interés. Sigue el interés propio en las maneras de ver el mundo en el grupo social al que se pertenece; maneras que varían socialmente de un grupo humano a otro y dentro de sectores diferentes de la misma sociedad. Interviene sobre el interés personal y cohesiona el grupo donde se asienta, porque construye una identidad ficticia como forma de vivir y valorar una realidad construida al margen de ella misma. De ahí que en la mayoría de los casos lleve a una superposición de discursos según el grado de realidad y a la construcción de utopías. En el terreno político, y en casos extremos, acarrea la mentira repetida, la mendacidad. En general se observa que fácilmente se pasa por un interés desmedido, centrado en la falsa conciencia, hacia la imagen o forma de la idea de la vida interpretada solamente en función de esas ideas, en definitiva, hacia una ideología que tiende al totalitarismo. El origen de las ideologías El origen de la mayoría de las ideologías se encuentra en una corriente filosófica cuando asume una versión muy simplificada y distorsionada, por falsa creencia, de la filosofía original. En este sentido se produce, de forma general, un carácter insincero, cuando un pensamiento original se convierte en «,ismo» (Platón → platonismo; Marx → marxismo; capital → capitalismo; anarquía → anarquismo; etc.). Su origen se sitúa en el ámbito personal, de acuerdo con las necesidades que sustentan socialmente un determinado pensamiento. Se separa y disocia de la realidad, porque la manipula en forma de propio interés. Los primeros filósofos que estudiaron la «ideología», los psicologistas franceses (Condillac, Cabanis, Destutt de Tracy), situaron esa necesidad en el «yo interior», interpretado de diversas formas (psicologismo y psicofisiologismo). El sujeto se opone a lo exterior, que se da como suceso, puesto que requiere la reflexión individual. Estos filósofos franceses pretendían estructurar una teoría sobre el materialismo primitivo de las sensaciones y de ahí su derivación en emociones, pasiones y sentimientos. De manera que del hecho, del suceso o del acontecimiento exteriores se pasa psicológicamente a la manera interior de captar las cosas y apreciar estas categorías de la psicología personal. Más tarde el compromiso político de filósofos sociales (socialistas utópicos, Saint-Simon, Fourier, Proudhon) situó el interés en las necesidades de la vida social. El vuelco que protagonizó al extenderse al ámbito de la sociedad fue considerable. Del interés del individuo se pasó al interés del grupo. Esto provocó que se acuñase el calificativo de «doctrinarios» para referirse a los «ideólogos» en su enfrentamiento con el poder, lo que confirió a la palabra un sentido peyorativo que hasta el momento no ha perdido. Después del psicologismo de los franceses, se pasó, primeramente, a las formas filosóficas propias y, posteriormente, a las relaciones económicas. El sentido más elaborado de ideología, en el primer sentido, es el de Hegel y, en el segundo, de Marx. Se consideró la ideología como una «escisión de la conciencia», que produce la alienación, bien sea ésta considerada como meramente dialéctica del pensamiento, en el idealismo de Hegel o dialéctica material en el materialismo de Marx. En el siglo XX, la ideología es considerada como problema de comunicación social. Para los frankfurtianos, de manera especial para Habermas, la ideología expresa la violencia de la dominación que distorsiona la comunicación. Este habla de la relación entre el conocimiento y el interés. Esto produce una distorsión que es consecuencia de una razón instrumental, como conocimiento interesado, y que es la responsable de la ciencia y la tecnología falsas como ejes de la dominación social. Es pues necesaria una hermenéutica de la emancipación y liberación. De la misma forma, Marcuse subraya este hecho en el seno de las clases sociales, en particular políticamente dentro de los partidos y sindicatos. Karl Mannheim y Max Scheler enmarcan la ideología en el marco de la sociología del saber. El saber enmarcado dentro de la dominación política genera tal cúmulo de intereses que configura la cosmovisión de los grupos sociales. No hay posibilidad de escapar a una ideología bien construida. Todo gira a su alrededor. Mannheim distingue entre ideología parcial, de tipo psicológico, e ideología total, de tipo social. Sartre, por su parte, introduce una idea de «ideología» completamente diferente. Para Sartre la ideología es fruto de un pensador «creador», capaz de generar un modo de ver la realidad. Por otro lado, Willard van Orman Quine trata la relación entre los objetos exteriores, de ahí fuera, y los sujetos interiores, de ahí dentro. En otros términos, liga la ideología a un modo razonado de considerar la ontología. A finales del siglo XX, sin embargo, se entra en una época de infravaloración de lo ideológico, de la mano de las ideologías conservadoras, de forma que algunos han proclamado el ocaso de los ídolos, como El fin de las ideologías, incluso proclamando el triunfo del pensamiento único y el fin de la historia o el choque de civilizaciones. La ideología como falsa creencia debe estudiarse en términos de su lógica degradada, más que en la filosofía de la que se deriva. Sin embargo, es difícil comprender cuándo y en qué términos una filosofía pasa a ser ideología. Max Weber afirma que las filosofías se seleccionan primero para ser ideologías después, pero no explica, cuándo, cómo ni por qué. Lo que sí puede asegurarse es que existe una relación dialéctica, es decir, de discurso, entre ideas y necesidades sociales, y que ambas son indispensables para configurar una ideología. Así nace el interés y las necesidades sentidas por el cuerpo social (o un grupo de este); no obstante pueden fracasar por no tener ideas claras que lo sustenten. Al igual que hay ideas que pueden pasar inadvertidas por no ser relevantes para las necesidades sociales, se requiere una falsa creencia aparentemente útil para que sea ideología. Marx, en su Crítica de la Filosofía del Derecho de Hegel, señala lo siguiente: ...Es cierto que el arma de la crítica no puede sustituir a la crítica de las armas, que el poder material tiene que derrocarse por medio del poder material, pero también la teoría se convierte en poder material tan pronto como se apodera de las masas. Y la teoría es capaz de apoderarse de las masas cuando argumenta y demuestra ad hominem; y argumenta y demuestra ad hominem cuando se hace radical. Ser radical es atacar el problema por la raíz. Y la raíz, para el hombre, es el hombre mismo... Marx. Contribución a la crítica de la filosofía del derecho de Hegel. Anales franco alemanes. 1970. Barcelona. Ed. Martínez-Roca, p 103 Concepto marxista de ideología Artículo principal: Crítica de la ideología Karl Marx plantea que la ideología dominante de una sociedad es parte integral de su Infraestructura y superestructura. Friedrich Engels define la ideología como un proceso que se opera por el llamado pensador pero con una conciencia falsa. Tal como el materialismo histórico define el concepto, la ideología forma parte de la superestructura, junto con el sistema político, la religión, el arte y el campo jurídico. Según la interpretación clásica, está determinada por las condiciones materiales de las relaciones de producción o estructura económica y social. Para Karl Marx, las ideologías son cuerpos de ideas que aspiran a la universalidad y a la verdad más lata y abstracta que representan los intereses históricos de una clase social, que en su mayoría son hipótesis idealistas. Desde esta perspectiva, son formas de falsa conciencia, porque solo reflejan los intereses económicos y preferencias de la clase dominante. Marx pone el ejemplo de la división de poderes como idea dominante, proclamada ahora como «ley eterna» en la época en la que se disputan el poder en un país la corona, la aristocracia y la burguesía. El concepto marxista de ideología se suele datar en las obras La sagrada familia y La ideología alemana como crítica de la filosofía idealista alemana posterior a Hegel. Esta crítica llegó a la economía política burguesa en La miseria de la filosofía y más tarde El capital. aunque ya se aprecia en Crítica de la filosofía del derecho de Hegel con la hipótesis de la negación de la filosofía como filosofía. La clase que tiene a su disposición los medios para la producción material dispone con ello, al mismo tiempo, de los medios para la producción espiritual, lo que hace que se le sometan, al propio tiempo, por término medio, las ideas de quienes carecen de los medios necesarios para producir espiritualmente. Las ideas dominantes no son otra cosa que la expresión ideal de las relaciones materiales dominantes, las mismas relaciones materiales dominantes concebidas como ideas; por tanto, las relaciones que hacen de una determinada clase la clase dominante, o sea, las ideas de su dominación. [...] La división del trabajo [...] se manifiesta también en el seno de la clase dominante como división del trabajo espiritual y material, de tal modo que una parte de esta clase se revela como la que da sus pensadores (los ideólogos conceptivos activos de dicha clase, que hacen del crear la ilusión de esta clase acerca de sí mismo su rama de alimentación fundamental), mientras que los demás adoptan ante estas ideas e ilusiones una actitud más bien pasiva y receptiva, ya que son en realidad los miembros activos de esta clase y disponen de poco tiempo para formarse ilusiones e ideas acerca de sí mismos. K. Marx y F. Engels (1845) La ideología alemana, Capítulo 1, Parte III, 1. La clase dominante y la conciencia dominante. Friedrich Engels explica que las verdaderas fuerzas propulsoras que lo mueven, permanecen ignoradas para el ideólogo”. Sus ideas le parecen al ideólogo como creación, sin buscar otra fuente más alejada e independiente del pensamiento; para él, esto es la evidencia misma, puesto que para él todos los actos, en cuanto les sirva de mediador el pensamiento, tienen también en este su fundamento último. Estos impulsores incluyen tanto intereses subjetivos oscuros como la constelación económica objetiva. Para Engels, la moral y la religión son ejemplos de ideologías. La moral siempre fue una moral de clase; o bien justificaba el dominio y los intereses de la clase dominante, o bien, en cuanto que la clase oprimida se hizo lo suficientemente fuerte, representó la irritación de los oprimidos contra aquel dominio y los intereses de dichos oprimidos, orientados al futuro. El origen de la forma ideológica de la religión es la impotencia del hombre hacia la naturaleza. El bajo nivel de dominio de la naturaleza y la dependencia de eventos naturales desconocidos conducen a prácticas religioso-mágicas para compensar el subdesarrollo económico, técnico y científico: Estas diversas ideas falsas acerca de la naturaleza, el carácter del hombre mismo, los espíritus, las fuerzas mágicas, etc., se basan siempre en factores económicos de aspecto negativo; el incipiente desarrollo económico del período prehistórico tiene, por complemento, y también en parte por condición, e incluso por causa, las falsas ideas acerca de la naturaleza. El desarrollo de una ideología sigue una cierta lógica propia, se desarrolla por medio de la imaginación. Así, la filosofía de cada época tiene como premisa un determinado material de ideas que le legan sus predecesores y del que arranca. Sin embargo, la economía determina el modo cómo se modifica y desarrolla el material de ideas preexistente indirectamente, ya que son los reflejos políticos, jurídicos, morales, los que en mayor grado ejercen una influencia directa sobre la filosofía. El papel de la ideología, según esa concepción marxista de la historia, es actuar de lubricante para mantener fluidas las relaciones sociales, proporcionando el mínimo consenso social necesario mediante la justificación del predominio de las clases dominantes y del poder político. Por otro lado, Engels también enfatiza la efectividad histórica de la ideología. La negación de un desarrollo histórico independiente no significa que no pueda ser puesto en el mundo, una vez por otras causas, en última instancia económicas, y puede tener un efecto en su entorno, de hecho su propia causa. Marx reconoció que dentro de formas ideológicas puede darse elementos de verdad. La existencia de ideas revolucionarias en una determinada época presupone ya la existencia de una clase revolucionaria [...] como representante de toda la sociedad, como toda la masa de la sociedad, frente a la clase única, a la clase dominante. Ibíd. Esta crítica ha contribuido a una desconfianza académica hacia nociones como objetividad, neutralidad, universalidad y semejantes. Entre los marxistas que se han dedicado al estudio de la ideología, o han hecho comentarios significativos sobre el tema, están Marx y Engels, Lenin, Kautsky, Lukács, Althusser, Gramsci, Theodor Adorno y, más recientemente, Slavoj Zizek. Lenin diferenció en ¿Qué hacer? una ideología burguesa que socaba a una ideología socialista mediante el rechazo de la difusión en masa de una conciencia política de clase, siendo imposible que exista una ideología al margen de las clases ni por encima de las clases. Gramsci decía que los análisis culturales e históricos del “orden natural de las cosas en la sociedad” establecido por la ideología dominante permitirían a hombres y mujeres con sentido común percibir intelectualmente las estructuras sociales de la hegemonía cultural burguesa. Pese a que comúnmente suele hablarse de una teoría de la ideología homogénea en el marxismo, ligada al esquema base-superestructura, existen numerosas variaciones teóricas que tratan este tema. Algunos analistas de la teoría de la ideología marxista, por ejemplo Terry Eagleton, han afirmado que en los escritos del propio Marx existen teorías diferentes sobre este tema. Durante la etapa estalinista de la URSS, el marxismo quedó reducido al materialismo dialéctico (o diamat) y a la concepción materialista de la historia. Dichas doctrinas, codificadas y poco cuestionables, eran enseñadas académicamente, con una sección incluso en la Academia de Ciencias. Para los marxistas occidentales, y especialmente para los historiadores de orientación no ortodoxa, que suele llamarse marxiana, sobre todo en Francia e Inglaterra (más o menos ligados a la renovación historiográfica de mediados del siglo XX que supuso la Escuela de los Annales), es imposible explicar la historia de un modo tan determinista. Desde ese punto de vista, suelen encontrarse en la historiografía interpretaciones de la ideología en el sentido de que la inadecuación de la ideología dominante a nuevas condiciones o el surgimiento de ideologías alternativas que entran en competencia con ella, produce una crisis ideológica. Así suele admitirse que, aunque desde un punto de vista marxista clásico suene herético, cuando una ideología dominante no cumple eficazmente su función hace aumentar la tensión social (lucha de clases) que contribuye a la crisis de un modo de producción y su transición al siguiente. La ideología como crítica totalitaria Este artículo o sección tiene referencias, pero necesita más para complementar su verificabilidad. Busca fuentes: «Ideología» – noticias · libros · académico · imágenes Este aviso fue puesto el 20 de febrero de 2023. El contemporáneo filósofo político australiano Kenneth Minogue se dedicó a observar la noción marxista de ideología en su obra La teoría pura de la ideología. Para este autor, El marxismo presupone por ideología un conjunto de ideas funcionales de un individuo que dan justificación y validez universal a sus intereses. Estos intereses se entienden principalmente como la preservación de sus medios económicos de subsistencia una vez adoptados; excluyendo de esta categoría su uso o los fines de consumo, que volverían a los intereses socialmente teleológicos e infraestructuralmente culturales. Los intereses en estas reducidas condiciones materiales de existencia estarían predeterminados tecnológicamente por la particular relación social del individuo con su ubicación en la división del trabajo, cuya forma no sería modificable ni elegible, esto es: sus fines serían necesarios en vez de contingentes. Estos intereses tienen la característica de no ser comunes (salvo con miembros de la misma clase) y contrarios con las otras clases en forma intrínseca, ya que su naturaleza es la de participar en una relación orgánica dual de opresores-oprimidos. Minogue plantea inmediatamente una versión inversa a esta poniendo de cabeza sus premisas básicas: Las verdaderas ideologías son pseudo-revelaciones que reducen toda la realidad a la existencia de grupos y géneros con predeterminados intereses opuestos. Intereses que encarnarían en sí mismos un sistema de opresión (que incluye la opresión de unas ideas funcionales por otras). Requieren interpretar ciegamente el concepto de liberación como eliminación de dichas clases de intereses opuestos. Y el trato pragmático-revolucionario de todo pensamiento funcional como sistemas de ideas (como ideologías) basadas en falsas racionalizaciones (siendo la verdad incognoscible salvo en la realización de la lucha revolucionaria). Las características de esta noción de ideología como dogma crítico se destacan particularmente en el marxismo, y todas tendrían como particular característica su tendencia a degenerar en sociologismos y psicologismos autocontradictorios (teorías de conspiración en las cuales las formas de organización social no serían necesidades históricas que generarían los grupos sociales dominantes y sus ideologías, sino a la inversa serían elites las que crearían la sociedad con una ideología que haría posible su poder; idea esta última que el epistemólogo Karl Popper ya había denunciado como parte de un marxismo vulgarizado y malinterpretado). También la comunidad de intereses entre grupos no solo es arbitraria (clases sociales, géneros, razas), sino que la misma visión ideológica de la sociedad es en realidad la sociedad ideológica que esta genera, ya que aunque presuma combatir un sistema de opresión donde sus elementos son orgánicamente funcionales, dicha opresión dependería solo de su ocultamiento (cuando en realidad tal ocultamiento requeriría de una opresión preexistente) y no sería realmente funcional en tanto no fuera planificada (planificación que la ideología sí necesita generar). Debido a ello, la comunidad de intereses inter-individuales de la que presume el revolucionario ideológico es una ficción útil (el leninismo habría sincerado este hecho al afirmar que los burgueses compiten para vender la soga con la que los van a ahorcar),[cita requerida] pero termina siendo una realidad forzada cuando la ideología llega al poder. Minogue vuelve así, contra las propias doctrinas sistémico-clasistas (que tratan de ideológico a todo pensamiento), la acusación de reificación ideológica en nuevos términos, particularmente al marxismo, la generación y dependencia para con sus propios intereses revolucionarios en una opresiva sociedad sin clases. La tesis de Minogue fue de gran influencia a fines del siglo XX en los círculos políticos e intelectuales más cercanos al pensamiento demoliberal, conservador y neoconservador, por haber dado sistematicidad a la dialéctica de las democracias liberales occidentales en su confrontación con las democracias populares marxistas a lo largo de la Guerra Fría. El siglo de las ideologías La expresión siglo de las ideologías para definir el siglo XX fue acuñada por el filósofo Jean Pierre Faye en 1998. El término ideología, reservado en el siglo XIX al debate intelectual, se convierte en el siglo XX en el vehículo de grandes movimientos sociales y de pensamiento, sobre el soporte de grandes masas que son adoctrinadas por los nuevos medios de comunicación, la propaganda, la violencia y la represión. En el periodo de entreguerras las ideologías políticas enfrentadas son fascismo y comunismo fundamentalmente, aunque del siglo XIX hayan sobrevivido el liberalismo en su versión democrática (frente al que ambos se definen), el conservadurismo, el socialismo democrático, el anarquismo y los nacionalismos. El feminismo, el pacifismo, el ecologismo y los movimientos por la igualdad racial y el reconocimiento de la identidad sexual son ideologías no estrictamente políticas, con fuerte vocación transformadora de la sociedad. El mundo religioso parece estar ausente de la mayor parte de las nuevas visiones del mundo (en alemán Weltanschauung) hasta el final del siglo XX, cuando André Malraux profetizó poco antes de morir (1976): el siglo XXI será religioso o no será. Es pronto para confirmarlo, pero desde entonces el cristianismo integrista, tanto católico como protestante, y el fundamentalismo islámico se han renovado, tanto en los países desarrollados (donde va más allá del interclasismo de la democracia cristiana de posguerra) como en los subdesarrollados (donde sustituye al tercermundismo dominante en el periodo de la descolonización o a la teología de la liberación de los años 1970). Lo mismo ocurre con el nacionalismo hindú. El europeísmo o movimiento europeo ha entrado en una clara crisis ideológica de la que es síntoma la incapacidad de definición de los valores y las fronteras continentales en los debates reformistas que rodean el Tratado de Lisboa dentro de la Unión Europea. El pensamiento débil Por otra parte, desde las décadas de 1980 y 1990, el concepto de ideología sufre una devaluación por su inadecuación a nuevos paradigmas intelectuales emergentes, como el deconstructivismo (Jacques Derrida), o lo más genéricamente llamado postmodernidad, que proponen un pensamiento débil (Gianni Vattimo), en cierto modo una ideología flexible y acomodable a las situaciones de cambio desconcertante que ocurren en el periodo de final del siglo y del milenio (especialmente la caída del muro de Berlín). En ese contexto cultural se entiende la formulación del concepto de la tercera vía (Anthony Giddens), una adaptación a la globalización y el liberalismo económico triunfante desde posiciones socialdemócratas (el laborismo británico de Tony Blair o incluso la presidencia de Bill Clinton) que en la práctica es una aproximación a muchas concepciones del conservadurismo. La ideología en la educación Desde la posición pedagógica crítica, la ideología está presente en los procesos educativos y sus esquemas posteriores de formación, los cuales están cargados de conceptos y narrativas que constituyen formas dominación al difundir aprehensiones que condensan intereses y sesgos propios del conocimiento o de los grupos que detentan el poder. Como constructo político, hace problemático al significado y cuestiona por qué los seres humanos tienen acceso desigual a los recursos materiales e intelectuales que constituyen las condiciones para la producción, consumo y distribución del significado. En esta interpretación, la ideología puede ser vista como un conjunto de representaciones producidas e inscritas en la conciencia y en la conducta humana, en el discurso y en las experiencias vividas [...] afecta y es concretizada en varios textos, prácticas materiales y formas materiales.En este sentido, la ideología y su vínculo con lo educativo se vale de la reflexión constituir procesos emancipatorios ante los intentos de dominación que representa para la formación de docentes, académicos y estudiantes. Uso despectivo del término En ocasiones se usa el concepto ideología para desprestigiar o descalificar a un sistema de pensamiento, concepción del mundo o autor, señalando que está ideologizado. En principio, una ideología es una postura fundamentada que propone un punto de vista superior y programa de acción propositivo ante una situación social. Sin embargo, una ideología en manos de un grupo dominante corrompido opera como un sistema de creencias y racionalizaciones que refuerza su propia posición de privilegio. El uso despectivo del término entiende a la ideología como un discurso de control social que: Obedece a los intereses y al egoísmo grupal de sus postulantes, en lugar de responder a una búsqueda del bien común, Posee un conjunto de soluciones fijas y preestablecidas para los problemas sociales, Es dogmático, planteando premisas normativas irrefutables y que no pueden ser comprobadas, Se acompaña del proselitismo, propaganda y, en grados extremos, del adoctrinamiento. Cuenta con justificaciones internas y causas ajenas a su control para explicar sus propios fracasos, Egoísmo grupal En su disertación sobre el bien humano, Bernard Lonergan detalla la relación entre ideología corrompida y egoísmo grupal de quien la postula, y declara: Mientras que el egoísta individual tiene que soportar la pública censura de su modo de proceder, el egoísmo de grupo no solamente dirige el desarrollo a su propio engrandecimiento, sino que también abre un mercado para las opiniones, doctrinas y teorías que justifican su proceder, y revelarán al mismo tiempo que los infortunios de otros grupos se deben a la depravación que los corroe. Es decir, la ideología se convierte en un medio práctico que habilita a la vez la aprobación de las mayorías, su sometimiento, la autojustificación de conductas y el error de los oponentes, aunque el conjunto de ideas no respondan a la realidad, al interés genuino de la población ni al bien común. Dogmatismo y totalitarismo Según este uso peyorativo, las ideologías ven el mundo como algo estático. Es por este hecho que cualquier ideología se ve a sí misma como la depositaria de las ideas que pueden resolver cualquier problema de la sociedad, ya sea presente o futuro. Esto convierte a la ideología en un dogmatismo, pues se cierra a las ideas de los demás como posible fuente de soluciones a los problemas que se plantean en el día a día, siendo ella la explicación total y última; lo que algunos llaman explicación feroz. En casos extremos, una ideología puede llevar a negar la posibilidad de disentir, dando por verdad irrefutable sus postulados. Llegados a considerar la ideología como verdad irrefutable, se abre el camino al totalitarismo, ya sea político o religioso, también llamado teocracia. Cualquiera que disienta pasa a ser un problema para el grupo dominante, pues va contra la verdad dogmática que proclama la ideología. Tal es el problema que plantean disidentes, facciones y sectas."
ksampletext_wikipedia_poli_capitalismo: str = "Capitalismo. El capitalismo es un sistema económico y sistema social basado en la propiedad privada de los medios de producción y en la asignación de recursos a través del mercado. Su dinámica principal consiste en la inversión de capital con el objetivo de obtener beneficios, lo cual genera relaciones económicas centradas en la producción, el intercambio y el trabajo, ya sea asalariado o autónomo. Desde una perspectiva histórica, el término ha sido objeto de diversas definiciones y aproximaciones teóricas. Autores como Adam Smith, David Ricardo, Karl Marx, Werner Sombart o Max Weber han contribuido a su conceptualización desde enfoques económicos, sociológicos y filosóficos, estableciendo tanto sus fundamentos como sus críticas. En el capitalismo, los individuos y las empresas habitualmente representadas por los mismos, llevan a cabo la producción de bienes y servicios de forma privada e interdependiente, dependiendo así de un mercado de consumo para la obtención de recursos. El intercambio de los mismos se realiza básicamente mediante comercio libre y, por tanto, la división del trabajo se desarrolla de forma mercantil y los agentes económicos dependen de la búsqueda de beneficio. La distribución se organiza, y las unidades de producción se fusionan o separan, de acuerdo a una dinámica basada en un sistema de precios para los bienes y servicios. A su vez, los precios se forman mayoritariamente en un mercado que depende de la interacción entre una oferta y una demanda dadas por la opción elegida de productores y consumidores, y estos, son necesarios para la coordinación ex-post de una economía basada en el intercambio de mercancías. De acuerdo con el concepto de la mano invisible de Adam Smith, la interdependencia existente entre oferta y demanda incentiva a que los productores produzcan solo lo que es necesario para la sociedad debido a la información que obtienen de millones de transacciones individuales. De esta manera, el sistema productivo se autorregula de una manera más eficiente de lo que podría regularse de una manera centralizada. El origen etimológico de la palabra capitalismo proviene de la idea de capital y su uso para la propiedad privada de los medios de producción, sin embargo, se relaciona mayormente al capitalismo como concepto con el intercambio dentro de una economía de mercado que es su condición necesaria, y a la propiedad privada absoluta o burguesa que es su corolario previo. El origen de la palabra puede remontarse antes de 1848 pero no es hasta 1860 que llega a ser una corriente como tal y reconocida como término, según las fuentes escritas de la época. Se denomina sociedad capitalista a toda aquella sociedad política y jurídica originada basada en el respeto a la propiedad privada, los datos de trabajo a domicilio, la organización racional del trabajo, el dinero y la utilidad de los recursos de producción, caracteres propios de aquel sistema económico. Max Weber afirma que el protestantismo contribuyó en conjunto con otros factores al desarrollo del capitalismo, describiendo en su obra que, los valores del calvinismo, como la predestinación y la ética del trabajo, fueron fundamentales en la formación del espíritu capitalista. La doctrina de la predestinación generó una búsqueda constante de señales de elección divina, y la acumulación de riqueza se consideraba un posible signo de ello. La ética del trabajo, basada en el «llamado» o vocación, promovía el trabajo duro y la excelencia profesional como deber religioso, contribuyendo así al desarrollo del capitalismo al fomentar la acumulación de capital y la inversión en el crecimiento económico. En el orden capitalista, la sociedad está formada por clases socioeconómicas en vez de estamentos como son propios del feudalismo y otros órdenes premodernos. Se distingue de aquel y de otras formas sociales por la posibilidad de movilidad social de los individuos, por una estratificación social de tipo económica, y por una distribución de la renta que depende casi enteramente de la funcionalidad de las diferentes posiciones sociales adquiridas en la estructura de producción. El nombre de sociedad capitalista se adopta habitualmente debido a que el capital como relación de producción se convierte dentro de esta en un elemento económicamente predominante. La discrepancia sobre las razones de este predominio divide a las ideologías políticas modernas: el enfoque liberal smithiano se centra en la utilidad que el capital como relación social provee para la producción en una sociedad comercial con una amplia división del trabajo, entendida como causa y consecuencia de la mejora de la oferta de consumo y los mayores ingresos por vía del salario respecto del trabajo autónomo, mientras que el enfoque socialista marxista considera que el capital como relación social es precedido (y luego retroalimentado) por una institucionalizada imposibilidad social de sobrevivir sin relacionarse con los propietarios de un mayor capital físico mediante el intercambio de trabajo asalariado. La clase social conformada por los creadores y/o propietarios que proveen de capital a la organización económica a cambio de un interés se la describe como capitalista, a diferencia de las funciones empresariales cuyo éxito se traduce en forma de ganancia y de las gerenciales ejecutadas a cambio de un salario. Vulgarmente se describe desde el siglo XVIII como burguesía tanto a este conjunto social como al de los empleadores de trabajo de una moderna sociedad industrial, pero la burguesía se origina en las ciudades de la sociedad rural medieval y está constituida por propietarios autoempleados cuya naturaleza da origen al capitalismo moderno. Características Si bien el capitalismo no encuentra su fundador en un pensador sino en las relaciones productivas de la sociedad, la obra La riqueza de las naciones concedió a Adam Smith el título de fundador intelectual del capitalismo. John Locke, con su obra Dos tratados sobre el gobierno civil, establece los principios que posteriormente servirán para identificar el capitalismo como sistema productivo y el liberalismo como sistema de pensamiento que lo respalda. El capitalismo, o más concretamente los sistemas económicos capitalistas, se caracterizan por la presencia de unos ciertos elementos de tipo socioeconómico, pues si un número importante de ellos está ausente el sistema no puede ser considerado como propiamente capitalista. Entre los factores que acaban haciendo que un sistema sea considerado capitalista están: El tipo de propiedad de los medios de producción y el tipo de acceso a los factores de producción. La presencia de dinero, capital y acumulación capitalista. La presencia de mercados de capital y transacciones financieras societarias relacionadas así como el papel asignado a los mismos. La existencia de salarios monetarios y una estructura de clases ligada a las diferentes funciones dentro de la actividad económica. Factores macroeconómicos varios. La Internet Encyclopedia of Philosophy define el capitalismo como un sistema económico que tiene las siguientes características: El tipo de propiedad de los medios de producción son en su mayor parte privada; Las personas son legalmente poseedores de su fuerza de trabajo, y libres de venderla (o retenerla) a otros; La producción está generalmente orientada más hacia la obtención de beneficio que para satisfacer las necesidades humanas; Los mercados desempeñan un papel importante en la asignación de insumos a la producción de productos básicos y en la determinación de la cantidad y la dirección de la inversión. En términos más descriptivos, los sistemas capitalistas son sistemas socioeconómicos donde los activos de capital están básicamente en manos privadas y son controlados por agentes o personas privadas. El trabajo es proporcionado mediante el ofrecimiento de salarios monetarios y la aceptación libre por parte de los empleados. La actividad económica frecuentemente está organizada para obtener un beneficio neto que permita a las personas propietarias que controlan los medios de producción incrementar su capital. Los bienes y servicios producidos son además distribuidos mediante mecanismos de mercado. Si bien todos los sistemas capitalistas existentes presentan un mayor o menor grado de intervención estatal y se alejan por diversas razones del modelo de mercado idealmente competitivo, razón por la cual se definen conceptos como la competitividad o el índice de libertad económica, para caracterizar hasta qué punto difieren unos sistemas capitalistas de otros. Capital, trabajo y régimen de propiedad En los sistemas capitalistas la titularidad de la mayor parte de medios de producción es privada, entendiéndose por esto su construcción sobre un régimen de bienes de capital industrial y de tenencia y uso de la tierra basado en la propiedad privada. Los medios de producción operan principalmente en función del beneficio y en la de los intereses directivos. Se acepta que en un sistema capitalista, la mayor parte de las decisiones de inversión de capital están determinadas por las expectativas de beneficio, por lo que la rentabilidad del capital invertido tiene un papel muy destacado en la vida económica. Junto con el capital, el trabajo se refiere al otro gran conjunto de elementos de producción (algunos autores añaden un factor tradicionalmente llamado «tierra» que en términos generales puede representar cualquier tipo de «recurso natural»). El papel decisivo del trabajo, junto el capital, hacen que uno de los aspectos importantes del capitalismo sea la competencia en el llamado mercado de trabajo asalariado. Sobre la propiedad privada, los sistemas capitalistas tienden a que los recursos invertidos por los prestadores de capital para la producción económica, estén en manos de las empresas y personas particulares (accionistas). De esta forma a los particulares se les facilita el uso, empleo y control de los recursos que se utilizan la producción de bienes y servicios. En los sistemas capitalistas se busca que no existan demasiadas restricciones para las empresas sobre cómo usar mejor sus factores de producción (capital, trabajo, recursos disponibles). Entre las características generales del capitalismo se encuentra la motivación basada en el cálculo costo-beneficio dentro de una economía de intercambio basada en el mercado, el énfasis legislativo en la protección de un tipo específico de apropiación privada (en el caso del capitalismo particularmente lockeano), o el predominio de las herramientas de producción en la determinación de las formas socioeconómicas. Contrato libre, ganancias y movilidad social El capitalismo se considera un sistema económico en el cual el dominio de la propiedad privada sobre los medios de producción desempeña un papel fundamental. Es importante comprender lo que se entiende por propiedad privada en el capitalismo ya que existen múltiples opiniones, a pesar de que este es uno de los principios básicos del capitalismo: otorga influencia económica a quienes detentan la propiedad de los medios de producción (o en este caso el capital), dando lugar a una relación voluntaria de funciones y de mando entre el empleador y el empleado. Esto crea a su vez una sociedad de clases móviles en relación con el éxito o fracaso económico en el mercado de consumo, lo que influye en el resto de la estructura social según la variable de capital acumulada; por tal razón en el capitalismo la pertenencia a una clase social es movible y no estática. Las relaciones económicas de producción y el origen de la cadena de mando ,incluyendo la empresaria por delegación, es establecida desde la titularidad privada y exclusiva de los propietarios de una empresa en función de la participación en su creación en tanto primeros propietarios del capital. La propiedad y el usufructo queda así en manos de quienes adquirieron o crearon el capital volviendo interés su óptima utilización, cuidado y acumulación, con independencia de que la aplicación productiva del capital se genere mediante la compra del trabajo, esto es, el sueldo, realizado por los asalariados de la empresa. Una de las interpretaciones más difundidas señala que en el capitalismo, como sistema económico, predomina el capital ,actividad empresarial, mental, sobre el trabajo ,actividad corporal, como elemento de producción y creador de riqueza. El control privado de los bienes de capital sobre otros factores económicos tiene la característica de hacer posible negociar con las propiedades y sus intereses a través de rentas, inversiones, etc. Eso crea el otro distintivo del capitalismo que es el beneficio o ganancia como prioridad en la acción económica en función de la acumulación de capital que por vía de la compra del trabajo puede separarse del trabajo asalariado. Libre mercado, empresas, competencia y trabajo El capitalismo se basa ideológicamente en una economía en la cual el mercado predomina, esto usualmente se da, aunque existen importantes excepciones además de las polémicas sobre qué debe ser denominado libre mercado o libre empresa. En este se llevan a cabo las transacciones económicas entre personas, empresas y organizaciones que ofrecen productos y las que los demandan. El mercado, por medio de las leyes de la oferta y la demanda, regula los precios según los cuales se intercambian las mercancías (bienes y servicios), permite la asignación de recursos y la distribución de la riqueza entre los individuos. La libertad de empresa propone que todas las empresas sean libres de conseguir recursos económicos y transformarlos en una nueva mercancía o servicio que será ofrecido en el mercado que estas dispongan. A su vez, son libres de escoger el negocio que deseen desarrollar y el momento para entrar o salir de este. La libertad de elección se aplica a las empresas, los trabajadores y los consumidores, pues la empresa puede manejar sus recursos como crea conveniente, los trabajadores pueden realizar un trabajo cualquiera que esté dentro de sus capacidades y los consumidores son libres de escoger lo que desean consumir, buscando que el producto escogido cumpla con sus necesidades y se encuentre dentro de los límites de su ingreso. Esto en un contexto teórico capitalista es denominado cálculo económico. Competencia se refiere a la existencia de un gran número de empresas o personas que ofrecen y venden un producto (oferentes) en un mercado determinado. En dicho mercado también existe un gran número de personas o empresas (demandantes), las cuales, según sus preferencias y necesidades, compran o demandan esos productos o mercancías. A través de la competencia se establece una «rivalidad» o antagonismo entre productores. Los productores buscan acaparar la mayor cantidad de consumidores/compradores para sí. Para conseguir esto, utilizan estrategias de reducción de precios, mejoramiento de la calidad, etc. Al hacer referencia a una fuerza de trabajo libre, se entiende a una mano de obra con la libertad de vender su capacidad de trabajo a cambio de un salario a cualquier patrono potencial. La empresa por sociedad de capitales El tipo de empresa actual suele resultar de una asociación. A principios del siglo XIX, las empresas eran generalmente de un individuo que invertía en ellas capitales, fueran estos propios o procedentes de préstamos, y los ponía al servicio de una capacidad técnica, que generalmente él mismo tenía. Sin embargo, el posterior desarrollo o auge del capitalismo demostraron claramente la superioridad de la empresa, que supera los límites de la personalidad individual o de la continuidad familiar. Este sistema permite al mismo tiempo agrupar capacidades que se completan y disociar las aportaciones de capital de las aptitudes puramente técnicas, antes confundidas. Hay que distinguir dos grandes categorías de sociedades: 1. Las de personas, constituidas por un pequeño número de individuos que aportan al fondo social capitales, llamados (partes) o capacidades técnicas (caso del socio industrial opuesto al capitalista), que, como son en realidad fracciones casi materiales de la empresa no pueden ser cedidas sin el acuerdo de los copartícipes. 2. Las de capitales, en las que las partes llamadas (acciones),se consideran como simples pruebas materiales de la aportación de cierto capital por los asociados, en general numerosos y tienen por tanto la posibilidad de transmitirse o negociarse libremente en la bolsa de valores. Crecimiento económico Teóricos y políticos han enfatizado la habilidad del capitalismo para promover el crecimiento económico a partir de la búsqueda del beneficio propio en un libre mercado, tal como se mide por el producto interno bruto (PIB), utilización de la capacidad instalada o calidad de vida. Sin embargo, debe notarse el análisis de la tasa de crecimiento ha revelado que el progreso técnico y causas no asignables a la intensividad del capital o la asignación de trabajo, parecen ser responsables de gran parte de la productividad (ver productividad total de los factores). Igualmente los sistemas de economía planificada lograron entre 1945-1970 tasas muy superiores a la mayor parte de países capitalistas.[cita requerida] Aun dejando a un lado el peso de los diferentes factores en el crecimiento económico, la posible benéfica influencia de la organización capitalista de la producción ha sido históricamente el argumento central, por ejemplo, en la propuesta de Adam Smith de dejar que el libre mercado controle los niveles de producción y de precio, y distribuya los recursos. Diversos autores han sostenido que el rápido y consistente crecimiento de los indicadores económicos mundiales desde la revolución industrial se debe al surgimiento del capitalismo moderno. Aun cuando parece que parte del crecimiento recogida dentro de la productividad total de los factores no necesariamente está ligada al modo de organización capitalista, sino podría deberse simplemente a factores técnicos cuyo desarrollo obedece a causas más complicadas. Los defensores de que la organización capitalista es el factor principal en el crecimiento argumentan que incrementar el PIB (per cápita) ha demostrado empíricamente una mejora en la calidad de vida de las personas, tal como mejor disponibilidad de alimentos, vivienda, vestimenta, atención médica, reducción de horas de trabajo, y libertad de trabajo para niños y ancianos. Sí parece ampliamente demostrado, que la especialización tanto en la agricultura como en otras áreas, produce un aumento de la producción existente, y la actividad comercial de materias primas aumenta. La consecuencia de este hecho, es el incremento de la circulación de capital, que fue un estímulo a la banca, y por tanto de la riqueza de la sociedad, aumentando el ahorro y con ello la inversión. Este fue fundamentalmente el origen de la banca actual, la cual tenía dos funciones: prestar el dinero que custodiaban a cambio de un interés y la emisión de promesas de pago al contado al portador que circulaban como dinero. Argumentos favorables al capitalismo también afirman que una economía capitalista brinda más oportunidades a los individuos de incrementar sus ingresos a través de nuevas profesiones o negocios que otras formas de economía. Según esta manera de pensar, este potencial es mucho mayor que en las sociedades feudales o tribales o en las sociedades socialistas.[cita requerida] Igualmente, diversos trabajos modernos han enfatizado las dificultades de los sistemas capitalistas no sometidos a regulación, los efectos de la información asimétrica, y la ocurrencia de crisis económicas cíclicas. Organizaciones por interés individual De acuerdo con los argumentos de los defensores del capitalismo, cada uno de los actores del mercado actuaría según su propio interés; por ejemplo, el empleador, quien posee recursos productivos y capital, buscaría maximizar el beneficio económico por medio de la acumulación y producción de mercancías. Por otra parte, los empleados, quienes estarían vendiendo su trabajo a su empleador a cambio de un salario; y, por último, los consumidores, que estarían buscando obtener la mayor satisfacción o utilidad adquiriendo lo que desean o necesitan en función a la calidad del producto y de su precio. De acuerdo con numerosos economistas, el capitalismo podría organizarse a sí mismo como un sistema complejo sin necesidad de un mecanismo de planeamiento o guía externa. A este fenómeno se lo llama laissez faire. Otros economistas modernos han señalado la conveniencia de las regulaciones, especialmente si se tienen en cuenta que las economías están insertas en sistemas sociopolíticos y medioambientales que también es necesario preservar. A este respecto el propio presidente Franklin D. Roosevelt, en un mensaje al Congreso del 29 de abril de 1938 llegó a afirmar: La libertad de una democracia no está a salvo si la gente tolera el crecimiento del poder en manos privadas hasta el punto de que se convierte en algo más fuerte que el propio estado democrático. En cualquier caso es innegable, que para unos y otros el proceso de búsqueda de beneficios tiene un rol importante (ya se prefiera una economía con cierta regulación o una totalmente desregulada). Se admite que a partir de las transacciones entre compradores y vendedores emerge un sistema de precios, y los precios frecuentemente surgen como una señal de cuáles son las urgencias y necesidades insatisfechas de las personas, si bien algunos autores señalan que pueden existir fallos de mercado bajo circunstancias específicas. La promesa de beneficios les da a los emprendedores el incentivo para usar su conocimiento y recursos para satisfacer esas necesidades. De tal manera, las actividades de millones de personas, cada una buscando su propio interés, se coordinan y complementan entre sí. Liberalismo y papel del Estado La doctrina política que históricamente ha encabezado la defensa e implantación de este sistema económico y político ha sido el liberalismo económico y clásico del cual se considera sus padres fundadores a John Locke, Juan de Mariana y Adam Smith. El pensamiento liberal clásico sostiene en economía que la intervención del gobierno debe reducirse a su mínima expresión. Solo debe encargarse del ordenamiento jurídico que garantice el respeto de la propiedad privada, la defensa de las llamadas libertades negativas: los derechos civiles y políticos, el control de la seguridad interna y externa (justicia y protección), y finalmente la implantación de políticas para garantizar el libre funcionamiento de los mercados, ya que la presencia del Estado en la economía perturbaría su funcionamiento. Sus representantes contemporáneos más prominentes son Ludwig von Mises y Friedrich Hayek por parte de la llamada Escuela austríaca de economía; George Stigler y Milton Friedman por parte de la llamada Escuela de Chicago, existiendo profundas diferencias entre ambas. Existen otras tendencias dentro del pensamiento económico que asignan al Estado funciones diferentes. Por ejemplo los que se adscriben a lo sostenido por John Maynard Keynes, según el cual el Estado puede intervenir para incrementar la demanda efectiva en época de crisis. También se puede mencionar a los politólogos que dan al Estado y a otras instituciones un papel importante en controlar las deficiencias del mercado (una línea de pensamiento en este sentido es el neoinstitucionalismo). El economista de la Escuela Kennedy de Harvard, Dani Rodrik, distingue entre tres variantes históricas del capitalismo: El capitalismo 1.0 durante el siglo XIX implicó mercados en gran parte no regulados con un papel mínimo para el Estado (además de la defensa nacional y la protección de los derechos de propiedad). El capitalismo 2.0 durante los años posteriores a la Segunda Guerra Mundial implicó el keynesianismo, un papel sustancial para el Estado en la regulación de los mercados y fuertes Estados de bienestar. El capitalismo 2.1 implica una combinación de mercados no regulados, globalización y diversas obligaciones nacionales de los Estados. Relación con la democracia La relación entre democracia y capitalismo es un área polémica en la teoría y en los movimientos políticos populares. La extensión del sufragio masculino adulto en Gran Bretaña en el siglo XIX ocurrió junto con el desarrollo del capitalismo industrial y la democracia representativa se generalizó al mismo tiempo que el capitalismo, lo que llevó a los capitalistas a postular una relación causal o mutua entre ellos. Sin embargo, según algunos autores del siglo XX, el capitalismo también acompañó a una variedad de formaciones políticas bastante distintas de las democracias liberales, incluidos los regímenes fascistas, las monarquías absolutas y los estados de partido único. La teoría de la paz democrática afirma que las democracias rara vez luchan contra otras democracias. Los críticos moderados argumentan que aunque el crecimiento económico bajo el capitalismo ha llevado a la democracia en el pasado, es posible que no lo haga en el futuro, ya que los regímenes autoritarios han sido capaces de gestionar el crecimiento económico utilizando algunos de los principios competitivos del capitalismo sin hacer concesiones. a una mayor libertad política. Los politólogos Torben Iversen y David Soskice consideran que la democracia y el capitalismo se apoyan mutuamente. En su libro The Road to Serfdom (1944), Friedrich Hayek (1899-1992) afirmó que la comprensión del libre mercado de la libertad económica como presente en el capitalismo es un requisito de la libertad política. El mecanismo es la única forma de decidir qué producir y cómo distribuir los artículos sin utilizar la coacción. Milton Friedman, Andrew Brennan y Ronald Reagan también promovieron este punto de vista. Friedman afirmó que las operaciones económicas centralizadas siempre van acompañadas de represión política. En su opinión, las transacciones en una economía de mercado son voluntarias y la amplia diversidad que permite la actividad voluntaria es una amenaza fundamental para los líderes políticos represivos y disminuye en gran medida su poder de coacción. Algunas de las opiniones de Friedman fueron compartidas por John Maynard Keynes, quien creía que el capitalismo era vital para que la libertad sobreviviera y prosperara. Freedom House, un grupo de expertos estadounidenses que realiza investigaciones internacionales y defiende la democracia, la libertad política y los derechos humanos, ha argumentado que existe una correlación alta y estadísticamente significativa entre el nivel de libertad política medido por Freedom House y libertad económica medida por la encuesta del Wall Street Journal / Heritage Foundation . Milton Friedman, uno de los mayores partidarios de la idea de que el capitalismo promueve la libertad política, argumentó que el capitalismo competitivo permite que el poder económico y político estén separados, asegurando que no chocan entre sí. Los críticos moderados han desafiado esto recientemente, afirmando que la influencia actual que los grupos de presión han tenido en las políticas en los Estados Unidos es una contradicción. Esto ha llevado a la gente a cuestionar la idea de que el capitalismo competitivo promueve la libertad política. El sistema legal de EE. UU. permite a las corporaciones gastar cantidades de dinero no divulgadas y no reguladas en campañas políticas, cambiando los resultados a favor de intereses especiales y socavando la verdadera democracia. Como se explica en los escritos de Robin Hahnel, la pieza central de la defensa ideológica del sistema de libre mercado es el concepto de libertad económica y que los partidarios equiparan la democracia económica con la libertad económica y afirman que solo el sistema de libre mercado puede proporcionar libertad económica. Según Hahnel, hay algunas objeciones a la premisa de que el capitalismo ofrece libertad a través de la libertad económica. Estas objeciones están guiadas por preguntas críticas sobre quién o qué decide qué libertades están más protegidas. A menudo, la cuestión de la desigualdad se plantea cuando se habla de como el capitalismo promueve la democracia. Un argumento que podría sostenerse es que el crecimiento económico puede conducir a la desigualdad dado que el capital puede ser adquirido a diferentes ritmos por diferentes personas. En El capital en el siglo XXI (2013), Thomas Piketty de la Escuela de Economía de París afirmó que la desigualdad es la consecuencia inevitable del crecimiento económico en una economía capitalista y que la concentración de riqueza resultante puede desestabilizar las sociedades democráticas y socavar los ideales de justicia social sobre los que se basan. Los estados con sistemas económicos capitalistas han prosperado bajo regímenes políticos considerados autoritarios u opresivos. Singapur tiene una economía de mercado abierta exitosa como resultado de su clima competitivo y favorable a las empresas y de un sólido estado de derecho. Sin embargo, a menudo es criticado por su estilo de gobierno que, aunque democrático y consistentemente uno de los menos corruptos, opera en gran parte bajo un gobierno de partido único. Además, no defiende enérgicamente la libertad de expresión como lo demuestra su prensa regulada por el gobierno, y su inclinación por defender las leyes que protegen la armonía étnica y religiosa, la dignidad judicial y la reputación personal. El sector privado (capitalista) en la República Popular China ha crecido exponencialmente y ha prosperado desde sus inicios, a pesar de tener un gobierno autoritario. El gobierno de Augusto Pinochet en Chile condujo al crecimiento económico y altos niveles de desigualdad mediante el uso de medios autoritarios para crear un entorno seguro para la inversión y el capitalismo. De manera similar, el reinado autoritario de Suharto y la extirpación del Partido Comunista de Indonesia permitieron la expansión del capitalismo en Indonesia. Origen Skyline de la ciudad inglesa de Mánchester en 1857. Durante el siglo XIX en medio de la Revolución industrial esta ciudad desarrolló tal cantidad de industria textil que fue llamada Cottonopolis, y se convirtió en modelo de la prosperidad provocada por el capitalismo de libre empresa para el movimiento social y político denominado Escuela de Mánchester. Artículo principal: Historia del capitalismo Tanto los mercaderes como el comercio existen desde que existe la civilización, pero el capitalismo como sistema económico, en teoría, no apareció hasta el siglo XVII en Inglaterra sustituyendo al feudalismo. Según Adam Smith, los seres humanos siempre han tenido una fuerte tendencia a «realizar trueques, cambios e intercambios de unas cosas por otras». De esta forma al capitalismo, al igual que al dinero y la economía de mercado, se le atribuye un origen espontáneo o natural dentro de la Edad Moderna. La sustitución del feudalismo tuvo como impulso a poderosas fuerzas del cambio que sirvieron para introducir de forma gradual la estructura de una sociedad de mercado, dentro de las principales fuerzas se encuentran: El papel del mercader ambulante en la introducción del comercio, el dinero y el espíritu adquisitivo. El proceso de urbanización como una fuente de actividad económica y como punto central de un poder nuevo y orientado al comercio. Las cruzadas como una interrupción de la vida feudal y la introducción de nuevas ideas. El surgimiento de estados nacionales que apoyaban y facilitaban el comercio. El estímulo de la edad de la exploración y del oro. El surgimiento de nuevas ideas religiosas que simpatizaban más con la actividad de los negocios que con el catolicismo. La monetización de los tributos dentro del sistema feudal. Todas estas fuerzas del cambio crearon un aspecto económico en la vida de las personas que antes no existía, con estos cambios se empieza a marcar la separación del aspecto social de la vida con el aspecto económico, con este nacimiento del aspecto económico la sociedad empieza a tener fuertes transformaciones, por ejemplo, el siervo ya no está atado a la tierra sino que se convierte en un trabajador libre, el maestro gremial ahora es un empresario independiente, el señor feudal se convierte ahora en un simple arrendatario, estas transformaciones son cruciales para el nacimiento del capitalismo ya que empiezan a introducir las bases de este nuevo sistema económico. El nacimiento de estos trabajadores libres, capitalistas y terratenientes cada uno vendiendo sus servicios en el mercado del trabajo, el capital y la tierra hicieron que nacieran los factores de producción. El orden económico resultante de estos acontecimientos fue un sistema en el que predominaba lo comercial o mercantil, es decir, cuyo objetivo principal consistía en intercambiar bienes y no en producirlos. La importancia de la producción no se hizo patente hasta la Revolución industrial que tuvo lugar en el siglo XIX. El camino hacia el capitalismo a partir del siglo XIII fue allanado gracias a la filosofía del Renacimiento y de la Reforma. Estos movimientos cambiaron de forma drástica la sociedad, facilitando la aparición de los modernos Estados nacionales que proporcionaron las condiciones necesarias para el crecimiento y desarrollo del capitalismo en las naciones europeas. Este crecimiento fue posible gracias a la acumulación del excedente económico que generaba el empresario privado y a la reinversión de este excedente para generar mayor crecimiento, lo cual generó industrialización en las regiones del norte. Tipos de sistemas capitalistas Como se ha indicado anteriormente, existen distintas variantes del capitalismo que se diferencian de acuerdo a la relación entre el mercado, el Estado y la sociedad. Por supuesto, todas comparten características como la producción de bienes y servicios por beneficio, asignación de recursos basada principalmente en el mercado, y estructuración en torno a la acumulación de capital. Es importante destacar que entre los círculos ligados a la Escuela austríaca de economía se conoce como «capitalismo» a su variante más pura, el laissez faire. Otros defensores del capitalismo han adoptado visiones del capitalismo más moderadas y más matizadas con respecto a su implementación práctica. Algunas de las formas de capitalismo históricamente existentes o propuestas son: Mercantilismo y proteccionismo Laissez faire y capitalismo desregulado Capitalismo corporativo Economía social de mercado Economía mixta En gran medida en la mayoría de países modernos predominan formas de capitalismo más cercanas a las dos últimas formas, la economía social de mercado y la economía mixta. El mercantilismo y el proteccionismo parecen casi universalmente abandonados aunque tuvieron su auge durante los siglos XVIII y XIX. Mercantilismo Artículos principales: Mercantilismo y Proteccionismo. Esta es una forma nacionalista del capitalismo temprano que nació aproximadamente en el siglo XVI. Se caracteriza por el entrelazamiento de intereses comerciales de interés para el Estado y el imperialismo y, consecuentemente, por el uso del aparato estatal para promover las empresas nacionales en el extranjero. Un buen ejemplo lo entrega el caso del monopolio comercial impuesto por España a sus territorios de ultramar en 1504 prohibiéndoles comerciar con otras naciones. El mercantilismo sostiene que la riqueza de las naciones se incrementa a través de una balanza comercial positiva (en que las exportaciones superan a las importaciones). Corresponde a la fase de desarrollo capitalista llamada Acumulación originaria de capital. Capitalismo de libre mercado Artículos principales: Laissez faire y Libre mercado. El capitalismo laissez faire se caracteriza por contratos voluntarios en ausencia de intervención de terceros (como pudiere ser el Estado). Los precios de los bienes y servicios son establecidos por la oferta y la demanda, llegando naturalmente a un punto de equilibrio. Implica la existencia de mercados altamente competitivos y la propiedad privada de los medios de producción. El rol del Estado se limita a la producción de seguridad y al resguardo de los derechos de propiedad. Economía social de mercado Artículo principal: Economía social de mercado En este sistema la intervención del Estado en la economía es mínima, pero entrega servicios importantes en cuanto a la seguridad social, prestaciones de desempleo y reconocimiento de derechos laborales a través de acuerdos nacionales de negociación colectiva. Este modelo es prominente en los países de Europa occidental y del norte, aunque variando sus configuraciones. La gran mayoría de las empresas son de propiedad privada. Capitalismo corporativo Artículo principal: Capitalismo corporativo Caracterizado por la dominación de corporaciones jerárquicas y burocráticas. El término «capitalismo monopolista de Estado» fue originalmente un concepto marxista para referirse a una forma de capitalismo en que la política de estado es utilizada para beneficiar y promover los intereses de corporaciones dominantes mediante la imposición de barreras competitivas y la entrega de subsidios. Economía mixta Artículo principal: Economía mixta Una economía mixta está basada en gran medida en el mercado, y consiste en la convivencia de la propiedad privada y la propiedad pública de los medios de producción, y en el intervencionismo a través de políticas macroeconómicas destinadas a corregir los posibles fallos de mercado, reducir el desempleo y mantener bajos los niveles de inflación. Los niveles de intervención varían entre los diferentes países, y la mayoría de las economías capitalistas son mixtas hasta cierto punto. En términos políticos informales se considera que los sistemas capitalistas son opuestos a los sistemas de inspiración socialista. Presuntamente los sistemas socialistas difieren de los sistemas capitalistas en varias maneras: propiedad pública de los medios de producción, los recursos monetarios obtenidos mediante la producción pueden ser utilizados con fines sociales no relacionados con la inversión o la obtención de beneficios. En muchos sistemas históricos de inspiración socialista muchas decisiones importantes de producción fueron directamente planificadas por el estado lo cual dio lugar a sistemas de economía planificada. Tampoco pueden considerarse sistemas capitalistas muchos sistemas socioeconómicos de la antigüedad y la Edad Media, ya que en ellos tenía un papel destacado la mano de obra forzada (como en el feudalismo) o directamente la mano de obra esclava (presente en la antigüedad, la Edad Moderna e incluso perduró inicialmente en las sociedades capitalistas). Tampoco existía en muchos de esos sistemas movilidad social, al tratarse de sociedades estamentarias; ni la producción estaba orientada o racionalizada a la obtención de beneficio económico o a crear sistemas de acumulación capitalista, sino que otros objetivos socialmente deseables para una parte de la sociedad podían tener mayor peso en las decisiones de producción y la actividad económica. Capitalismo de riesgo Comprendido también como sociedad de riesgo, ha sido un vocablo introducido por el sociólogo alemán Ulrich Beck quien comprendía que luego de Chernobyl la sociedad entró en una nueva fase de producción. El riesgo era la base angular de la sociedad que hacía a todas las clases iguales. Este proceso de desjerarquización ha llevado a un fenómeno conocido como proceso de reflexibilidad. En el capitalismo descrito por Beck, los sistemas de producción son descentralizados, en parte como resultado del proceso de reflexibilidad que da origen a formas donde el lego tiene acceso a información que en épocas anteriores eran exclusivas de los expertos. No obstante, la introducción de la tecnología para detectar y reducir ciertos riesgos, engendraba otros no tenidos en cuenta o planificados por los expertos. Anthony Giddens explora el capitalismo del riesgo como una consecuencia del empalme entre la globalización y el mercantilismo. Por su parte, Richard Sennett sugiere que la discursividad del riesgo es útil para que los grupos privilegiados no asuman los riesgos de sus decisiones. El ciudadano moderno debe gestionarse su propia seguridad como signo de estatus, que le permite ingresar al mundo de los buenos ciudadanos. Quienes así no pueden gestionarlo, son tildados de «incapaces» o «personas vulnerables». Ser vulnerable implica no tener autonomía respecto de otros que si pueden autoprotegerse. Este cambio en las políticas de protección se asocia a una tendencia económica que pondera y valoriza a quienes no se apegan a una empresa por muchos años. Los expertos en organizaciones o sociología laboral sugieren que las personas deben cambiar de trabajo en forma periódica debido a que ello sugiere una adaptación sana a lo diferente. Más allá de este discurso subyace una lógica de explotación que intenta romper con los lazos sociales y con el apego tradicional de un sujeto a una organización. Por ese motivo, no es extraño observar que dentro del culto al cambio prime una atmósfera de precarización laboral. Ante el mismo problema Zygmunt Bauman acuña el término «sociedad líquida» para expresar la dinámica del capitalismo moderno. En la sociedad sólida las economías y los lazos institucionales estaban orientados a largo plazo, en forma de una producción de escala. Pero la modernidad ha cambiado a formas más descentralizadas, móviles y menos estables en los canales productivos. Eso ha dado como resultado una sociedad donde los lazos sociales son adaptables al momento y a los intereses individuales de las personas. En la sociedad líquida la seguridad es empleada como una forma discursiva que denota exclusividad y estatus social. Los medios tecnológicos vigentes son usados por los grupos privilegiados no solo para protegerse de ciertos grupos marginales, sino para demostrar ejemplaridad. Capitalismo mortuorio George H. Mead afirmaba que existía una fascinación por las malas noticias, los periódicos y los accidentes porque de esa forma el yo exorciza a la muerte. Se siente una sana alegría ante la tragedia de los demás debido a que se ha evitado ser afectado por el evento. En este sentido, Joy Sather-Wagstaff sugiere que los desastres provocados por el hombre o naturales generan un gran trauma para la sociedad, el cual debe ser regulado por medio de la solidaridad entre las víctimas y los supervivientes. En ciertas ocasiones, el poder político intenta manipular el discurso con el fin de ganar legitimidad frente a los miembros de la comunidad. Se da, entonces, una patrimonialización del dolor que distorsiona las razones reales del desastre. Rememorar la muerte es el primer hecho político que da origen a la cultura. Estas mismas observaciones fueron validadas por la profesora Rodanthi Tzanelli de la Universidad de Leeds, quien sostiene que el cine ha hecho de la muerte una principal mercancía (en inglés: commodity) para ser comercializado por los diferentes agentes del capitalismo al punto de imponer mensajes discursivos hegemónicos. En diversas prácticas como la visita a lugares de extrema pobreza, o a santuarios donde abunda la muerte masiva, estos dispositivos apelan al sufrimiento humano para dotar al consumidor de una realidad apocalíptica. La función de retratar la miseria ajena radica en el reforzamiento de la propia posición de clase ejercida por la élite capitalista. Phillipe Aries por su parte sostiene que el hombre moderno ha perdido la familiaridad con la muerte y a diferencia de sus predecesores ha hecho de ella algo incontrolable, cuyos efectos adquieren una naturaleza desestabilizadora. Por último, la muerte funcionaría según Geoffrey Skoll como un importante discurso para mantener a la masa trabajadora bajo control. Zygmunt Bauman sostiene que el estado de hiper-vigilancia que se ha fundamentado en el uso de tecnologías cumple una doble función. Por un lado protege a los ciudadanos deseables de los indeseables, pero también sirve como criterio de exclusión donde solo unos pocos se aíslan del resto de la sociedad. La exclusividad confiere estatus a ciertos grupos y la vigilancia es el instrumento por medio del cual ese estatus se hace visible a otros quienes no poseen los recursos necesarios para protegerse. Críticas al capitalismo Pyramid of Capitalist System, póster del sindicato Industrial Workers of the World. Critica el capitalismo representándolo como una estructura jerárquica de clases sociales. Artículo principal: Anticapitalismo Parte de la crítica al capitalismo es la opinión de que es un sistema caracterizado por la explotación de la fuerza de trabajo humano al constituir el trabajo como una mercancía más. Esta condición sería su principal contradicción: medios de producción privados con fuerza de trabajo colectiva, de este modo, mientras en el capitalismo se produce de forma colectiva, el disfrute de las riquezas generadas es privado, ya que el sector privado compra el trabajo de los obreros con el salario. La alternativa histórica al capitalismo con mayor acogida ha estado representada por el socialismo.[cita requerida] Marxismo Artículo principal: Modo de producción capitalista Para el materialismo histórico (el marco teórico del marxismo), el capitalismo es un modo de producción. Los marxistas creen que las desigualdades sociales se deben a una continua lucha social, la lucha de clases que tendría una inevitable evolución en el comunismo, en este sistema se plantea una mejora en las relaciones socioeconómicas que mejoraría las condiciones laborales de los trabajadores y evitaría la injusticia social que ellos creen que tiene lugar en el capitalismo. Esta construcción intelectual es originaria del pensamiento de Karl Marx (Manifiesto Comunista, 1848, El Capital, 1867) y deriva de la síntesis y crítica de tres elementos: la economía clásica inglesa (Adam Smith, David Ricardo y Thomas Malthus), la filosofía idealista alemana (fundamentante la dialéctica hegeliana) y el movimiento obrero de la primera mitad del siglo XIX (representado por autores que Marx calificaba de socialistas utópicos). Capitalismo e imperialismo Los críticos del capitalismo lo responsabilizan de generar numerosas desigualdades económicas. Tales desigualdades eran muy acusadas durante el siglo XIX, sin embargo, a lo largo de la industrialización (principalmente en el siglo XX) se experimentaron notables mejorías materiales y humanas. Los críticos del capitalismo (John A. Hobson, Imperialism, a study, Lenin, El imperialismo, fase superior del capitalismo) señalaron desde finales del siglo XIX que tales avances se obtuvieron por un lado a costa del colonialismo, que permitió el desarrollo económico de las metrópolis, y por otro lado gracias al estado del bienestar, que suavizó los efectos negativos del capitalismo e impulsó toda una serie de políticas cuasisocialista. Otras críticas al capitalismo que se enlazan a décadas anteriores con el mismo matiz antiimperialista (a partir del pensamiento centro-periferia) provienen de los movimientos antiglobalización, que denuncian al modelo económico capitalista y las empresas transnacionales como el responsable de las desigualdades entre el Primer Mundo y el Tercer Mundo, teniendo el tercer mundo una economía dependiente del primero. El mercado como institución no natural Desde una perspectiva no estrictamente marxista, Karl Polanyi (La gran transformación, 1944) insiste en que lo crucial en la transformación capitalista de economía, sociedad y naturaleza fue la conversión en mercancía de todos los factores de producción (tierra, o naturaleza y trabajo, o seres humanos) en beneficio del capital. Capitalismo como religión Artículo principal: Capitalismo como religión Capitalismo como religión es un escrito póstumo de 1921 del filósofo alemán Walter Benjamin que contiene una crítica profunda al capitalismo. El texto indaga en la naturaleza religiosa del capitalismo como una dogmática inhumana: la identificación del pecado y la culpa religiosa y la deuda impuesta por el capitalismo (el término alemán utilizado en el escrito Schuld significa a la vez deuda y culpa). Para Michael Löwy el escrito es una lectura anticapitalista de Max Weber. En este sentido, se ha afirmado con relación al vínculo entre capitalismo y religión: Con relación a la percepción religiosa de la riqueza, podría decirse que el dilema del capital ,su pecado capital cabría decir, y el origen de la hostilidad que en ocasiones genera, podría explicarse por su desapego crónico hacia la auténtica riqueza: toda aquella que no consista en dinero. Algo que ha quedado sintetizado en el conocido adagio atribuido al emperador Vespasiano: «pecunia non olet». Frente a este planteamiento, resulta comprensible que haya quien entienda que la acumulación de riquezas nada tiene que ver con la moral (aunque, en realidad, esta tenga que ver con todo), sin embargo, no cabe duda de que un rasgo del capitalismo es el de desatender el origen y destino del capital, sirviendo lo mismo para financiar un hospital para refugiados que para financiar la guerra que los ha convertido en tales. Ecologismo La crítica ecologista argumenta que un sistema basado en el crecimiento y la acumulación constante es insostenible, y que acabaría por agotar los recursos naturales del planeta, muchos de los cuales no son renovables; más aún si el consumo de estos recursos es desigual entre los países y en sus respectivas clases económicas. Hasta hace algunas décadas, se pensaba que los recursos naturales eran virtualmente inagotables y que la contaminación, pérdida de la biodiversidad y de paisajes eran costes asumibles del progreso. Actualmente existen dos tendencias principales relacionadas con la crítica ecologista: aquella que defiende un desarrollo sostenible de la economía (que consistiría en adaptar el actual modelo al nuevo problema medioambiental) y otra que defiende un decrecimiento de la economía (que apunta directamente a nuevos sistemas de organización económica). Como contraparte al ecologismo colectivista, surge el ecologismo de mercado con base en la libertad individual. Este ecologismo plantea la protección de los ecosistemas desde el punto de vista del capitalismo libertario. Los libertarios dicen que una clara definición de la propiedad privada en todos los recursos escasos da como resultado que cada recurso escaso sea usado más eficientemente, y por lo tanto, sea regulado por el mercado; de igual manera, el propietario siempre estaría interesado en que su tierra y animales estén sanos, y usan el ejemplo de la privatización de los elefantes en Kenia y la recuperación de la población de estos para demostrar que una economía de mercado con propiedad privada siempre tiene interés en preservar un ecosistema sano. Desde el punto de vista de los libertarios, cuando no hay derechos de propiedad definidos ocurre la denominada tragedia de los comunes, donde el recurso es usado por todos de manera irresponsable y este se agota."

ksampletext_wikipedia_juri_ley: str = "Ley. La ley (del latín lex, legis) es una norma jurídica de carácter general, abstracto y obligatorio, dictada por el poder legislativo u otra autoridad competente, que establece derechos y obligaciones para los miembros de una comunidad, conforme a los principios del ordenamiento jurídico. La ley constituye una de las principales fuentes del derecho en los sistemas jurídicos modernos, especialmente en aquellos de tradición continental europea, donde ocupa un lugar preeminente en la jerarquía normativa, por debajo únicamente de la Constitución y, en su caso, de los tratados internacionales. Las leyes pueden clasificarse según diversos criterios: Por su jerarquía: ley ordinaria, ley orgánica, ley marco. Por su ámbito: leyes nacionales, autonómicas, locales, etc. Por su función: leyes sustantivas, procesales, penales, civiles, administrativas, entre otras. El proceso de elaboración de la ley varía según el sistema político, pero en general incluye las siguientes etapas: iniciativa legislativa, debate parlamentario, aprobación, sanción y promulgación. Una vez publicada oficialmente, la ley entra en vigor y su incumplimiento puede conllevar la imposición de sanciones por parte de la autoridad competente. Además de regular la convivencia social, las leyes reflejan valores, principios y objetivos fundamentales del Estado, como la justicia, la seguridad jurídica y el respeto por los derechos fundamentales. Descripción Las leyes son delimitadoras del libre albedrío de las personas dentro de la sociedad. Se puede decir que la ley es el control externo que existe para la conducta humana, las normas que rigen nuestra conducta social. Constituye una de las principales fuentes del derecho. Definiciones históricas Aristóteles: El común consentimiento de la ciudad. Gayo: Es lo que el pueblo manda y establece. Aftalión: Es la norma general establecida mediante la palabra por el órgano competente (legislador). Kelsen: En sentido específico, legislación significa establecimiento de normas jurídicas generales, cualquiera que sea el órgano que lo realice. Planiol: Regla social obligatoria establecida con carácter permanente por la autoridad pública y sancionada por la fuerza. Santo Tomás de Aquino: Ordenación de la razón dirigida al bien común y promulgada por quien tiene a su cargo a la comunidad (Suma de teología, I-II, q. 90, a. 4). Características Generalidad: la ley comprende a todos aquellos que se encuentran en las condiciones previstas por ella, sin excepciones de ninguna clase, salvo que se indique excepciones. Obligatoriedad: tiene carácter imperativo-atributivo, que por una parte establece obligaciones o deberes jurídicos y por la otra otorga derechos. La ley impone sus mandatos, incluso en contra de la voluntad de sus destinatarios. Su incumplimiento da lugar a una sanción; un castigo impuesto por ella misma. Permanencia: se dictan con carácter indefinido, permanente, para un número indeterminado de casos y de hechos, y solo dejará de tener vigencia mediante su abrogación, subrogación y derogación por leyes posteriores. Abstracta e impersonal: las leyes no se emiten para regular o resolver casos individuales, ni para personas o grupos determinados, su impersonalidad y abstracción las conducen a la generalidad. Ignorancia no es excusa: El hecho de no conocer la ley no es una razón válida para no recibir sanción. Irretroactiva: como norma general, regula los hechos que ocurren a partir de su publicación, hacia lo futuro, jamás hacia lo pasado, salvo ciertas excepciones (como la retroactividad en materia penal). Ley natural Artículo principal: Derecho natural Las leyes naturales son juicios enunciativos cuyo fin estriba en mostrar las relaciones indefectibles que en la naturaleza existen, la cual es dictada por la correcta razón. Thomas Hobbes difiere entre razón y pasión como objetos de la ley natural del hombre en el cual la razón garantiza la búsqueda de paz, la renuncia a mis derechos positivos (en pos de obtener seguridad y vida) y el cumplimiento con los pactos (voluntario, único y racional). En cambio, la pasión despierta por sensaciones y necesidades naturales del hombre, como el temor a la muerte. Nace del propio instinto humano y no hace posible el pacto, ya que el hombre queda como un animal insatisfecho, siempre con ganas de más y más para mejorar. Ley positiva En Derecho, el origen de la definición de la ley se debe a Tomás de Aquino en su Summa Theologiae al concebirla como: «La ordenación de la razón dirigida al bien común y promulgada por el que tiene a su cargo el cuidado de la comunidad». Más modernamente, se denomina ley a la norma de mayor rango tras la Constitución que emana de quien ostenta el poder legislativo. Mientras no está aprobada es un proyecto de ley. Clasificaciones de las leyes En sentido material y formal: esta diversificación en material y formal se particulariza por observar cuál es el contenido de la ley y cuál es su estructura. Material: refiere a cuál es el contenido de la norma (su finalidad, la regla de conducta que fija y las facultades y deberes que otorga e impone). Formal: refiere a cómo debe ser la estructura de la norma (toda norma debe ser general, obligatoria, escrita, emanada desde el congreso o autoridad competente conforme al mecanismo constitucionalmente determinado y debe formar parte del derecho de un Estado). De derecho estricto y de derecho equitativo, también se denominan rígidas o flexibles. En las primeras la norma es taxativa y no deja margen para apreciar las circunstancias del caso concreto ni graduar sus consecuencias. En las segundas, resultan más o menos indeterminados los requisitos o los efectos del caso regulado, dejando un cierto margen para apreciar las circunstancias de hecho y dar al Derecho una configuración adecuada al caso concreto. a) Por el sistema al que pertenecen: internacionales; nacionales; provinciales; locales. b) Según el modo de operar: Permisivas: son aquellas disposiciones que autorizan, permiten a un sujeto realizar determinadas conductas. Prohibitivas: son aquellas que vedan, niegan al sujeto la posibilidad de realizar determinados actos o de tener ciertas conductas. Declarativas: son aquellas cuyo contenido encierra definiciones. Son personas todos los entes susceptibles de adquirir derechos y contraer obligaciones. c) Según cómo actúa la voluntad individual: Imperativas: son leyes que se imponen a la voluntad individual o colectiva, que no dan opción para el sujeto. Supletorias: son leyes que rigen solamente cuando las personas no expresan su voluntad en otro sentido. Están estrechamente ligadas al principio de autonomía de la voluntad. Algunos tipos de leyes son: Ley fundamental: es la que establece principios por los que deberá regirse la legislación de un país; suele denominarse Constitución. La Constitución es la norma suprema del ordenamiento jurídico, ya que está por encima de cualquier ley. Ley orgánica: cuando nace como consecuencia de un mandato constitucional para la regulación de una materia específica. Ley ordinaria: entre las que se incluye la ley de presupuestos. Legislación delegada Son normas jurídicas con rango de ley dictadas por el Gobierno sobre determinadas materias. No son propiamente leyes, aunque tienen todos los efectos de éstas, ya que tienen valor, rango y fuerza de ley. Entre ellas encontramos al: Decreto Ley (España y en regímenes de excepción) Decreto Legislativo Decreto con fuerza de ley (Chile) Decreto de necesidad y urgencia (Argentina)."
ksampletext_wikipedia_juri_constitucion: str = "Constitución. Una constitución (del latín constitutio) es un texto codificado de carácter jurídico-político, surgido de un poder constituyente, que tiene el propósito de constituir una separación de poderes, definiendo y creando los poderes constituidos (legislativo, ejecutivo y judicial), que anteriormente estaban unidos o entremezclados, y sus respectivos controles y equilibrios (checks and balances), además de ser la ley fundamental de un Estado, con rango superior al resto de las normas jurídicas, fundamentando ,según el normativismo, todo el ordenamiento jurídico, incluye el régimen de los derechos y libertades de los ciudadanos, también delimitando los poderes e instituciones de la organización política. En la actualidad también se tiene como costumbre añadir normas ajenas a la regulación del poder político, dependiendo de la ideología, tales como los fundamentos del sistema económico. La constitución no constituye al Estado o la nación, debido a que ambos ya son hechos anteriores constituidos. En ciencia política, a los Estados que tienen separación de poderes se les añade a su forma de Estado el término constitucional (como es el caso de la monarquía constitucional o la república constitucional). En el uso cotidiano del término, se llama constitución a todas las leyes supremas de los Estados aunque no cuenten con separación de poderes. También se usa como sinónimo carta magna, en referencia a un tratado de paz de 1215 que ha influido profusamente en el common law anglosajón. Clasificación Este artículo o sección necesita referencias que aparezcan en una publicación acreditada. Busca fuentes: «Constitución» – noticias · libros · académico · imágenes La constitución, como toda norma jurídica, puede definirse tanto desde el punto de vista formal como desde el punto de vista material. Desde el punto de vista material, la constitución es el conjunto de reglas fundamentales que se aplican al ejercicio del poder estatal. Desde el punto de vista formal, la constitución se define a partir de los órganos y procedimientos que intervienen en su adopción, derivándose así una de sus características principales, la supremacía sobre cualquier otra norma del ordenamiento jurídico. Definición material La democracia ateniense fue la primera en adquirir las características de una constitución. La Antigua Roma, a través de sus instituciones políticas complejas y organizadas, también tiene una constitución. Una constitución material se define de acuerdo con sus reglas y contenido, en lugar de un texto o documentos específicos. Por lo tanto, una constitución material es un conjunto de reglas que se pueden agrupar en un solo documento, pero no necesariamente. Estas reglas generalmente se clasifican en una o más de las siguientes categorías: por un lado, todas las reglas que organizan las autoridades públicas y sus relaciones entre ellas (gobierno, parlamento, presidente, rey, etc. ), por otro lado, las libertades públicas (o libertades fundamentales) se pueden otorgar a cualquier persona que resida en el territorio o nacional del Estado de que se trate. En esta última categoría, a menudo encontramos derechos o libertades tales como la libertad de ir y venir, libertad de expresión, etc.. Este es el significado del Artículo 16 de la Declaración de los Derechos del Hombre y del Ciudadano de 1789: Una sociedad en la que la garantía de los derechos no está asegurada, ni la separación de poderes determinada, no tiene Constitución. Asamblea Nacional Constituyente (Francia), 26 de agosto de 1789. La mayoría, si no todos, los estados tienen una constitución material, en el sentido de que tienen reglas que organizan y gobiernan sus instituciones políticas. Según su reformabilidad Según su reformabilidad, las constituciones se clasifican en rígidas y flexibles. Las constituciones rígidas son aquellas que requieren de un procedimiento especial y complejo para su reformabilidad; es decir, los procedimientos para la creación, reforma o adición de las leyes constitucionales es distinto y más complejo que los procedimientos de las leyes ordinarias. Constituciones semirrígidas, rígidas o pétreas Son aquellas constituciones que para modificarse establecen un procedimiento más agravado que el procedimiento legislativo ordinario. Según el grado de complejidad del mismo se denominarán rígidas o semirrígidas. En la práctica, las constituciones escritas son también constituciones rígidas; es decir, cuando en un Estado encontramos que existe Constitución escrita, descubrimos que esta tiene un procedimiento más complejo de reforma o adición que el procedimiento para la creación, reforma o adición de una ley ordinaria. Constitución de Chile de 1828. Constituciones flexibles Se modifican mediante el procedimiento legislativo ordinario, lo que significa que una ley del parlamento puede cambiarlas en cualquier momento. Según su origen Las Constituciones se diferencian también en función de su origen político: pueden ser creadas por contrato entre varias partes, por imposición de un grupo a otro, por decisión soberana, etc. Constituciones otorgadas Las Constituciones otorgadas se dice que corresponden tradicionalmente a un Estado monárquico, donde el propio soberano es quien precisamente otorga; es decir, son aquellas en las cuales el monarca, en su carácter de titular de la soberanía, las otorga al pueblo. En este caso, se parte de las siguientes premisas: Desde la perspectiva del monarca, es él quien la otorga, por ser el depositario de la soberanía. Es una relación entre el titular de la soberanía –monarca, y el pueblo, quien simplemente es receptor de lo que indique el monarca Se trata de una Constitución en la cual se reconocen los derechos para sus súbditos, con respecto al poder legislativo. Constituciones impuestas Hay Constituciones que son impuestas por el Parlamento al monarca, refiriéndose al Parlamento en sentido amplio, con lo que se alude a la representación de las fuerzas políticas de la sociedad de un Estado, de los grupos reales de poder en un Estado que se configuran en un órgano denominado Parlamento. En este tipo de Constitución, es la representación de la sociedad la que le impone una serie de notas, determinaciones o de cartas políticas al rey, y este las tiene que aceptar. Por lo tanto, en el caso de las Constituciones impuestas existe una participación activa de la representación de la sociedad en las decisiones políticas fundamentales. Constituciones pactadas En las Constituciones pactadas, la primera idea que se tiene es el consenso. Nadie las otorga en forma unilateral, ni tampoco las impone debido a que si son impuestas y no se pactan, carecerían de un marco de legitimidad. Estas Constituciones son multilaterales, ya que todo lo que se pacte implica la voluntad de dos o más agentes; por lo tanto, son contractuales y se dice que parten de la teoría del pacto social. Así, se puede pactar entre comarcas, entre provincias, entre fracciones revolucionarias, etc. Las constituciones pactadas o contractuales implican una mayor evolución política que en aquellas que son impuestas u otorgadas en las pactadas hay una fuerte influencia de la teoría del pacto social en aquellas que son pactadas, este pacto o consenso se puede dar entre diversos agentes políticos o todos aquellos grupos de poder real que estén reconocidos por el Estado. Así, aun tratándose de una monarquía, cuando se pacta los gobernados dejan de ser súbditos y se consagran como un pueblo soberano. Constituciones aprobadas por voluntad de la soberanía popular Son aquellas elegidas por el pueblo de un Estado, la cual por lo general se dan a conocer por una asamblea y se reafirman con la votación máxima de la población en un proceso electoral. Por lo tanto, no es que la sociedad pacte con los detentadores del poder público, sino que la Constitución surge de las necesidades sociales y de la fuerza popular. Fuentes de la constitución El poder constituyente: Sieyes reconoce que todos los ciudadanos tienen derecho a establecer su gobierno. Pero este gobierno o el manejo de los intereses generales de la comunidad es un trabajo humano y es de primordial importancia ya que la voluntad de todos y cada uno cuestiona el bienestar y la libertad de todos. Como resultado, sin un mandato expreso, los legisladores no deben tocar esta gran jurisdicción del Estado que uno llama la constitución. Este poder está prohibido, con buena razón, en las asambleas ordinarias, para evitar posibles usurpaciones y ciertas agitaciones. Y cuando es necesario tocar la ley suprema, las personas, suficientemente conscientes, dan un mandato especial a una asamblea constituyente, a una convención, cargada expresamente, y con exclusión de cualquier otro cuerpo, para revisar la constitución. La primacía de la nación: Sieyes -a diferencia de Rousseau- se sostiene por lo racional y lo construido. El estado social, en relación con el estado de naturaleza, perfecciona y ennoblece al hombre. Se extiende y protege la libertad. Defiende y garantiza la igualdad de derechos. Para él, las verdaderas relaciones de una constitución política son «con la nación que permanece» en lugar de «la generación que pasa; con las necesidades de la naturaleza humana, común a todos, en lugar de las diferencias individuales». En ¿Qué es el Tercer Estado? Sieyes proclama: «Considerado en forma aislada, el poder de los ciudadanos sería nulo, reside solo en el todo». La nación no se crea a sí misma, existe; es una ley natural, pero necesita una organización política y administrativa, o en palabras de Sieyès de un establecimiento público, es decir, un conjunto de medios formados por personas y cosas, destinados a darse cuenta de los fines sociales: La constitución política es posterior a la formación en nación. Viene cuando ya existe una voluntad común, anterior a ella. Control de constitucionalidad Artículo principal: Control de constitucionalidad El control de constitucionalidad trata de los mecanismos de revisión de la adecuación de las leyes y de los actos del Estado. Sistema concentrado: en algunos países es realizado por una Corte Suprema o Tribunal Constitucional, que es el encargado de resolver los planteamientos o recursos de inconstitucionalidad presentados por los ciudadanos frente a las violaciones a alguna norma legal por parte del Estado, o de otro particular. Sistema concentrado en Corte Suprema, sistema imperante en Uruguay. Sistema concentrado en Tribunal Constitucional, caso de vigencia en Bolivia. Sistema difuso: Este sistema establece que el control de constitucionalidad de una norma o de un acto jurídico puede ser realizado por cualquier tribunal del país. Los jueces inferiores no tienen minusvalía alguna para este mecanismo respecto de la Suprema Corte de Justicia de la Nación. Tal es el caso de la República Argentina, o de Estados Unidos. Sin embargo, será el máximo tribunal quien resolverá si son apelados los fallos de los tribunales inferiores. Sistema mixto. Sistema de control difuso en tribunales ordinarios y control concentrado en Corte Suprema, como en Brasil. Sistema de control difuso en tribunales ordinarios y control concentrado en Tribunal Constitucional, como en Perú y Colombia. Sistema de control concentrado de constitucionalidad en Tribunal Constitucional (preventivo) y Corte Suprema (represivo). Clasificación según el efecto de la sentencia Puede ser que la sentencia surta efecto solo entre las partes intervinientes en el caso concreto. En este caso se dice que la declaración de inconstitucionalidad tiene efecto inter partes. O puede acontecer que la sentencia sea válida para todos los ciudadanos, en cuyo caso se dice que surte efecto erga omnes. Esto generalmente sucede en los países en los que se aplica un sistema concentrado de control. Historia del constitucionalismo Antecedentes Sería anacrónico entender como constituciones modernas los sistemas políticos de la Antigüedad griega (democracia ateniense), la obra de sus legisladores (logógrafos) o los estudios legislativos de sus filósofos (Aristóteles, Athenaion politeia ,cuyo título se traduce habitualmente como Constitución de los atenienses,). Lo mismo puede decirse de los fueros locales o estamentales que se redactaron en la Europa medieval o de la Carta de Medina (Mahoma, año 2 de la Hégira ,622 d. C.,) Carta Magna (Inglaterra, 1215 -versión con sello de 1297-). Carta Magna (Inglaterra, 1215 -versión con sello de 1297-). Bula de Oro de 1222 (Hungría). Bula de Oro de 1222 (Hungría). Privilegio de Juan I de Castilla confirmando el Fuero de Castrojeriz (ca. 1379-1390). Privilegio de Juan I de Castilla confirmando el Fuero de Castrojeriz (ca. 1379-1390). Nova compilatio dels usatges de Barcelona, constitutions, capitols y actes de cort y altras leys de Cathalunya (libro impreso a partir de la versión de 1413 -constituciones catalanas, usatges-). Nova compilatio dels usatges de Barcelona, constitutions, capitols y actes de cort y altras leys de Cathalunya (libro impreso a partir de la versión de 1413 -constituciones catalanas, usatges-). El Fuero, Privilegios, Franquezas y Libertades de los Cavalleros hijosdalgo del Señorío de Vizcaya, confirmados por el Rey don Felipe II nuestro señor, Y por el Emperador y Reyes sus predecesores (1575). El Fuero, Privilegios, Franquezas y Libertades de los Cavalleros hijosdalgo del Señorío de Vizcaya, confirmados por el Rey don Felipe II nuestro señor, Y por el Emperador y Reyes sus predecesores (1575). Pese a ello, se pueden considerar tales estatutos de la Edad Antigua y la Edad Media como precedentes a la constitución moderna por fungir como leyes fundamentales (que intentaban ser leyes supremas) de un estado. Por ejemplo, el majestas en el Imperio romano que no podía ser contrariado (un ejemplo de las consecuencias de contrariarlo fue la Crucifixión de Jesús); o el Privilegium Generale Aragonum de 1283 en el Reino de Aragón, que intentaba ser una ley suprema, impuesta por los Hidalgos, por el que el rey estaba sometido y su falta de cumplimiento generaba una sanción al rey en tanto que se invalidaban sus actos reales. Sin embargo, no se pueden considerar como constituciones políticas en el sentido moderno, puesto que en la época medieval estaba presente el Iusnaturalismo Escolástico sustentado por Santo Tomas de Aquino y el Agustinismo político, en el que el derecho positivo (de donde emana la soberanía de un estado constituyente, en el Poder temporal) no podía estar por encima del derecho natural (que emanaba de una Ley eterna, cuya supremacía era trascendente a la soberanía del estado). Por ende, ni una de esas leyes fundamentales premodernas podrían cumplir el principio de supremacía constitucional del Estado moderno (en el que la constitución emana de la soberanía inmanente del estado, y sería ley suprema por no reconocer algo trascendente al propio estado), ya que todas las leyes supremas en realidad estaban aún sometidas a un orden superior de carácter Metafísico. En el contexto cristiano, se identificaba a la Ley divina como lo de carácter supremo (y superior a la propia soberanía del estado constituido), pues dicha ley divina podía ser intuida incluso sin necesidad de la Biblia o Dogmas de la Iglesia católica, pues las Cinco vías afirmaban un conocimiento natural de la existencia de Dios (que precede a la Revelación divina del Cristianismo) con solo deducir por medio de la razón que el mundo estaba perfectamente ordenado por sus propiedades de los entes en su esencia de ser (por tanto, las verdades de la fe no contradecían las verdades de la razón, y se asumía que hasta un no-creyente podía percatarse de la obra creadora de Dios, razón por la que existían los Paganos), por lo que la Ley Natural entonces emanaba de un Orden natural al que los estados debían respetar y ajustarse en base al Realismo moral, o no se podría lograr el Bien común si el hombre organizado en Sociedad política no respetaba dicho orden natural, el cual debía justificar la soberanía del estado (mientras el enfoque modernista es al revés, la soberanía del estado justifica el orden social, y se pone en duda o se niega la existencia de un orden natural). Aun así, la ausencia de constituciones modernas no quiere decir que en dichas épocas no hubieran existido mecanismos que reglamentaran el ejercicio del poder político. Con la Edad Media esta la construcción historiográfica de la constitución medieval, en la que existieron una serie de vínculos, convenciones, pacto, contratos, límites, reglas y relaciones (basados en la praxis social) entre las distintas personas de una sociedad para lograr la limitación de los poderes públicos y las garantías del Señor en proteger el orden social pactado con sus Vasallos. Historia moderna Ya en el contexto de las revoluciones burguesas de la Edad Moderna, algún texto ha sido retrospectivamente calificado de constitución, como la Ley Perpetua del Reino de Castilla que intentaron promulgar los comuneros de la Junta de Ávila (1520). Con mayor trascendencia, el régimen político establecido en los Países Bajos sublevados desde finales del siglo XVI incorpora algunas características propias de un sistema constitucional, aunque los textos que generó (Pacificación de Gante -1576-, Unión de Utrecht -1579-) no pueden considerarse constituciones. El régimen político inglés establecido paulatinamente desde finales del siglo XVII (Revolución Gloriosa y Bill of Rights de 1689 ,los Tratados sobre el gobierno civil de John Locke se publican ese mismo año,) se basa en el derecho consuetudinario, y se define como un régimen constitucional sin constitución escrita cuyos precedentes pueden remontarse a la Carta Magna de 1215, que en sí misma no puede calificarse de constitución, pero es considerada coloquialmente como sinónimo. Bill of rights inglés de 1689. Bill of rights inglés de 1689. Alegoría de la Pacificación de Gante, 1576. Alegoría de la Pacificación de Gante, 1576. Firmas al pie de la Unión de Utrecht, 1579. Firmas al pie de la Unión de Utrecht, 1579. Los primeros desafíos contra la Teología política, anti-constitucionalista, se darían con el Renacimiento y las ideas Maquiavelistas que harían florecer el Estado Moderno (con la distinción de leyes fundamentales y ordinarias en el siglo XVII), antecedente necesario para el desarrollo del constitucionalismo basándose en la concepción de soberanía suprema del estado. Aquello sería reforzado con el deseo de los monarcas (sobre todo franceses) de imitar el Cesarismo y que los reyes sean grandes legisladores (tanto por motivos de aumentar el prestigio nacional como el deseo de una mayor eficiencia administrativa), por lo que el estado debía centralizarse y abolir toda legislación feudal autónoma (e incluso el de la Iglesia católica con el Regalismo) para consolidar su soberanía del estado por medio del Rey absoluto. Así, en la Francia Absolutista apareció la doctrina del lheureuse impuissance (la feliz impotencia del rey absoluto, Luis XV de Francia, en violar las leyes constitucionales del reino), ganando terreno la supremacía del derecho positivo por sobre el derecho natural con las ideas de soberanía en Jean Bodin. Consecuencia de la crítica ilustrada a los sistemas políticos del Antiguo Régimen (la monarquía absoluta o autoritaria), las constituciones actuales comienzan con los proyectos para Córcega y para Polonia que redactó Jean-Jacques Rousseau en 1755 y 1771-1772 respectivamente; y, ya como documento que efectivamente entró en vigor, la Constitución de Estados Unidos (17 de septiembre de 1787, cuyo antecedente directo es la del Estado de Virginia, 1776), estableciendo los límites de los poderes gubernamentales, y que en sus primeras enmiendas (Bill of Rights de 15 de diciembre de 1791) protege los derechos y libertades fundamentales. Previamente, los colonos de las Trece Colonias estaban convencidos de la supremacía de las leyes inglesas y que no había forma de que una ley colonial pudiera contradecirlas (lo cual motivo a la Revolución de las Trece Colonias). La palabra constitución en Francia se utilizaría con su significado contemporáneo en las protestas del Parlamento de París y en las ordenanzas reales de la década de 1720. La Declaración de los Derechos del Hombre y del Ciudadano aprobada por la Asamblea Nacional Constituyente francesa (26 de agosto de 1789), documento precursor de los derechos humanos, menciona explícitamente en su artículo XVI el concepto de constitución: «Una sociedad en la que la garantía de los derechos no está asegurada, ni la separación de poderes determinada, no tiene Constitución». Finalmente, la Revolución francesa se encargaría de consolidar las nociones de supremacía constitucional del estado soberano, en el que nada ni nadie esta por encima, en contra o fuera de ella, con la Constitución francesa de 1791. Al respecto, lo único que se habría hecho era transferir la titularidad, de la soberanía absoluta del estado moderno, del rey hacia el pueblo a través de la Asamblea Nacional Constituyente (pero ambos, el rey en la Monarquía constitucional francesa, o el pueblo en la Primera República francesa, estaban sometidos a la supremacía de la constitución). El siglo XIX supuso un desarrollo constante de esta idea de constitución, de división de poderes y de establecimiento del derecho moderno como hoy lo conocemos. Así, con el liberalismo, las constituciones se concretan y desarrollan (Ustawa Rządowa o constitución polaca del 3 de mayo de 1791, Constitución francesa del 3 de septiembre de 1791, Constitución Política de la Monarquía Española del 19 de marzo de 1812, etc.); diseñando sistemas políticos muy diversos (Acta constitucional del pueblo francés o Constitución del año I -1793, una constitución republicana y democrática, que reconocía el derecho al trabajo y a la insurrección, y que no llegó a entrar en vigor-, Carta constitucional francesa de 1814 ,una carta otorgada que no reconocía la soberanía nacional,). We, the people (nosotros, el pueblo) son las palabras con las que comienza la Constitución de los Estados Unidos, 1787. We, the people (nosotros, el pueblo) son las palabras con las que comienza la Constitución de los Estados Unidos, 1787. Déclaration des droits de lhomme et du citoyen francesa de 1789. Déclaration des droits de lhomme et du citoyen francesa de 1789. La primera constitución española, de Cádiz o de 1812, llamada la Pepa por promulgarse el 19 de marzo. La primera constitución española, de Cádiz o de 1812, llamada la Pepa por promulgarse el 19 de marzo. Charte constitutionnelle de 1814, la carta otorgada por Luis XVIII durante la restauración borbónica en Francia. Charte constitutionnelle de 1814, la carta otorgada por Luis XVIII durante la restauración borbónica en Francia. Más allá de los derechos civiles y políticos, la introducción de la llamada segunda generación de derechos (los derechos sociales) comienza en las constituciones surgidas en el primer tercio del siglo XX (Constitución Política de los Estados Unidos Mexicanos de 31 de enero de 1917, Constitución soviética de 10 de julio de 1918 ,precedida por una Declaración de derechos del pueblo trabajador y explotado,, Constitución de Weimar de 11 de noviembre de 1919, Constitución española de 1931). Simultáneamente se promulgaron las constituciones o pseudo-constituciones fascistas (Carta del Lavoro italiana de 1927, Ley Habilitante alemana de 1933, Leyes Fundamentales españolas de 1938-1967), caracterizadas por no imponer límites al poder del gobernante. El siguiente hito fundamental fue la Segunda Guerra Mundial, tras la cual se produjo la Declaración Universal de los Derechos Humanos (10 de diciembre de 1948), cuya ratificación por los distintos Estados le otorgó cierto valor constitucional que algunas constituciones explícitamente reconocen. Constitución mexicana de 1917. Constitución mexicana de 1917. Pico Constitución, de 5284 metros, el mayor de Kuyljutau (cordillera Tian Shan, en el actual Kirgizistán), denominado así en honor a la Constitución soviética. Pico Constitución, de 5284 metros, el mayor de Kuyljutau (cordillera Tian Shan, en el actual Kirgizistán), denominado así en honor a la Constitución soviética. Die Verfassung des Deutschen Reichs, la llamada Constitución de Weimar de 1919. Die Verfassung des Deutschen Reichs, la llamada Constitución de Weimar de 1919. Declaración Universal de Derechos de Hombre de 1948 (ejemplar en español, sostenido por Eleanor Roosevelt). Declaración Universal de Derechos de Hombre de 1948 (ejemplar en español, sostenido por Eleanor Roosevelt). Reconocimiento de derechos Este artículo o sección necesita referencias que aparezcan en una publicación acreditada. Busca fuentes: «Constitución» – noticias · libros · académico · imágenes Este aviso fue puesto el 8 de febrero de 2016. La norma fundamental no solo es una norma que controla y estructura el poder y sus manifestaciones en una sociedad, sino que además es la norma que reconoce los derechos que el Estado advierte en todas las personas. La Constitución no otorga los derechos, como tampoco lo hacen las múltiples declaraciones que internacionalmente se han pronunciado sobre el tema. Los derechos humanos son precedentes a cualquier Estado y superiores a cualquier expresión de poder que este tenga. Hasta el día de hoy el proceso demostró un desarrollo, gracias al cual el modelo inicial del sujeto poderoso y violento pasó al pueblo soberano y superior en sus derechos a cualquier expresión del Estado. Hoy el sujeto poderoso no es una persona, sino que es una entelequia creada por el pueblo y ocupada por él según las normas que este mismo estableció a través de una Constitución. El punto más novedoso de este desarrollo se da con la certeza de que la mera declaración de derechos no hace a estos invulnerables a cualquier violación o intento de violación por parte tanto del Estado como de otras personas. En ese sentido el desarrollo del Constitucionalismo moderno se dedica al estudio de procedimientos que aseguren una adecuada protección a los derechos reconocidos. Algunos de estos procedimientos tienen un gran desarrollo histórico y teórico (como el habeas corpus que data del siglo XIII) y otros son aún novedosos y tienen poco desarrollo (como el habeas data y la acción de cumplimiento)."
ksampletext_wikipedia_juri_derecho: str = "Derecho. El derecho (o ciencias jurídicas) puede definirse como un sistema de principios y normas, generalmente inspirados en ideas de justicia y orden, que regulan la conducta humana en toda sociedad y cuyo cumplimiento puede imponerse de forma coactiva por el poder público. No obstante, no hay una definición del derecho generalmente aceptada o consensuada. Por ello, el derecho ha sido considerado simultáneamente una ciencia y un arte. El derecho existe desde las primeras civilizaciones. El ordenamiento jurídico es el conjunto o la suma de principios y normas jurídicas vigentes en un Estado. El derecho comparado analiza las diferencias entre los diferentes ordenamientos coexistentes. El derecho guarda una íntima conexión con la ciencia política, la economía, la sociología y la historia, y es el centro de problemas humanos importantes y complejos, como concretar el significado de ideas como igualdad, libertad o justicia en casos concretos. Las cuestiones más generales sobre el derecho han sido estudiadas por la filosofía, la historia y la teoría del derecho. Existen muchas formas de clasificar, analizar y ordenar el derecho para proceder a su estudio, aunque el derecho es en esencia un conjunto de normas unitario. Una división clásica es la acuñada en el siglo iii d. C por el jurista romano Ulpiano: el derecho público regula la actuación de los poderes públicos; y el derecho privado, que regula las relaciones privadas entre personas físicas y jurídicas. Esta división, a grandes rasgos, persiste en la actualidad, aunque la división es menos clara en algunos sistemas jurídicos, como el derecho anglosajón. Algunas ramas importantes del derecho público contemporáneo son, entre otras, el derecho constitucional, el derecho administrativo o el penal, mientras que el derecho privado se compone principalmente del derecho civil, el mercantil y el derecho laboral. Algunas disciplinas como el derecho internacional también obedecen a esta clasificación, dividiéndose en derecho internacional público y privado. Terminología Etimología El Código de Hammurabi, creado en el año 1785 a. C. por el rey homónimo de Babilonia, es uno de los conjuntos de leyes más antiguos que se han encontrado. En él aparece la ley del Talión, que estableció la regla de la proporcionalidad como criterio de justicia. Se encuentra en el Museo del Louvre, París. La palabra derecho deriva de la voz latina directum, «lo que está conforme a la regla, a la ley, a la norma», o como expresa el jurista mexicano Villoro Toranzo, «lo que no se desvía ni a un lado ni otro». La expresión aparece, según el español Pérez Luño, en la Edad Media para definir al derecho con connotaciones morales o religiosas, el derecho «conforme a la recta razón». Esto es así si tenemos en cuenta frases como non omne quod licet honestum est (no todo lo que es lícito es honesto), en palabras del jurista romano Paulo, que demuestra el distanciamiento del derecho respecto a la moral. Esta palabra surge por la influencia estoico-cristiana tras la época del secularizado derecho de la época romana, y es el germen y raíz gramatical de la palabra «derecho» en los sistemas actuales: diritto, en italiano; direito, en portugués; dreptu, en rumano; droit, en francés; a su vez, right, en inglés; recht en alemán y en neerlandés, donde han conservado su significación primigenia de «recto» o «rectitud». La separación posterior del binomio ius - directum no pretende estimar que la palabra ius se halle exenta de connotaciones religiosas: téngase en cuenta que en la época romana temprana, según Pérez Luño, los aplicadores del derecho fueron, prácticamente de forma exclusiva, los pontífices. Definición Véanse también: Filosofía del derecho y Teoría del derecho. Los juristas han elaborado numerosas definiciones del derecho a lo largo de los siglos. Sin embargo, no existe una definición que goce de aceptación generalizada. La Real Academia Española define el derecho como el «conjunto de normas, principios, costumbres y concepciones jurisprudenciales y de la comunidad jurídica, de los que se derivan las normas de organización de la sociedad y de los poderes públicos, así como los derechos de los individuos y sus relaciones con aquellos». El derecho es un conjunto o sistema de normas jurídicas, de carácter general, que se dictan para regir sobre toda la sociedad o sectores preestablecidos por las necesidades de la regulación social, que se imponen de forma obligatoria a los destinatarios y cuyo incumplimiento debe acarrear una sanción coactiva o la respuesta del Estado a tales acciones. Estas normas no son resultado solamente de elementos racionales, sino que en la formación de las mismas inciden otros elementos, tales como intereses políticos y socioeconómicos, de valores y exigencias sociales predominantes, que condicionan una determinada voluntad política y jurídica, que en tanto se haga dominante se hace valer a través de las reglas de derecho. A su vez esas normas expresan esos valores, conceptos y exigencias, y contendrán los mecanismos para propiciar la realización de los mismos a través de las conductas permitidas, prohibidas o exigidas en las diferentes esferas de la vida social. La diversidad social y de esferas en que metodológica y jurídicamente se pueden agrupar, es consecuencia del nivel de desarrollo no solo de las relaciones, sino también de la normativa y de las exigencias de progreso de las mismas, pero aún con esta multiplicidad de normativas existentes, el derecho ha de ser considerado como un todo, como un conjunto armónico. Esa armonía interna puede producirse por la existencia de la voluntad política y jurídica que en ellas subyace. En sociedades plurales, la armonía de la voluntad política depende de la coincidencia de intereses de los grupos políticos partidistas predominantes en el poder legislativo y en el poder ejecutivo, así como de la continuidad de los mismos en el tiempo. Cambios también se pueden producir con las variaciones de los intereses socioeconómicos y políticos predominantes, al variar la composición parlamentaria o del gobierno. Asimismo, en sociedades monopartidistas y con presupuesto de la unidad sobre la base de la heterogeneidad social existente, la armonía de la voluntad normativa es mucho más factible si bien menos democrática, lo que no quiere decir que se logre permanentemente. Doctrinariamente se defiende la existencia de unidad y coherencia; pero lo cierto es que en la práctica lo anterior es absolutamente imposible en su aspecto formal, a pesar de los intereses y valores en juego, por cuanto las disposiciones normativas se promulgan en distintos momentos históricos, por órganos del Estado diferentes, e incluso dominados estos por mayorías políticas o con expresiones de voluntades políticas muy disímiles. Igualmente no siempre hay un programa preelaborado para la actuación normativa del Estado (programas legislativos), sino que la promulgación de una u otra disposición depende de las necesidades o imposiciones del momento. En tales situaciones se regulan relaciones sociales de una forma, con cierto reconocimiento de derechos e imposiciones de deberes, con determinadas limitaciones, se establecen mandatos de ineludible cumplimiento; y estas disposiciones pueden ser cuestionadas por otros órganos del Estado, derogadas por los superiores, o modificadas por los mismos productores meses o años después. Es decir, en el plano formal, haciendo un análisis de la existencia de una diversidad de disposiciones, encontraremos disposiciones que regulan de manera diferente ciertas instituciones, las prohíben, las admiten, introducen variaciones en su regulación, o que también en el proceso de modificación o derogación, se producen vacíos o lagunas, es decir, esferas o situaciones desreguladas. En el orden fáctico, y usando argumentos de la teoría política, las bases para la armonía las ofrece, ciertamente, la existencia de una voluntad política predominante, y de ciertos y determinados intereses políticos en juego que desean hacerse prevalecer. Y desde el punto de vista jurídico-formal, la existencia de un conjunto de principios que en el orden técnico jurídico hacen que unas disposiciones se subordinen a otras, que la producción normativa de un órgano prime sobre la de otros, que unas posteriores puedan dejar sin vigor a otras anteriores, como resulta de los principios de jerarquía normativa, no por el rango formal de la norma, sino por la jerarquía del órgano del aparato estatal que ha sido facultado para dictarla o que la ha dictado; de prevalencia de la norma especial sobre la general; que permita que puedan existir leyes generales y a su lado leyes específicas para ciertas circunstancias o instituciones y que permitan regularla de forma diferenciada, y aun así ambas tengan valor jurídico y fuerza obligatoria; o el principio de derogación de la norma anterior por la posterior, etcétera. Clasificaciones Derecho objetivo y derecho subjetivo El derecho objetivo es el conjunto de normas jurídicas (leyes, reglamentos, entre otras) de carácter obligatorio, y que son creadas por el Estado para conservar el orden social. Siempre teniendo en cuenta la validez, es decir, si se ha llevado a cabo el procedimiento adecuado para su creación, independientemente de su eficacia y de su ideal axiológico (si busca concretar un valor como la justicia, la armonía, el bien común, etcétera). El derecho subjetivo es la facultad que ha otorgado el ordenamiento jurídico a un sujeto (por ejemplo, derecho a la nacionalidad, derecho a la salud, derecho a demandar, derecho a manifestarse libre y pacíficamente, derecho a la libre expresión, derecho al sindicalismo, etc.). El derecho objetivo puede responder a distintas significaciones: El conjunto de reglas que rigen la convivencia de los hombres en sociedad. Norma o conjunto de normas que por una parte otorgan derechos o facultades y por la otra, correlativamente, establecen o imponen obligaciones. Conjunto de normas que regulan la conducta de los hombres, con el objeto de establecer un ordenamiento justo de convivencia humana. El derecho subjetivo se puede decir que es: La facultad que tiene un sujeto para ejecutar un acto con determinada conducta o abstenerse de ella, o para exigir de otro sujeto el cumplimiento de su deber. La facultad, la potestad o autorización que conforme a la norma jurídica tiene un sujeto frente a otro u otros sujetos, ya sea para desarrollar su propia actividad o determinar la de aquellos. Es una concesión de autoridad otorgada por una norma jurídica una persona o grupo en virtud de la cual queda legitimada para desplegar una determinada conducta sobre una o más personas o cosas. El derecho subjetivo se clasifica en: Derechos políticos (derecho de sufragio, a ser electo, a asociarse políticamente y a ser jurado). Derechos privados, que se subclasifican en: Derechos personales o créditos (aquellos de donde nacen las obligaciones correlativas). Derechos reales (aquellos que se ejercen sobre cosas, sin respecto a determinada persona). Creación y evolución del derecho Mosaico de Justiniano I, célebre por ordenar la compilación del Corpus iuris civilis, la más importante recopilación de derecho romano de la historia. La producción del derecho tal como lo conocemos hoy es básicamente estatal y tiene su origen en la institucionalización del Estado moderno, a partir de la Edad Moderna, siendo su ejemplo clásico la hegemonía del Estado español tras la unificación de Castilla y Aragón con los reyes católicos. Aunque el derecho como norma de conducta coactiva surge ya desde las primeras civilizaciones con una organización política, como las ubicadas en Mesopotamia, Fenicia, Palestina, Egipto y Grecia fundamentalmente como un derecho consuetudinario, es decir, basado en la costumbre, sin lugar a dudas que los romanos fueron la primera y mayor civilización en dedicar sus mayores esfuerzos a condicionar la generalidad de sus conductas, incluso las más cotidianas, al imperio del derecho, como sus relaciones de familia, el matrimonio, la adopción, la emancipación y la patria potestad; o las normas patrimoniales del derecho civil, como los contratos y los derechos reales, donde los romanos aún no han encontrado otra civilización que los alcance en profusión y creación jurídica, ni siquiera el derecho francés, que junto al derecho canónico y a la pandectística alemana del siglo XIX, son los siguientes mayores contribuyentes en dicha rama jurídica. Ya a partir de la Edad Moderna y bajo la fuerte influencia de los clásicos del humanismo como Nicolás Maquiavelo, Thomas Hobbes y John Locke, el derecho comienza a moldearse como un instrumento y elemento del Estado, adquiriendo la fisonomía positivista que tiene en la actualidad en la mayor parte de los Estados no descendientes del archipiélago británico, como Latinoamérica y Europa continental (derecho continental). Realización del derecho Para que una norma pueda ser eficaz, para que se realice, han de crearse, además, los medios e instituciones que propicien la realización de la disposición, y de los derechos y deberes que de tales situaciones resulten. Pero la eficacia de una norma no puede exigirse solo en el plano normativo (coerción), también ha de ser social, material, para que haya correspondencia entre la norma y el hecho o situación, para que refleje la situación existente o que desee crearse, manifestándose así la funcionalidad del derecho. Como resultado de lo anterior, será posible, entonces, que la norma obtenga el consenso activo de sus destinatarios, que sea acatada y respetada conscientemente, sin requerir la presión del aparato coercitivo del Estado. Requisito previo de la validez normativa es la publicidad en el sentido antes expuesto. La publicación de las normas se hace no solo para dar a conocer el nacimiento de la disposición, el inicio de su vida jurídica formal, sino también para declarar la posibilidad de su exigencia y obligatoriedad para el círculo de destinatarios de la normativa. Aún más, si toda disposición normativa se dicta, por regla general, para que tenga vida indeterminada, para que sea vigente y por tanto válida a partir de la fecha de su publicación, si ella no establece lo contrario, el acto de la publicación es vital en su nacimiento y acción posterior. La validez de una norma de derecho y de la disposición que la contiene y expresa, entonces, es un elemento importante para la eficacia de la misma, para el hallazgo de su realización en la sociedad, tal y como se previó. Interesan no solo la observación de los principios, sino también de ciertas reglas relativas a su elaboración racional, a la creación de instituciones para asegurar su cumplimiento, así como la finalidad que con ellas se persigue, a saber: conservar, modificar, legitimar cambios, así como de la observancia de principios básicos que rigen en cada ordenamiento jurídico. Por lo tanto, las disposiciones normativas, de cualquier rango, han de ser resultado del análisis previo con el objetivo de conocer los hechos, sus causas y efectos, regulaciones posibles, sus efectos, para poder determinar cuál es la forma precisa que ha de exigirse o propiciarse, o de la Institución jurídica que desea regularse; del cumplimiento de ciertos requisitos formales en su creación y de la observancia de principios técnicos jurídicos que rigen en un ordenamiento jurídico determinado. Han de crearse, además, los medios e instituciones que propicien el cumplimiento de la disposición, y de los derechos y deberes que de tales situaciones resulten, tanto en el orden del condicionamiento social-material, proveniente del régimen socioeconómico y político imperante, de los órganos que hacen falta para su aplicación, como la normativa legal secundaria y necesaria para instrumentar la norma de derecho. También ha de tenerse en forma clara los objetivos o finalidades que se persiguen con la norma o para qué se quiere regular esa relación, si existen las condiciones antes expuestas para su realización, y entonces la validez de la norma, será no solo manifestándose así la funcionalidad del Derecho, sino que también lo será en el orden formal, siendo posible, entonces, que la norma obtenga el consenso activo de sus destinatarios, su aceptación, cumplimiento y hasta su defensa. Nacida la norma, se ha de aplicar y de respetar no solo por los ciudadanos, sino también por el resto de las instituciones sociales, y en particular por los órganos inferiores, los cuales están impedidos formalmente, gracias a la vigencia del principio de legalidad, de regular diferente o contrario, de limitar o ampliar las circunstancias en que se ha de aplicar la normativa anterior, salvo que la propia disposición autorice su desarrollo. En consecuencia, la eficacia del derecho depende no solo del proceso de formación, aunque es muy importante, sino que depende también de las medidas adoptadas para hacer posible la realización de lo dispuesto en la norma y del respeto que respecto a él exista, principalmente por los órganos del Estado y en particular de la administración a todos los niveles. Por último, para que las normas emitidas por el Estado no solo sean cumplidas ante la amenaza latente de sanción ante su vulneración, sino que se realicen voluntariamente, el creador de las mismas ha de tener siempre presente que el destinatario general y básico de las normas es el dueño del poder, que mediante el acto electoral ha otorgado a otros un mandato popular para que actúen a su nombre y, en tanto hacia él van dirigidas las normas, han de preverse los instrumentos legales, así como las instituciones y medios materiales que permitan hacer efectivos los derechos que las disposiciones reconocen jurídicamente y permitan la defensa de los mismos ante posibles amenazas o vulneraciones que la administración o terceras personas puedan provocar. En otras palabras: la necesidad de garantías para el ejercicio de los derechos y su salvaguarda como vía para que se realice el derecho, para garantizar, entre otras, las relaciones bilaterales individuo-Estado, individuo-individuo que se han regulado. Así entonces la salvaguarda del orden, la defensa de los derechos y la legalidad, irán de la mano. Características del derecho El derecho presenta las siguientes características: normativo, bilateral, coercible, con una pretensión de inviolabilidad, se manifiesta como un sistema y posee una proyección de justicia. Normatividad Se traduce en que el derecho se encuentra inmerso dentro de la realidad social, el marco cultural. El derecho pertenece a la familia de las normas y está constituido por normas, más específicamente dentro de las reglas obligatorias de conducta. Bilateralidad El derecho es bilateral porque requiere de interactividad de dos o más personas. Uno de los rasgos distintivos de las normas jurídicas frente a las normas morales es la bilateralidad. En efecto, la bilateralidad del derecho se hace evidente no solo por su necesidad primordial de interrelacionar, cuando menos, dos personas, sino también en la heteronomía, condición de la voluntad que se rige por imperativos que están fuera de ella misma, pues, una es la fuente de la norma jurídica y la otra la persona sujeta a su cumplimiento. En la coercibilidad, igualmente resalta la misma dicotomía: quien dispone de la fuerza y quien es compelido por ella. Giorgio Del Vecchio enuncia: Se puede decir que este concepto de la bilateralidad es el elemento fundamental del edificio jurídico. Eduardo García Máynez corrobora: La diferencia esencial entre normas morales y normas jurídicas estriba en que las primeras son unilaterales y las segundas bilaterales. Coercibilidad La coercibilidad es la exigencia de amparar el derecho en la fuerza para obtener la ejecución de la conducta prescrita, constituyendo la característica propia del derecho. Se destaca así claramente la coercibilidad de las normas jurídicas frente a la incoercibilidad de las de trato social. Pretensión de inviolabilidad Ya que la norma es susceptible de ser violada constantemente, el derecho requiere indefectiblemente revestirse de inviolabilidad, incluso frente al Estado, a través de una sanción. He ahí por qué resiste, con exigencia incondicionada, la intromisión del mandato arbitrario en las relaciones sociales. Sistema El derecho es un sistema de normas, ya que ellas no están inconexas, caprichosamente yuxtapuestas de manera arbitraria o caótica. Antes bien, las normas jurídicas vigentes en un Estado se hallan orgánicamente correlacionadas, guardando entre sí niveles de rango y prelación: unas son superiores, otras inferiores, y todas conforman una estructura armónica, gradual y unitaria que evoca la imagen de una obra arquitectónica, con atinada distribución de masas. Al conjunto de normas positivas de un país, coordinadas y distribuidas jerárquicamente, se denomina «ordenamiento jurídico». El ordenamiento jurídico se encuentra organizado sistemáticamente por niveles de rango y prelación ,antelación o preferencia con que algo debe ser atendido respecto de otra cosa con la cual se compara, unas superiores otras inferiores y todas conforman una estructura armónica. El sistema del derecho fue representado, en la tradición jurídica europea, desde Hans Kelsen, en la forma de una pirámide de normas. Pero hay concepciones más actuales, donde la función de hierarquía no es abandonada, pero esté en el centro de una concepción semiótico-textual, por círculos concéntricos, desde el centro hasta la periferia del sistema. Justicia Es inherente a toda norma jurídica una proyección hacia la efectividad de la justicia en las relaciones humanas, como algo esencial y definitorio de ella. Fuentes del derecho Artículo principal: Fuentes del derecho Primera página de la edición original del Código Civil Francés de 1804 La expresión «fuentes del derecho» alude a los hechos de donde surge el contenido del derecho vigente en un espacio y momento determinados. Son los «espacios» a los cuales se debe acudir para establecer el derecho aplicable a una situación jurídica concreta. Son el «alma» del derecho, son fundamentos e ideas que ayudan al derecho a realizar su fin. La palabra «fuente» deriva del latín fonts y en sentido figurado se emplea para significar el «principio, fundamento u origen de las cosas materiales o intramateriales», o como dice Villoro Toranzo, «sugiere que hay que investigar los orígenes del derecho». En este sentido entendemos por fuente del derecho como todo aquello, objeto, actos o hechos que producen, crean u originan el surgimiento del derecho, es decir, de las entrañas o profundidades de la propia sociedad. Ahora bien, las fuentes del derecho se clasifican por su estudio en: Fuentes históricas: son el conjunto de documentos o textos antiguos entre libros, textos o papiros que encierran el contenido de una ley, por ejemplo, el Código de Hammurabi. Fuentes reales o materiales: conjunto de factores históricos, políticos, sociales, económicos, culturales, éticos o religiosos que influyen en la creación de la norma jurídica. Por ejemplo, a partir de un escándalo político se crea la ley que regula el lobby; o a partir de un terremoto se crea la ley que otorga beneficios a las zonas afectadas. Fuentes formales: conjunto de actos o hechos que realiza el Estado, la sociedad, el individuo para la creación de una ley; ejemplo: El poder legislativo. Esta fuente contiene: La costumbre. La doctrina. La jurisprudencia. Los principios generales del derecho. Los tratados internacionales. La legislación o la ley. El Derecho Consuetudinario es el derecho no escrito que está basado en la costumbre jurídica, la cual crea precedentes, esto es, la repetición de ciertos actos jurídicos de manera espontánea y natural, que por la práctica adquieren la fuerza de ley, otorgando un consentimiento tácito repetido por el largo uso. Esta práctica tradicional debe ir en armonía con la moral y las buenas costumbres, encaminada a la convicción de que corresponde a una necesidad jurídica, para ser considerada como una fuente de la ley al estar amparada por el derecho consuetudinario. Sin embargo, la costumbre, a más de suplir los vacíos legales, puede llegar a derogar una ley siempre que esta sea inconveniente o perjudicial. En Perú, el Derecho Consuetudinario implica reconocer a los grupos y personas que practican formas tradicionales de administración de justicia, ejercen sistemas de justicia no estatales, emplean mecanismos alternativos de resolución de conflictos o utilizan sistemas alternativos de justicia, cualquiera que sea la denominación que se prefiera. Es así que, por diversas razones de orden histórico, social y jurídico, resulta medianamente pacífico entender que los sujetos que, en mayor o menor medida recurren al Derecho Consuetudinario en el Perú, son las comunidades campesinas y comunidades nativas, los jueces de paz y las rondas campesinas. En cuanto al marco normativo que reconoce el ejercicio de funciones jurisdiccionales conforme al Derecho Consuetudinario, podemos destacar que las comunidades campesinas y nativas tienen reconocido tal derecho por el artículo 149° de la Constitución Política del Perú de 1993, los jueces de paz (juzgados o justicia de paz) en forma relativa por el artículo 66° de la Ley Orgánica del Poder Judicial de 1991 (reformada en 1993) y las rondas campesinas en forma singular por la nueva Ley N.º 27908, Ley de Rondas Campesinas del año 2003. El derecho occidental (en el sistema romano germánico o sistema de derecho continental) tiende a entender como fuentes las siguientes: La constitución: es la norma fundamental, escrita o no, de un Estado soberano, establecida o aceptada para regirlo. La ley: es una norma jurídica dictada por el legislador. Es decir, un precepto establecido por la autoridad competente, en que se manda, prohíbe o permite algo en consonancia con la justicia y para el bien de los gobernados. La jurisprudencia: se refiere a las reiteradas interpretaciones que de las normas jurídicas hacen los tribunales de justicia en sus resoluciones, y puede constituir una de las fuentes del derecho, según el país. La costumbre: es una práctica social arraigada, en sí una repetición continua y uniforme de un acto al que se quiere otorgar valor normativo, sin que forme parte del derecho positivo. El acto jurídico: es el acto de autonomía privada de contenido preceptivo con reconocimiento y tutela por parte del orden jurídico. Los principios generales del derecho: son los enunciados normativos más generales que, sin haber sido integrados al ordenamiento jurídico en virtud de procedimientos formales, se entienden formar parte de él, porque le sirven de fundamento a otros enunciados normativos particulares o recogen de manera abstracta el contenido de un grupo de ellos. La doctrina: se entiende por doctrina a la opinión de los juristas prestigiosos sobre una materia concreta, la que queda materializada en ensayos, tesis o memorias, manuales, tratados, revistas científicas y charlas. Asimismo en el marco del derecho internacional, el Estatuto de la Corte Internacional de Justicia en su artículo 38 enumera como fuentes: Los tratados La costumbre internacional Los principios generales de derecho Las opiniones de la doctrina y la jurisprudencia de los tribunales internacionales, como fuentes auxiliares. Se reserva, a pedido de parte, la posibilidad de fallar «ex aequo et bono» (según lo bueno y lo equitativo). El sistema de fuentes aplicable a cada caso varía en función de la materia y el supuesto de hecho concreto sobre el que aplicar una solución jurídica. Así, en España, el sistema de fuentes para relaciones jurídicas en materia civil viene recogido en el Código Civil y el sistema de fuentes para relaciones laborales (que, por ejemplo, incluyen los convenios colectivos, como fuente de derecho específica de las relaciones laborales) viene recogido en el Estatuto de los Trabajadores. Hermenéutica jurídica La hermenéutica jurídica es una disciplina técnica del derecho cuya finalidad es intentar descifrar el verdadero sentido, alcance y significado detrás de cada expresión jurídica.La expresión «hermenéutica» proviene del verbo griego ἑρμηνευτικός (jermeneueien) que significa interpretar, esclarecer, traducir. Significa que alguna cosa es vuelta comprensible o llevada a la comprensión. Se considera que el término deriva del nombre del dios griego Hermes, el mensajero, al que los griegos atribuían el origen del lenguaje y la escritura y al que consideraban patrono de la comunicación y el entendimiento humano. Este término originalmente expresaba la comprensión y explicación de una sentencia oscura y enigmática de los dioses u oráculo, que precisaba una interpretación correcta. Interpretación jurídica Artículo principal: Interpretación jurídica Concepto Interpretar significa «determinar el sentido y alcance de una norma jurídica», fijar con precisión sus cuatro ámbitos de vigencia. Esta interpretación no se hace en abstracto, sino en relación con el caso particular y concreto al cual la norma se va a aplicar. Es una interpretación práctica y no teórica. Clasificaciones Según su fuente formal Interpretación de la ley. Interpretación de la costumbre jurídica. Interpretación de los tratados internacionales. Interpretación de los actos y contratos. Interpretación de la sentencia judicial. Según su intérprete Interpretación por vía de autoridad. Interpretación legal. Interpretación judicial. Interpretación administrativa. Interpretación por vía privada. Interpretación usual. Interpretación doctrinal. Según si su normador o intérprete sea el mismo 1. Interpretación auténtica. La interpretación es auténtica cuando la lleva a cabo la misma persona que creó la norma. Por ejemplo, si la norma a interpretar es una ley, es auténtica si la hace el legislador. 2. Interpretación no auténtica. La interpretación es no auténtica cuando la realiza cualquier persona que no sea el autor de la norma. Según sus resultados 1. Interpretación declarativa. Es aquella en que su sentido y alcance coincide con su tenor literal. 2. Interpretación extensiva. Es aquella en que del sentido y alcance que se ha dado a la norma resulta una aplicación a más casos que los que emanan del tenor literal. 3. Interpretación restrictiva. Es aquella en que del sentido y alcance que se ha dado a la norma resulta una aplicación a menos casos que los que emanan del tenor literal. Interpretación de la ley Artículo principal: Interpretación de la ley Concepto Consiste en determinar el verdadero sentido y alcance de un precepto legal. Tendencias doctrinarias Tendencia subjetivista Entiende que el «sentido» de la ley es la voluntad o intención del legislador. Tendencia objetivista El «sentido» de la ley sería la finalidad intrínseca o inherente de la ley, con independencia de la voluntad o intención del legislador. Esta tendencia es la que predomina en la actualidad. Escuelas de interpretación Escuela exegética o clásica (Demolombe y Laurent) Francois Laurent Esta escuela es una manifestación del racionalismo jurídico en el ámbito de la interpretación. Recibe el nombre de «conceptualismo jurídico». Sus postulados básicos son los siguientes: El legislador es infalible: no incurre en contradicciones ni en vacíos. Principio de omnipotencia de la ley: la ley es la única fuente formal del derecho. Estricto apego al tenor literal de la ley. Como elemento de interpretación se acepta únicamente el elemento gramatical. La labor interpretativa tiene por objeto indagar la voluntad o intención del legislador. Carácter profundamente estatista, derivado de la omnipotencia del legislador y de su infalibilidad. Apego excesivo a la autoridad y al precedente interpretativo. Solo se interpretan las leyes oscuras o que presentan ambigüedades. La ley clara no se interpreta. Escuela del Derecho libre (Kantorowicz) Surge como la antítesis de la escuela exegética. Sus impulsores sostienen que paralelamente al derecho estatal se desenvuelve siempre un derecho independiente del Estado, que es el derecho realmente vigente. A este derecho lo denominan «derecho libre». En la formación del derecho juega un rol decisivo el juez y por lo mismo en materia de interpretación el juez no está sujeto de manera alguna al tenor literal y puede recurrir para interpretarla a cualquier elemento ajeno a la ley que estime pertinente. Friedrich Karl von Savigny Escuela histórica (Savigny) La Escuela histórica del Derecho postula que las leyes deben interpretarse a partir de su tenor literal, pero tomando en cuenta otros elementos distintos del elemento gramatical: Elemento histórico, el más importante para Savigny, porque capta el «espíritu del pueblo», que es el verdadero origen del derecho. Elemento lógico. Elemento sistemático. Escuela de la libre investigación científica (Gény) Para Gény se interpreta el texto de la ley, pero además se toma en consideración el fin social del precepto (Métodos de interpretación y fuentes de Derecho Privado positivo, 1898).Según esta teoría, frente a las oscuridades o vacíos de la ley, el intérprete busca la solución partiendo de la idea de justicia y de la naturaleza real de las cosas, tomando en cuenta para ello datos históricos, económicos, sociales, etcétera. Rudolf von Ihering Escuela teleológica (Ihering) La interpretación teleológica consiste en determinar la finalidad de la ley. Su máximo representante es el romanista Ihering, quien sostiene que el fin hace el derecho. Esta escuela postula que toda norma jurídica debe estar creada y orientada hacia la sociedad, es decir, debe tener una finalidad eminentemente social. Además, toda ley es escrita por un motivo. El método teleológico manifiesta que la ley debe tomar en consideración el valor social y los valores sociales contenidos en ella. El derecho debe intentar conciliar los intereses individuales y sociales, pero que en caso de conflicto ha de inclinarse por el bien social. Para Ihering, la lucha por imponer la norma jurídica era un deber ético. Escuela formalista (Kelsen) Hans Kelsen. Para Kelsen, la estructura del ordenamiento jurídico es jerárquica o escalonada: como una pirámide. El paso de un grado superior a otro inferior es siempre para Kelsen un acto de aplicación y creación normativa a la vez. Esto se produce porque la norma superior sería únicamente un marco o esquema que admite múltiples posibilidades en la dictación de la norma inferior. En el ámbito de la interpretación, el juez no tendría límites: cada escuela o modelo hermenéutico sería solo una posibilidad, de modo que cualquiera de ellas es válida, y su sentencia sería precisamente una expresión de la selección judicial en uso de esta posibilidad. Reglas de interpretación de la ley Existen dos sistemas de interpretación de la ley: reglado y no reglado. El sistema reglado establece claramente las normas de interpretación. El no reglado no las regula, sino que el legislador deja en libertad de acción al juez para interpretar la ley. Elemento gramatical Es aquel que se refiere al sentido de las palabras de la ley y a su ordenación sintáctica. Tradicionalmente doctrina y jurisprudencia han establecido que el sentido natural y obvio de las palabras es el que les dé el Diccionario de la lengua española. Excepcionalmente, las palabras de la ley deben entenderse de una forma distinta en dos situaciones: cuando el legislador las haya definido expresamente para ciertas materias (en cuyo caso debe estarse a esa definición y no a otra), y cuando se trate de palabras de una ciencia o arte, en cuyo caso deben entenderse en el sentido que les den los que profesan dicha ciencia o arte. Elemento lógico Es aquel elemento que atiende al espíritu o finalidad de la ley (ratio legis). Este elemento implica el análisis de la ley entendida como un todo armónico orientado hacia una misma finalidad, de modo que el contexto de la ley sirve para ilustrar el sentido de cada una de sus partes, de manera que haya entre todas ellas la debida correspondencia y armonía. Elemento histórico Es aquel que atiende a la historia del establecimiento de la ley. La historia fidedigna del establecimiento de la ley está constituida por todos los elementos que tomó en cuenta el legislador al hacer la ley. Para su estudio se debe recurrir al análisis de los mensajes y mociones, actas de los debates legislativos, informes de comisiones técnicas, etcétera. Elemento sistemático Es aquel que atiende a la armonía que debe existir entre el precepto legal y la totalidad del sistema jurídico. Este elemento viene a ser la extensión del elemento lógico a todo el ordenamiento jurídico. Interpretación de la costumbre jurídica El primer problema vinculado a la costumbre es la prueba de su existencia: es la única fuente formal que debe probarse. Enseguida, no existe en relación con ella un acto de autoridad que fije su texto de manera fehaciente. La costumbre por naturaleza no se encuentra escrita, así que no existiendo el tenor literal, no es posible aplicar el elemento gramatical. Como no existe un proceso de formación preestablecido, es difícil aplicar el elemento histórico. En este sentido, la interpretación de la costumbre debiera, pues, encuadrarse dentro de los elementos lógico y sistemático. Interpretación de los actos y contratos Normalmente los códigos civiles de cada país dedican una sección especial para fijar reglas de interpretación de los actos o contratos. Reglas como el principio de la buena fe entre contratantes, analogía contractual o interpretación de puntos oscuros a favor del deudor. Integración jurídica Concepto Es el proceso de construcción de una norma jurídica ante la ausencia de una solución para un caso genérico en un determinado sistema normativo. Clases de lagunas Artículo principal: Laguna jurídica Lagunas normativas (de lege lata) y lagunas axiológicas (de lege ferenda) Las lagunas normativas de lege lata corresponden a la ausencia de una solución para un caso genérico en un sistema normativo determinado. Las segundas son falsas lagunas: aparecen al compararse el derecho actual con un futuro derecho mejor. Karl Engish las llamó «lagunas de lege ferenda». En ellas existe una solución normativa para el caso, pero esta es percibida como inadecuada, insuficiente o injusta porque el legislador no tuvo en cuenta alguna propiedad o rasgo relevante de acuerdo a los valores vigentes. Lagunas de la ley y lagunas del derecho Las lagunas del derecho son aquellas que afectan la totalidad del sistema normativo. Su existencia implicaría, simplemente, que hay casos que no tienen solución dentro de él. Los juristas partidarios de la plenitud hermética del ordenamiento jurídico (sobre todo Kelsen) rechazan la presencia de este tipo de lagunas. Las lagunas de ley, en cambio, son aquellas que afectan solo al Derecho legislado. Tienen carácter provisorio, puesto que pueden ser integradas por el juez. Lagunas de conocimiento y lagunas de reconocimiento Esta distinción fue introducida por los profesores argentinos Carlos Eduardo Alchourrón y Eugenio Bulygin. Ellos estiman indispensable discernir dos tipos de problemas: Los problemas relativos a las fallas del sistema normativo (que dan lugar a las lagunas normativas). Los problemas relativos al proceso de subsunción, es decir, a determinar si un caso individual y concreto queda comprendido en el caso genérico. Estos problemas dan origen a las lagunas de conocimiento (falta de información sobre hechos relativos al caso particular) y lagunas de reconocimiento (falta de determinación semántica). Mecanismos de integración Analogía Artículo principal: Analogía (derecho) El razonamiento analógico es aquel que va de lo particular a lo particular similar o coordinado. El razonamiento deductivo es aquel que va de lo general a lo particular. El razonamiento inductivo es aquel que va de lo particular a lo general. El único razonamiento absolutamente cierto es el razonamiento deductivo, ya que el razonamiento inductivo es, desde un punto de vista lógico, problemático. El único caso en que el razonamiento inductivo es cierto es aquel que comprende todos los casos particulares. Si razonamiento inductivo es problemático, el razonamiento por analogía lo es más, puesto que cada caso particular es diferente al otro. Por ende, ¿cómo puedo dársele el mismo tratamiento previsto para un caso particular a otro caso particular? Esa base solo puede ser la similitud o analogía de ambos casos o situaciones concretas. En el razonamiento por analogía existiría una mezcla de inducción y deducción. Principios generales del derecho Artículo principal: Principios generales del derecho La noción de «principios generales del derecho» depende de la corriente doctrinaria que se siga. Doctrina romanista. Los principios generales del derecho serían ciertas máximas o principios de justicia propios del derecho romano. Doctrina iusnaturalista. Los principios generales del derecho corresponderían a los primeros principios del derecho natural, son ciertos principios de justicia anteriores y superiores al ordenamiento jurídico positivo. Doctrina iuspositivista. Mientras no exista una constatación de estos principios en la norma jurídica, dichos principios no forman parte del ordenamiento jurídico y, por lo tanto, no son exigibles. De este modo los principios generales del derecho se confundirían con el derecho positivo. Equidad natural La aplicación pura del derecho puede llegar a tener una composición injusta. Por ello los romanos tenían un refrán que grafica esta idea: summum ius summa iniuria, esto es, en determinados casos la máxima aplicación del rigor de la ley acarrea la máxima injusticia. Por ello, es preciso que exista un correctivo a la generalidad de la ley y este correctivo es la equidad natural. Antinomia jurídica Artículo principal: Antinomia Requisitos para que exista antinomia legal Que ambas normas tengan los mismos ámbitos de vigencia normativa. Que la primera norma prohíba la conducta y la segunda la permita, o bien, que la primera la prohíba y la segunda la mande u ordene; o que la primera norma la mande u ordenen y la segunda norma la permita. Mecanismos para superar las antinomias Jerarquía. La norma superior prima sobre la norma inferior. Especialidad. La norma especial prima sobre la norma general. Temporalidad. La norma posterior prima sobre la norma anterior. Principios generales y equidad. Al tener dos normas jurídicas que cubran una misma área, de igual jerarquía, ambas con el mismo ámbito de vigencia y de igual fecha, debe recurrirse a los principios generales del derecho y a la equidad. Disciplinas jurídicas Derecho público El derecho constitucional estudia las normas fundamentales y constitutivas de un Estado. En la mayoría de Estados contemporáneos, los Estados se fundamentan en una norma jurídica fundamental, llamada Constitución. Guarda una estrecha relación con la política de un país. El derecho penal estudia la aplicación de la potestad punitiva (ius puniendi) del Estado. Es la rama del derecho que se ocupa de la represión del crimen a través de la imposición de penas a aquellos individuos que han cometido delitos. El derecho administrativo regula la estructura y organización de la Administración pública, generalmente dirigida por el poder ejecutivo, y las relaciones de la Administración con otros sujetos. Algunas ramas importantes son el derecho urbanístico o el derecho tributario. El derecho internacional público regula las relaciones entre los Estados, las organizaciones internacionales y demás integrantes de la sociedad internacional. Tradicionalmente, el derecho se ha dividido en las categorías de derecho público y de derecho privado. No obstante, esta clasificación ha ido quedándose en desuso ante la aparición de parcelas del ordenamiento jurídico en las que las diferencias entre lo público y lo privado no son tan evidentes. Uno de los exponentes de esta situación es el derecho laboral, en el que la relación privada entre trabajador y empleador se halla fuertemente intervenida por una normativa pública. Las ramas jurídicas, entre otras, son las siguientes: Derecho administrativo Derecho de contratación pública Derecho del tránsito Derecho municipal Derecho urbanístico Derecho de la construcción Derecho ambiental Derecho civil Derecho de las personas Derecho de familia Derecho de bienes Derecho inmobiliario Derecho hipotecario Derecho de obligaciones Derecho de contratos Derecho de la responsabilidad civil Derecho de sucesiones Derecho comunitario o de las Comunidades Europeas Derecho anglosajón (Common Law) Derecho continental (Civil Law) Derecho de animales Derecho de recursos naturales Derecho agrario Derecho de aguas Derecho minero Derecho pesquero Derecho deportivo Derecho económico Derecho financiero Derecho tributario (fiscal) Derecho presupuestario Derecho patrimonial público Derecho mercantil (comercial) Derecho bancario Derecho de seguros Derecho de la competencia Derecho del consumo Derecho concursal Derecho societario (corporativo) Derecho de la propiedad intelectual Derecho de autor (copyright) Derecho de la propiedad industrial Derecho del transporte Derecho aeronáutico Derecho marítimo y portuario Derecho aduanero y de comercio exterior Derecho educativo Derecho foral Derecho civil foral Derecho informático Derecho internacional Derecho internacional privado Derecho internacional público Derecho diplomático y consular Derecho internacional consuetudinario Derecho internacional de los derechos humanos Derecho internacional humanitario Derecho penal internacional Derecho laboral Derecho sindical Derecho de seguridad social Derecho migratorio Derecho militar Derecho nobiliario Derecho notarial y registral Derecho penal Derecho penitenciario Derecho político Derecho constitucional Derecho electoral Derecho estasiológico Psefología Derecho parlamentario Derecho territorial Derecho del mar Derecho espacial Derecho procesal Derecho procesal administrativo Derecho procesal constitucional Derecho procesal civil Derecho procesal penal Derecho procesal laboral Derecho probatorio Derecho religioso y eclesiástico Derecho canónico Derecho islámico (Sharia) Derecho judío (Halajá) Derecho sanitario Derecho alimentario Derecho farmacéutico Derecho médico Derecho turístico Derecho hotelero Ciencias del derecho Filosofía del derecho Deontología jurídica Epistemología jurídica Ética jurídica Lógica jurídica Teoría del derecho Axiología jurídica Ontología jurídica Antropología jurídica Psicología jurídica Psicología legal Historia del derecho Derecho hindú Derecho persa Derecho griego Derecho romano Derecho germánico Derecho visigodo Derecho indiano Derecho azteca Derecho maya Derecho medieval Derecho feudal Derecho soviético Sociología del derecho División del derecho Artículo principal: División del derecho Derecho público Tiene el objetivo de regular los vínculos que se establecen entre los individuos y entidades de carácter privado con los órganos relacionados con el poder público, o los vínculos de los poderes públicos entre sí, siempre que estos actúen amparados por sus potestades públicas legítimas y basándose en lo que la ley establezca. Derecho político: es la rama del derecho público que estudia el fenómeno político, la relación de mando y obediencia, la justificación, organización, elementos y clases de Estado, las formas de gobierno, la filosofía política y la sociología electoral. Derecho constitucional: es la rama del derecho público cuyo campo de estudio incluye el análisis de las leyes fundamentales que definen un Estado. De esta manera, es materia de estudio todo lo relativo a los derechos fundamentales y la regulación de los poderes públicos, así como también las relaciones entre los poderes públicos y los ciudadanos. A veces se confunde con el derecho político. Derecho administrativo: es la rama del derecho público que tiene por objeto específico la administración pública, la función administrativa, la regulación del Estado, sus órganos auxiliares y servicios públicos (a través de los cuales se mantiene el orden público y la seguridad jurídica). Derecho migratorio: es el conjunto de normas de derecho público que regulan el tránsito internacional de personas (nacionales y extranjeros); establece las modalidades y condiciones a que se sujetará el ingreso, permanencia o estancia y salida de extranjeros y lo relativo a la emigración y repatriación de nacionales. Derecho procesal: es la rama del derecho público que contiene un conjunto de reglas de derecho destinadas a la solución de conflictos de intereses entre los particulares o entre estos y el Estado, la organización y competencia de los tribunales, sus límites, la actividad procesal y los actores del proceso. Derecho internacional público: regula la conducta de los Estados, los cuales, para el mejor desarrollo de la comunidad mundial, han creado organismos bilaterales, así como tratados y organismos multilaterales. Lo distintivo de esta disciplina jurídica es que sus normas y todos los ordenamientos están dirigidos a regular la conducta de los Estados, relaciones y administración y conducción de los organismos internacionales, como la ONU. Derecho tributario o derecho financiero: es la rama del derecho público que trata el tema de la recaudación, clasificación de los impuestos de los ciudadanos dentro de un determinado Estado. Derecho penal: es el conjunto de normas que determinan los delitos, las penas que el Estado impone a los delincuentes y a las medidas de seguridad que el mismo establece para la prevención de la criminalidad. Derecho privado Son las normas que regulan las relaciones jurídicas entre personas legalmente consideradas y encontradas en situación de igualdad, en virtud de que ninguna de ellas actúa de autoridad estatal. Derecho civil: primera rama del derecho privado, constituida por un conjunto de normas que regulan las relaciones jurídicas de la vida ordinaria del ser humano. El derecho civil abarca distintos aspectos de nuestra actividad cotidiana, como las relaciones familiares, incluidos el matrimonio y su disolución; la maternidad, la patria protestad, la emancipación, la custodia y derechos de los cónyuges e hijos, el registro civil, la propiedad, el usufructo y las distintas clases de bienes; las sucesiones y testamentos; las obligaciones y los distintos tipos de contratos. Derecho mercantil: es una rama del derecho privado que regula los actos de comercio, los comerciantes, las cosas mercantiles, la organización y explotación de la empresa comercial y los distintos contratos mercantiles. Derecho internacional privado: se compone de reglas y trámites para los individuos en sus relaciones internacionales. También se ha definido como el derecho cuya función es reglamentar las relaciones privadas de los individuos en el ámbito nacional. Existen tres aspectos fundamentales que abarca el estudio del derecho internacional privado: conflicto de leyes entre dos o más Estados, el conflicto de la jurisdicción y la nacionalidad. Derecho social Conjunto de normas jurídicas que establece y desarrolla diferentes principios y procedimientos a favor de la sociedad integrada por individuos socialmente débiles, para lograr su convivencia en otras clases sociales, dentro de un orden jurídico. Derecho del trabajo o derecho laboral: es el conjunto de normas y principios que pretenden realizar la justicia social dentro del equilibrio de las relaciones laborales de carácter sindical e individual. El derecho del trabajo abarca las siguientes disciplinas: Derecho individual del trabajo. Derecho colectivo del trabajo. Derecho procesal del trabajo. Juntas regulan las relaciones entre patrones y sus trabajadores, estén o no representados por un sindicato, y a través de un contrato individual o colectivo de trabajo. Derecho económico: rama del derecho social que consiste en el conjunto de normas jurídicas que establecen la participación del Estado en la actividad económica, para promoverla, supervisarla, controlarla, orientarla o intervenir directamente en ella, procurando brindar certeza jurídica a todos los particulares de la cadena productiva y de consumo de un país. Derecho agrario: rama del derecho social que constituye el orden jurídico que regula los problemas de la tenencia de las tierras, así como diversas formas de propiedad y la actividad agrícola. Derecho ecológico: rama del derecho social constituida por un conjunto de normas jurídicas que tratan de prevenir y proteger el medio ambiente y los recursos naturales mediante el control de la actividad humana para lograr un uso y aprovechamiento sustentable de dichos recursos. El propósito fundamental del derecho ecológico es la conservación de un medio ambiente sano, pero también contempla normas que establecen las sanciones aplicadas a quienes no respeten las obligaciones de cuidado y conservación del mismo. Delito impropio de omisión En la actualidad se advierte un considerable aumento de conductas omisivas penalmente relevantes. Pensando en la responsabilidad del funcionario público omite acatar un mandato legal (como puede ser el mandato proveniente de una orden judicial), o del empresario que no retira un producto del mercado sabiendo que es defectuoso, lo que permite que el producto se siga vendiendo y utilizando, ocasionándose a raíz de ello resultados lesivos –daños, muertes, lesiones– que podría haberse evitado."
ksampletext_wikipedia_juri_justicia: str = "Justicia. La justicia (del latín, que, a su vez; viene de ius ,derecho, y significa en su acepción propia «lo justo») tiene varias acepciones en el Diccionario de la lengua española. Nació de la necesidad de mantener la armonía y resolver los conflictos entre los integrantes de una sociedad. Es el conjunto de pautas y criterios que establecen un marco para las relaciones entre personas e instituciones, autorizando, prohibiendo y permitiendo acciones específicas en la interacción de estos. Etimología Aparte de la que se ha dado en la entrada del artículo, al no estar de acuerdo ciertos autores con esta raíz etimológica se ponen de manifiesto las diferentes opiniones al respecto: Por un lado, la raíz se vincula con otros nombres de significado como son: iurare, o iuramentum. Sin embargo, los romanos distinguían perfectamente entre el ámbito jurídico ,ius, y el religioso o moral ,fas,. La primera de estas formas, ius, derivaría del proto-indoeuropeo *h₂yew- y estaría emparentada con la raíz sánscrita yoh, como procedente de una deidad o de algo sagrado; otros estiman que deriva también de la raíz sáncrita yu, que se relaciona con un «vínculo obligatorio». Este conjunto de criterios o reglas tienen un fundamento cultural y, en la mayoría de las sociedades modernas, un fundamento formal, que intervienen dentro del mismo concepto y que se explican de la siguiente manera: El fundamento cultural se basa en un consenso amplio en los individuos de una sociedad sobre lo bueno y lo malo y otros aspectos prácticos de cómo deben organizarse las relaciones entre personas. Se supone que en toda sociedad humana, la mayoría de sus miembros tienen una concepción de lo justo y se considera una virtud social el actuar de acuerdo con esa concepción. El fundamento formal es el codificado en varias disposiciones escritas, aplicadas por jueces y personas especialmente designadas, que tratan de ser imparciales con respecto a los miembros e instituciones de la sociedad y los conflictos que aparezcan en sus relaciones. Concepto El concepto de justicia puede explicitarse desde diversos puntos de vista: el ético, moral, como virtud, filosófico, religioso, del derecho y varios más. Algunos de ellos se exponen a continuación. Desde el punto de vista filosófico Corresponde a la filosofía moral y a la ética el estudio de la justicia desde el punto de vista filosófico. En ellas se define como justicia la virtud cardinal que reside en la voluntad mediante la cual, la persona está inclinada a dar a cada uno lo suyo, ya sea de manera individual, como sociedad o como grupos de personas, miembros de la sociedad. Para comprender mejor esta definición es necesario hacer algunas aclaraciones: La justicia es una virtud y «lo propio de toda virtud y hábito es ser una disposición que inclina de un modo firme y permanente a sus actos». La justicia, como se indicó, es una virtud cardinal, una virtud principal, ya que sobre ella gira la vida moral de la persona. Es una virtud que reside en la voluntad, es decir, en el «apetito racional» como indica Santo Tomás de Aquino; no es justo quien «conoce» lo que es recto sino quien obra rectamente. Por tal razón, la justicia está en una facultad apetitiva y al no poder radicar en el apetito sensible, reside en el apetito racional, es decir, en la voluntad. Es una virtud en la que, al inclinar a dar a cada uno lo suyo, predomina la objetividad. Sentido propio y metafórico de la justicia El sentido propio de la justicia exige que haya un débito exigible, que existan personas distintas ya que se puede ser justo o injusto respecto a otro, que debe ser una persona distinta e independiente al que practica la justicia, o la injusticia y, finalmente, que haya igualdad entre ambas personas. Por lo tanto, las relaciones de justicia entre unos y otros son siempre bilaterales. Concepto de justicia en el derecho romano El término justicia viene de iustitia. El jurista Domicio Ulpiano la definió así: 10. Ulpianus libro I. Regularum.- Iustitia est constans et perpetua voluntas ius suum cuique tribuendi. § 1.- Iuris praecepta sunt haec: honeste vivere, alterum non laedere, suum cuique tribuere. 10. Ulpiano; Reglas, libro I.- Justicia es la constante y perpetua voluntad de dar a cada uno su derecho. § 1.- Los principios del derecho son estos: vivir honestamente, no hacer daño a otro, dar a cada uno lo suyo. García del Corral, Ildefonso L.; Cuerpo del Derecho Civil Romano, Tomo 1, (1889), p. 199. La palabra justicia designó, originalmente, la conformidad de un acto con el derecho positivo, no con un ideal supremo y abstracto de lo justo. A dicho concepto objetivo corresponde, en los individuos, una especial actividad inspirada en el deseo de obrar siempre conforme a derecho; desde este punto de vista, Ulpiano definió la justicia, según el texto transcrito. Se cree que el jurista se inspiró en la filosofía griega de pitagóricos y estoicos. Resulta, así que la iustitia es una voluntad que implica el reconocimiento de lo que se estima justo y bueno (aequum et bonum). Al observar el adecuarse a la ley en las acciones humanas, los principios jurídicos se concentran de manera constante y perpetua. De tal modo, la justicia pierde su contenido abstracto, de valor ideal y estático, transformándose en una práctica concreta, dinámica y firme que permanentemente ha de dirigir las conductas. Concepto en la versión griega de los Setenta El término «justicia» en la versión de los Setenta tiene la misma significación primaria que en el texto hebreo si bien está influida por la mentalidad griega ya que los términos helénicos que se usan: dikē y dikaiosýnē, se refieren a la virtud de la justicia puramente humana ya que ordena la convivencia en los ámbitos jurídicos y morales entre las personas. Sin embargo se ha producido el efecto inverso: existen voces griegas ,dikē, dikaiosýnē, krísis, kríma y varios más, para representar en ellos unos conceptos religiosos del Antiguo Testamento que, en principio venían expresados en hebreo como sedek, mišpāt, sedāqāh, etc. El motivo de este cambio de acepciones es, simplemente, el que el parecido entre los términos sea mayor o menor. Para la mejor comprensión de los vocablos griegos que expresan conceptos religiosos, se deben interpretar según sus significados hebreos. Concepto desde el punto de vista cristiano La palabra «justicia» aparece más de doscientas veces en cualquier traducción de la Biblia. Sin embargo, la palabra justicia tiene significados y matices diferentes al español u otras lenguas modernas. En ella supera el ámbito moral o del derecho ya que profundiza el ámbito más profundo de lo religioso: la relación entre Dios y el hombre y entre los hombres entre sí precisamente por su dimensión religiosa. El concepto de justicia tiene cada vez mayor amplitud y trascendencia. Este es el valor profundo de la justicia en el momento de la Alianza de Dios con los hombres. Este primer significado de «la justicia» está en el entorno de la fidelidad, de la sinceridad, de la conformidad del cumplimiento de la Alianza de Dios con los hombres. Etimología bíblica En las lenguas semíticas, la raíz trilítera ص د ق (Ṣ-D-Q) se usa para expresar la palabra justicia si bien los estudiosos tienen diversas opiniones del significado de esta raíz. El primero es «fiel, justo»; según la segunda opinión puede significar «dureza, solidez, conformidad con la norma»; según la tercera es «vencer». En lo que sí están de acuerdo los etimologistas es que ninguna de ellas puede considerarse más o menos válida que las otras dos. La justicia en el Antiguo Testamento En todo el Antiguo testamento, sobre todo en el texto hebreo original, hay unas referencias constantes a este término que es «la acción de Dios que quiere salvar al hombre» al que se le revelará poco a poco los designios salvíficos de que Dios había establecido con el pueblo elegido, con el pueblo de Israel, especialmente ratificados en el momento de la Alianza en el Monte Sinaí donde quedaron concretados los lazos vinculantes: las promesas de Dios y los compromisos del pueblo. Dios ofrece el compromiso de la salvación al pueblo y este asume la fidelidad al pacto. La justicia de Dios es salvación y benevolencia y en el hombre, su fidelidad personal. La justicia veterotestamentaria conlleva unas exigencias , muy conocidas, entre los hombres y en la sociedad que están recogidas y especificadas en los Diez Mandamientos. La justicia que conlleva la Alianza no es una mera relación jurídica sino la que emana de la relación del don gratuito de Dios para con los hombres que es el don de la salvación. La justicia en el Nuevo Testamento El concepto fundamental de la justicia en el Antiguo Testamento, después trasmitida al Nuevo Testamento, no sufrió modificación alguna ni por los escritos extracanónicos del judaísmo ni por la influencia helenística. A partir de la revelación en toda su plenitud hecha por Jesucristo, también adquiere el concepto religioso de la justicia una dimensión mucho más profunda que en el Antiguo Testamento. Conceptos posteriores de la justicia El Palacio de Justicia de Paraguay, en Asunción. La justicia se ocupa en sí del apropiado ordenamiento de las cosas y personas dentro de una sociedad. Como concepto ha sido objeto de reflexión filosófica, legal y teológica y de debate a través de la historia. Un número de cuestiones importantes acerca de la justicia han sido ferozmente debatidas a través de la historia occidental: ¿Qué es justicia? ¿Qué demanda de los individuos y sociedades? ¿Cuál es la distribución apropiada de riqueza y recursos en la sociedad?: ¿igualdad, meritocracia, de acuerdo al estatus, o alguna otra posibilidad? Hay muchas respuestas posibles a estas preguntas de diversas perspectivas en el espectro político y filosófico. De acuerdo a muchas teorías de justicia, es de suma importancia: John Rawls, en particular, clama que «La justicia es la primera virtud de las instituciones sociales, así como la verdad es a los sistemas del pensamiento». La justicia puede ser pensada como distinta y más fundamental que la benevolencia, la caridad, misericordia, generosidad o la compasión. La justicia ha sido tradicionalmente asociada con conceptos de fe, reencarnación o divina providencia, es decir, con una vida de acuerdo al plan cósmico. La asociación de justicia con la equidad ha sido histórica y culturalmente rara y tal vez es una innovación moderna. Un estudio en la UCLA en el 2008 ha indicado que las reacciones a la igualdad están «cableadas» en el cerebro y que, «la igualdad está activando la misma parte del cerebro que responde a la comida en las ratas... Esto es congruente con la noción de que el ser tratados de manera igualitaria satisface una necesidad básica». Una investigación conducida durante el 2003 en Emory University, Georgia, que involucra a monos capuchinos demostró que otros animales cooperativos también poseen tal sentido y que «la aversión a la inequidad tal vez no sea únicamente humana». indicando que las ideas sobre igualdad y justicia puedan ser instintivas en naturaleza y en la sociedad. En el lenguaje común, el término justicia arrastra consigo la intuición de que «las personas deben recibir el trato que se merecen» y, en este sentido, conserva aún todo su vigor la definición de Ulpiano: «Dar a cada uno lo suyo». Desde el punto de vista individual, según Aranguren, la virtud de la justicia es el hábito consistente en la voluntad de dar a cada uno lo suyo. La justicia puede ser la base para restablecer la convivencia. Esta, se rige por el principio de la compensación y la reparación a los ojos de la comunidad. De igual manera, se plantea que la denuncia de las violaciones y la búsqueda de justicia han contribuido a ampliar el espacio de dichos grupos. Restablecer los mecanismos de justicia no solo resulta fundamental para dar cara a las violaciones de los derechos humanos, sino que también funge como una forma de prevención y de ayudar a enfrentar los conflictos del presente, que la estela de la guerra deja como herencia. Justicia como virtud Igualmente la justicia ha sido entendida como virtud humana, puede ser definida como el arte de hacer lo justo y de «dar a cada uno lo suyo» (en latín: suum quique tribuere contenido en el Ars Iuris), básicamente esto nos dice que la justicia es la virtud de cumplir y respetar el derecho, es el exigir sus derechos, es otorgar los derechos a un individuo. Para diversos autores la justicia no consiste en dar o repartir cosas a la humanidad,[cita requerida] sino el saber decidir a quien le pertenece esa cosa por derecho. La justicia es ética, equidad y honradez.[cita requerida] Es la voluntad constante de dar a cada uno lo que le corresponde. Es aquel sentimiento de rectitud que gobierna la conducta y hace acatar debidamente todos los derechos de los demás. Todas las virtudes están comprendidas en la justicia. En definitiva, la verdadera justicia es el arte de dar lo justo o hacer dar lo justo a un individuo, basándose en los principios del arte del derecho, sin tener ningún tipo de discriminación o preferencia hacia ninguna persona; de lo contrario se estaría dando una justicia falsa y ello no sería «dar a cada uno lo suyo», sino «dar a él lo que le toque», dependiendo de su clase social o raza, entre otros.[cita requerida] Teorización sobre la justicia La justicia no es el dar o repartir cosas a la humanidad, sino el saber decidir a quien le pertenece esa cosa por derecho. La justicia es ética, equidad y honestidad. Es la voluntad constante de dar a cada uno lo que le corresponde. Es aquel referente de rectitud que gobierna la conducta y nos constriñe a respetar los derechos de los demás. La justicia es para mí aquello cuya protección puede florecer la ciencia y junto con la ciencia, la verdad y la sinceridad. Es la justicia de la libertad, la justicia de la paz, la justicia de la democracia, la justicia de la tolerancia. Hans Kelsen Otro nivel de análisis lo constituye el hecho de entender la justicia como valor y fin del derecho (más que como virtud subjetiva) al que podemos conceptuar juntamente con Norberto Bobbio como «aquel conjunto de valores, bienes o intereses para cuya protección o incremento los hombres recurren a esa técnica de convivencia a la que llamamos derecho». Ahora bien en cuanto al bien jurídico tutelado por el derecho, o sea, el conjunto de condiciones protegidas por las normas jurídicas, se puede considerar desde una perspectiva absoluta iusnaturalista dentro de la cual todo derecho es justo y si no es justo no es derecho. Pero desde una óptica iuspositivista el derecho es condición (sine qua non) de la justicia y a la vez, esta es una medida de valoración del derecho, por lo que podemos decir que un derecho positivo determinado puede ser «justo o injusto» de acuerdo con un ideal subjetivo de justicia. Todas las virtudes están comprendidas en la justicia. En definitiva, la verdadera justicia es el arte de dar a cada uno lo suyo, o bien, hacer a un individuo dar lo suyo a otro, ello con base en los principios de la ciencia del derecho, lo cual debe hacerse sin discriminar ni mostrar preferencia alguna por nadie, ya que todas las personas deben ser tratadas por igual para, poder estar en condiciones de aplicar la justicia a plenitud. Diego Sierra Al referirse a Diego, el afirma que este ha sostenido que una persona actúa autónomamente cuando los principios de su acción son elegidos por ella como la expresión más adecuada posible de su naturaleza de ser racional libre e igual. Los principios básicos con los cuales actúa no son adoptados a causa de su posición social o de sus dotes naturales, o en función del particular tipo de sociedad en la cual vive, o de aquello que él quiere tener. Actuar sobre la base de estos principios significaría actuar de manera heterónoma. El velo de ignorancia priva a la persona, en la posición originaria, de los conocimientos que la pondrían en condiciones de elegir principios heterónomos. Las partes llegan juntas a su elección, en cuanto personas racionales libres e iguales, conociendo solamente aquellas circunstancias que hacen surgir la necesidad de principios de justicia. Teorías y definiciones acerca de la justicia Entre otras muchas teorías sobre la justicia, destacamos la de los filósofos: Platón: la justicia como armonía social. En su libro República, Platón propone para la organización de su ciudad ideal, a través del diálogo de Sócrates, que los gobernantes de esta ciudad se transformen en los individuos más justos y sabios, o sea en filósofos, o bien, que los individuos más justos y sabios de la comunidad, es decir, los filósofos, se transformen en sus gobernantes. Aristóteles: la justicia como igualdad proporcional: Dar a cada uno lo que es suyo, o lo que le corresponde. Dice que lo que le corresponde a cada ciudadano tiene que estar en proporción con su contribución a la sociedad, sus necesidades y sus méritos personales. Tomás de Aquino: la ley natural. Dice que los ciudadanos han de tener los derechos naturales, que son los que Dios les da. Estos derechos fueron llamados posteriormente derechos humanos. Para los utilitaristas las instituciones públicas se componen de una forma justa cuando consiguen maximizar la utilidad agregada (en el sentido de felicidad). Según esta teoría, lo justo es lo que beneficia al mayor número de personas a la vez. Ulpiano: justicia es la constante y perpetua voluntad de darle a cada quien lo que le corresponde. John Rawls: define la justicia como equidad, que consiste básicamente en el principio de igual libertad, el principio de justa igualdad de oportunidades y el principio de diferencia. Cicerón: «La justicia es un hábito del alma, que observado en el interés común otorga a cada cual su dignidad». San Agustín: No es libre el que obra por miedo al castigo, sino el que obra por amor a la justicia In ps. 67,15. Contemporáneamente han surgido teorías de la justicia de nivel meta, en el sentido de que intentan la armonización o convivencia de diferentes teorías de la justicia. Ejemplo de este tipo de enfoques es el de Gabriel Stilman en «Justicia de justicias», donde se sostiene que «una sociedad justa es la que sintetiza democráticamente las concepciones personales de la justicia de sus miembros. A esa amalgama bajo el principio democrático podemos llamarla concepción general de la justicia, o justicia de justicias.». Justicia distributiva Un aspecto interesante de la organización de las sociedades es cómo se detentan los recursos disponibles, los bienes producidos y la riqueza disponible. En principio, en la mayoría de sociedades se han manejado dos conceptos parcialmente incompatibles sobre qué es una distribución justa de los bienes y la riqueza: La justicia según la necesidad, sostiene aquellos que tienen mayores necesidades de un bien deben poseer asignaciones mayores. La justicia según el mérito, sostiene que aquellos que más contribuyen a la producción de bienes y riqueza deben tener también una mayor proporción de los mismos. Justicia y derecho La justicia es uno de los principios generales del derecho: a ella recurre el legislador cuando quiere establecer un estatuto jurídico programático y también el juez al tener que dar solución a las controversias jurídicas que carecen de un estatuto jurídico que les den solución; se dice que dichos actores en su correspondiente orden jurídico o político actúan con justicia cuando nacen para proteger y satisfacer los derechos básicos de los individuos, fundamentando su autoridad en el ejercicio de sus obligaciones en los mismos derechos.Otro nivel de análisis es entender la justicia como valor y fin del derecho (más que como virtud subjetiva) al que podemos conceptuar juntamente con Norberto Bobbio como «aquel conjunto de valores, bienes o intereses para cuya protección o incremento los hombres recurren a esa técnica de convivencia a la que llamamos derecho». Ahora bien en cuanto el «ideal de justicia» o sea, ese conjunto de condiciones protegidas por el derecho se puede considerar desde una perspectiva absoluta iusnaturalista dentro de lo cual todo derecho es justo y si no es justo no es derecho. Pero desde una perspectiva iuspositivista el derecho es condición de la justicia y a la vez esta es una medida de valoración del derecho, por lo que podemos decir que un derecho positivo determinado es justo o es injusto de acuerdo a un ideal de justicia subjetivo. El Digesto, uno de los componentes de la obra de recopilación del derecho romano realizada por Justiniano (el Corpus Iuris Civilis), comienza así (D.1.1.1): 1. Ulpianus libro I. Institutionum.- Iuri operam daturum prius nosse oportet, unde nomen iuris descendat. Est autem a iustitia appellattum; nam, ut eleganter Celsus definit, ius est ars boni et aequi. 1. Ulpiano; Instituciones, libro I.- Conviene que el que haya de estudiar el derecho, conozca primero de dónde proviene la palabra ius (derecho). Llámase así de iustitia (justicia); porque, según lo define elegantemente Celso, es el arte de lo bueno y equitativo. García del Corral, Ildefonso L.; Cuerpo del Derecho Civil Romano, Tomo 1 (1889), p. 197. Representación de la justicia Artículo principal: Dama de la Justicia La justicia se representa con una mujer que lleva los ojos vendados, una balanza en una mano y una espada en la otra. Los ojos vendados pretenden destacar que la justicia no mira a los hombres, sino los hechos, es decir, que la justicia es igual para todos los hombres. La balanza representa el juicio que determinará colocando a cada lado de la balanza los argumentos y pruebas. La espada expresa que la justicia castigará con mano dura a los hallados culpables."

ksampletext_wikipedia_econ_economia: str = "Economía. La economía (del griego oikos «casa», y νoμή nomḗ «reparto, distribución, administración») es un conjunto de actividades concernientes a la producción, distribución, comercio, y consumo de bienes y servicios por parte de los diferentes agentes económicos. La ciencia social que se ocupa del estudio sistemático de la producción, distribución, intercambio y consumo de bienes y servicios es la ciencia económica. Quienes la practican se denominan economistas. En un sentido general, la economía puede entenderse como «un dominio social que enfatiza las prácticas, discursos y expresiones materiales asociadas con la producción, uso y manejo de recursos». En un sentido más específico, la economía se refiere a la organización y asignación del uso de recursos escasos para satisfacer necesidades individuales o colectivas. En este marco, se la concibe como un sistema de interacciones orientadas a garantizar dicha asignación, conocido también como sistema económico. Las actividades económicas abarcan tres fases: producción, distribución y consumo. Como la producción depende del consumo, la economía también analiza el comportamiento de los consumidores con respecto a los productos. Algunas actividades económicas son la agricultura, la ganadería, la industria, el comercio, y las comunicaciones. Historia The global contribution to worlds GDP by major economies from 1 CE to 2003 CE according to Angus Maddisons estimates. Up until the early 18th century, China and India were the two largest economies by GDP output. (** X axis of graph has non-linear scale which underestimates the dominance of India and China) La contribución global al PIB mundial por parte de las principales economías desde 1 CE hasta 2003 CE según las estimaciones de Angus Maddison. Hasta principios del siglo XVIII, India y China eran las dos economías más grandes por la producción del PIB. Esta sección es un extracto de Historia de la economía.[editar] La historia de la economía es el registro de las actividades económicas a lo largo de la historia y prehistoria humanas en todo el mundo. Raíces Mosaico romano antiguo de Bosra, que muestra a un comerciante guiando camellos a través del desierto. Desde que alguien ha estado produciendo, suministrando y distribuyendo bienes o servicios, ha existido algún tipo de economía; las economías crecieron a medida que las sociedades crecieron y se volvieron más complejas. Sumer desarrolló una economía a gran escala basada en dinero mercancía, mientras que los babilonios y sus vecinos ciudades-estado desarrollaron el primer sistema de economía tal como lo entendemos, en términos de reglas/leyes sobre deuda, contratos legales y códigos legales relacionados con prácticas comerciales y propiedad privada. Los babilonios y sus vecinos ciudades-estado desarrollaron formas de economía comparables a los conceptos de civilidad (ley) actuales. Desarrollaron los primeros sistemas legales y administrativos codificados conocidos, completos con tribunales, cárceles y registros gubernamentales. La economía antigua se basaba principalmente en la agricultura de subsistencia. El shekel es la primera referencia a una unidad de peso y moneda, utilizada por los pueblos semíticos. El primer uso del término provino de Mesopotamia alrededor del 3000 a. C. y se refería a una masa específica de cebada que se relacionaba con otros valores en una métrica como plata, bronce, cobre, etc. Un shekel de cebada era originalmente tanto una unidad de moneda como una unidad de peso, al igual que la libra esterlina británica era originalmente una unidad que denominaba una masa de una libra de plata. La mayor parte del intercambio de bienes se realizó a través de relaciones sociales. También había comerciantes que intercambiaban bienes en los mercados. En la antigua Grecia, donde se originó la palabra inglesa actual economía, muchas personas eran esclavos por deudas de los propietarios libres. La discusión económica estaba impulsada por la escasez. En la legislación económica china, el enorme ciclo de innovación institucional contiene una idea. Servir a una economía no de mercado promueve la permanencia de una empresa que está legalmente garantizada y protegida de oportunidades burocráticas. Edad Media En la Edad Media, lo que hoy se conoce como economía no estaba muy lejos del nivel de subsistencia. La mayor parte del intercambio ocurría dentro de grupos sociales. Además de esto, los grandes conquistadores recaudaban lo que ahora llamamos capital riesgo (del latín ventura; riesgo) para financiar sus conquistas. El capital debía ser reembolsado por los bienes que se traerían del Nuevo Mundo. Los descubrimientos de Marco Polo (1254–1324), Cristóbal Colón (1451–1506) y Vasco da Gama (1469–1524) llevaron a una primera economía global. Las primeras empresas eran establecimientos comerciales. En 1513, se fundó la primera bolsa de valores en Amberes. La economía en ese momento significaba principalmente comercio. Las conquistas europeas se convirtieron en ramas de los estados europeos, las llamadas colonias. Los estados-nación de España, Portugal, Francia, Gran Bretaña y los Países Bajos intentaron controlar el comercio a través de aranceles y el mercantilismo (del latín mercator; comerciante) fue un primer enfoque para mediar entre la riqueza privada y el interés público. La secularización en Europa permitió a los estados utilizar la inmensa propiedad de la iglesia para el desarrollo de las ciudades. La influencia de los nobles disminuyó. Los primeros Secretarios de Estado de economía comenzaron su trabajo. Bancarios como Amschel Mayer Rothschild (1773–1855) comenzaron a financiar proyectos nacionales como guerras e infraestructura. A partir de entonces, la economía significaba economía nacional como un tema para las actividades económicas de los ciudadanos de un estado. Revolución Industrial El primer economista en el verdadero sentido moderno de la palabra fue el escocés Adam Smith (1723–1790), quien se inspiró parcialmente en las ideas de la fisiócracia, una reacción al mercantilismo y también en el estudiante de Economía, Adam Mari. Definió los elementos de una economía nacional: los productos se ofrecen a un precio natural generado por el uso de competencia - oferta y demanda - y la división del trabajo. Mantuvo que el motivo básico para el comercio libre es el interés propio humano. La llamada hipótesis del interés propio se convirtió en la base antropológica de la economía. Thomas Malthus (1766–1834) transfirió la idea de oferta y demanda al problema de la sobrepoblación humana. La Revolución Industrial fue un período desde el siglo XVIII hasta el XIX donde los grandes cambios en agricultura, manufactura, minería y transporte tuvieron un profundo efecto en las condiciones socioeconómicas y culturales comenzando en el Reino Unido, extendiéndose posteriormente a Europa, América del Norte, y finalmente al mundo. El inicio de la Revolución Industrial marcó un importante punto de inflexión en la historia humana; casi cada aspecto de la vida cotidiana fue finalmente influenciado de alguna manera. En Europa, el capitalismo salvaje comenzó a reemplazar el sistema de mercantilismo (hoy: proteccionismo) y llevó al crecimiento económico. El período se llama Revolución Industrial porque el sistema de producción y división del trabajo permitió la producción en masa de bienes. Siglo XX El concepto contemporáneo de la economía no era ampliamente conocido hasta la Gran Depresión estadounidense en la década de 1930. Después del caos de dos Guerras Mundiales y la devastadora Gran Depresión, los responsables de políticas buscaron nuevas formas de controlar el curso de la economía. Esto fue explorado y discutido por Friedrich August von Hayek (1899–1992) y Milton Friedman (1912–2006), quienes abogaron por un libre comercio global y se les considera los padres del llamado neoliberalismo. Sin embargo, la visión predominante era la sostenida por John Maynard Keynes (1883–1946), quien abogó por un mayor control del mercado por parte del estado. La teoría de que el estado puede aliviar los problemas económicos e instigar el crecimiento económico a través de la manipulación estatal de la demanda agregada se llama keynesianismo en su honor. A fines de la década de 1950, el crecimiento económico en América y Europa,frecuentemente llamado Wirtschaftswunder (alemán para milagro económico),dio lugar a una nueva forma de economía: economía de consumo masivo. En 1958, John Kenneth Galbraith (1908–2006) fue el primero en hablar de una sociedad opulenta en su libro La Sociedad Opulenta. En la mayoría de los países, el sistema económico se llama economía social de mercado. Siglo XXI Bolsa de Valores de Frankfurt en 2015. Con la caída del Telón de Acero y la transición de los países del bloque del Este hacia gobiernos democráticos y economías de mercado, la idea de la sociedad postindustrial cobra importancia al marcar el papel que recibe el sector servicios en lugar de la industrialización. Algunos atribuyen el primer uso de este término al libro de Daniel Bell de 1973, El Advenimiento de la Sociedad Postindustrial, mientras que otros lo atribuyen al libro del filósofo social Iván Illich, Herramientas para la Convivencia. El término también se aplica en filosofía para designar el desvanecimiento del postmodernismo a fines de los 90 y especialmente al inicio del siglo XXI. Con la expansión de Internet como medio de comunicación y masa, especialmente después de 2000–2001, se da lugar a la idea de Internet y economía de la información debido a la creciente importancia del comercio electrónico y los negocios electrónicos, también se crea el término para una sociedad global de información como comprensión de un nuevo tipo de sociedad todo-conectado. A fines de la década de 2000, el nuevo tipo de economías y expansiones económicas de países como China, Brasil e India atraen la atención y el interés hacia economías y modelos económicos diferentes de los dominantes en Occidente. Factores productivos Trabajo Esta sección es un extracto de Trabajo (economía).[editar] Se ha sugerido que este artículo o sección sea fusionado con mano de obra. Para más información, véase la discusión. Una vez que hayas realizado la fusión de contenidos, pide la fusión de historiales aquí. Este aviso fue puesto el 25 de junio de 2024. Trabajador mecánico ajustando una máquina de vapor. Fotografía de Lewis Hine (1920). Alegoría del trabajo, en el monumento al marqués de Larios (Málaga, España). En teoría económica, el concepto trabajo se refiere a la actividad, normalmente física aunque también puede ser intelectual, que las personas desempeñan ya sea como deber o actividad dependiente de la profesión, necesidades y deseos de una comunidad más amplia. Alternativamente, el trabajo puede verse como la actividad humana que contribuye (junto con otros factores de producción) hacia los bienes y servicios dentro de una economía. A lo largo de la historia y coexistiendo entre ellas ha habido muchas formas de organización de la labor y de la producción como, por ejemplo, el trabajo de casa o desde un establecimiento, desde la esclavitud al pequeño taller artesano, pasando por la servidumbre y la aparcería. Pero desde el siglo XIX y la revolución industrial y sin desaparecer otras formas, el trabajo asalariado es la forma dominante. Actualmente aún coexisten el trabajo asalariado, el trabajo autónomo (profesiones liberales, comerciantes y otros), el trabajo informal o irregular (el cual sigue siendo salario pero sin control del fisco), la servidumbre, así como un nivel de desempleo (personas que buscan y no consiguen empleo). El salario o sueldo es el valor del trabajo del pago en el mercado de trabajo, determinado en un contrato de trabajo que puede realizarse en forma individual (contrato individual de trabajo) o colectiva (contrato colectivo de trabajo). El trabajo está esencialmente relacionado con la construcción y el uso de herramientas, y por lo tanto con la técnica y la tecnología, así como con el diseño de los procesos de trabajo y producción (véase: fordismo, taylorismo, toyotismo). En economía, el trabajo es en general una medida del esfuerzo real. Según la visión de la economía clásica, es uno de los tres factores de producción, junto con la tierra y el capital. Grandes economistas como Adam Smith, David Ricardo, entre otros, concedieron al trabajo un lugar central en sus teorías. Karl Marx y John Maynard Keynes desarrollaron sus teorías económicas alrededor del trabajo y el empleo. Desde la economía marxista se refiere a la fuerza de trabajo y la teoría del valor-trabajo. Capital Esta sección es un extracto de Capital (economía).[editar] En economía, se entiende por capital un componente material de la producción, básicamente constituido por maquinaria, utillaje o instalaciones, que, en combinación con otros factores, como el trabajo, materias primas y los bienes intermedios, permite crear bienes de consumo. Según Michael Parkin, el capital son las herramientas, los instrumentos, la maquinaria, los edificios y demás construcciones que se utilizan para producir bienes y servicios, pero una parte de la teoría económica sostiene que el capital también puede ser humano, cuando las personas agregan a su trabajo, educación y experiencia. En sentido contable, se concreta en los bienes y derechos (elementos patrimoniales del activo) menos las deudas y obligaciones (pasivo), de todo lo cual es titular el capitalista. Así se dice que se capitaliza o se amplía capital cuando aumenta su activo o disminuye su pasivo o se incorporan nuevas aportaciones de socios o se reduce el endeudamiento con terceros. Cuando el pasivo es superior al activo se resuelve que la unidad económica está en situación de capital negativo (negative equity, en inglés). El capital debe distinguirse analíticamente de la empresa en sí y de la gerencia, aunque en muchos casos los papeles sociales de capitalista o empresario y gerente se puedan dar simultáneamente en una misma persona, como suele suceder en las unidades productivas más pequeñas. Igualmente debe diferenciarse el interés que obtiene el prestamista, de la ganancia que obtiene el empresario o el inversor por una exitosa actividad productiva o especulativa en el mercado, de la renta que percibe el propietario de la tierra, así como del salario y el honorario que se recibe por el trabajo y el capital humano. Tierra Esta sección es un extracto de Tierra (economía).[editar] Tierra, en economía, es el concepto que abarca a todos los recursos naturales cuyo suministro está inherentemente fijado (es decir, no cambia respondiendo a las variaciones de sus precios en el mercado). En ese conjunto se incluyen las tierras propiamente dichas, definidas por su localización geográfica en la superficie terrestre (concepto que excluye las mejoras debidas a las infraestructuras y el capital natural, que puede ser degradado por las acciones humanas -factores biogeográficos, como el suelo, el clima, la hidrología, etc.-), los depósitos minerales del subsuelo, e incluso las localizaciones en órbita geoestacionaria y una parte del espectro electromagnético. En economía clásica se considera la tierra como uno de los tres factores de producción, siendo los otros el capital y el trabajo; la remuneración derivada de la propiedad o el control de la tierra (o de los recursos naturales en ella incluidos) se suele denominar renta o renta de la tierra. La tierra, particularmente los yacimientos mineros y los campos o localizaciones geográficas de especial valor para su uso agrícola (tierras de cultivo), ganadero o forestal (el sector primario que identifica principalmente al paisaje rural); ha sido históricamente la causa de todo tipo de conflictos sociales, políticos y bélicos. Agentes económicos Sector privado Esta sección es un extracto de Sector privado.[editar] Panes en una tienda de Génova. La producción de pan es una actividad típicamente realizada por el sector privado. El sector privado, que se contrapone al sector público, es aquella parte de la economía que busca el lucro en su actividad y que no está controlada por el Estado. Por contraste, las empresas que pertenecen al Estado son parte del sector público. Las organizaciones privadas sin ánimo de lucro están incluidas en el sector voluntario o tercer sector. Las formas jurídicas en que se pueden desarrollar las actividades del sector privado son muy variadas y van desde el ejercicio individual por una persona de una actividad empresarial (trabajador autónomo) hasta las grandes compañías que cotizan en bolsa y son propiedad de miles de accionistas, pasando por otras formas como la sociedad de responsabilidad limitada, la comunidad de bienes, la unión temporal de empresas (UTE), etc. En cada país la legislación recoge unas formas y les asigna unas determinadas características. Aunque 2 países compartan el mismo idioma y sea válida en ellas la misma forma jurídica, puede adoptar denominaciones oficiales diferentes. Asimismo, hay formas jurídicas que existen en unos países y no en otros. Incluso aunque la misma forma jurídica exista en 2 países distintos con la misma denominación, puede tener en ellos tratamiento y obligaciones diferentes. Sector público Esta sección es un extracto de Sector público.[editar] El Ayuntamiento del municipio de Lyngby-Taarbæk en Copenhague, Dinamarca. El sector público es el conjunto de organismos administrativos mediante los cuales el Estado cumple, o hace cumplir la política o voluntad expresada en las leyes del país. Esta clasificación incluye dentro del sector público: El poder legislativo, poder ejecutivo, poder judicial y organismos públicos autónomos, instituciones, empresas y personas que realizan alguna actividad económica en nombre del Estado y que se encuentran representadas por el mismo, es decir, que abarca todas aquellas actividades que el Estado (Administración local y central) poseen o controlan. Economía social Esta sección es un extracto de Economía social.[editar] Economía social es un concepto polisémico que puede significar: Economics La expresión Economía social puede referirse a distintos conceptos dentro del ámbito de la ciencia económica y de las prácticas socioeconómicas. Economía social (ciencia económica) (en inglés: Social economics), como campo de estudio de la ciencia económica, analiza la influencia de las preferencias, normas e interacciones sociales en los comportamientos económicos. Economía social como denominación descriptiva del funcionamiento de una actividad económica desarrollada por un agente económico institucional, caracterizada por una estructura organizacional y una gestión empresarial participativa en los órganos directivos (véase Organización de empresas). Economía social como denominación descriptiva del sector institucional conformado por organizaciones con fines no lucrativos, tales como cooperativas, mutuales y asociaciones civiles. Economía social como denominación descriptiva de las prácticas empresariales que consideran el impacto de sus operaciones sobre el entorno social y ambiental, en el marco de la responsabilidad social corporativa. Sectores de actividad Esta sección es un extracto de Sector económico.[editar] Sector económico es cada una de las ramas de la producción, clasificada según sus características relativas, se tienen en cuenta sobre todo el modo en que los bienes o servicios son producidos. La noción aparece bien temprano en el pensamiento económico, pero no se desarrolla hasta la primera mitad del siglo XX, cuando Allan Fisher, Colin Clark, y Jean Fourastié establecen la división clásica de sector primario, productor de materia prima o de bienes no elaborados, secundario, vinculado al procesamiento de los bienes obtenidos por el primer sector, esto es, la industria, y terciario, relacionado con los servicios que posibilitan el desarrollo de los anteriores de manera directa, transporte, logística o administración, e indirecta, cuidado de los trabajadores, educación y servicios legales, por ejemplo. A partir del desarrollo de las nuevas tecnologías de la información y la comunicación, así como de la importancia de los servicios vinculados al crédito, se ha introducido el llamado sector cuaternario, relacionado con las finanzas y las propiedades, pero también con el conocimiento como activo económico. Algunos proponen también un sector quinario, formado por las actividades sin pago directo, como el trabajo hogareño, o aquellas que se realizan sin ánimo de lucro; también se engloban en este sector los servicios de reciclaje. Desde otro punto de vista se considera al sector quinario de manera completamente diferente como los servicios ligados a los sectores más concentrados de la economía y la administración, por ejemplo: gerenciamiento de alto nivel. También se denomina sector económico a la clasificación de los bienes y servicios en función de la propiedad de los medios de producción. En este caso se tiene un sector privado, cuando los dueños de empresas son parte de la sociedad civil, en forma particular o compartida por medio de acciones, y que las poseen para realizar una actividad comercial, un sector público, en los cuales la propiedad está en manos del Estado,por lo general sin propósito de ganancias, y un tercer sector, conformados por aquellas empresas productoras de bienes o servicios que no pertenecen al Estado, pero tampoco son gestionadas por particulares con una finalidad comercial (esto es con ánimo de lucro). Según la hipótesis de los tres sectores, cuanto más avanzada o desarrollada es una economía, más peso tiene el sector terciario y menos el sector primario. La actividad económica se diferencia del acto económico. Sector primario Esta sección es un extracto de Sector primario.[editar] Empaquetamiento de heno en Estados Unidos. La agricultura fue uno de los primeros integrantes del sector primario. El sector primario está formado por las actividades económicas relacionadas con la recolección o extracción y la transformación de los recursos naturales con poca o ninguna manipulación. Las principales actividades del sector primario son el sector agropecuario (agricultura y ganadería), la silvicultura (explotación forestal), la apicultura, la acuicultura, la minería, la pesca, y la caza. Usualmente, los productos primarios son utilizados como materia prima en las producciones industriales. Los procesos industriales que se limitan a dar un valor añadido a los recursos naturales suelen ser considerados parte del sector primario, especialmente si dicho producto es difícil de ser transportado en condiciones normales a grandes distancias. El dominio del sector primario, tanto si se reduce al sector agrario como si se considera la totalidad de los sectores extractivos, suele ser una característica definitoria de la economía de los países subdesarrollados. No obstante, varios países desarrollados también poseen sectores primarios pujantes, a los que se añade producción de mayor valor agregado. Este sector viene del latín agri (campo) y cultura (cultivo) es una actividad dedicada a la explotación de bosques y selvas, la cría y el desarrollo de ganado Sector secundario Esta sección es un extracto de Sector secundario.[editar] Cadena de montaje de Ford en Ontario, Canadá. La industria de automoción es una de las más lujosas representantes del sector secundario. El sector secundario es el sector de la industria que transforma la materia prima, extraída o producida por el sector primario, en productos de consumo, o en bienes de equipo. Es decir: mientras que el sector primario se limita a obtener de manera directa los recursos de la naturaleza, el sector secundario ejecuta procedimientos industriales para transformar dichos recursos. Sector terciario Esta sección es un extracto de Sector servicios.[editar] Interior de una tienda de Zara. El comercio minorista es uno de los más cercanos representantes del sector terciario. El sector servicios o sector terciario es el sector económico que engloba las actividades relacionadas con los servicios no productores o transformadores de bienes materiales. Generan servicios que se ofrecen para satisfacer las necesidades de cualquier población en el mundo. Incluye subsectores como comercio, comunicaciones, centro de llamadas, finanzas, turismo, hostelería, ocio, cultura, espectáculos, la administración pública y los denominados servicios públicos, los presta el Estado o la iniciativa privada (sanidad, educación, atención a la dependencia, entre otros). Dirige, organiza y facilita la actividad productiva de los otros sectores (sector primario y sector secundario). Aunque se le considera un sector de la producción, propiamente su papel principal se encuentra en los dos pasos siguientes de la actividad económica: la distribución y el consumo. El predominio del sector terciario frente a los otros dos en las economías más desarrolladas permite hablar del proceso de terciarización. El Premio Nobel de economía, Paul Krugman, argumenta que la menor productividad del sector servicios y la dificultad para mejorar su productividad es el principal factor del estancamiento de los niveles de vida en muchos países. Sector cuaternario Esta sección es un extracto de Sector cuaternario.[editar] La investigadora LEONA Laufey Hrólfsdóttir utilizando un microscopio. Las empresas biotecnológicas son algunas de las primeras integrantes del sector cuaternario. El sector cuaternario es una parte de la economía cuyas características más mencionadas son basarse en el conocimiento y ofrecer servicios imposibles de mecanizar, tales como la generación e intercambio de información, tecnología, consultoría, educación, investigación y desarrollo, planificación financiera entre otros servicios o actividades principalmente intelectuales. El término se ha utilizado asimismo para describir a los medios de comunicación, la cultura y el gobierno: puede ser clave en el desarrollo de una mejor juventud ya que incluye también la educación. En el sector cuaternario las empresas invierten para asegurar una mayor expansión, lo que se ve como un medio de generar mayores márgenes o rentabilidad de las inversiones. La investigación se dirige hacia la reducción de costes, aprovechamiento de los mercados, la producción de ideas innovadoras, nuevos métodos de producción o métodos de fabricación más eficientes, entre otros. Para muchos ramos, como la industria farmacéutica, el sector cuaternario es el de más alto valor añadido, ya que crea productos de los que se beneficiará en un futuro la empresa. El sector cuaternario incluye actividades vinculadas al desarrollo y la investigación de nuevas tecnologías. Estas tecnologías de punta se aplican a todos los sectores de la economía y llevan la delantera en la investigación científico-tecnológica. Debido a esta naturaleza, el sector requiere de importantes inversiones en capital humano altamente capacitado. Conceptos económicos básicos Beneficio: resultado de la diferencia de gastos e ingresos derivados de una actividad económica, si los gastos son mayores se producirán pérdidas. Bien: un bien es todo aquello que satisface una necesidad y tiene un valor. Servicio: es una prestación destinada a satisfacer una necesidad personal o social pero que no consiste en la producción de un objeto. Coste de la vida e inflación: el coste de la vida es la cuantía de los gastos mínimos indispensables para obtener los bienes y servicios básicos; para calcularlo se suma el valor de un conjunto de productos y con este resultado se define el IPC (Índice de Precios de Consumo). La subida de los precios de los productos es lo que llamamos inflación. Inversión y especulación: la inversión es la cantidad de dinero que se destina a iniciar un negocio o a mantenerlo y mejorarlo con el objeto de obtener un beneficio. Cuando se obtiene un rápido beneficio de una operación comercial basado solo en el precio de los bienes se habla de especulación. Mercado: institución social conformada por los consumidores y los productores, donde los primeros demandan los bienes y servicios que ofrecen los segundos. Producción y productividad: llamamos producción a los bienes y servicios generados por una actividad económica. La relación entre lo producido y los medios empleados determina la productividad. Si se obtiene una elevada productividad utilizando pocos medios, se dice que la producción es alta. Cuando se emplean muchos medios pero la productividad es pequeña, la producción es baja. Producto interior bruto (PIB) y PIB per cápita: el PIB es el valor total de los bienes y servicios producidos en un territorio durante todo un año. El PIB refleja la riqueza o renta generada en ese territorio, pero para conocer cuál es la riqueza o renta media de su población, es decir, PIB per cápita, hay que dividir el PIB del territorio considerado entre el número de sus habitantes. Este no considera la depreciación de los equipos, solo su valor inicial o nuevo, cuando se desea conocer el valor total menos la depreciación, se llama PIN Producto interno neto. Objetivos sociales de la economía Estabilidad económica: La estabilidad de los precios tiene que ver con evitar la inflación o deflación, ya que estas pueden causar desigualdades en la economía. Pleno empleo: El pleno empleo existe cuando se utilizan completamente los recursos que son escasos de una economía (mano de obra). Crecimiento económico sostenido, sin variaciones drásticas. Equidad económica: Las acciones y políticas económicas se evalúan tomando en cuenta lo que las personas consideran correcto e incorrecto, mediante la distribución conveniente de los recursos económicos, lo que va a permitir que estos recursos fueran combinados en la mejor manera posible para suministrar los bienes y servicios necesarios. Eficiencia económica: La eficiencia económica es el estado en el que un sistema económico logra y utiliza recursos escasos de una manera más productiva, generando mayores o mejores resultados en servicios o bienes, que los previstos, sin que se incremente los costos de producción. Libertad económica: Se refiere a aspectos como la libertad de los consumidores para decidir cómo gastar su dinero o ahorrarlo y la libertad de los trabajadores para cambiar de empleo. Crecimiento económico: Tiene que ver con el aumento en la producción de bienes y servicios a través del tiempo. Se relaciona con el ritmo de aumento de su población y su productividad. El crecimiento económico se mide según los cambios en el nivel del producto interno bruto real (PIB). Seguridad económica: Protege a los consumidores, productores y propietarios de los recursos de los riesgos que hay en la sociedad. Cada sociedad decide que riesgos son los que necesitan protección y si deben pagarlos los individuos, empresarios o gobierno. Distribución satisfactoria de los ingresos: Se refiere a una distribución acorde a las diferencias en la iniciativa, el esfuerzo y la habilidad. El estado sirve de equilibrio en la redistribución de la riqueza mediante su poder de gasto. Microeconomía y macroeconomía La ciencia económica se divide en dos ramas centrales según su alcance: Microeconomía Esta sección es un extracto de Microeconomía.[editar] La microeconomía es una rama de la teoría económica que estudia el comportamiento de los agentes económicos individuales, como los consumidores, las empresas, los trabajadores y los inversores, así como su interacción en los mercados. Analiza las decisiones que cada agente toma para alcanzar determinados objetivos y cómo dichas decisiones afectan la oferta y demanda de bienes y servicios, la producción, la fijación de precios y el equilibrio del mercado. Los elementos fundamentales del análisis microeconómico incluyen los bienes, los precios, los mercados y los agentes económicos. En contraposición, la macroeconomía se ocupa del estudio global de la economía, analizando las variables agregadas como el producto total de bienes y servicios, el nivel de empleo, la balanza de pagos, el tipo de cambio y el comportamiento general de los precios. Macroeconomía Esta sección es un extracto de Macroeconomía.[editar] La macroeconomía es la parte de la teoría económica que estudia los indicadores globales de la economía mediante el análisis de las variables agregadas, como el monto total de bienes y servicios producidos, el total de los ingresos, el nivel de empleo, de recursos productivos, la balanza de pagos, el tipo de cambio y el comportamiento general de los precios. La macroeconomía se distingue así de la otra gran rama de la teoría económica, la microeconomía, que estudia el comportamiento económico de agentes individuales, como consumidores, empresas, trabajadores e inversores. Herramientas de análisis económico Los economistas para estudiar la ciencia económica, así como para aplicar sus conocimientos a la realidad, utilizan la teoría económica y diversas herramientas de análisis como las siguientes: Econometría Esta sección es un extracto de Econometría.[editar] La econometría (del griego οἰκονόμος oikonómos regla para la administración doméstica y μετρία metría, relativo a la medida) es la rama de la economía que hace un uso extensivo de modelos matemáticos y estadísticos así como de modelos y técnicas del aprendizaje automático, la programación lineal, la teoría de juegos y la teoría económica para analizar, interpretar y realizar estimaciones y pronósticos sobre sistemas económicos, tratando variables como el precio de bienes y servicios, tasas de interés, tipos de cambio, las reacciones del mercado, el coste de producción, la tendencia de los negocios, las consecuencias de la política económica, entre otras. Estadística Esta sección es un extracto de Estadística.[editar] Una distribución normal La estadística es la disciplina que estudia la variabilidad de los fenómenos y los procesos aleatorios que la generan, conforme a las leyes de la probabilidad. El término proviene del alemán Statistik ,forma femenina derivada a su vez del italiano statista (hombre de Estado),, haciendo alusión a sus orígenes como herramienta para el análisis de datos del Estado. En el ámbito de las ciencias fácticas, la estadística se emplea como herramienta metodológica para la recolección, organización, análisis, interpretación y presentación de los datos obtenidos durante la investigación. Este proceso permite extraer conclusiones a partir de muestras y formular inferencias sobre las poblaciones de origen. Teoría de juegos Esta sección es un extracto de Teoría de juegos.[editar] Este artículo o sección tiene referencias, pero necesita más para complementar su verificabilidad. Busca fuentes: «Economía» – noticias · libros · académico · imágenes Este aviso fue puesto el 26 de enero de 2017. La teoría de juegos es la rama de las matemáticas que estudia modelos matemáticos de interacciones estratégicas. Utiliza modelos para estudiar interacciones en estructuras formalizadas de incentivos (los llamados «juegos»). Constituye teóricamente a la ciencia económica, informática y biología; con importantes aplicaciones en la sociología, la politología, la psicología, la filosofía, la administración, la ingeniería y otras ciencias de la vida. Fue desarrollada inicialmente por Ernst Zermelo, Émile Borel, John von Neumann y Oskar Morgenstern; mientras su primera formulación implícita fue presentada por Antoine Augustin Cournot. La Teoría de juegos se divide en dos grandes ramas, Teoría de Juegos No Cooperativos y la Teoría de Juegos Cooperativos, otras divisiones del campo se caracterizan por la estructura del juego (Juegos de Suma Cero y Suma No Cero; Simétricos y Asimétricos; Estáticos y Dinámicos; Información Perfecta e Imperfecta; Información Completa e Incompleta; De un periodo, Finito o Infinitos; etc.). Su profundidad se manifiesta no solo en su rica base teórica, sino también en su vigoroso desarrollo empírico y experimental. Así, mientras que la modelización matemática sienta las bases, el análisis de datos reales (como se evidencia en trabajos como Game Theory for Applied Econometricians: Data analytics with R de Christopher P. Adams) y la investigación experimental (pionera en obras como Behavioral Game Theory: Experiments in Strategic Interaction de Colin F. Camerer) validan y refinan su capacidad explicativa sobre el comportamiento humano y las dinámicas estratégicas en diversos contextos. Tipos de interacción aparentemente distintos pueden presentar en realidad una estructura de incentivo similar y, por lo tanto, se puede representar mil veces conjuntamente un mismo juego. Desarrollada en sus comienzos como una herramienta para entender el comportamiento de la economía, la teoría de juegos se ha ido extendiendo a muchos otros campos, como la biología, las ciencias de la computación, la sociología, la politología, la psicología y la filosofía. Experimentó un crecimiento sustancial y se formalizó por primera vez a partir de los trabajos de John von Neumann y Oskar Morgenstern, antes y durante la Guerra Fría, debido sobre todo a su aplicación a la estrategia militar, en particular a causa del concepto de destrucción mutua garantizada. Desde los setenta, la teoría de juegos se ha aplicado a la conducta animal, incluyendo el desarrollo de las especies por la selección natural. A raíz de juegos como el dilema del prisionero, en los que el egoísmo generalizado perjudica a los jugadores, la teoría de juegos ha atraído también la atención de los investigadores en informática, usándose en inteligencia artificial y cibernética. Los conflictos entre seres racionales que recelan uno del otro, o la pugna entre competidores que interactúan y se influyen mutuamente, que piensan y que, incluso, pueden ser capaces de traicionarse uno al otro, constituyen el campo de estudio de la teoría de juegos, la cual se basa en un análisis matemático riguroso pero que, sin embargo, surge de manera natural al observar y analizar un conflicto desde un punto de vista racional. Desde el enfoque de esta teoría, un «juego» es una situación conflictiva en la que priman intereses contrapuestos de individuos o instituciones, y en ese contexto una parte, al tomar una decisión, influye sobre la decisión que tomará la otra; así, el resultado del conflicto se determina a partir de todas las decisiones tomadas por todos los actuantes. La teoría de juegos plantea que debe haber una forma racional de jugar a cualquier «juego» (o de negociar en un conflicto), especialmente en el caso de haber muchas situaciones engañosas y segundas intenciones; así, por ejemplo, la anticipación mutua de las intenciones del contrario, que sucede en juegos como el ajedrez o el póquer, da lugar a cadenas de razonamiento teóricamente infinitas, las cuales pueden también trasladarse al ámbito de resolución de conflictos reales y complejos. En síntesis, y tal como se comentó, los individuos, al interactuar en un conflicto, obtendrán resultados que de algún modo son totalmente dependientes de tal interacción. Así, desde que Von Neumann, Morgenstern y John Nash delinearon los postulados básicos de esta teoría durante las décadas del 40 y 50, varias han sido las aplicaciones que se le han otorgado a esta herramienta en el campo de las decisiones económicas, llegando incluso a modificar el modo en que los economistas interpretaban la toma de decisiones y la consecución del bienestar común. Ciencia de datos Esta sección es un extracto de Ciencia de datos.[editar] La existencia del Cometa Neowise (aquí representado como una serie de puntos rojos) fue descubierta mediante el análisis de datos (provenientes de un reconocimiento astronómico) adquiridos por un telescopio espacial, el Wide-field Infrared Survey Explorer. La Ciencia de Datos es un campo de conocimiento interdisciplinario que utiliza matemáticas, estadística, computación científica, método científico, procesos ingenieriles y algoritmos para obtener (recolectar o extraer), tratar, analizar y presentar informes a partir de datos ruidosos, estructurados y no estructurados. La ciencia de datos es multifacética y puede describirse como una ciencia, un paradigma de investigación, un método de investigación, una disciplina, un flujo de trabajo o una profesión. La ciencia de datos integra el conocimiento del dominio de la aplicación subyacente (por ejemplo, economía aplicada, investigación de mercados, finanzas, investigación de operaciones, medicina, tecnologías de la información, ciencias naturales) con la estadística, el análisis de datos, la informática, las matemáticas y sus métodos relacionados para comprender y analizar fenómenos reales con datos. Utiliza técnicas y teorías extraídas de muchos campos dentro del contexto de las matemáticas, las estadísticas, las ciencias de la computación, las ciencias de la información y el conocimiento del dominio. Sin embargo, la ciencia de datos es diferente de la informática, la estadística y la ciencia de la información. El ganador del premio Turing, Jim Gray, imaginó la ciencia de datos como un cuarto paradigma de la ciencia (empírico, teórico, computacional y ahora basado en datos) y afirmó que todo sobre la ciencia está cambiando debido al impacto de la tecnología de la información y la avalancha de datos. Un científico de datos es el profesional que mediante la escritura y aplicación de código de programación y conocimientos en estadística trabaja en la recolección de datos, la limpieza de datos, la exploración de datos, la modelación de datos, visualización de datos, la implementación de soluciones de aprendizaje automático y en la interpretación de resultados. Los científicos de datos provienen de diferentes profesiones o backgrounds: matemáticos, ingenieros, economistas, actuarios, físicos, químicos, y en algunas ocasiones de campos que pudieran parecer muy distantes como la medicina. Crítica de la economía política Esta sección es un extracto de Crítica de la economía política.[editar] La crítica de la economía política o crítica de la economía es una crítica que cuestiona el objeto mismo de la economía y, por lo tanto, rechaza los axiomas, las instituciones y las categorías sociales, las abstracciones y todo el paradigma de lo que generalmente se denomina «la economía». Los críticos de la economía tienden a estar de acuerdo en que las prácticas, los supuestos y los conceptos que son comunes dentro del campo de la economía no son científicos. Afirman, a su vez, que estos fenómenos son más bien el producto de prácticas sociales y normativas, más que el resultado de una ley autoevidente. Para los críticos de la economía política, lo que se suele llamar «la economía» no es más que un conjunto de conceptos metafísicos y prácticas sociales."
ksampletext_wikipedia_econ_dinero: str = "Dinero. Dinero es todo activo o bien generalmente aceptado como medio de pago por los agentes económicos para sus intercambios y que además cumple las funciones de ser unidad de cuenta y depósito de valor. Algunos ejemplos de dinero son: las monedas, las divisas y los billetes, las tarjetas de débito y crédito, y las transferencias electrónicas, entre otros. El dinero tal como lo conocemos hoy (billetes y monedas sin valor propio), debe estar avalado o certificado por la entidad emisora. Para su aceptación necesita de la construcción de mecanismos de legitimidad y de confianza. Actualmente son los gobiernos, a través de las leyes, quienes determinan cuál es el tipo de dinero de curso legal. Pero son otras entidades, como los bancos centrales y las casas de la moneda, los que se encargan, primero, de regular y controlar la política monetaria de una economía, y segundo, de crear las monedas y billetes según la demanda y la necesidad de tener en circulación dinero físico. Desde un punto de vista de las Ciencias Sociales entra en juego el factor social ya que la moneda al ser «un bien público», en tanto que presta servicios de tal naturaleza, debe ser regulada por las autoridades públicas (mediante los bancos centrales) en cuanto representantes del interés público, y no solamente a través de los mecanismos de mercado. Etimología El término «dinero» deriva del nombre de la moneda romana de plata, denarius. Tenía un valor diez ases. Derivaría del adjetivo distributivo latino deni (cada diez), a su vez, derivado del numeral decem (diez). Fue el nombre de diversas monedas utilizadas durante la Edad Media en casi todos los reinos de España, Francia y Bizancio. Así mismo, el dinar usado en varios países árabes actuales, comparten la misma etimología latina. Actualmente en los países de habla hispana, «dinero» conlleva el significado de «moneda corriente». Historia Artículo principal: Historia del dinero Entre las diferentes culturas del mundo se han empleado gran variedad de materiales y objetos con la función que hoy estamos habituados a ver con monedas, billetes, tarjetas de crédito, cheques o talones, etc. Cierto que puede parecer, en muchos casos, que guarda relación con el intercambio de mercancías que desde la prehistoria el ser humano lleva empleando: el trueque. Trueque Artículo principal: Trueque En el neolítico, con la aparición de la agricultura y la ganadería, apareció la primera economía de producción y se produjo un excedente; una cantidad de bienes que no necesitan ser consumidos. Esto dio lugar a la posibilidad de también alimentar a personas que no necesitaban trabajar la agricultura o la ganadería para su propia subsistencia, y que por tanto podían dedicarse a producir otros productos, como por ejemplo la cerámica o las armas, e intercambiar los mismos por el excedente producido. Ello permitió la primera forma de comerciar, el trueque, intercambiando directamente bienes y servicios por otros, basándose en el consenso de los participantes acerca de la valoración de los objetos a intercambiar. Con el tiempo, esta forma de intercambio se consideró ineficiente. Monedas de un tercio de estatero, acuñadas a principios del siglo VI a. C. Metales Con el paso del tiempo, el oro y la plata fueron los metales preciosos más ampliamente usados como dinero debido a que su valor es aceptado mundialmente, y también debido a la facilidad de transporte, a las ventajas de la conservación, etcétera. Para garantizar o certificar que un trozo de metal o moneda contenía una cierta cantidad de oro y/o plata, se comenzó la acuñación, a modo de garantía o certificación, por parte de entidades reconocidas y respetadas (reinos, gobiernos, bancos), que avalaban el peso y la calidad de los metales que contenían. De acuerdo con Heródoto, el pueblo lidio fue el primero en introducir el uso de monedas de oro y plata, y también el primero en establecer tiendas de cambio en locales permanentes. Se cree que fueron los primeros en acuñar monedas estampadas, durante el reinado de Giges, en la segunda mitad del siglo VII a. C. Otros numismáticos remontan la acuñación a Ardis II. La primera moneda fue hecha de electro (aleación de oro y plata). con un peso de 4,76 gramos, para poder pagar a las tropas de un modo regulado. El motivo del estampado era la cabeza de un león, el símbolo de la realeza. El estándar lidio eran 14,1 gramos de electro, y era la paga de un soldado por un mes de servicio; a esta medida se le llamó estatero. Pero el bronce, el cobre, el hierro, incluso el plomo, también han sido algunos de los metales frecuentemente empleados para acuñación de monedas. En 1896 William McKinley se postuló a la presidencia de los Estados Unidos sobre la base del Patrón oro. Tipos Dinero mercancía Artículo principal: Dinero mercancía A veces llamado dinero real, 1 es aquella clase de dinero cuyo valor, a diferencia del dinero representativo y del dinero fiat, proviene fundamentalmente del bien del cual se compone. El dinero mercancía consiste en bienes u objetos que tienen valor por sí mismos, además del valor de cambio al ser utilizado como moneda. Dinero representativo Artículo principal: Dinero representativo Tipo de dinero que, a diferencia del dinero mercancía, se basa en otro activo, como, por ejemplo, el dinero respaldado en oro, plata, petróleo u otra moneda, que tiene la cualidad de ser convertible al activo al cual representa, el cual puede ser una especie de dinero metálico. Dinero fíat Artículo principal: Dinero fíat También conocido como dinero por decreto, es una forma de dinero sin valor intrínseco. Su valor se basa en su declaración como dinero por el Estado. El término fíat frecuentemente se utiliza de forma intercambiable con el de dinero fiduciario, sin embargo los términos no son equivalentes y el matiz puede ser considerable. El dinero fiat es el tipo de dinero del dólar, euro, yen y principales monedas de curso internacional. Dinero fiduciario Artículo principal: Dinero fiduciario El dinero llamado fiduciario (del latín fiduciarĭus, de fiducĭa confianza y ésta a su vez de fides fe), es el que se basa en la fe o confianza de la comunidad, es decir, que no se respalda por metales preciosos ni nada que no sea una promesa de pago por parte de la entidad emisora.1 Es importante tener en cuenta que entendemos la confianza de la comunidad como el conjunto de la riqueza aparente que presenta la comunidad emisora de la moneda. Es el modelo monetario que predomina actualmente en el mundo, y es del dólar estadounidense, el euro y todas las otras monedas de reserva.2 Moneda Artículo principal: Moneda La moneda es una pieza de un material resistente, de peso y composición uniforme, normalmente de metal acuñado en forma de disco y con los distintivos elegidos por la autoridad emisora, que se emplea como medida de cambio (dinero) por su valor legal o intrínseco y como unidad de cuenta. Papel moneda Artículo principal: Papel moneda El billete de papel más antiguo conservado lo fabricó la dinastía Ming, hacia 1375 y equivalía a 1000 monedas de cobre. Fue necesaria una evolución en la cual los estados emitían billetes y monedas, que daban derecho a su portador a intercambiarlos por oro o plata de las reservas del país. Los cambios en las dinámicas económicas durante el siglo XX dieron fin a la hegemonía de los metales en el dinero, el cual tomó otros aspectos (billetes, tarjetas, entre otros), La evolución del respaldo del papel moneda es el siguiente: En los siglos XVIII y XIX, varios países tenían un patrón de dos metales, basado en oro y plata. Entre 1870 y la Primera Guerra Mundial se adoptó principalmente el Patrón oro, de forma que cualquier ciudadano podría transformar el papel moneda en una cantidad de oro equivalente. En el periodo entre guerras mundiales se trató de volver al Patrón oro, si bien la situación económica y la crisis o crac del 29 terminó con la convertibilidad de los billetes en oro para particulares. Al finalizar la Segunda Guerra Mundial, los aliados establecieron un nuevo sistema financiero en los Acuerdos de Bretton Woods, en los cuales se establecía que todas las divisas serían convertibles en dólares estadounidenses y solo el dólar estadounidense sería convertible en lingotes de oro a razón de 35 dólares por onza para los gobiernos extranjeros. En 1971, las políticas fiscales expansivas de los Estados Unidos, motivadas fundamentalmente por el gasto bélico de Vietnam, provocaron la abundancia de dólares, planteándose dudas acerca de su convertibilidad en oro. Esto hizo que los bancos centrales europeos intentasen convertir sus reservas de dólares en oro, creando una situación insostenible para los estadounidenses. Ante ello, en diciembre de 1971, el presidente de Estados Unidos, Richard Nixon, suspendió unilateralmente la convertibilidad del dólar en oro para el público y devaluó el dólar un 10 %. En 1973, el dólar se vuelve a devaluar otro 10 %, hasta que, finalmente, se termina con la convertibilidad del dólar en oro también para los gobiernos y bancos centrales extranjeros. Desde 1973 el dinero utilizado en el mundo tiene un valor que está en la creencia subjetiva de que será aceptado por los demás habitantes de un país, o zona económica, como forma de intercambio. Las autoridades monetarias y Bancos Centrales no pretenden defender ningún nivel particular de tipo de cambio, pero intervienen, individualmente o en coordinación, en los mercados de dinero y en la oferta monetaria para suavizar las fluctuaciones especulativas de corto plazo, con el objetivo de mantener a corto plazo la estabilidad de precios, y evitar situaciones como la hiperinflación, que hacen que el valor de ese dinero se destruya, al desaparecer la confianza en el mismo, o como la deflación. Dinero electrónico Artículo principal: Dinero electrónico El dinero electrónico (también conocido como e-money, efectivo electrónico, moneda electrónica, dinero digital, efectivo digital o moneda digital) se refiere a dinero que, o bien se emite de forma electrónica, a través de la utilización de una red de ordenadores, Internet y sistemas de valores digitalmente almacenados como el caso del Bitcoin, o es un medio de pago digital equivalente de una determinada moneda, como en el caso del Ecuador o Perú Las transferencia electrónica de fondos y los depósitos directos son ejemplos de dinero electrónico. Cajero automático de bitcoins, una criptomoneda concebida en 2009. Funciones El dinero es un activo financiero neutro o plenamente líquido, que sirve de plataforma intermedia para optimizar el intercambio de bienes y servicios, evitando las inexactitudes propias del trueque, es decir, del intercambio directo de bienes y servicios. Por tanto, para que un bien pueda ser calificado como dinero se deben satisfacer los siguientes tres criterios y que son las tres principales funciones que cumple el dinero en un sistema económico moderno (funciones que ya en su libro Ética a Nicómaco, Aristóteles define de forma similar unos siglos antes): Medio de intercambio: para evitar las ineficiencias de un sistema del trueque. Cuando un bien es requerido con el solo propósito de usarlo para ser intercambiado por otras cosas, posee esta propiedad. Además, el dinero debe ser un bien ligero y fácil de almacenar y de transportar. Unidad de cuenta: Cuando el valor de un bien es utilizado con frecuencia para medir y comparar el valor de otros bienes o cuando su valor es utilizado para denominar deudas, se dice que el bien posee esta propiedad. La unidad de cuenta significa que es la unidad de medida que se utiliza en una economía para fijar los precios. Conservación de valor: Cuando un bien es adquirido con el objetivo de conservar el valor comercial para futuro intercambio, entonces se dice que es utilizado como un depósito de valor. El dinero es un depósito de valor pero no el único, cualquier activo que mantenga su poder adquisitivo a lo largo de tiempo servirá como depósito de valor. Oferta de dinero Artículo principal: Oferta de dinero La oferta de dinero o masa monetaria, en macroeconomía, es la cantidad de dinero disponible en una economía para comprar bienes, servicios y títulos de ahorro, en un momento determinado. La oferta monetaria es determinada de manera conjunta por el sistema bancario privado y el banco central del país. El banco central opera a través del mercado abierto y de otros instrumentos para proveer de reservas al sistema bancario. El dinero y la liquidez de los activos Cualquiera puede crear su propio dinero. Por ejemplo, en los comercios minoristas con la emisión de los vales de compra, pero este tipo de dinero tiene un inconveniente, que solo es aceptado por la misma tienda que los emitió, por lo que su liquidez es muy limitada. El único tipo de dinero propiamente dicho que se considera legalmente como tal, es el emitido por los bancos centrales (billetes y monedas), también denominado dinero legal, y siempre ha de ser aceptado como forma de pago. Se considera que el dinero legal es el activo más líquido al cual se pueden convertir el resto de activos (bienes, servicios, deudas u obligaciones) y viceversa. El dinero de papel de diferentes países. En sentido amplio, hay que entender por dinero mucho más que el mero dinero legal. Hay diferentes tipos de activos financieros que constituyen el dinero en sentido amplio; se pueden clasificar según su grado de liquidez. El dinero en sentido estricto sería solo el plenamente líquido. Para empezar a hablar, las cantidades en cuentas bancarias corrientes o depósitos a la vista también han de considerarse como dinero convertible en dinero legal en una relación de 1:1, de forma inmediata; estos depositantes dispondrán de su dinero a través de su chequera de cuenta corriente, pero el banco depositario, en el ejercicio de su actividad comercial, se lo entregará a un tercero que podrá disponer de él con otra chequera, la de una cuenta de crédito. De este modo, las entidades de depósito y, en general, todas las entidades de crédito, multiplican el dinero legal (cfr. creación de dinero bancario, cuasidinero). Cuando se realiza un depósito en efectivo en una cuenta corriente o a la vista, el Activo (Tesorería) del banco se incrementa pero, también, aparece una anotación contable en el Pasivo para representar la deuda de esa cantidad de dinero con el depositante. Desde el punto de vista del patrimonio de este, lo que ha sucedido es un cambio de un activo plenamente líquido (dinero legal) por un derecho de crédito de máxima liquidez contra el banco depositario, ya que existe el compromiso por parte del banco de convertirlo en dinero legal a instancia del depositante. Con el dinero legal depositado por los clientes de pasivo, el banco hace su negocio (cfr. depósito irregular), prestando una parte a sus clientes de activo o invirtiéndolo, pero debe mantener reservada una parte en Caja (Encaje) para poder hacer frente a los eventuales reintegros. Lo dicho en relación con las cuentas corrientes y depósitos a la vista puede aplicarse, con las correspondientes variaciones, a los depósitos de ahorro, a plazo fijo y demás activos financieros emitidos por el sistema financiero. La clasificación completa de los activos financieros, de mayor a menor liquidez, agrupados en los llamados agregados monetarios, no es más que la forma ordenada de referirse la Oferta Monetaria. Hay que añadir que, al igual que los bancos, las empresas también pueden emitir títulos negociables de deuda que pueden ser considerado un tipo especial de cuasidinero. Este tipo de emisiones se denominan de diversas formas, por ejemplo, papel comercial (letras de cambio endosables), pero no forman parte de ninguno de los agregados monetarios, reservados al sistema financiero. Parte de la doctrina denomina dinero financiero a este conjunto de activos financieros no bancarios. Desde el punto de vista material, no cabe duda de su naturaleza dineraria; piénsese, por ejemplo, en mecanismos tales como la pignoración de títulos-valores o en que, con las acciones cotizadas de una empresa, pueden adquirirse directamente participaciones en otras empresas, como se ve muchas veces. Creación de dinero En los sistemas económicos actuales, el dinero es creado por dos procedimientos: Dinero legal es el creado por el Banco Central mediante la acuñación de monedas e impresión de billetes, es el dinero en efectivo. La cantidad de dinero legal es medida por el M1. Dinero bancario, es el creado por los bancos privados mediante la anotación en cuenta de los créditos como depósitos de los clientes prestatarios, con un respaldo parcial indicado por el coeficiente de caja. Normalmente en la actualidad el dinero bancario se crea como dinero electrónico. La cantidad de dinero bancario es medida por los agregados monetarios distintos del M1. La cantidad de dinero creada es medida mediante los agregados monetarios. La forma actual de creación y control de la cantidad de dinero es inspirada en el monetarismo. Véanse también: Oferta de dinero, Base monetaria y Multiplicador monetario. Respaldo del dinero Se considera que el valor del dinero debería estar respaldado en metales preciosos (oro, plata, etc.) o en divisas extranjeras; sin embargo, ninguno de estos métodos es seguro, considerando que su valor está sujeto a la oferta y la demanda, y no se puede garantizar que de repente no se descubran grandes reservas minerales del metal o se genere una aplicación que aumente su demanda. Lo mismo sucede con las divisas. Puesto que el dinero que no está actualmente respaldado por ningún activo tangible es denominado dinero fiduciario. La doctrina ha cuestionado ambas relaciones, tanto del respaldo oro como la ley de la oferta y demanda, en el valor del dinero.[cita requerida] De hecho, el dinero es resultado de un pacto social, donde todos aceptan entregar sus bienes o servicios a otros, a cambio de los símbolos monetarios (billetes, monedas, etc.); por lo tanto, el respaldo del dinero es la suma de los bienes y servicios de la Población; o sea, el producto interno bruto o PIB. El gobierno debe impedir que el avance del agregado monetario esté descorrelacionado con el del PIB, para sostener su valor. Sin embargo, el gobierno puede optar por imprimir más billetes, lo que derivaría en inflación y la devaluación de su moneda, como una manera de financiarse, de manera recíproca."
ksampletext_wikipedia_econ_teoriadelvalortrabajo: str = "Teoría del valor-trabajo. La teoría laboral del valor o valor-trabajo (TVT) es una teoría económica heterodoxa que considera que el valor de un bien o servicio está determinado por la cantidad de «tiempo de trabajo socialmente necesario» para su producción. La teoría del valor-trabajo es usualmente asociada a la economía marxista, aunque esta también aparece en las primeras teorías formuladas por los economistas clásicos, como Adam Smith o David Ricardo, y posteriormente retomada en la economía anarquista. Smith describió el precio de una mercancía en términos de trabajo necesario para adquirirlo, que personifica el concepto de cuanto trabajo es requerido para la obtención de un bien y como una herramienta por ejemplo, puede facilitar su adquisición. La teoría del valor-trabajo es un elemento central de la teoría marxista, que sostiene que la clase trabajadora es explotada bajo el capitalismo y disocia valor y precio. Karl Marx sin embargo, no se refiere a su propia teoría como teoría del valor-trabajo sino como teoría del valor. El resurgimiento en la interpretación de Marx conocida como Neue Marx-Lektüre también rechaza la economía marxista y la teoría del valor-trabajo, llamándola sustancialista. Esta revisión afirma que la teoría del valor-trabajo es una mala interpretación del concepto de fetichismo de la mercancía con relación al valor y que esta interpretación nunca aparece en la obra de Marx. Esta escuela enfatiza en que El Capital es explícitamente una crítica a la economía política, en lugar de una teoría más correcta. La economía ortodoxa moderna rechaza la TVT y utiliza una teoría del valor basada en preferencias subjetivas. Valor y trabajo Cuando se habla en términos de una teoría del valor-trabajo, valor, sin ningún adjetivo calificativo, debería teóricamente referirse a la cantidad de trabajo necesaria para producir una mercancía comercializable, incluido el trabajo necesario para desarrollar cualquier capital real utilizado en la producción. El precio que paga deberá ser siempre el mismo, cualquiera sea la cantidad de bienes que recibe a cambio. En realidad a veces comprará más de éstos y a veces menos; pero lo que cambia es su valor, no el del trabajo que los compra. En todo tiempo y lugar lo caro es lo que es difícil de conseguir, o lo que cuesta mucho trabajo adquirir, y lo barato es lo que se obtiene fácilmente o con muy poco trabajo. El trabajo exclusivamente, entonces, al no variar nunca en su propio valor, es el patrón auténtico y definitivo mediante el cual se puede estimar y comparar el valor de todas las mercancías en todo tiempo y lugar. Es su precio real; y el dinero es tan sólo su precio nominal. El precio real de todas las cosas, lo que cada cosa cuesta realmente a la persona que desea adquirirla, es el esfuerzo y la fatiga que su adquisición supone. Lo que cada cosa verdaderamente vale para el hombre que la ha adquirido y que pretende desprenderse de ella o cambiarla por otra cosa, es el esfuerzo y la fatiga que se puede ahorrar y que puede imponer sobre otras personas. Aquello que se compra con dinero o con bienes se compra con trabajo, tanto como lo que compramos con el esfuerzo de nuestro propio cuerpo. La riqueza de las naciones (1776) Adam Smith Tanto David Ricardo como Karl Marx intentaron cuantificar y encarnar todos los componentes del trabajo para desarrollar una teoría del precio real o precio natural de una mercancía. Trabajo como valor La TVT defiende el carácter especial del trabajo como valor debido a su omnipresencia en los productos del mercado, es un insumo directo e indirecto para cada mercancía, por lo que es un factor necesario para la producción. En consecuencia, Adam Smith argumentó que la riqueza de una nación, en consecuencia, depende de la administración del trabajo social y útil en ella. Como explica Dave Zachariah: El trabajo social es especial en este sentido. [...] El trabajo es un recurso universal pero escaso. Durante un período de tiempo determinado hay una cantidad limitada de horas de trabajo disponibles para la producción, lo que restringe el patrón de consumo factible. Esto impone una necesidad práctica de asignarlo en varias ramas de producción para satisfacer las demandas sociales cambiantes. Por lo tanto, determinar la demanda en términos de trabajo se vuelve funcionalmente útil para organizar y asignar el trabajo con cierto grado de eficiencia. El trabajo a su vez asume la forma de mercancía, ya que se compra y se vende, pero no se produce como mercancía. En la economía marxista el trabajo humano posee además la capacidad de crear un excedente, un plusvalor, del cual surge el beneficio económico. Por otro lado, el valor de la maquinaria solo transfiere el valor del trabajo previo ya contenido en ella. Aunque el avance tecnológico ha permitido el remplazo del trabajo humano mediante la automatización todavía se requiere de la intervención humana para su funcionamiento y eficiencia. En este sentido, Ian Wright describe que los seres humanos tienen poderes causales universales mientras que las máquinas tienen poderes causales particulares. Marx hipotetizó que en una sociedad completamente mecanizada sería el «intelecto general» sería la nueva fuerza productiva. Wright también llegó a hipotetizar que una máquina podría ser capaz de crear plusvalor si superase el test de Turing. Valor de uso y valor de cambio El valor en uso es la utilidad del bien en cuestión. A menudo surge una paradoja clásica al considerar este tipo de valor. En palabras de Adam Smith: Hay que destacar que la palabra VALOR tiene dos significados distintos. A veces expresa la utilidad de algún objeto en particular, y a veces el poder de compra de otros bienes que confiere la propiedad de dicho objeto. Se puede llamar a lo primero «valor de uso» y a lo segundo «valor de cambio». Las cosas que tienen un gran valor de uso con frecuencia poseen poco o ningún valor de cambio. No hay nada más útil que el agua, pero con ella casi no se puede comprar nada; casi nada se obtendrá a cambio de agua. Un diamante, por el contrario, apenas tiene valor de uso, pero a cambio de él se puede conseguir generalmente una gran cantidad de otros bienes. La riqueza de las naciones (1776) Adam Smith El valor en cambio es la proporción relativa con la que esta mercancía se intercambia por otra (es decir, su precio en el caso del dinero). Es relativo al trabajo como lo explica Adam Smith: [E]l valor de cualquier mercancía, para la persona que la posee y que no pretende usarla o consumirla sino intercambiarla por otras, es igual a la cantidad de trabajo que le permite a la persona comprar u ordenar. El trabajo es, así, la medida real del valor de cambio de todas las mercancías. La riqueza de las naciones (1776) Adam Smith El valor (sin evaluación) es el trabajo incorporado en una mercancía bajo una estructura de producción dada. Marx definió el valor de la mercancía con esta tercera definición. En sus términos, el valor es el trabajo abstracto socialmente necesario encarnado en una mercancía. Para David Ricardo y otros economistas clásicos, esta definición sirve como una medida de costo real, valor absoluto o una medida de valor invariable ante cambios en la distribución y la tecnología. Ricardo, otros economistas clásicos y Marx comenzaron sus exposiciones con el supuesto de que el valor de cambio era igual o proporcional a este valor del trabajo. Pensaron que esta era una buena suposición a partir de la cual explorar la dinámica del desarrollo en las sociedades capitalistas. Otros partidarios de la teoría del valor-trabajo utilizaron la palabra valor en el segundo sentido para representar el valor de cambio. Distinciones del trabajo económicamente pertinente Trabajo concreto y abstracto Artículo principal: Trabajo abstracto y trabajo concreto El trabajo concreto y abstracto son dos categorías utilizadas centrales en El Capital de Karl Marx, con la que se pretende señalar que así como en la mercancía están contenidos dos tipos o formas del valor, el valor de cambio y el valor de uso, el trabajo que produce una mercancía también contiene dos facetas diferentes, dos caras de la misma moneda. Este es el eje en torno al cual gira la comprensión de la economía política. Todo trabajo es, por un lado, gasto de fuerza humana de trabajo en un sentido fisiológico, y es en esta condición de trabajo humano igual, o de trabajo abstractamente humano, como constituye el valor de la mercancía. Todo trabajo, por otra parte, es gasto de fuerza humana de trabajo en una forma particular y orientada a un fin, y en esta condición de trabajo útil concreto produce valores de uso. K. Marx El capital, tomo I, Sección I, Cap. I, La Mercancía. El trabajo, como trabajo concreto o trabajo útil, es creador de valores de uso; por otro lado, como trabajo abstracto, creador de valores de cambio. Por trabajo abstracto se refiere a una característica del trabajo productor de mercancías que es compartida por todos los diferentes tipos de trabajos heterogéneos (concretos). Es decir, el concepto se abstrae de las características particulares de todo el trabajo, volviéndose una masa de trabajo cristalizada o mera gelatina de trabajo humano indiferenciado. Este permite comparar e intercambiar distintos productos de distintos trabajos. De la misma forma que el valor de uso no necesita de la existencia del valor de cambio, pero el valor de cambio sí necesita de la existencia de un valor de uso (un producto sólo puede realizarse como mercancía si es útil), el trabajo concreto puede existir sin el trabajo abstracto, pero no a la inversa. El trabajo abstracto no existe sino en contraposición al trabajo concreto o trabajo útil, como su manifestación en el valor de cambio. El trabajo abstracto no existe sin su contrapuesto. Trabajo directo e indirecto Artículo principal: Trabajo vivo y trabajo muerto La cantidad de trabajo requerida de la mano de obra para una mercancía se denomina trabajo directo. Sin embargo, los bienes de capital necesarios en una mercancía que han sido a su vez producidos previamente por el trabajo y otros bienes de capital se denomina trabajo indirecto. Juntando los insumos de trabajo directos e indirectos, se obtiene finalmente el insumo total de trabajo en la mercancía, que también puede llamarse el trabajo total incorporado en ella, o su contenido de trabajo directo e indirecto. Marx se refería como trabajo vivo al trabajo directo y trabajo muerto al trabajo indirecto. Trabajo simple y complejo Esta sección es un extracto de Trabajo simple y trabajo complejo.[editar] La profesión del arquitecto se suele usar como ejemplo de trabajo complejo frente al trabajo simple del obrero. El trabajo simple y trabajo complejo son dos conceptos que se utilizaron en la economía política clásica principalmente en los siglo XVIII y XIX, que fueron desarrollados posteriormente por Karl Marx en su crítica de la economía política. Ambos términos se refieren al valor del trabajo humano, pero con una distinción clave en cuanto a la cualificación y productividad del trabajador. Estos términos sobreviven hoy en día hasta cierto punto en las discusiones modernas de sociología económica, análisis económico marxista y en el debate sobre el cálculo económico en el socialismo. Si una clase de trabajo requiere un extraordinario grado de destreza e ingenio, el aprecio que los hombres tengan por tales talentos naturalmente dará valor a su producción, un valor superior al que se derivaría sólo del tiempo empleado en la misma. Esos talentos casi nunca pueden ser adquiridos sin una larga dedicación, y el mayor valor de su producción con frecuencia no es más que una compensación razonable por el tiempo y trabajo invertidos en conseguirlos. La riqueza de las naciones. Adam Smith Se considera que el trabajo más complejo es igual sólo a trabajo simple potenciado o más bien multiplicado, de suerte que una pequeña cantidad de trabajo complejo equivale a una cantidad mayor de trabajo simple. La experiencia muestra que constantemente se opera esa reducción El capital, tomo I, Sección primera, Capítulo 1 (1867), Karl Marx Las diferencias entre mano de obra calificada y no calificada se denomina como trabajo simple y complejo o compuesto. Marx sostuvo que los diversos trabajos concretos en el mercado puede reducirse a un trabajo abstracto. Este último puede ser simple, es decir no requiere ninguna destreza o conocimiento especial; o complejo, que requiere un aprendizaje o práctica especial. Luego, una cantidad de trabajo complejo equivale a una cantidad de trabajo simple mayor. Para Marx la relación entre el trabajo simple y el complejo depende en gran medida de la costumbre (tradiciones) que, a su vez, se forma mediante un proceso que se desarrolla a espaldas de los productores. Los críticos de la teoría del valor-trabajo han señalado la ausencia de un procedimiento clara para poder igualar todos los diversos tipos de trabajos a una sustancia o magnitud de trabajo homogéneo que permitir medirlo. Esto se le denomina como problema de la homogenización. El propio Marx señaló que ya en el siglo XVIII los economistas ingleses habían llegado a la concepción de trabajo simple (unskilled labour), “el trabajo que puede efectuar cualquier individuo medio de una sociedad dada”, considerado como gasto de “músculo, nervio, cerebro humano”. La mano de obra calificada cuesta más producir que la mano de obra no calificada, y puede ser más productiva. Smith y Ricardo pensaron que los ingresos relativos como medida razonablemente buena para reducir todas las variedades de trabajo especializado a trabajo no especializado. Por ello usaron en el mercado como índice de cualificación, pero consideraron que la relación entre los diferentes niveles de trabajo especializado era una propiedad material y presocial del trabajo como factor natural de producción. Marx sostuvo que esta igualación de diferentes tipos de trabajos es inconsciente. Asumió que, independientemente del precio al que se venda, la fuerza de trabajo calificada tenía un valor más alto (cuesta más producirla, en dinero, tiempo, energía y recursos), y que el trabajo calificado podía producir un producto con un valor más alto en la misma cantidad de tiempo, en comparación con el trabajo no calificado. Esto se reflejó en una jerarquía de habilidades y una jerarquía de niveles salariales. En este sentido, Friedrich Engels comenta en Anti-Dühring: El producto de la hora de trabajo compuesto es una mercancía de valor superior, doble o triple, comparado con el producto de la hora de trabajo simple. Mediante esa comparación, el valor de los productos del trabajo compuesto se expresa en determinadas cantidades de trabajo simple; pero esta reducción del trabajo compuesto tiene lugar por un proceso social que se realiza a espaldas de los productores, por un mecanismo que en este punto, en el desarrollo de la teoría del valor, no se puede sino comprobar, no explicar. [...] ¿Cómo se resuelve esta importante cuestión del salario más alto del trabajo compuesto? En la sociedad de productores privados, los particulares o las familias cargan con los costes de formación del trabajador calificado; por eso corresponde a los particulares el precio, más alto, de la fuerza de trabajo calificada: el esclavo hábil se vende más caro, y el obrero hábil cobra salario más alto. F. Engels (1878), Anti-Dühring, Sección segunda, ECONOMIA POLITICA. V. Teoría del valor Trabajo productivo e improductivo Esta sección es un extracto de Trabajo productivo e improductivo.[editar] Este artículo o sección necesita referencias que aparezcan en una publicación acreditada. Busca fuentes: «Teoría del valor-trabajo» – noticias · libros · académico · imágenes Este aviso fue puesto el 2 de agosto de 2025. El trabajo productivo se refiere a aquel trabajo que tiene la capacidad de modificar el valor de uso de las mercancías o de proporcionar servicios, generando la riqueza material y no material que sostiene a la sociedad. En un sistema socioeconómico capitalista, además, los capitalistas pueden apropiarse de otra parte de esa riqueza en forma de plusvalor. En contraposición, el trabajo improductivo es aquel orientado al mantenimiento del orden social basado en clases sociales y que no genera riqueza por sí mismo (como el gobierno, las fuerzas armadas, el mantenimiento de la propiedad privada o las operaciones financieras entre otras). Tanto el trabajo productivo como el improductivo pueden ser asalariados, siendo la riqueza generada por el productivo la responsable del mantenimiento tanto de los trabajadores productivos como de los improductivos, así como de la clase capitalista. Se trata de conceptos centrales en economía política clásica cuya concreción ha ido variando a lo largo del tiempo. Fueron planteados primeramente por la escuela fisiocrática, para, más tarde, tener importantes aportes de Adam Smith y ser finalmente precisados por la crítica de Karl Marx y el posterior desarrollo de la teoría marxista. Relación entre valor y precio La oferta y la demanda determina el precio de una mercancía y éste es igual a su valor si ambos factores se equilibran (precio natural). Un problema al que se enfrenta el TvT es la relación entre las cantidades de valor, por un lado, y los precios, por otro. Si el valor de una mercancía no es el mismo que su precio y, por lo tanto, las magnitudes de cada una probablemente difieran, ¿Cuál es la relación entre las dos, si es que existe alguna? En el capítulo V de su libro La riqueza de las naciones, Adam Smith escribe: En todo tiempo y lugar lo caro es lo que es difícil de conseguir, o lo que cuesta mucho trabajo adquirir, y lo barato es lo que se obtiene fácilmente o con muy poco trabajo. El trabajo exclusivamente, entonces, al no variar nunca en su propio valor, es el patrón auténtico y definitivo mediante el cual se puede estimar y comparar el valor de todas las mercancías en todo tiempo y lugar. Es su precio real; y el dinero es tan sólo su precio nominal. La riqueza de las naciones (1776) Adam Smith Varias escuelas de pensamiento de TvT brindan diferentes respuestas a esta pregunta. Sin embargo, la mayoría de los economistas dirían que los casos en los que el precio se da aproximadamente igual al valor del trabajo incorporado, son de hecho solo casos especiales. Escribe Smith en el capítulo VI: Debe destacarse que el valor real de todos los varios componentes del precio viene medido por la cantidad de trabajo que cada uno de ellos puede comprar u ordenar. El trabajo mide el valor no sólo de aquella parte del precio que se resuelve en trabajo sino de la que se resuelve en renta y la que se resuelve en beneficio. La riqueza de las naciones (1776) Adam Smith La formulación estándar es que los precios normalmente incluyen un nivel de ingresos para capital y tierra. Estos ingresos se conocen como ganancia y alquiler, respectivamente. Sin embargo, Marx señaló que no se puede asignar valor al trabajo como una mercancía, porque el capital es una constante, mientras que la ganancia es una variable, no un ingreso; explicando así la importancia del beneficio en relación con las variables de precios. En general, los precios suelen fluctuar. La TvT no niega el papel de la oferta y la demanda que influyen en el precio, ya que el precio de una mercancía es algo diferente a su valor. Smith define precio de mercado como el precio efectivo al que se vende habitualmente una mercancía. Este precio está determinado por la proporción entre la cantidad que de hecho se trae al mercado y la demanda de los que están dispuestos a pagar. Por otro lado Smith distingue el precio natural, que es el precio correspondiente al valor real. El precio de mercado puede estar por encima o por debajo, o ser exactamente igual al precio natural. Por ejemplo, si las personas deciden que prefieren comprar más abrigos y menos zapatos, el precio de mercado de los abrigos subirá por encima de su valor y el precio de los zapatos caerá por debajo del suyo. Solo cuando hay equilibrio económico el precio de mercado es igual al precio natural. Otras industrias, como los monopolios naturales, divergen los valores del precio de sus productos debido a los efectos de la renta diferencial y renta absoluta. Si interpretamos el precio de mercado de un bien o servicio como una señal estocástica, el LTV sugiere que su valor laboral es un componente determinista subyacente al ruido. Esto implica un mecanismo de control en el sentido de que las desviaciones entre los precios de mercado y los precios proporcionales al valor laboral, causadas por discrepancias entre la oferta y la demanda, conducirán a ajustes contrarrestantes en la producción. La TvT busca explicar el nivel de este equilibrio. Esto podría explicarse por un argumento de costos de producción, que señala que todos los costos son en última instancia costos laborales, pero esto no tiene en cuenta las ganancias y es vulnerable al cargo de tautología en el sentido de que explica los precios por precios. Smith sostuvo que los valores laborales son la medida natural de intercambio para productores directos como cazadores y pescadores. Marx, por otro lado, usa una analogía de medición, argumentando que para que las mercancías sean comparables deben tener un elemento o sustancia común por la cual medirlas, y que el trabajo es una sustancia común de lo que Marx eventualmente llama valor de la mercancía. Para Marx, el precio supone el valor y es en la circulación donde la forma valor aparece transformada en la forma precio. Según Engels, en el dinero está ya incluido en germen en el concepto de valor, y no es más que el valor desplegado. Así, Marx conectó la teoría del valor trabajo con la teoría monetaria. La teoría marxista es por una parte un sistema de cálculo de valores y por otra parte un sistema de cálculo de precios, más concretamente, de precios de producción. Tanto Smith como Ricardo consideraron el precio de costo, o precio de producción, como idéntico al valor y, por lo tanto, al precio natural. Marx consideró errónea tal igualación, ya que el valor es el capital invertido más un plusvalor mientras que el precio de producción es el capital invertido más una ganancia promedia de todas las industrias que reparten su plusvalor. Entonces, l precios naturales serían costos laborales transformados, o distorsionados, debido a las relaciones de producción capitalistas. Marx argumentó que en dicha transformación la suma total de los precios se iguala al valor agregado de las mercancías. Así, estos dos son distintos pero es en última instancia la ley del valor la que se impone como centro de gravedad donde oscilan los precios de producción. La demostración de la relación entre los valores unitarios de las mercancías y sus respectivos precios se conoce en la terminología marxista como el problema de la transformación o la transformación de los valores en precios de producción. El problema de la transformación probablemente ha generado la mayor parte del debate sobre el TvT. El problema de la transformación es encontrar un algoritmo en el que la magnitud del valor agregado por el trabajo, en proporción a su duración e intensidad, se contabilice suficientemente después de que este valor se distribuya a través de precios que reflejen una tasa igual de rendimiento del capital adelantado. Si hay una magnitud adicional de valor o una pérdida de valor después de la transformación, entonces la relación entre valores (proporcionales al trabajo) y precios (proporcionales al capital total adelantado) es incompleta. Se han ofrecido varias soluciones y teoremas de imposibilidad para la transformación, pero el debate no ha llegado a ninguna resolución clara. Historia Antecedentes Aristóteles. Aunque suele atribuírsele a Karl Marx o a Ricardo, la TvT no tiene un único creador, sino que muchos pensadores diferentes han llegado a conclusiones similares de forma independiente. Se ha sostenido que Aristóteles compartía parte de esta visión afirmando que el valor de cada bien surge debido a la necesidad de un único estándar universal de medición. De igual manera, diferenció el precio del valor y distinguió entre valor de uso y valor de cambio, ambos conceptos presentes en la teoría del valor-trabajo. Puede representarse esta reciprocidad proporcional de servicios por una figura cuadrada, en la que se combinen los términos opuestos en el sentido de la diagonal. Sea, por ejemplo, el arquitecto A, el zapatero B, la casa C, el calzado D. El arquitecto recibirá del zapatero la obra que es propia del zapatero; y en cambio, le dará la obra que él mismo hace. Si hay desde luego entre los servicios cambiados una igualdad proporcional, y en seguida hay reciprocidad de buenos servicios, las cosas pasarán como ya lo he dicho. De otra manera, no hay ni igualdad ni estabilidad en las relaciones, porque puede suceder que la obra del uno valga más que la del otro, y es necesario igualarlas. Esta regla tiene aplicaciones en todas las demás artes [...] Sea una casa, A; diez minas, B; una cama, C. Sea A la mitad de B, es decir, que la casa valga cinco minas o sea igual a cinco minas. Supongamos también que la cama C sólo valga la décima de B. Con estos datos se ve fácilmente cuántas camas se necesitan para igualar el valor de la casa, es decir, que se necesitan cinco. Se comprende, que de esta manera habrán tenido naturalmente lugar los cambios, antes que existiese la moneda; porque importa poco que las cinco camas se cambiaran por la casa, o por cualquier otro objeto que tuviese el valor de cinco camas. Ética a Nicómaco, libro V, capítulo V Aristóteles diferenció entre la forma natural de las artes adquisitivas (oikonomiké), que se centra en los valores de uso, y la forma antinatural, que sirve al fin del enriquecimiento ilimitado (chrematistiké). Tenía una opinión desfavorable del comercio minorista, ya que creía que el uso del dinero para obtener ganancias a través del interés era antinatural, al obtener una ganancia del dinero en sí y no de su uso. El filósofo por una parte sostenía la necesidad de un único estándar universal de medición para un intercambio recíproco, pero al mismo tiempo lo rechazaba porque no puede suceder que cosas tan diferentes sean conmensurables entre sí. La asociación de la teoría del valor con el aristotelismo ha sido discutida, afirmándose que era más bien una teoría subjetiva o premarginal. Karl Marx criticó en los Grundrisse y Una contribución a la crítica de la economía política la visión individualista del Estado de naturaleza y la sociedad civil en la economía clásica, enfatizando en su lugar la opinión de Aristóteles en la que el hombre es por naturaleza un ser social (zoon politikón). Escribió en El Capital: “la brillantez del genio de Aristóteles se demuestra solo con esto, que descubrió, en la expresión del valor de las mercancías, una relación de igualdad”, pero carecía del concepto de valor del trabajo humano como un algo igual, en la medida en que esto representa en ambos porque la sociedad griega se fundaba en el trabajo esclavo y por consiguiente su base natural era la desigualdad de los hombres y de sus fuerzas de trabajo. John Ball. La idea de que toda la riqueza es creada por el trabajo humano se remonta a la Edad Media en los sermones del lolardo John Ball: Mi buena gente, las cosas no pueden ir bien en Inglaterra, ni lo harán hasta que todos los bienes se mantengan en común, y hasta que no haya siervos ni caballeros, y todos seamos iguales. ¿Por qué razón ellos, a quienes llamamos señores, han sacado lo mejor de nosotros? ¿Cómo se lo merecían? ¿Por qué nos mantienen en esclavitud? Si todos descendemos de un padre y una madre, Adán y Eva, ¿cómo pueden afirmar o probar que son más amos que nosotros? ¡Excepto quizás que nos hacen trabajar y producir para que ellos gasten! Están vestidos con terciopelos y con abrigos adornados con armiño y pieles, mientras que nosotros usamos lino tosco. Tienen vino, especias y buen pan, mientras que nosotros obtenemos pan de centeno, despojos, paja y agua. Tienen residencias, hermosas mansiones, y nosotros la molestia y el trabajo, y debemos desafiar la lluvia y el viento en los campos. Y es de nosotros y de nuestro trabajo de donde obtienen los medios para mantener su pompa; sin embargo, se nos llama siervos y se nos golpea rápidamente si no cumplimos con sus órdenes. R. H. Tawney remonta su origen hasta la tradición escolástica de Tomás de Aquino como responsable de asociar el precio justo con el coste de producción. En la Suma teológica expresa Tomás la opinión de que el valor puede, aumenta y debe aumentar en relación con la cantidad de trabajo que se ha gastado en la mejora de las mercancías. En la Edad Media, los escolásticos siguieron a Aristóteles al condenar la usura y, en parte, el comercio como medios antinaturales de enriquecimiento. Pero con el desarrollo del capitalismo todas las formas de enriquecimiento empezaron a parecer naturales, permisibles por ley natural. (Ver: Homo œconomicus) Ibn Jaldún. Durante la Edad Media islámica, Ibn Jaldún sentó las bases del pensamiento económico en su Muqaddimah (1377) como un precursor importante de la economía clásica y teoría económica moderna. Ibn Jaldún describió el trabajo como la fuente de valor, necesaria para todas las ganancias y la acumulación de capital. Argumentó que incluso si la ganancia resulta de algo que no sea un oficio, el valor de la ganancia resultante y del capital adquirido debe (también) incluir el valor del trabajo por el cual se obtuvo. Sin mano de obra, no se habría adquirido. También analizó la división del trabajo a nivel industrial concentrándose en el desarrollo de la artesanía como una forma de mejorar la producción y destacó que el trabajo es la fuente del valor y la medida real del valor de cambio de todas las mercancías. Pero lo que se obtiene mediante la cooperación de un grupo de seres humanos satisface las necesidades de un número muchas veces mayor que ellos. Por ejemplo, nadie, por sí mismo, puede obtener la parte del trigo que necesita para comer, sino cuando seis o diez personas entre ellas un herrero y un carpintero para hacer las herramientas, y otras que están a cargo de los bueyes, el arado de la tierra, la cosecha del grano maduro y todas las demás actividades agrícolas, se encargan de obtener su alimento... esa cantidad será alimento para un número de personas mucho mayor que el suyo propio. El trabajo combinado produce más que las necesidades y necesidades de los trabajadores. Entrada la Edad Moderna el filósofo inglés Thomas Hobbes escribió en su obra Leviatán que el trabajo y los dones de la naturaleza son la única fuente de toda riqueza. Dios (naturaleza) o da gratuitamente, o por trabajo vende a la humanidad. Durante la Revolución inglesa, Gerrard Winstanley declaró que “los hombres ricos reciben todo lo que tienen de la mano del trabajador, y lo que dan, lo dan del trabajo de otros hombres, no del suyo propio; por lo tanto, no son actores justos en la tierra”. Bertrant Russell sostuvo la presencia de la TVT en el pensamiento económico de John Locke, quien formuló anteriormente una teoría de la propiedad basada en el trabajo en su Segundo tratado sobre el gobierno civil (1689) donde: cada Hombre tiene una Propiedad en su propia Persona. Nadie tiene derecho a esto sino a sí mismo. El trabajo de su cuerpo y el trabajo de sus manos, podemos decir, son propiamente suyos (ver: Teoría de la propiedad-trabajo). John Thelwall proclamaba que: “La propiedad no es otra cosa que el trabajo humano”. David Hume en sus Discursos políticos (1752) también afirmó que: Todo en el mundo se compra con el trabajo. Vernon Louis Parrington señaló a William Petty como uno de los primeros expositores de la teoría del valor trabajo como se analiza en el Tratado de impuestos (1692). En su Anatomía política de Irlanda (1667) afirmó que “la comida diaria de un hombre adulto, en un nivel medio, y no el trabajo diario, es la medida común del valor”. Petty continuó el debate iniciado por Aristóteles sobre el valor y optó por desarrollar una teoría del valor basada en insumos: todas las cosas deben ser valoradas por dos denominaciones naturales, que es la tierra y el trabajo. Ambos serían fuentes principales de ingresos imponibles. Sus escritos tuvieron una gran influencia en economistas posteriores como el Richard Cantillon, Adam Smith, Jean-Baptiste Say e incluso Marx y Keynes. Los fisiócratas veían como única fuente del valor la tierra, donde sale el excedente agrícola. Por ello, según François Quesnay la clase productiva son los trabajadores agrícolas, mientras que los terratenientes son la classe propriétaire. Richard Cantillon trató de idear alguna ecuación o par entre la madre y el padre de la producción, la tierra y el trabajo, y expresar el valor en consecuencia. Cantillon escribió en su Ensayo sobre la naturaleza del comercio en general (1755) que La Tierra es la Fuente o Materia de donde se produce toda la Riqueza. El Trabajo del hombre es la Forma que lo produce: y la Riqueza misma no es nada Mantenimiento, Conveniencia y Superfluidad de la Vida. Marx llamó a los fisiócratas “los verdaderos padres de la economía política moderna” al posicionar el origen de la ganancia en la producción en lugar de la circulación o intercambio. Benjamín Franklin declaró en Una modesta investigación sobre la naturaleza y la necesidad del papel moneda (1729-1731) como necesario buscar otra medida de valor que los metales preciosos, y que esta medida es trabajo. Por el trabajo puede medirse el valor de la plata, así como otras cosas. Supongamos que un hombre está empleado para cultivar maíz, mientras que otro está excavando y refinando plata; al final del año, o en cualquier otro período de tiempo, el producto de maíz y el de plata son el precio natural el uno del otro; y si uno son veinte fanegas y el otro veinte onzas, entonces una onza de esa plata vale el trabajo de levantar una fanega de ese maíz. Ahora bien, si por el descubrimiento de algunas minas más cercanas, más fáciles o abundantes, un hombre puede obtener cuarenta onzas de plata tan fácilmente como antes obtenía veinte, y todavía se requiere el mismo trabajo para cultivar veinte fanegas de maíz, entonces dos onzas de plata serán no vale más que el mismo trabajo de cultivar una fanega de maíz, y esa fanega de maíz será tan barata a dos onzas como antes a una, caeteris paribus [en igualdad de condiciones]. Así, las riquezas de un país deben valorarse por la cantidad de trabajo que sus habitantes pueden comprar”. Críticos de la época como John Cazenove escribió en 1812: “Que el trabajo es la única fuente de riqueza parece ser una doctrina tan peligrosa como falsa, ya que desgraciadamente ofrece un asidero a quienes representarían toda la propiedad como perteneciente a las clases trabajadoras, y la parte que reciben los demás como un robo o fraude a las mismas”. Economía clásica Artículo principal: Economía clásica Adam Smith. En 1776, Adam Smith entendía en La riqueza de las naciones que el trabajo era la calidad de medida exacta para cuantificar el valor del bien producido. Para él, el valor era la cantidad de trabajo que uno podía recibir a cambio de su mercancía. Todo trato es: dame esto que deseo y obtendrás esto otro que deseas tú; y de esta manera conseguimos mutuamente la mayor parte de los bienes que necesitamos. No es la benevolencia del carnicero, el cervecero, o el panadero lo que nos procura nuestra cena, sino el cuidado que ponen ellos en su propio beneficio. Dada una movilidad laboral perfecta, el intercambio de las mercancías estaría determinada por la dificultad relativa de producirlas. Los bienes económicos podían aumentar de valor en el mercado, pero lo que siempre permanece invariable es el trabajo, o sea el desgaste de energías física e intelectual del trabajador para producirlos, siendo entonces el trabajo el patrón definitivo e invariable del valor. Se trata de la teoría del valor comandado o adquirido. Todo bien producido necesariamente contiene trabajo, este trabajo es la fuerza de los hombres que han interactuado en el proceso de producción de dicho bien, o sea que en todo bien se vende la fuerza de trabajo (de cada hombre que interactuó en el proceso de producción). Esto no significa que a nivel individual un bien valga más por tener más trabajo en él. Aunque no era el factor determinante de los precios, estos oscilaban hacia su precio de producción gracias al juego de la oferta y la demanda. Por ejemplo: El ganado procedente de los páramos incultos se vende en el mercado, en proporción a su peso o calidad, al mismo precio que el criado en las tierras más mejoradas. Smith se refiere a la cantidad de trabajo “como una noción abstracta, que aun siendo bastante inteligible, no es tan natural y obvia”. La teoría del valor trabajo, tal como la presentó Adam Smith, no requería la cuantificación del trabajo pasado, ni se ocupaba del trabajo necesario para crear las herramientas (capital) que podrían usarse para producir una mercancía. Para Smith, el valor solo corresponde a su trabajo o precio natural en el estado rudo y primitivo de la sociedad previo a la división del trabajo, acumulación de capital y apropiación de la tierra. Este valor está sujeto a la oferta y la demanda en la sociedad moderna por un momento determinado. La teoría del valor de Smith era muy similar a las teorías de la utilidad posteriores en que Smith proclamó que una mercancía valía cualquier trabajo que exigiría en otros (valor en el comercio) o cualquier trabajo que útil a uno mismo (valor en uso), o ambas cosas. De hecho, la primera obra de Smith, Teoría de los sentimientos morales, elabora el papel de las estimaciones individuales en la determinación de los valores estéticos y morales. La teoría del precio de Smith no tiene ninguna relación con el trabajo pasado gastado en producir una mercancía. Habla solamente del trabajo que puede ser dado o rescatado en el presente. Si un látigo de hule no sirve para nada, entonces el artículo no tiene valor económico en el comercio o en el uso, independientemente de todo el trabajo invertido en su creación. Sin embargo, Smith no logró explicar con la teoría del valor-trabajo, los conceptos de beneficio, renta, interés y salarios. Además, la venta de la fuerza de trabajo no tiene un comprador común (no se tiene en cuenta la competencia) por lo que se torna insostenible dicha teoría. Esto lo lleva a desarrollar una segunda: Teoría de los costes de producción, donde el valor de cambio de un bien depende del gasto invertido en el mismo (es decir, la suma de beneficios, rentas y salarios). Piero Sraffa más tarde llamó a esto la teoría de la suma de Smith. Al mismo tiempo, suponiendo una movilidad laboral perfecta, Smith tenía una noción clara de un precio de equilibrio a largo plazo como resultado de un proceso de arbitraje competitivo que produce una tasa de ganancia igualada. Así los precios no oscilan ahora según los valores de los bienes, sino según sus precios naturales. “Cuando el precio de cualquier producto no es ni más ni menos que lo que es suficiente para pagar la renta de la tierra, los salarios de la mano de obra, y los beneficios del capital empleado en su preparación, elaboración y transporte al mercado, de acuerdo a sus tasas naturales, el producto se vende por lo que puede llamarse su precio natural”. Jeffrey Young sugiere que el abandono de Smith de la teoría del valor por el costo de producción está ligada a su teoría de la justicia, donde un espectador imparcial en el estado primitivo, cuando ésta [la tierra] era común costaban al trabajador sólo la molestia de recogerlos simpatizaba solo con las molestias del trabajo, pero en la sociedad moderna considera factores adicionales como el riesgo y la posesión. Al intercambiar la manufactura completa sea por dinero, trabajo, u otros bienes, en una cantidad superior a lo que costaron los materiales y los salarios de los trabajadores, algo debe quedar como beneficio del empresario que arriesga en esta aventura su capital. La teoría del valor de Smith fue una critica a los terratenientes cuyo trabajo es improductivo y su beneficio parasitario por la renta que el empresario y el trabajador deberá entregar al terrateniente una parte de lo que su trabajo recoge o produce. Smith también reconoció la desaventajada posición de los trabajadores frente al poder desigual que los empleadores tienen en la negociación. [A]unque el interés del trabajador está íntimamente vinculado al de la sociedad, él es incapaz de comprender ese interés o de percibir su conexión con el suyo propio. Su condición no le deja tiempo para adquirir la información necesaria, y su educación y costumbres lo vuelven por lo general incapaz de juzgar incluso si estuviese plenamente informado. En las deliberaciones públicas, por lo tanto, su voz es poco escuchada y menos atendida. La teoría de la suma de Smith implicaba que las ganancias y los salarios tenían una determinación independiente. Como resultado, no se pueden determinar las ganancias independientemente de los precios. David Ricardo. Unos cuarenta años después, David Ricardo desarrolló una teoría del valor-trabajo explicada en su obra Principios de economía política y tributación (1817). Continuando los razonamientos de Smith, Ricardo adoptó la primera de sus dos teorías del valor y trata de explicar cómo funciona el beneficio en la sociedad capitalista. El primer paso fue comprender las leyes del valor. Poseyendo utilidad, las cosas derivan su valor en cambio de dos causas: de su escasez y de la cantidad de trabajo necesaria para obtenerla. [...] Así, pues, al hablar de las cosas, de su valor en cambio y de las leyes que regulan y especias respectivos, nos referimos siempre a aquellas cuya cantidad puede ser aumentada por el esfuerzo de la industria humana y en cuya producción la competencia actúa sin restricciones. Principios de economía política y tributación (1817) David Ricardo Criticó además la definición dada sobre el patrón invariable trabajo. Ricardo explica que el valor del trabajo también varía, oponiéndose a Smith, quien sostenía que las mercancías varían su valor pero no el trabajo para producirlas. Expuso la idea de que lo único que puede servir de norma para el intercambio de bienes es la cantidad de distintas clases de trabajo que se necesitan para producirlos. Las únicas cualidades necesarias para que una medida de valor sea perfecta son que ella misma tenga valor y que ese valor sea invariable, de la misma manera que en una medida de longitud perfecta la medida debe tener longitud y esa longitud no debe estar sujeta a aumentar ni disminuir; o en una medida de peso debe tener peso y que dicho peso sea constante. Thomas Malthus criticó que el valor de cambio de las mercancías no es exactamente proporcional al trabajo que se ha empleado en ellas, cosa que Ricardo admitió al no poder vincular el sistema de precios a los costos de la mano de obra. Malthus sugirió el valor del salario de una jornada laboral, cualquiera que sea, como una medida perfecta de valor. Sin embargo, Ricardo no conocía otro criterio para juzgar si una cosa es cara o barata, que los sacrificios de trabajo realizados para obtenerla” y consideró el trabajo como la aproximación más cercana a una medida perfecta. Hay que confesar, pues, que no existe en la naturaleza una medida perfecta del valor, y que todo lo que le queda al economista político es admitir que la gran causa de la variación de las mercancías es la mayor o menor cantidad de trabajo que puede ser necesaria para producirlas, pero que hay también otra causa, aunque mucho menos poderosa, de su variación, que surge de las diferentes proporciones en que las mercancías terminadas pueden distribuirse entre el patrón y el trabajador como consecuencia de la condición mejorada o deteriorada del trabajador o de la mayor dificultad o facilidad de producir los artículos necesarios esenciales para su subsistencia. Así, aunque la gran causa de la variación del valor de las mercancías es la cantidad de trabajo para producirlas, Ricardo sugirió que otra causa eran las utilidades acumuladas como capital, y es únicamente una justa compensación para el tiempo durante el cual fueron retenidas las utilidades. George Stigler argumentó que la teoría de Ricardo sostenía una interpretación empírica en vez de analítica del valor-trabajo. Sin embargo, el lector observará que esta causa de los bienes produce efectos relativamente leves. [...] Los mayores efectos que podrían producirse: sobre los precios relativos de dichos bienes a consecuencia de salarios, no podrían exceder del 6 al 7%, porque las utilidades no podrían, en ninguna otra circunstancia, descender en forma general y permanente por debajo de dicha proporción. Ricardo encontró que según su teoría la ganancia es una sustracción del producto del trabajo. Pero, dada la existencia de la ganancia, y aceptando la existencia de capitales de diferente duración o tiempo de rotación, Ricardo antes que nada procedió a demostrar que el resultado es una relación de cambio (precios) que no obedece mínimamente a las exigencias de la teoría del valor del trabajo. En otras palabras, tal teoría del valor del trabajo constituye el punto de partida de una cadena de razonamientos que lleva a conclusiones en desacuerdo con la teoría misma. La teoría de Ricardo fue una predecesora de la teoría moderna de que los precios de equilibrio están determinados únicamente por los costos de producción asociados con el neo-ricardianismo. Desde Ricardo el economista John Stuart Mill escribió en 1848: No hay nada en las leyes del valor que queda para que el escritor presente o futuro lo aclare. Basándose en la discrepancia entre los salarios del trabajo y el valor del producto, los socialistas ricardianos ,Charles Hall, Thomas Hodgskin, John Gray, John Francis Bray, y Percy Ravenstone, aplicaron la teoría de Ricardo para desarrollar teorías de la explotación. Hodgskin consideraba que la teoría ricardiana del valor-trabajo tendría lugar en una economía estricta de libre mercado que hubiese provocado la desaparición del capitalismo. Anarquismo Artículos principales: Economía anarquista y Costo como límite del precio. El mutualismo de Pierre Joseph Proudhon y los anarquistas individualistas estadounidenses como Josiah Warren, Lysander Spooner y Benjamin Tucker adoptaron la teoría del valor trabajo de la economía clásica y la utilizaron para criticar el capitalismo mientras favorecían un sistema de mercado no capitalista. Vale de trabajo del Cincinnati Time Store. Escaneado del Equitable Commerce (1846) por Josiah Warren. Warren es ampliamente considerado como el primer anarquista estadounidense, a y el semanario de cuatro páginas que editó durante 1833, The Peaceful Revolutionist, fue el primer periódico anarquista publicado. El costo como límite del precio fue una máxima acuñada por Warren, que indica una versión (prescriptiva) de la teoría del valor trabajo. Warren sostenía que la compensación justa por el trabajo (o por su producto) sólo podía ser una cantidad equivalente de trabajo (o un producto que incluyera una cantidad equivalente). Por lo tanto, las ganancias, la renta y los intereses se consideraban acuerdos económicos injustos. De acuerdo con la tradición de La riqueza de las naciones de Adam Smith, el costo del trabajo se considera el costo subjetivo; es decir, la cantidad de sufrimiento involucrado en ello. Puso sus teorías a prueba estableciendo una tienda de trabajo por trabajo experimental llamada Cincinnati Time Store en la esquina de las calles 5th y Elm en lo que ahora es el centro de Cincinnati, donde el comercio se facilitaba mediante billetes respaldados por una promesa. para realizar labores. Todos los productos puestos a la venta en la tienda de Warren se ofrecieron al mismo precio que el propio comerciante había pagado por ellos, más un pequeño recargo, de entre el 4 y el 7 por ciento, para cubrir los gastos generales de la tienda (ver: Vale de trabajo). La tienda permaneció abierta durante tres años; Después de su cierre, Warren podría intentar establecer colonias basadas en el mutualismo. Entre ellos se encontraban Utopía y Tiempos modernos. Warren dijo que The Science of Society de Stephen Pearl Andrews, publicado en 1852, era la exposición más lúcida y completa de las propias teorías de Warren. Pierre-Joseph Proudhon. El mutualismo es una teoría económica y escuela de pensamiento anarquista que se basa en una teoría del valor trabajo que sostiene que cuando se vende trabajo o su producto, a cambio, debe recibir bienes o servicios que incorporen la cantidad de trabajo necesaria para producir un artículo de utilidad exactamente similar e igual. El mutualismo se originó de las obras del filósofo Pierre-Joseph Proudhon. El problema del sistema capitalista, para Proudhon, era la violación de la proporcionalidad. En Filosofía de la miseria, describe que el progreso de la sociedad consiste en resolver incesantemente el problema de la constitución de los valores, o sea de la proporcionalidad y solidaridad de los productos. En el mutualismo proudhoniano, fundado en la reciprocidad, el tiempo empleado en la elaboración de todo producto, valor constituido, da la medida de su valor y es la base para el intercambio. Así, al capital se le priva de su poder usuario y se ata completamente al esfuerzo de su trabajo. Por tanto, la apropiación del trabajo desaparecería y los trabajadores serían remunerados según su aporte. El mutualismo aboga por una sociedad en la que cada persona pueda poseer un medio de producción, ya sea individual o colectivamente, y el comercio represente cantidades equivalentes de trabajo en el libre mercado. Una parte integral del plan fue el establecimiento de un banco de crédito mutuo que prestaría a los productores a una tasa de interés mínima, lo suficientemente alta como para cubrir la administración. El mutualista Kevin Carson en Estudios sobre economía política mutualista intentó de integrar las críticas marginalistas en la teoría del valor-trabajo. El anarquismo colectivista defendido por Mijaíl Bakunin defendió una forma de teoría del valor trabajo cuando defendía un sistema donde todos los elementos necesarios para la producción son propiedad común de los grupos laborales y las comunas libres... basado en la distribución de bienes de acuerdo con la mano de obra aportada. Economía marxista Artículos principales: Economía marxista y Teoría del valor-trabajo marxista. Karl Marx. Contrariamente a la creencia popular, Karl Marx nunca usó el término teoría del valor-trabajo en ninguna de sus obras, sino que usó el término ley del valor (Wertgesetz). Aunque a menudo se equipara ambos términos, esto es, en sentido estricto, la ley del valor es el principio regulador del intercambio económico mientras que la teoría del valor-trabajo tiene como objetivo explicar cómo funciona esa determinación en la producción e intercambio de mercancías. Algunos economistas sostiene que Marx no tenía propiamente una teoría del valor, sino que perfeccionó una teoría ya existente pero que nunca antes se había presentado de manera coherente. La base, el punto de partida de la fisiología del sistema burgués ,para la comprensión de su sistema orgánico interno la coherencia y el proceso de la vida, es la determinación del valor por el tiempo de trabajo. Teorías sobre la plusvalía (c. 1862/3), Karl Marx Marx utilizó el concepto de tiempo de trabajo socialmente necesario para introducir una perspectiva social distinta de sus predecesores y de la economía neoclásica. Mientras que la mayoría de los economistas comienzan con la perspectiva del individuo, Marx comenzó con la perspectiva de la sociedad en su conjunto. La producción social implica una división del trabajo complicada e interconectada de una amplia variedad de personas que dependen unas de otras para su supervivencia y prosperidad. En Marx la teoría del valor-trabajo es una teoría sociológica del trabajo abstracto, ya que la categoría valor posee un carácter social e histórico. El punto de vista marxiano del valor es macroeconómico. [I]ncluso si en mi libro [El capital] no hubiera ningún capítulo acerca del «valor», el análisis de las condiciones reales que yo hago contendría la prueba y la demostración de relaciones reales de valor. Marx: Carta a Ludwig Kugelmann; 11 de julio de 1868 Smith vio el valor de un producto en relación con el trabajo del comprador o consumidor, en oposición a Marx, quien vio que el valor de un producto es proporcional al trabajo del trabajador o productor. Y al valorar las cosas les ponemos precio basándonos en la cantidad de trabajo que podemos evitar u ordenar, y podemos ordenar mano de obra no solo de una manera simple sino también intercambiando cosas para obtener una ganancia. La definición de valor y su forma se encuentra en su obra El Capital, la cual es la base fundamental para entender el modo de producción capitalista. El valor en Marx es pues una relación entre personas, una relación social, pero bajo el capitalismo aparece en una forma fantástica como una relación entre cosas (véase Fetichismo de la mercancía). De hecho es la ley del valor, tal como se impone no con relación a las mercancías o artículos en particular, sino a los productos globales originados en cada una de las esferas sociales particulares de la producción, autonomizadas en virtud de la división del trabajo, de modo que no sólo se emplea únicamente el tiempo de trabajo necesario para cada mercancía, sino que sólo se emplea la cantidad proporcional necesaria del tiempo de trabajo social global en los diversos grupos. El capital, tomo III, Sección sexta, Capítulo 37 (1894), Karl Marx Marx «definía» el valor como la cantidad de trabajo directa (trabajo vivo) e indirectamente (trabajo muerto materializado en los medios de producción) consumida en la producción de la mercancía, pero se opuso a atribuir un poder creativo sobrenatural al trabajo. Al igual que los fisiócratas, Marx vio la naturaleza como la fuente de la riqueza. El trabajo no es valor sino lo que produce valor a través de ella exclusivamente por la organización social en el cual es empleado. La naturaleza es la fuente de los valores de uso (¡que son los que verdaderamente integran la riqueza material!), ni más ni menos que el trabajo, que no es más que la manifestación de una fuerza natural, de la fuerza de trabajo del hombre. Crítica del Programa de Gotha (1875) K. Marx En su producción, el hombre sólo puede proceder como la naturaleza misma, vale decir, cambiando, simplemente, la forma de los materiales. Y es más: incluso en ese trabajo de transformación se ve constantemente apoyado por fuerzas naturales. El trabajo, por tanto, no es la fuente única de los valores de uso que produce, de la riqueza material. El trabajo es el padre de ésta, como dice William Petty, y la tierra, su madre. El capital, tomo I (1867), Karl Marx Una característica intrínseca del trabajo es producir, crear y transformar. La naturaleza no construye máquinas, ni locomotoras, ferrocarriles, telégrafos eléctricos, hiladoras automáticas, etc. Son estos, productos de la industria humana”. Como consecuencia, tanto trabajo generalizado en una mercancía puede ser comparado con tanto trabajo generalizado en otra. El trabajo es la fuente de todo valor de cambio. En contradicción directa con la objetividad sensorialmente grosera del cuerpo de las mercancías, ni un solo átomo de sustancia natural forma parte de su objetividad en cuanto valores. De ahí que por más que se dé vuelta y se manipule una mercancía cualquiera, resultará inasequible en cuanto cosa que es valor. Si recordamos, empero, que las mercancías sólo poseen objetividad como valores en la medida en que son expresiones de la misma unidad social, del trabajo humano; que su objetividad en cuanto valores, por tanto, es de naturaleza puramente social, se comprenderá de suyo, asimismo, que dicha objetividad como valores sólo puede ponerse de manifiesto en la relación social entre diversas mercancías. El capital, tomo I (1872), Karl Marx Ahora bien, el valor de las mercancías se mide por el número total de horas de trabajo indiferenciadas y socialmente necesarias empleadas en ellas. Ello se debe al estadio histórico alcanzado de desarrollo económico en los diversos Estados, de las fuerzas productivas y de las relaciones sociales de producción de un determinado modo de producción (en este caso, el capitalista). Además, dado los avances tecnológicos en la producción capitalista, el valor de las mercancías no está determinado por el tiempo de trabajo originalmente gastados en su producción, sino por el tiempo de trabajo empleado en su reproducción, y ésta disminuye continuamente debido al desarrollo de la productividad social del trabajo. El carácter socialmente necesario es una de las diferencias radicales introducidas por Marx con respecto a sus predecesores, los cuales no concebían o no introducían el mercado y la competencia dentro de sus respectivas teorías del valor de las mercancías. Marx sí tenía presente el mercado y la competencia en su teoría del valor de las mercancías, como dejó constancia en una de sus obras principales escrita contra Proudhon: Es importante insistir aquí en que el valor no es determinado por el tiempo en que una cosa ha sido producida, sino por el mínimo de tiempo en que puede ser producida, y este mínimo es establecido por la competencia. Supongamos por un momento que haya desaparecido la competencia y que, por consiguiente, no exista medio de establecer el mínimo de trabajo necesario para la producción de una mercancía. ¿Que ocurrirá? Bastará invertir en la producción de un objeto seis horas de trabajo para tener derecho, según el señor Proudhon, a exigir a cambio seis veces más que quien no haya empleado más de una hora en la producción del mismo objeto. Miseria de la filosofía (1846), Karl Marx Valor de las mercancías y trabajo Una mercancía es un objeto o servicio por cuyas características satisface necesidades, ya sean reales o imaginarias. La mercancía es el producto que solo existe en las sociedades mercantiles, y en el capitalismo, por ser la economía mercantil más compleja y desarrollada, la producción se presenta la mercancía como su forma celular. Las sociedades mercantiles tienen como principal característica la producción no para la satisfacción propia sino para el intercambio. Al igual que los fisiócratas, Marx insistía en que el proceso productivo es un proceso circular, es decir, las mercancías se producían por medio de mercancías. Las características de la mercancía son el valor de uso y el valor de cambio: Valor de uso: es la capacidad de un objeto o servicio de satisfacer alguna necesidad. Porque cuando un objeto es útil (funcional), puede satisfacer la necesidad de alguna persona. Marx sostenía que los valores de uso producidos en el capitalismo eran valores concretos, pues responden a cualidades físicas sensibles de las mercancías, por ejemplo, las propiedades nutritivas del trigo. Valor de cambio: es la expresión x cantidad de la mercancía A igual a y cantidad de la mercancía B. Aquí las mercancías en vez de ser un valor concreto e individual, pasan a ser un valor abstracto y social. Esto significa que algo hace equivalentes a dos cantidades de diferentes mercancías. Marx en su análisis del valor de uso y el valor de cambio en El capital se basó en la Lógica del filósofo alemán Hegel, particularmente la categoría de medida. Como explica Rolando Astarita: [E]n Hegel la medida alude a una proporción, que a su vez nos llevará a la razón o ley que la gobierna. Marx toma esta idea cuando parte de los valores de cambio, esto es, de lo que aparece a primera vista al examinar el mercado. [...] Pero a poco que se examine, se encontrará que determinadas relaciones se mantienen [...] esto significa que hay proporción, medida, no es una relación arbitraria. Por lo tanto, hay que preguntarse por la ley que gobierna esta proporción, lo que lleva a pasar de la superficie a lo que está por debajo. El valor de uso y el valor de cambio se deben al carácter bifacético (dual) del trabajo. El trabajo es útil porque transforma materias primas y las convierte en cosas útiles, por ejemplo, como convierte el hombre una semilla en un cítrico, o el mineral de hierro en utensilios y herramientas. El trabajo abstracto es el gasto de trabajo humano indiferenciado. Es la propiedad que queda si se separa el valor de uso de las mercancías y hace que las mercancías sean comparables entre sí. El trabajo abstracto se vincula orgánicamente con el valor como gasto de trabajo humano en general. Las cualidades individuales de una mercancía no es lo que permite su igual intercambio. Lo que las hace comparables, conmensurables, medibles es la sustancia del valor. Es este trabajo abstracto social la sustancia del valor de todas las mercancías y su medición la magnitud del valor. La sustancia del valor es el trabajo socialmente necesario para producir cierta cantidad de una mercancía con las condiciones medias de trabajo. Podría parecer que si el valor de una mercancía se determina por la cantidad de trabajo gastada en su producción, cuanto más perezoso o torpe fuera un hombre tanto más valiosa sería su mercancía, porque aquél necesitaría tanto más tiempo para fabricarla. Sin embargo, el trabajo que genera la sustancia de los valores es trabajo humano indiferenciado, gasto de la misma fuerza humana de trabajo. El capital, tomo I (1867), Karl Marx La magnitud del valor es la magnitud de la sustancia del valor, que es el tiempo de trabajo invertido en la mercancía, por ejemplo: si x cantidad de hierro es igual a n cantidad de pan, es porque, el tiempo necesario para producir n cantidad de pan y x cantidad de hierro son proporcionalmente equivalentes. Esta propiedad común puede calcularse sobre la según el tiempo de trabajo socialmente necesario, que es aquel que se realiza bajo la fuerza productiva del trabajo y la intensidad del trabajo promedio. Como los valores de cambio de las mercancías no son más que funciones sociales de las mismas y no tienen nada que ver con sus propiedades naturales, lo primero que tenemos que preguntarnos es esto: ¿cuál es la sustancia social común a todas las mercancías? Es el trabajo. Para producir una mercancía hay que invertir en ella o incorporar a ella una determinada cantidad de trabajo. Y no simplemente trabajo, sino trabajo social. [...] Pero, para producir una mercancía, no sólo tiene que crear un artículo que satisfaga alguna necesidad social, sino que su mismo trabajo ha de representar una parte integrante de la suma global de trabajo invertido por la sociedad. Ha de hallarse supeditado a la división del trabajo dentro de la sociedad. No es nada sin los demás sectores del trabajo, y, a su vez, tiene que integrarlos. Salario, precio y ganancia (1865), Karl Marx La fuerza productiva del trabajo depende de varios factores: la destreza del obrero, el desarrollo de la ciencia y sus aplicaciones tecnológicas, las condiciones naturales, etc. Por lo tanto, los valores varían según las sociedades pues dependen del desarrollo tecnológico, el desarrollo de la división del trabajo, entre otras cosas, todas determinadas por factores objetivos externos, como el clima, la posición geográfica, el acceso a los recursos, etcétera. Si 1 levita equivale a 20 lienzos de tela; dicho fenómenos puede medirse en tiempo de trabajo para observar la equivalencia; de tal suerte que si un lienzo requiere 5 horas de trabajo, entonces, 1 levita equivale a 100 horas de trabajo de lienzo. Por otra parte si la levita requiere de 30 horas de trabajo del sastre, eso comprueba que 30 horas del trabajo del sastre equivalen a 100 horas de trabajo del lienzo; por tanto, se trata de una equivalencia directamente proporcional; expresada como: 30 K = 100. k = 100 / 30- k = 3.3333 Las mercancías como valores de cambio son meramente expresiones cuantitativamente diferentes del trabajo social”, y se relacionan de todas ellas mediante ese trabajo social. Entonces, en el intercambio, los productores están igualando, inconscientemente, diferentes tipos de trabajos. Eso implica, que en determinado momento, la sociedad valora el trabajo del sastre para fabricar una levita como un trabajo complejo, mientras que el trabajo del hilador del lienzo, lo considera como un trabajo simple; una hora de trabajo del sastre es 3.3333 veces más que una hora del trabajo del hilador. Marx también explica que una cosa puede ser valor de uso y no ser valor, como el aire, la tierra virgen, las praderas y bosques naturales, cuya utilidad no se mide en trabajo pero puede ser vendido (véase Renta diferencial y renta absoluta). A su vez algo puede tener trabajo y no tener valor por ser inútil; y algo puede ser útil, y además producto del trabajo humano, y no ser mercancía si es para autoconsumo o nadie la quiere. Si nuestro individuo produce una cosa que no tenga ningún valor de uso para otros, toda su energía no conseguirá producir ni un átomo de valor; y si se empeña en fabricar con la mano un objeto producido veinte veces más barato por una máquina, entonces diecinueve vigésimos de la energía que ha puesto en ello no producen ni una determinada cantidad de valor ni valor en absoluto. F. Engels (1878), Anti-Dühring, Sección segunda, ECONOMIA POLITICA. V. Teoría del valor La teoría de Marx difiere de la teoría propuesta por Ricardo, pues conecta la teoría del valor-trabajo con la teoría monetaria. En Marx, el dinero surge de forma endógena al mercado: el producto de los individuos sólo se manifiestan como productos del trabajo general tomando la forma de dinero. En el dinero la cuota de tiempo de trabajo que ella representa es al mismo tiempo medida y colocada en su forma intercambiable universal, correspondiente al concepto”. El trabajo humano abstracto que descubrió Marx no es, en su forma más desarrollada, sino el dinero. Rosa Luxemburgo (1899), Reforma o revolución Se diferencian el valor de los precios, que no son equivalentes. Los valores “se hallan detrás de los precios de producción y, en última instancia, los determinan”, pero estos dos no coinciden nunca, o sólo ocasionalmente y como excepción, ya que el precio está determinado por la relación entre la oferta y la demanda. Sin embargo, cuando la oferta y la demanda se equilibran (equilibrio económico) y dejan, por tanto, de actuar, el precio de una mercancía en el mercado coincide con su valor real (precio natural). Basta decir que si la oferta y la demanda se equilibran, los precios de mercado de las mercancías se corresponderán con sus precios naturales, es decir, con sus valores determinados por las respectivas cantidades de trabajo necesarias para su producción. Karl Marx (1865) Salario, precio y ganancia Fuerza de trabajo y plusvalor Caricatura La relación entre los trabajadores de dos empresarios exprimiendo a trabajadores. Su dinero cae en una bañera con la inscripción cuenca colectora del capitalismo. Al fondo hay un hombre desempleado atrapado por los espectros de la miseria y el hambre. «Neue Postillon» Zúrich, Suiza (1896). El modo de producción capitalista surgió con posteridad a otros modos de producción que ya habían desarrollado la fuerza productiva del trabajo. La fuerza de trabajo es la capacidad de realizar alguna actividad laboral ya sea física o intelectual. Por ser la capacidad de trabajar, es una mercancía especial que solo pueden desempeñar los seres humanos. Marx diferenció entre el capital constante, la masa de capital invertida en medios de producción y materias primas; y el capital variable, el capital incurrido en los salarios para la contratación de fuerza de trabajo. Adam Smith vio una falla en su aplicación de la TvT al capitalismo contemporáneo. Señaló que si el trabajo incorporado en un producto era igual al trabajo ordenado (es decir, la cantidad de trabajo que se podía comprar vendiéndolo), entonces la ganancia era imposible. David Ricardo (secundado por Marx) respondió a esta paradoja argumentando que Smith había confundido el trabajo con el salario. El trabajo mandado, argumentó, siempre sería más que el trabajo necesario para sostenerse a sí mismo (los salarios). El valor del trabajo, desde este punto de vista, abarcaba no sólo el valor de los salarios (lo que Marx llamaba el valor de la fuerza de trabajo), sino el valor de todo el producto creado por el trabajo. Marx investigó de un modo minucioso por vez primera la propiedad que tiene el trabajo de crear valor, y descubrió que no todo trabajo aparentemente y aun realmente necesario para la producción de una mercancía añade a ésta en todo caso un volumen de valor equivalente a la cantidad de trabajo consumido. Trabajo asalariado y capital, Introducción de Federico Engels a la edición de 1891 Al igual que el valor de cualquier mercancía, el valor de la fuerza de trabajo es el tiempo de trabajo socialmente necesario para su producción o, más bien, reproducción. Este se encuentra determinado por el valor de las mercancías que consume el trabajador, las mercancías que constituyen las necesidades promedio en un estado dado de la sociedad, bajo ciertas condiciones medias sociales o de producción, con una determinada intensidad social media y una habilidad media del trabajo empleado. Es decir, las necesidades van cambiando más por los estándares sociales que por las condiciones individuales, por los cuales incluyen: alimentación, vivienda, transporte, entretenimiento, etc. La cantidad de mercancías que consume un trabajador por día pueden ser producidas en menos tiempo. Entonces, el tiempo de trabajo necesario para que el trabajador fabrique las mercancías que consume es menor al tiempo de trabajo que labora en la empresa que lo contrató. Esto explica por qué los avances tecnológicos reducen el precio de los productos básicos y dejan sin trabajo a los productores menos avanzados. En Marx no es el trabajo per se lo que crea valor, sino la explotación de la fuerza de trabajo vendida por los trabajadores libres a los capitalistas. Solo los trabajadores asalariados de los sectores productivos de la economía producen valor. “La producción capitalista no es meramente la producción de mercancías. Es, por su propia esencia, la producción de plusvalía”, explica Marx en el tomo I de El capital. “El trabajador no produce para sí mismo, sino para el capital. Por tanto, ya no le es suficiente con producir. Debe producir plusvalía. El único trabajador productivo es el que produce plusvalía para el capitalista”. La fuerza de trabajo humana crea valor pero no se convierte en valor hasta que se objetiva en el mercado. Solo cuando se vende un producto el productor sabrá si su trabajo privado es sancionado como trabajo socialmente necesario, y por lo tanto, como trabajo que ha generado valor. El trabajo abstracto socialmente necesario materializado se representa como el atributo social que tiene su producto para relacionar socialmente a su productor a través del cambio, o sea, como el valor de su producto específicamente determinado como mercancía. En el momento en que el productor de mercancías ejerce el control sobre su proceso individual de trabajo propio del individuo libre, debe someter su conciencia y voluntad a las potencias sociales encarnadas en el producto de su trabajo (Véase: Fetichismo de la mercancía). La jornada laboral incluye el trabajo necesario y el plustrabajo. Durante el primero el trabajador repone su valor, durante el segundo trabaja sin remuneración alguna para él y sí para el capitalista. Por ejemplo, la jornada laboral legal actual en muchos países es de 8 horas, entonces si el trabajo necesario es de 4 horas, el plustrabajo será de 4 horas, tiempo del cual se apropia el capitalista por su posición social como dueño de los medios de producción. Por tanto, dentro de la economía política marxista, se establece concepto de tiempo de trabajo socialmente necesario (TTSN) para producir una mercancía, la cual condiciona el valor del trabajo desagregando el plusvalor generado por la fuerza de trabajo. Astarita también explica que las nociones de renta e interés son partes del plusvalor, lo que también permite explicar el precio de la tierra y de activos financieros (ver: capital ficticio). Tasa de ganancia Marx diferenció en su obra El capital, entre el capital constante, la masa de capital invertida en medios de producción y materias primas; y el capital variable, el capital incurrido en los salarios para la contratación de fuerza de trabajo. Mientras del primero solo transfiere su valor a la mercancía, solo el segundo es capaz de crear plusvalor, y en consecuencia ganancias. La relación entre el capital variable y el constante se denomina como composición orgánica del capital; y la tasa de plusvalor o tasa de explotación como la relación del plusvalor entre el capital variable. La tasa de plusvalor explica el origen de la ganancia bruta para el capital total, enfrentado a la clase obrera en su conjunto. Sin embargo, para determinar el éxito de una inversión de un capital individual se mide la tasa de ganancia, que es la relación del plusvalor obtenido entre el capital constante y capital variable adelantado en un ciclo productivo. La tasa de ganancia es directamente proporcional a la tasa de plusvalor y es inversamente proporcional a la composición orgánica del capital. Entonces: Tasa de ganancia Tasa de pluvalor Composición orgánica del capital La cantidad de plusvalía producida en diferentes industrias será muy diferente según las composiciones del capital constante. Sin embargo, Marx notó que diferentes industrias tienden a compartir una tasa de ganancia media. Según Marx, esta contradicción se resolvería mediante la competencia y la reasignación de capitales; y con su teoría de los precios de producción expuesta en el tomo III de El capital, donde la suma de todos los precios es igual a la suma de todos los valores agregados. Como explica León Trotski: [E]n último término únicamente los valores que han sido creados por el trabajo humano se encuentran a disposición de la sociedad, y los precios no poden franquear este límite, inclusive si se tiene en cuenta el “monopolio de los precios” o el “trust”; de allí donde el trabajo no ha creado un valor nuevo, ni el mismo Rockefeller puede sacar nada. Véase también: Problema de la transformación Solamente en una sociedad socialista con una economía planificada se podrá lograr el equilibrio entre la oferta y la demanda; y así entre valor y precio. [C]uando la producción esté bajo el control real y predeterminado de la sociedad, establece la sociedad la relación coherente entre la cantidad de trabajo social de trabajo empleado en la producción de definidos artículos y la cantidad de demanda de la sociedad que ha de ser satisfecha por ellos... El cambio venta de mercancías según su valor es la ley racional y natural de su equilibrio. Véase también: Debate sobre el cálculo económico en el socialismo Interpretación de David Harvey De acuerdo a David Harvey, las pocas ocasiones en que Marx se refirió directamente a esta teoría, lo hizo bajo la denominación de teoría del valor y no de teoría del valor-trabajo ni de teoría laboral del valor. Para Harvey, la teoría del valor, además de ocuparse del proceso del trabajo, se ocupa de las condiciones de reproducción social del ejército industrial de reserva. Para sustentar esta visión, Harvey menciona reportes europeos de mediados del siglo XIX citados por Marx. La conclusión de Harvey es que una intensificación en la competencia capitalista en el mercado, el cual incluye la búsqueda de valor relativo excedente obtenido a través de innovaciones tecnológicas, conlleva un deterioro en las condiciones de reproducción social para la clase trabajadora, o al menos, para un sector significativo de la misma. Michael Roberts considera errónea la interpretación de Harvey porque equipara valores con precios y el proceso de valorización sea la circulación y no el intercambio. Citando a Marx: no es el intercambio el que regula la magnitud de valor de la mercancía, sino a la inversa la magnitud de valor de la mercancía la que rige sus relaciones de intercambio. Harvey respondió que el valor es más bien la unidad contradictoria de la producción y la realización. El valor no se produce en el intercambio, pero el valor creado en la producción es sólo un valor potencial hasta que se realiza en el intercambio. El capital entendido como valor en movimiento puede devaluarse si no entra en mercado. Harvey exploró este concepto como antivalor más a fondo en Companion to Capital Volume 2, el cual Marx en los Grundrisse denomina como capital negado, en barbecho, dormido o fijado. Por otro lado, Paul Cockshott criticó la interpretación de Harvey defendiendo que las teorías de Marx y Ricardo son sustancialmente idénticas, difiriendo sólo en la terminología. Interpretación de Michael Heinrich Esta sección es un extracto de Michael Heinrich § Teoría monetaria del valor.[editar] Heinrich rechaza la interpretación sustancialista de la teoría del valor de Marx, que entiende el valor como la propiedad de una mercancía individual ,: 54  es decir, el trabajo abstracto definido por Marx. Más bien, el valor es un medio para entender las relaciones sociales fetichizadas. Heinrich entiende la teoría de Marx como una teoría monetaria del valor, donde la mercancía y el valor no pueden existir y tampoco pueden conceptualizarse sin referencia al dinero, que marca un cambio paradigmático con respecto a la teoría del trabajo premonetaria de los economistas políticos clásicos anteriores, y también distingue a Marx de la teoría de la utilidad de la economía neoclásica.: 64  La magnitud del valor de una mercancía se expresa en su precio, y esta es la única posibilidad de expresar la magnitud del valor. Durante la época de Marx el patrón oro desempeñó el papel de mercancía dinero, el cual fue abolido por la década de 1970. Sin embargo, Heinrich argumenta, la existencia de tal mercancía no es de ninguna manera una consecuencia necesaria de su análisis de la mercancía y el dinero. La interpretación de Heinrich del tiempo de trabajo socialmente necesario es que: El trabajo debe ser técnicamente necesario, es decir, tener lugar en condiciones técnicas medias que no sobreutilicen ni subutilicen la mano de obra. El trabajo debe ser socialmente necesario en el sentido de que debe haber suficiente demanda monetaria para que se venda la producción total. Sólo el tiempo de trabajo empleado en las condiciones medias de producción existentes, así como para la satisfacción de la demanda social monetaria, constituye valor. La medida en que el trabajo gastado en el sector privado era realmente necesario para satisfacer la demanda depende, por una parte, de la cuantía de esta demanda y, por otra, del volumen de producción de otros productores, lo que se hace evidente por primera vez en el intercambio. Aunque el valor de una mercancía parezca una propiedad material, se trata de una relación social, es decir, la relación entre el trabajo individual de los productores y el trabajo total de la sociedad. Esto no significa que el intercambio produzca valor, sino que sólo en el intercambio el valor puede obtener una forma de valor objetiva. : 55  Paul Cockshott alaba el estilo claro y bien escrito de Heinrich pero criticó su interpretación como teleológica y un tanto controvertida pues Marx habla de mercancías que se venden por encima o por debajo de su valor, algo que no tendría sentido si el valor se constituyera en el proceso de intercambio, lo cual vacía la teoría del valor de cualquier estatus científico. Holger Wendt criticó la posición de Heinrich sobre el valor como una concepción positivista del valor y el precio. Investigación empírica Relación entre el valor monetario de los productos de la industria del Reino Unido y el contenido laboral de estos productos. Fuente: Paul Cockshott, 2019. Diagrama que muestra una correlación entre el valor agregado y la cantidad de empleados de distintos sectores de EE. UU en 2020. Fuente: Blair Fix, 2021. Durante los últimos años han surgido un grupo de estudios académicos empíricos que mediante matrices de insumo-producto y coeficientes de trabajo verticalmente integrado se han calculado los valores promedios con los precios de producción y afirman que las desviaciones de los valores a los precios son bastante pequeñas. véase: Shaikh (1984, 1998, 2021), Ochoa (1989), Petrovic (1987). Paul Cockshott y Allin Cottrell (1995, 1997, 1998) encontraron que para 49 industrias estadounidenses en la Clasificación Industrial Estándar una correlación muy fuerte (más del 98%) entre el contenido de trabajo de la producción industrial y la variación de los valores monetarios de esas industrias, por lo que la economía marxista no tiene nada que temer de una confrontación con los datos empíricos. Diego Guerrero (2000) observó que en 51 industrias de la economía española la relación entre precios directos y de producción, los resultados globales están plenamente de acuerdo con la TLV. Tsoulfidis y Paitaridis (2002) realizaron investigaciones empíricas positivas para la economía griega. Dave Zachariah (2004) observó en la economía sueca que los costes laborales están estrechamente relacionados con los precios de mercado (entre 0,930 y 0,965) y describió la TVT como una poderosa herramienta analítica para comprender cómo las economías de mercado regulan el trabajo social. Zachariah (2006) también mostró correlaciones positivas en economías de 18 países distintos. Yan Ma (2016) cita los resultados de un estudio según los cuales en un mercado libre y competitivo el costo de los bienes es inversamente proporcional a la productividad laboral; en un mercado libre monopolizado, se muestran resultados mixtos; en un mercado no competitivo monopolizado, el costo de los bienes es directamente proporcional a la productividad laboral. Estos resultados son consistentes con las predicciones de la TVT de Marx, con respecto a la relación entre la productividad laboral y el costo de los bienes. En ese mismo año, Tsoulfidis y Paitaridis (2016) realizaron investigaciones empíricas positivas desde la interpretación temporal y de sistema único. En un estudio de Güney Işıkara y Patrick Mokre (2020) se realiza un análisis estadístico extenso de las relaciones precio-valor que sugieren una considerable fortaleza de la teoría. El modelo que abarca 15 años y está basado en más de 36.000 vectores de precios de 42 países, revela apenas pequeñas variaciones respecto a las predicciones de la TVT del marxismo clásico. Diagramas de Bichler y Nitzan que muestra una correlación ilusoria entre el tiempo de trabajo y el precio de los productos al multiplicar los precios por el tamaño del sector. Fuente: Fix, Bichler y Nitzan. Críticos a las investigaciones empíricas como Andrew Kliman, Jonathan Nitzan y Shimshon Bichler señalan que caen a su vez en un razonamiento circular al asumir que el valor de la fuerza de trabajo es proporcional a la tasa salarial real, o que la relación entre el capital variable y la plusvalía viene dada por la relación entre el salario y la ganancia; lo que presupone que el tiempo de trabajo predice el valor monetario. Además, las correlaciones de precio y valores observadas pueden ser espurias pues podrían ser causa de otros factores (véase: Cum hoc ergo propter hoc). Como demostración visual de correlación ilusoria, Nitzan y Bichler mostraron en un gráfica donde las variables del precio y el valor unitario promedio de mercancías de distintos sectores son aleatorias pero se correlacionaban cuando se les multiplicaba por un tercer factor (la cantidad de la producción de cada sector en su caso). Kliman, Nitzan y Bichler sugieren el tamaño de las industrias como el tercer factor determinante. Juan Ramón Rallo sugiere a la utilidad marginal como factor determinante, además de ser buena aproximación a los precios. Por otro lado, George Soklis, desde un punto de vista sraffiano, arguyó desde un análisis de la economía francesa en favor de valores mercancía como mejores aproximaciones que el valor-trabajo. Otro problema, Kliman, Nitzan y Bichler señalan, es que la teoría del valor marxiana trata mercancías individuales, no sectores. En respuesta a las críticas se han realizados otros estudios que miden valores sin utilizar precios y defensas de las medidas transversales de correlación. Cockshott, Cottrell y Baeza defienden que la correlación entre dos vectores no cambian bajo una multiplicación escalar de uno de los vectores y que para sostener que una correlación es espuria se ha de identificar ese tercer factor independiente que induce tal fenómeno y el tamaño de la industria de Kliman no es independiente de la producción. Sin embargo, Blair Fix arguye que puede haber más de un tercer factor y defiende una teoría de la naturaleza humana que llama heurística igualitaria, en donde si los seres humanos son igualitarios, entonces una correlación valor-trabajo es inevitable, y la teoría del valor-trabajo es innecesaria (véase Juego del ultimátum). La mayoría de los marxistas, sin embargo, rechazan la interpretación de Bichler y Nitzan de Marx, argumentando que su afirmación de que las mercancías individuales pueden tener valores, en lugar de precios de producción, malinterpreta el trabajo de Marx. Por ejemplo, Fred Moseley argumenta que Marx entendía el valor como una variable macromonetaria (la cantidad total de trabajo agregado en un año determinado más la depreciación del capital fijo en ese año), que luego se concreta al nivel de los precios individuales de producción, lo que significa que los valores individuales de las mercancías no existen. Críticas Artículo principal: Críticas a la teoría del valor-trabajo Actualmente, la economía neoclásica ortodoxa utiliza la teoría del marginalismo, la cual rechaza todo este lazo e incluye el valor de uso en la esencia del valor de cambio basada en preferencias subjetivas. Friedrich Engels sostuvo que el auge marginalista es debido a que Marx mostró las peligrosas consecuencias de la economía clásica, lo cual lleva a una economía vulgar que renuncia a toda ciencia. Paul Cockshott sostiene que fue la evidente amenaza política del movimiento socialista lo que motivó el rechazo de la teoría del valor-trabajo ya que si se aceptase entonces la crítica de Marx a la explotación capitalista se vuelve inevitable, lo cual es políticamente intolerable en países capitalistas. Sin la teoría del valor del trabajo, explicó Steven Horwitz, no está claro cuánto de la crítica marxiana al capitalismo sigue siendo válida. Es por ello que teóricos antimarxianos se dedican a exponer los absurdos” de la teoría laboral del trabajo. Así, el economista austriaco Eugen von Böhm-Bawerk admitió que: Es cierto que creo que la teoría del valor del trabajo ha ganado durante algunos años una aceptación general, como resultado de la difusión de las ideas socialistas, pero en la época más reciente ha perdido decididamente terreno entre los círculos teóricos de todos los países. Y esto se debe, en particular, a la creciente importancia que ahora se concede a la teoría de la utilidad marginal. Aunque Marx nunca mencionó economistas marginalistas pero criticó previamente teorías económicas premarginalistas que equiparaban el valor de uso con valor de cambio y se centran solo en los precios, las cuales consideró como economía vulgar ya que se contenta en los fenómenos pero no en la esencia. Steven Horwitz consideró la situación al revés, donde la teoría del valor-trabajo explicaba los fenómenos similarmente como el geocentrismo, y el marginalismo fue revolución copernicana. Economía neoclásica y marginalismo Poco después de que Marx terminase su primer tomo de El capital en la década de 1870 surgió la denominada revolución marginalista por parte de los economistas William Stanley Jevons, Carl Menger, León Walras. El marginalismo sostiene que el valor de cualquier bien o servicio se determina por su utilidad marginal, es decir, la utilidad de la última unidad de bien consumida medida según su precio en la satisfacción de un deseo específico de un consumidor. Según el marginalismo, el valor es subjetivo (ya que las mismas horas de ocio y bienes de consumo tienen distintas utilidades marginales para diferentes consumidores, o incluso para el mismo consumidor en circunstancias diferentes) y por lo tanto no se puede determinar midiendo cuánto trabajo llevó la producción de una unidad de un bien. Mientras que Marx enfatizó la maximización del beneficio, los economistas neoclásicos vieron en la maximización de la utilidad a un nivel individual o social la fuerza motriz de la economía. Así, por ejemplo, Léon Walras desarrolló una teoría del valor basada en la escasez y Eugen von Böhm-Bawerk afirmó que el plusvalor o interés surge de la diferencia entre el valor presente y el valor futuro de los bienes. En consecuencia, los economistas neoclásicos se centraron tanto en los mercados y los precios de mercado, por lo que consideraron la noción de valor como algo metafísico, irreal e innecesario (y tal vez motivado políticamente) en la economía. Jevons trató de compatibilizar su teoría marginalista con la posición clásica, donde las mercancías se intercambian en cualquier mercado en proporción a las cantidades producidas por la misma cantidad de trabajo y los artículos se intercambiarán en cantidades inversamente al coste de producción de las porciones más costosas, pero en última instancia afirma que el trabajo puede medirse en términos de valor marginal, ya que en equilibrio la utilidad marginal iguala al valor marginal del trabajo. El hecho es, afirmó Jevons, que el trabajo, una vez gastado, no tiene ninguna influencia en el valor futuro de ningún artículo. Alfred Marshall postuló una teoría híbrida donde el valor depende del coste y de la utilidad, de la oferta y de la demanda. Marshall argumentó que su análisis marginalista no socavaba la teoría del valor-trabajo porque a largo plazo los productores cambian entre sectores que persiguen rendimientos anormalmente altos y huyen de rendimientos anormalmente bajos de sus inversiones, de modo que las condiciones de oferta determinan el precio. Propuso que en un contexto competitivo, el precio (y por ende el valor económico) no se pueden determinar considerando exclusivamente los procesos e individuos implicados en la producción de los bienes, sino también teniendo en cuenta a aquellos que los compran, y los fenómenos relacionados con su consumo. En otras palabras, la ley de la oferta y de la demanda afirman que los precios de los bienes son el resultado de una interacción y una medida de cuán arduo es para la sociedad ofertarlos y cuán útiles son para su consumo. Es una teoría imperfecta debido a la inexistencia de competencia perfecta, sin embargo esta aproximación se acerca mucho a la realidad en mercados no monopolizados.[cita requerida] Según óptimo de Pareto, por otra parte, las relaciones de intercambio entre bienes no sólo están determinadas por sus utilidades marginales, sino también por las productividades marginales de los factores de producción disponibles. El óptimo de Pareto se define como una situación en la que se maximiza la utilidad y al mismo tiempo todos los factores de producción se emplean de la manera más eficiente, conduciendo a una situación en donde todos los bienes se intercambian según sus utilidades marginales y las cantidades marginales de los factores productivos necesarios para producirlos. Eso significa que, en el marginalismo, los bienes se intercambian a la cantidad marginal de trabajo necesario para producirlos. En este sentido, se puede hablar de una teoría del valor marginal de los inputs de trabajo. Sin embargo, esto se aplica a todos los factores de producción y a la utilidad marginal. El trabajo no es nada especial. Que estas teorías del valor se pueden mantener al mismo tiempo es posible gracias al análisis marginalista. Si se pudiera corroborar empíricamente, el intercambio de bienes se lleva a cabo de acuerdo con la cantidad marginal de inputs de trabajo necesarios, lo cual confirmaría la teoría marginalista. Eso contradiría la teoría marxista, ya que según Marx los cocientes de intercambios están determinados por los precios de producción, que son diferentes de las cantidades de trabajo necesarias, es decir de su valor-trabajo. De forma implícita, Marx niega que el capitalismo sea un óptimo paretiano. Economistas marxistas y neorricardianos han criticado a su vez la aplicación y consistencia teórica del marginalismo (véase Controversia de Cambridge). Ernest Mandel consideró el marginalismo como coherente pero divorciado de la realidad, que no logra comprender estadísticamente o a fortiori, explicar en sus leyes de desarrollo. Paul Mattick sostuvo que el marginalismo como surgió como apología del capitalismo y justificación de las diferenciaciones de clase e ingresos imperantes. Según Fred Moseley, la teoría económica neoclásica se desarrolló, en parte, para atacar la noción misma de plustrabajo o plusvalía y para argumentar que los trabajadores reciben todo el valor incorporado en sus esfuerzos creativos. Defensores modernos de la teoría del valor-trabajo replicarían cómo el capitalismo solamente reconoce la demanda respaldada por el dinero, (el precio de un bien no sólo se mide por su utilidad sino también por la cantidad de dinero que tienen los consumidores). Eso depende del conjunto de relaciones de distribución preexistentes. Dichas relaciones de distribución se apoyan a su vez sobre un conjunto de relaciones de producción que determinan cómo obtienen dinero los consumidores, cómo obtienen beneficios los capitalistas, los obreros salarios, lo terratenientes rentas, y así sucesivamente. En consecuencia, el precio de un bien depende no solamente de su utilidad, sino de la cantidad de dinero que los diferentes consumidores poseen, de ahí sus diferentes demandas efectivas. Sin embargo, no está claro en absoluto si esto difiere de un efecto riqueza sobre la demanda derivado de un problema de maximización de la utilidad individual. En microeconomía, la maximización de la utilidad se produce teniendo presentes ciertas restricciones presupuestarias, que son las debidas a la cantidad disponible de factores de producción, por ejemplo, el trabajo (así con Marx la maximización del beneficio se produce bajo la restricción de las técnicas de producción disponibles y la tasa salarial). De hecho, la última restricción es el tiempo. Los hogares dividen su tiempo (24 horas) en ocio y trabajo. El tiempo de trabajo se dedica a ganar dinero para comprar bienes de consumo. El hogar elige la cantidad de tiempo libre (a través de las horas de trabajo) y la cantidad de bienes de consumo que maximizan su función de utilidad. Con Marx, el tiempo de trabajo no se basa en una decisión libre de los hogares, sino del producto de la lucha de clases entre trabajadores y capitalistas; los primeros tratan de reducir las horas de trabajo y los segundo de incrementarlas. Más aún, todo lo anterior no considera los efectos del proceso de acumulación. Dentro del marxismo analítico se ha intentado demostrar las tesis de Marx sin apelar a la teoría del valor-trabajo. John Roemer usó la elección racional y la teoría de juegos en su Teoría general de la explotación y de las clases para demostrar cómo la explotación y las relaciones de clase pueden presentarse en el desarrollo de un mercado laboral. Por otro lado también se ha intentado demostrar la teoría del valor trabajo a partir de las tesis marginalistas. Por ejemplo, Klaus Hagendorf defiende que bajo competencia perfecta los precios relativos son iguales a la relación entre los valores marginales del trabajo. Los intentos de compatibilizar el marginalismo con el marxismo fueron tildados por Engels de un marxismo disfrazado de economía vulgar, un socialismo vulgar. Transformación de valores y precios Artículo principal: Problema de la transformación La teoría del valor-trabajo predice que las ganancias serán mayores en las industrias intensivas en mano de obra que en las industrias intensivas en capital, lo que se contradiría con los datos empíricos. También podemos notar que no todos los productos tienen las mismas proporciones de valor agregado por capital amortizado. Las industrias intensivas en capital, como las finanzas, pueden tener una gran contribución de capital, mientras que las industrias intensivas en mano de obra, como la agricultura, tendrían una relativamente pequeña. Pero de acuerdo con Smith y Marx, hay una tendencia por la que se igualan las tasas de beneficio en el proceso de acumulación. Marx explicó en el tomo III de El capital por qué las ganancias no se distribuyen de acuerdo con las industrias que son más intensivas en mano de obra y por qué esto es consistente con su teoría de los precios de producción. Si el precio de un bien está por encima de su precio de producción, entonces los capitalistas de ese sector obtiene una plusganancia (la tasa de beneficio supera a la tasa de beneficio medio de la economía en su conjunto). Como resultado, se atrae capital al sector, la producción aumenta y los precios caen hasta que el beneficio extraordinario desaparece. Los precios de producción resultantes se basan en horas de trabajo a través de la transformación del valor del trabajo. Sin embargo, ahora las proporciones de estos precios de producción deben divergir de sus proporciones de trabajo. Si esto es o no consistente con la teoría del valor-trabajo, tal como se presenta en el tomo I, ha sido un tema de debate. Los críticos argumentan que esto convierte al LTV en una teoría macroeconómica, cuando se suponía que explicaba las relaciones de intercambio de las mercancías individuales en términos de su relación con sus proporciones de trabajo, lo que sería una teoría microeconómica. Por lo tanto, críticos como Böhm-Bawerk sostuvieron que la solución propuesta por Marx a la gran contradicción no era tanto una solución como una elusión de la cuestión. Economistas marxistas como Friedrich Engels y Ronald L. Meek consideraron que los valores pertenece a un nivel de abstracción más alto que los precios. Esta visión ha llevado a economistas como Conrad Schmidt, Wilhelm Lexis, Eduard Bernstein, Piero Sraffa, Paul Samuelson, Nobuo Okishio y Michio Morishima de que la TvT es parasitaria, centrándose en los precios y la explotación únicamente en su lugar (véase Teorema fundamental marxiano). Mano de obra y explotación Steve Keen argumenta que la idea de Marx de que sólo el trabajo puede producir valor se basa en la idea de que a medida que el capital se deprecia sobre su uso, entonces esto está transfiriendo su valor de cambio al producto. Keen argumenta que no está claro por qué el valor de la máquina debería depreciarse al mismo ritmo que se pierde. Keen utiliza una analogía con el trabajo: si los trabajadores reciben un salario de subsistencia y la jornada laboral agota la capacidad de trabajar, se podría argumentar que el trabajador se ha depreciado en una cantidad equivalente al salario de subsistencia. Sin embargo, esta depreciación no es el límite de valor que un trabajador puede agregar en un día (de hecho, esto es fundamental para la idea de Marx de que el trabajo es fundamentalmente explotado). Si así fuera, entonces la producción de un excedente sería imposible. Según Keen, una máquina podría tener un valor de uso mayor que su valor de cambio, lo que significa que podría, junto con el trabajo, ser una fuente de excedente. Keen afirma que Marx estuvo a punto de llegar a tal conclusión en los Grundrisse, pero nunca la desarrolló más. Keen observa además que, si bien Marx insistía en que la contribución de las máquinas a la producción es únicamente su valor de uso y no su valor de cambio, rutinariamente trataba el valor de uso y el valor de cambio de una máquina como idénticos, a pesar del hecho de que esto contradiría su afirmación de que los dos no estaban relacionados. Los marxistas responden argumentando que el valor de uso y el valor de cambio son magnitudes inconmensurables; afirmar que una máquina puede añadir más valor de uso del que vale en términos de valor es un error de categoría. Según Marx, una máquina, por definición, no puede ser una fuente de trabajo humano. Keen responde argumentando que la teoría del valor trabajo solo funciona si el valor de uso y el valor de cambio de una máquina son idénticos, ya que Marx argumentó que las máquinas no pueden crear plusvalía ya que su valor de uso se deprecia junto con su valor de cambio; simplemente lo transfieren al nuevo producto, pero no crean ningún valor nuevo en el proceso. El argumento de la maquinaria de Keen también se puede aplicar a los modos de producción basados en la esclavitud, que también se benefician de extraer más valor de uso de los trabajadores del que devuelven a los trabajadores. Además, el economista Joseph Alois Schumpeter señaló un par de cuestiones que, en su opinión, socavaban la validez de la teoría del valor-trabajo. En primer lugar, escribió que la teoría del valor-trabajo no tenía en cuenta las diferencias intrínsecas en la calidad del trabajo entre los individuos (una diferencia que, según él, no podía encapsularse adecuadamente mediante el uso de un multiplicador de valor). Además, afirma que la teoría del valor-trabajo, tanto en sus formulaciones marxistas como ricardianas, implicaría que el trabajo fuera el único insumo en una economía, junto con que todo el trabajo fuera de naturaleza homogénea, una tesis que Schumpeter descarta como poco realista y que de todos modos podría ser resuelta por el marginalismo. Schumpeter continúa desviando su atención hacia la supuesta naturaleza autocontradictoria de cómo la teoría del valor-trabajo permite la justificación de la tesis marxista de la explotación, destacando que el trabajo en sí mismo no podía ser valorado ya que no era producido por ningún trabajo y que la acumulación de plusvalía descrita por Marx no podía ocurrir de forma estática, mercado perfectamente competitivo. Así, aunque le da a Marx el crédito por ver la necesidad de cambio inherente a los mercados capitalistas, Schumpeter concluye que la teoría del valor-trabajo y sus consecuencias siguen siendo teorías problemáticas. Algunos economistas postkeynesianos han sido muy críticos con la teoría del valor-trabajo. Joan Robinson, quien a su vez era considerada una experta en los escritos de Marx, escribió que la teoría del valor-trabajo era en gran medida una tautología y un ejemplo típico de la forma en que operan las ideas metafísicas. Heinz D. Kurz señaló la heterogeneidad de las diversas mercancías y del trabajo mismo como problemático a la hora de buscar un sustancia común. Economía ecológica En la economía ecológica se ha criticado la teoría del valor-trabajo, donde se argumenta que el trabajo es en realidad energía a lo largo del tiempo. Tales argumentos generalmente no reconocen que Marx está indagando en las relaciones sociales entre los seres humanos, que no pueden reducirse al gasto de energía, al igual que la democracia no puede reducirse al gasto de energía que hace un votante para llegar al lugar de votación. Sin embargo, haciéndose eco de Joan Robinson, Alf Hornborg, un historiador ambiental, argumenta que tanto la dependencia de la teoría del valor energético como la teoría del valor trabajo son problemáticas, ya que proponen que los valores de uso (o la riqueza material) son más reales que los valores de cambio (o la riqueza cultural), sin embargo, los valores de uso están determinados culturalmente. Para Hornborg, cualquier argumento marxista que afirme que la riqueza desigual se debe a la explotación o al pago insuficiente de los valores de uso es en realidad una contradicción tautológica, ya que necesariamente debe cuantificar el pago insuficiente en términos de valor de cambio. La alternativa sería conceptualizar el intercambio desigual como una transferencia neta asimétrica de insumos materiales en la producción (por ejemplo, mano de obra incorporada, energía, tierra y agua), en lugar de en términos de un pago insuficiente de insumos materiales o una transferencia asimétrica de valor. En otras palabras, el intercambio desigual se caracteriza por la inconmensurabilidad, a saber: la transferencia desigual de insumos materiales; juicios de valor contrapuestos sobre el valor de la mano de obra, el combustible y las materias primas; la diferente disponibilidad de tecnologías industriales; y la descarga de las cargas ambientales sobre los menos recursos."
ksampletext_wikipedia_econ_microeconomia: str = "Microeconomía. La microeconomía es una rama de la teoría económica que estudia el comportamiento de los agentes económicos individuales, como los consumidores, las empresas, los trabajadores y los inversores, así como su interacción en los mercados. Analiza las decisiones que cada agente toma para alcanzar determinados objetivos y cómo dichas decisiones afectan la oferta y demanda de bienes y servicios, la producción, la fijación de precios y el equilibrio del mercado. Los elementos fundamentales del análisis microeconómico incluyen los bienes, los precios, los mercados y los agentes económicos. En contraposición, la macroeconomía se ocupa del estudio global de la economía, analizando las variables agregadas como el producto total de bienes y servicios, el nivel de empleo, la balanza de pagos, el tipo de cambio y el comportamiento general de los precios. Distinción entre Microeconomía y Macroeconomía Artículo principal: Historia del pensamiento microeconómico La diferencia entre microeconomía y macroeconomía fue introducida en 1933 por el economista noruego Ragnar Frisch, co-receptor del primer Premio Nobel de Ciencias Económicas en 1969. Sin embargo, Frisch en realidad no usó la palabra microeconomía, sino que hizo distinciones entre análisis micro-dinámico y macro-dinámico de una manera similar a como se usan hoy en día las palabras microeconomía y macroeconomía. El primer uso conocido del término en un artículo publicado fue Pieter de Wolff en 1941, quien amplió el término micro-dinámica a microeconomía. Ramas El modelo de oferta y demanda describe como varían los precios según el balance entre disponibilidad del producto a diferentes precios (oferta) y los deseos de aquellos con poder adquisitivo según el precio (demanda). La gráfica muestra un desplazamiento a la derecha de D1 a D2 con el correspondiente incremento en el precio y en la cantidad requerida para alcanzar un nuevo punto de equilibrio en el mercado en la curva de oferta (S). La microeconomía tiene varias ramas de desarrollo de las cuales destacan el equilibrio parcial, el equilibrio general y el equilibrio de Nash para entender los problemas del consumidor y del productor en mercados de bienes y servicios. Estas ramas o subdisciplinas no pueden considerarse enteramente separadas porque los resultados de unos aspectos influyen sobre los otros (en particular la teoría del equilibrio general habla de la interacción entre ellas). Por ejemplo, las empresas no solo ofertan bienes y servicios, sino que también demandan bienes y servicios para poder producir los suyos. La microeconomía propone modelos matemáticos que desarrollan ciertos supuestos sobre el comportamiento de los agentes económicos, las conclusiones a la que se llegue usando esos modelos solo será válida, en tanto en cuanto, se cumplan los supuestos, cosa que ocurre rara vez, especialmente si se trata de supuestos muy fuertes o restrictivos. Una de las incorporaciones más importantes al estudio de la microeconomía es el equilibrio de Nash, como concepto de solución a los juegos planteados. La teoría de juegos es una teoría matemática que estudia el comportamiento de varios agentes cuando las decisiones tomadas por cada uno influyen en las decisiones de los otros. A pesar de los éxitos de teoría microeconómica neoclásica en el entendimiento de algunos de nuestros problemas económicos, no los comprende todos porque en estos modelos están prácticamente ausentes las relaciones institucionales de las sociedades, más allá de los mercados de bienes y servicios. Al valorar sus alcances y límites, se podría decir que la teoría neoclásica es como el primer polinomio de Taylor, es una primera aproximación, valiosa pero insuficiente. Hay esperanzas de que los avances recientes en microeconomía institucional y microeconomía de economías complejas proporcionen una mejor comprensión sobre el rol de las instituciones, la evolución y el aprendizaje en el comportamiento de los agentes dentro y fuera de los mercados. Teoría del consumidor Artículo principal: Teoría del consumidor Esta sección es un extracto de Teoría del consumidor.[editar] La teoría del consumidor es la rama de la microeconomía, que estudia la conducta de los agentes económicos, en cuanto consumidores y cómo asignan su renta a la compra de diferentes bienes y servicios. La teoría relaciona las preferencias y las restricciones presupuestarias a las curvas de demanda del consumidor. Las decisiones de los consumidores sirven para entender cómo afectan las variaciones de la renta y de los precios a la demanda de bienes y por qué las demandas de algunos productos son más sensibles que otras a las variaciones de los precios y de la renta. En lo que sigue y, a menos que luego se diga lo contrario, todo lo dicho se referirá a la teoría del consumidor a la teoría neoclásica dominante. La teoría del consumidor neoclásica parte de unas preferencias que tiene un individuo. Con base en ellas el consumidor realizará una elección racional entre los bienes disponibles y los que puede adquirir con el presupuesto que tiene. Preferencias del consumidor Artículo principal: Preferencia Véase también: economía conductual Los consumidores tienen preferencias sobre los bienes y servicios, esto es, dadas cestas de bienes (conjuntos de bienes y servicios que un individuo consume. en las que, de cada tipo de bien puede ser cero, uno u otra cantidad de bienes, incluso una cantidad no entera), si le dieran a escoger entre dos cestas, un consumidor preferirá una sobre la otra (también puede ser indiferente entre ellas). Se supone entonces, que para la mayoría de los consumidores habrá unas preferencias que podrían manifestar para cualquier conjunto de cestas que se les presentara. Cada individuo tendrá sus preferencias y no tendrían por qué coincidir con las de otro sujeto, aunque pudiera ser así. Sin embargo, se espera que para la mayoría de los consumidores esas preferencias sí que tengan unas propiedades comunes. La teoría de la conducta del consumidor se basa en estos supuestos o propiedades básicas de las preferencias de los individuos por una cesta frente a otra, que se exponen. De manera que si estas condiciones no se cumpliesen, las conclusiones extraídas en la teoría del consumidor no serían plenamente válidas. Estas propiedades serían: Completitud: el consumidor podría clasificar todo los tipos de cestas, es decir todos los conjuntos de indiferencia no tienen fisuras. Universalidad: Dado cualquier par de cestas imaginable en una economía, un consumidor siempre podría decir si prefiere una cesta a otra. Nótese que es posible también que no pueda considerar a una cesta realmente mejor que la otra, pero se espera que pueda decir que una cesta es al menos tan buena como la otra. Es decir, no se necesitará que la preferencia sea siempre estricta, sino que dadas cualquiera dos cestas, el consumidor pueda siempre decir, o bien que lo mismo le da la una que la otra, o que considera una de las dos mejor que la otra. Transitividad: Generalmente, si un consumidor prefiere la cesta A a la cesta B, y la cesta B a la C, también debería preferir la cesta A a la C. Monotonicidad o cuanto más, mejor: Si una cesta A tiene los mismos bienes que otra cesta B, y alguno más, o bien mayor cantidad de alguno de ellos, entonces A se prefiere o se considera al menos tan buena como B. En consecuencia, los consumidores siempre prefieren una cantidad mayor de cualquier bien a una menor. Convexidad: Se espera, aunque este supuesto es algo restrictivo, que dadas dos cestas A y B de bienes, se prefiera a ambas una cesta C que fuera una combinación convexa de ambas. Es decir, una cesta que se compusiera en un porcentaje de las cantidades de cada uno de los bienes presentes en A y en el resto del porcentaje (hasta completar el 100%) de las cantidades de los bienes de B. Este supuesto está relacionado con el principio de utilidad marginal decreciente. Restricción presupuestaria Artículo principal: Recta de balance Teniendo en cuenta que los bienes tienen precios, y considerando estos datos, está claro que un consumidor no puede conseguir trivialmente la cesta que prefiera de entre todas las posibles. Si tenemos en cuenta además de los precios de los bienes la renta disponible del consumidor, tenemos lo que se llama la restricción presupuestaria. Esta es la que nos indica qué cestas de bienes son las que el consumidor puede elegir y conseguir, teniendo en cuenta el dinero de que dispone y los precios del mercado. La misión del consumidor será entonces conseguir de entre todas esas cestas aquella que prefiera a todas las demás (o alguna de las cestas que considere son al menos tan buenas como todas las demás). Encontrar esto es lo que se llama maximización del consumidor. Generalmente, es habitual que la cesta elegida del consumidor se encuentre en la frontera de la restricción presupuestaria, es decir, que sea una cesta cuyo valor (multiplicando los precios de los bienes por las cantidades de estos en la cesta) sea exactamente igual a la renta disponible del consumidor. Por tanto, el consumidor siempre elegirá la cesta que le proporcione la máxima utilidad, la que le produzca el mayor bienestar posible. Función de utilidad Artículos principales: Función de utilidad y Utilidad marginal. Una forma de representar las preferencias, cuando éstas tienen las propiedades adecuadas, es mediante lo que se llama una función de utilidad. En este caso, las canastas de bienes se pueden representar también como vectores numéricos, donde cada componente del vector nos dice qué cantidad de cada bien hay en esa cesta. Introduciendo dos vectores de bienes en una misma función de utilidad y viendo qué números nos devuelve esta, es posible ver si una canasta es preferida a la otra o considerada como igual a la otra desde el punto de vista del consumidor. Entonces, el problema del consumidor podría considerarse como el problema matemático de maximizar una función matemática (a menudo de varias variables), que sería la función de utilidad, dentro del conjunto representado matemáticamente por todas las canastas de bienes (vectores) que cumplieran la restricción presupuestaria, esto es, que su valor (resultado de multiplicar el vector de bienes de la canasta por el vector de los precios correspondientes) fuera igual o menor que el valor de la renta disponible. Nótese que la función de utilidad se considera una función monótona creciente de los bienes, pero que su valor es puramente ordinal, esto es, sirve para ordenar canastas , pero no para decir cuánto es mejor una canasta que otra, esto es, no es una función cardinal. De hecho, pueden usarse distintas funciones de utilidad para representar unas mismas preferencias, y al resolver el problema de maximización todas darían el mismo resultado. Curvas de indiferencia Artículo principal: Curva de indiferencia Esta sección es un extracto de Curva de indiferencia.[editar] Figura 1. Un ejemplo de mapa de indiferencia con dos bienes elegibles X e Y y tres curvas de indiferencia, es decir, tres niveles de satisfacción distintos. En microeconomía las curvas de indiferencia (también llamadas curvas de preferencia) son combinaciones de bienes, representadas como conjuntos de puntos en el espacio, para los cuales la satisfacción de un consumidor es idéntica. Esto quiere decir que para todos los puntos pertenecientes a una misma curva, el consumidor no tiene preferencia sobre una combinación u otra. La satisfacción del consumidor se caracteriza mediante la función de utilidad, donde las variables son las cantidades de cada bien, representadas por el valor sobre cada eje. Las preferencias del consumidor le permiten elegir entre diferentes canastas de consumo. Si se ofrece a un consumidor dos canastas diferentes, elegirá la que mejor satisface sus gustos. Si ambas satisfacen sus gustos en igual medida, se dice que el consumidor es «indiferente» entre las dos canastas. Existen discrepancias entre autores sobre si la continuidad, derivabilidad y convexidad de dichas curvas están garantizadas y ello tiene fuertes implicaciones en la discusión sobre la existencia o no de puntos de equilibrio. Desde un punto de vista matemático la discusión implica el axioma de elección. Figura 2. Deducción de las curvas de indiferencia. En primer lugar se muestra que la utilidad marginal es decreciente respecto a las dos variables (requisito para que las curvas sean convexas y exista equilibrio). A continuación se crea una representación de la función en la que Z sea la utilidad. Finalmente se proyectan las curvas de nivel en el plano XY. La representación gráfica más habitual presenta dos bienes alternativos X e Y (Figura 1) entre los cuales el consumidor puede elegir. Los puntos de cada curva representan las combinaciones de bienes X e Y que proporcionan la misma utilidad al consumidor. Otra cuestión de importancia en el estudio de la teoría del consumidor son las llamadas curvas de indiferencia. Una curva de indiferencia representaría a todas las cestas que para una función de utilidad dada tienen el mismo valor. La principal utilización de las curvas de indiferencia es encontrar los puntos de maximización de la utilidad al superponerlas con las restricciones presupuestarias del consumidor, que define los puntos al alcance de cada individuo dependiendo de su disponibilidad en unidades monetarias. Por otro lado la relación marginal de sustitución nos informa de cuanto es capaz de intercambiar un consumidor de un bien por otro de manera que su utilidad se mantenga igual. Tipos de bienes Artículo principal: Bien económico Se puede estudiar cómo cambian las soluciones al problema del consumidor cuando cambian los parámetros de la función de utilidad o bien cambian los precios o la renta disponible del consumidor. Por ejemplo, si cambia el precio de uno de los bienes, el cambio en la pendiente de la restricción presupuestaria llevará a cambiar de cesta de bienes escogida, en la que el bien cuyo precio ha cambiado, también cambiará en cantidad (y posiblemente las de otros de los bienes también cambien). Según el efecto que se produzca, se puede clasificar a los bienes. Así, normalmente los bienes disminuyen en cantidad demandada cuando aumenta su precio, aunque existen excepciones a esto, en las que aumentan (llamados bienes giffen). Lo que hace que un bien cambie es la suma de dos efectos, el efecto renta y el efecto sustitución. El efecto renta es el derivado del hecho de que al aumentar un precio, en cierto modo, es como si se perdiera renta, mientras que el efecto sustitución está relacionado con como el consumidor puede tender a sustituir el consumo de un bien por el de otro. Si aumenta el precio del bien, el efecto renta tenderá a hacer que disminuya su consumo, pero el efecto sustitución puede afectarle de dos maneras. Normalmente tenderá a hacer que también disminuya, porque el consumidor también vaya a consumir otro tipo de bienes que su precio no haya cambiado, pero en otras ocasiones podría ser que hiciera que aumentara. Nombrando lo anterior en términos marshalianos, podemos decir que se sustituye el valor de la mercancía sucedida por dinero equivalente, logrando así, que el consumidor tenga el mismo nivel de satisfacción con una curva diferente. En este último caso tendríamos lo que se llama un bien inferior (uno cuyo efecto sustitución tiende a aumentar el consumo cuando el precio sube). Si, en cambio, el efecto de sustitución fuera del mismo signo que el efecto renta, estaríamos ante un bien normal. Pero es la suma de los dos efectos lo que produciría el efecto total. En el caso de los bienes normales, el efecto renta hará que su consumo disminuya al aumentar el precio, y también ocurrirá así con los bienes inferiores, excepto cuando, en el caso de algunos de estos últimos, el efecto sustitución llegara a ser más fuerte que el del efecto renta, y por tanto tendríamos un bien giffen. Cuando aumenta la renta y los precios permanecen constantes, los bienes normales tienden a aumentar en consumo mientras que disminuye el de los bienes inferiores. Nótese que hemos mencionado que cuando sube el precio bajará el consumo de un bien, el análisis es completamente simétrico cuando baje el precio, es decir, aumentará el consumo con las particularidades ya dichas en los párrafos anteriores. Se ha de saber también que el consumo, por supuesto, también variará con la renta disponible, aumentando o disminuyendo conforme lo haga esta, hasta que se alcance para los bienes lo que se llama punto de saciedad, que sería el máximo posible para la función de utilidad, un punto más allá del cual al consumidor ya no le interesaría tener más de ninguno de los bienes. Otra forma en que se relacionan los bienes unos con otros es como complementarios o como sustitutivos. Los complementarios tienden a compartir el mismo destino cuando sube o baja el precio de uno de ellos, mientras que es al contrario en el caso de los sustitutivos. También es posible considerar algunos bienes como males, cuyo consumo produce desutilidad o utilidad negativa. Los males serían aquellos de los cuales al consumidor, al contrario que los otros, estaría interesado en tener lo menos posible. Por ejemplo, en ciertos análisis microeconómicos se puede presentar el salario como un bien y el trabajo como un mal y tener que estudiar la decisión de optimizar el tiempo teniendo en cuenta la restricción, es decir, más horas de trabajo (mal) producen más salario (bien) y el límite, restricción presupuestaria, es el tiempo disponible por un trabajador hipotético. Véanse también: Bienes complementarios, Bienes sustitutivos, Bien normal, Bien inferior y Bien de Giffen. Curva de demanda Artículo principal: Curva de la demanda La teoría de la demanda puede derivarse de la del consumidor, esto es, agregando las demandas individuales de un bien y viendo cuánto sería el total demandado para cada precio por cada consumidor. Esto nos llevaría a la curva de demanda total del bien, que generalmente se representa como una curva descendente (pendiente negativa), debido a que en el eje de ordenadas se representa el precio, y en el de abscisas la cantidad de bien demandada. Significa que cuanto menor es el precio, mayor es la cantidad demandada. La fórmula matemática simplificada que resume este concepto, que expresa la demanda como una recta es la siguiente: Véanse también: Ley de la oferta y la demanda, Efecto renta, Efecto sustitución, Elasticidad precio de la demanda y Elasticidad cruzada de la demanda. Representación matemática del problema del consumidor La microeconomía se estudia de forma matemática, usando modelos que eviten la ambigüedad del lenguaje hablado. La mayor parte de los desarrollos y estudios de la teoría del consumidor tienen como base el siguiente problema que se representa así: El significado de este problema es el siguiente: Se trata de maximizar, esto es, obtener el valor máximo de una función, el más alto de todos los que puede dar, así como qué valores son los que producen ese máximo. En este caso sería el de Este modelo se resuelve usando una técnica matemática llamada de los Multiplicadores de Lagrange (si se supone que finalmente se consumirá toda la renta disponible, lo que equivale a suponer que Las soluciones que se obtienen nos sirven para el análisis anteriormente dicho, para obtener cómo reaccionarían las cantidades demandadas si cambiaran los precios, y es posible estudiar también, mediante modificaciones a este problema básico, qué ocurre si se introducen impuestos sobre la renta, impuestos indirectos, subvenciones, que sucedería si consideramos el ahorro como un bien, que ocurre si consideramos también bienes cuyo valor es incierto (como en el caso de activos financieros), etcétera, y ver cómo influyen no solo sobre la cantidad de bien consumida sino también sobre la utilidad que recibe el consumidor. Teoría del productor Artículo principal: Teoría de la producción En microeconomía, la producción es simplemente la conversión de factores productivos en productos y una empresa es cualquier organización que se dedica a la planificación, coordinación y supervisión de la producción. La empresa es el agente de decisión que elige entre las combinaciones factores-producto de las cuales dispone y maximiza su beneficio. El problema comparte similitudes, con el del consumidor. En el caso del consumidor, la microeconomía lo reduce a menudo a la cuestión de maximizar una función de utilidad con una restricción presupuestaria. En el caso de la producción, se trata de maximizar la función de beneficios teniendo en cuenta restricciones tecnológicas (suponiendo, en principio, que los precios están dados, supuesto este muy fuerte que posteriormente se relaja). Función de producción Artículo principal: Función de producción Se empieza considerando, por razones de simplificación, que se produce un solo bien (o servicio) por una empresa y que para producirlo es necesario una serie de elementos denominados factores de producción (también pueden ser denominados insumos o inputs). El bien o servicio producido recibe el nombre de output. La función que relacionaría las cantidades de la cantidad de factores productivos utilizados con el output obtenido recibe el nombre de función de producción. Los inputs utilizados serían las materias primas, productos intermedios (comprados a otra empresa u obtenidos en otro proceso de producción de la misma empresa), el trabajo humano usado, los suministros de energía, agua y similares, el coste de reponer el capital utilizado, maquinaria, herramientas), ya que sufre desgaste por el uso en el proceso de fabricación. Una simplificación frecuente es reducir a dos los factores: capital y trabajo. Trabajo representaría el trabajo humano, capital el resto. Las funciones de producción también pueden tener una serie de propiedades que conviene destacar. Una de ellas es la de lo que se llaman Economías de escala. Véanse también: Isocuanta, Isocoste, Senda de expansión de la producción y Eficiencia productiva. El problema de maximización del beneficio Artículo principal: Maximización del beneficio El estilo de esta traducción aún no ha sido revisado por terceros. Si eres hispanohablante nativo y no has participado en esta traducción puedes colaborar revisando y adaptando el estilo de esta u otras traducciones ya acabadas. Generalmente el problema de maximización del beneficio se puede estudiar tanto a corto plazo como a largo plazo. A corto plazo se considera que uno de los inputs, como el capital, está ya decidido para la empresa y el precio por el mismo se ha pagado ya. A largo plazo, sin embargo, todos los inputs implicados pueden variar, por ejemplo, si la empresa varía la cantidad de capital disponible. Este problema se puede representar de forma matemáticamente de la siguiente manera: max con Donde además deben tenerse en cuenta las condiciones de uso de los inputs adquiridos por la empresa. [Pueden ser reescritas para algunos outputs, ver más adelante en (*)] El objetivo de la empresa es maximizar su beneficio, que es la diferencia entre los ingresos y los costes. Los ingresos totales son iguales a los outputs producidos por los precios a los que se venden (nótese que suponemos que se vende toda la producción de la empresa, cosa que no es siempre el caso en la realidad), y los costes son los de multiplicar los inputs utilizados por los precios de los outputs. Ahora bien, las restricciones serían que los outputs son función (de producción) de las cantidades de cada uno de los inputs utilizados, incluso si un input no se utilizara, se podría considerar que la cantidad utilizada de ese input es cero. (*) Si, por ejemplo, se usara del input de tipo 1 en la producción de los distintos outputs posibles, la suma del total de lo utilizado para cada uno de los outputs debería ser igual al total del input 1 adquirido por la empresa (Es decir, si usa x11 del input 1 para fabricar el output 1, x21 para fabricar del output 2, etcétera, entonces, x11+...+xn1=X1, y X1 sería el total del input 1 utilizado). No obstante, y esto es importante, en algunos casos es posible que al usar de algunos inputs no se consuman al usarlos en la fabricación de ciertos outputs, por lo que podría ser que estas condiciones no estuvieran escritas así. Por ejemplo, si consideráramos el tiempo de trabajo, en horas, de cierta máquina como un input, y esa máquina pudiera elaborar dos tipos o más de output al mismo tiempo, no se introduciría la restricción correspondiente en este modelo, es decir, si por ejemplo la máquina trabajara 8 horas haciendo dos outputs diferentes al mismo tiempo, no repartiría las 8 horas entre cada uno de ellos sino que las invertiría enteras en cada uno. Este problema se puede resolver también usando los Multiplicadores de Lagrange o la condiciones de Karush-Kuhn-Tucker. Curvas de costos Este artículo o sección necesita referencias que aparezcan en una publicación acreditada. Busca fuentes: «Microeconomía» – noticias · libros · académico · imágenes Artículo principal: Curva de costo Una forma habitual de simplificar el problema es suponer que sólo se produce un bien y que sólo va a haber un input que varíe según la producción de la empresa, estando todos los demás fijos (Nota: En un modelo determinado, suponer que un conjunto de variables puede cambiar mientras que el resto de variables van a permanecer constantes, independientemente de sus relaciones con el resto del modelo, es lo que se llama céteris páribus, una técnica simplificadora pero que puede llevar a error cuando se compara con la realidad, en la que en última instancia todo se relaciona e influye con todo) Con esto, por ejemplo, se puede estudiar cómo la producción de una empresa de un bien va a determinar la demanda de trabajo por parte de esa empresa. El valor total de los inputs por los que la empresa ha pagado ya y que no van a variar en el corto plazo daría lo que se llama el coste fijo. Por el contrario, el valor de los inputs que cambiará según se decida el nivel de producción, es el coste Variable. La suma de los dos es el coste Total. Como conforme varíe la producción de la empresa estos costes van a variar, se obtiene para el estudio microeconómico lo que se llaman curvas de costes. Las más importantes, serían la de coste variable, la de coste total, la de Coste Medio y la de coste marginal. La curva de coste variable relaciona el total de los costes variables con el nivel de producción. Generalmente es creciente, pero puede tender a crecer a menor velocidad. La de coste total es prácticamente idéntica, ya que sería una traslación de la variables en la magnitud del coste fijo, lo cual es importante sobre todo en teoría de la producción industrial porque unos costes fijos elevados disuaden a empresas de entrar en el mercado. La curva de coste medio, por el contrario, puede ser ascendente o descendente, incluso ascendente en unos tramos y descendente en otros, ya que esta curva informa de cuanto, por término medio, cuesta producir cada output dependiendo del nivel de producción. Por ejemplo, es posible que con cierta función de producción el valor de producir 300 unidades de output sea tal que cada una cueste 1.5 unidades monetarias, mientras que producir 600 pueda costar cada una sólo 1 unidad monetaria. Esto estaría relacionado posiblemente con la existencia de economías de escala, como se dijo antes. La curva de coste marginal, tiene para el análisis gran importancia, razón por la que a veces se llama a ciertos estudios de la economía marginalistas. Esta curva, que matemáticamente equivale a la derivada de la Curva de Coste Total, nos representa cuanto más nos cuesta producir una unidad de output a partir del nivel anterior de producción. Por ejemplo, si para producir 100 unidades de un bien tenemos un coste de 1000 unidades monetarias, y producir 101 unidades de ese mismo bien el coste fuera de 1020 unidades monetarias, la curva valdría 20 (1020-1000) en el nivel 100 de producción. El análisis más general para decidir el nivel de producción de una empresa parte de que la empresa quiere maximizar su beneficio. El beneficio es igual a los ingresos (I) menos los costes (C), ambos son funciones dependientes del nivel de producción. Desde el punto de vista matemático, maximizar una función supone igualar a cero la derivada esa función con respecto a la variable que queremos maximizar; si derivamos la función beneficio, sería la derivada de sus componentes: los ingresos menos los costes: Lo que lleva a que el ingreso marginal (obtenidos por la primera derivada de los Ingresos de la empresa en la función de beneficio) debe ser igual al coste marginal, que es la derivada de los Costes de la empresa, como condición para que el nivel de producción sea el que maximice el beneficio. Si suponemos que los precios del mercado no pueden cambiar por la actuación de la empresa, sino que están dados (porque estemos en lo que se llama competencia perfecta, en la que hay muchas empresas y ninguna puede influir en el precio), entonces la condición es: Precio ha de ser igual a coste marginal. Un ejemplo es: si la función de Beneficio de la empresa es Véanse también: Ley de los rendimientos decrecientes, Curva de oferta y Elasticidad precio de la oferta. Estructura de mercados Artículo principal: Estructura de mercado El mercado está constituido por cuatro elementos básicos: consumidores, productores, precios (relativos) y productos. Según los neoclásicos, el mercado puro no es (ni debe ser afectado) por agentes extraños como el gobierno o el sistema financiero. En el mercado de cada bien o servicio, se pueden dar distintos tipos de situaciones. Estas situaciones son conocidas como estructuras de mercado, que se agrupan de la siguiente forma: Estructura de mercado por cantidad de empresas (productores) Cantidad Inifinitas empresas Muchas empresas 2 empresas 1 empresa Pocas empresas Nombre Competencia perfecta Competencia monopolística Duopolio Monopolio Oligopolio El modelo de competencia perfecta Precio y cantidades de mercado en el caso de un monopolista y en el de competencia perfecta. Artículo principal: Competencia perfecta El modelo de competencia perfecta describe una estructura de mercado que cumple con los siguientes supuestos: No hay barreras a la entrada de nuevas empresas y el salir no implica un costo. Existe información perfecta sobre precios, bienes e insumos. Producto homogéneo, es decir, los bienes son sustitutos perfectos. No hay externalidades, es decir, los derechos de propiedad están perfectamente definidos. Los contratos se cumplen porque hay un aparato jurídico eficiente. No hay rendimientos crecientes a escala ni en la producción ni en el consumo. Si los supuestos se cumplen podemos estar seguros de que la asignación que genera el mercado es eficiente. De hecho, en un modelo de equilibrio general las asignaciones son eficientes en el sentido de Pareto. La condición de optimalidad del mercado exige que el precio sea igual al costo marginal. Si el precio es menor algunas empresas salen del mercado presionando el precio al alza por la reducción de la cantidad ofrecida y si el precio es mayor algunas empresas entran al mercado esperando beneficios positivos, pero al hacerlo, presionan el precio a la baja debido a que la oferta se expande. El modelo de competencia perfecta es un ente ideal que intenta capturar la esencia del comportamiento económico, tanto de las empresas como de los individuos. La mayor parte de la literatura se ocupa de analizar el impacto que tiene sobre el bienestar o la eficiencia el que alguno de los supuestos arriba mencionados no se cumpla. Quizá uno de los más importantes es el de la información. Véanse también: Principio de plena competencia, Teorema de la telaraña, Información asimétrica e Información perfecta. En un mercado competitivo millones de empresas (productores) y millones de hogares (consumidores) participan en el comercio voluntario, buscando mejorar sus propias situaciones económicas. Sus acciones están coordinadas, como por una mano invisible, por un sistema de precios, que tiene una lógica interna y funciona sin una dirección central. La principal función del mercado es determinar el precio de los bienes. El precio generalmente es entendido como el valor del bien en términos monetarios. Sin embargo, los modelos neoclásicos se conceptualiza en precios relativos, es decir la relación existente entre distintas mercancías. La importancia del mecanismo de precios es tan grande que a veces a la teoría microeconómica se le denomina “teoría de precios”. Competencia imperfecta Artículo principal: Competencia imperfecta Los mercados de competencia imperfecta son aquellos en los que los productores son los suficientemente grandes como para tener un efecto notable sobre el precio del mercado. Existen varios modelos de este tipo de mercado entre ellos el mercado monopolístico y los diversos modelos oligopolísticos. La diferencia fundamental con los mercados de competencia perfecta reside en la capacidad de influencia que tienen las empresas oferentes de controlar en precio. En estos mercados, el precio no se acepta como un dato ajeno, sino que los oferentes intervienen activamente en su determinación. En la realidad, casi todos los mercados son imperfectos, siendo la competencia perfecta casi un óptimo teórico. Por el contrario en mercados fuertemente monopolísticos la competencia se produce entre los capitales, que buscan el máximo beneficio en competencia con las inversiones en otros mercados. En general, puede afirmarse que cuanto más elevado resulte el número de participantes, más competitivo será el mercado, pero el monopolio no implica que no exista competencia. Monopolio Artículo principal: Monopolio El Monopolio (del griego, mono=único y polio=vendedor) es una estructura de mercado caracterizada por la presencia de una única empresa, que produce un bien homogéneo y que se comporta no paramétricamente en precios, y por la existencia de barreras de entrada y salida en el mercado. En general está probado, en los modelos microeconómicos que lo estudian, que, cuando el Monopolio no puede realizar discriminación entre sus compradores (es decir, cuando no puede poner precios distintos para cada consumidor en función de las posibilidades de este), sino que pone el mismo precio para todos los posibles compradores, en este caso, el precio de equilibrio en el mercado y la cantidad producida de ese bien, que se determinan a partir de donde se cruzan la Curva de Coste Marginal (que depende de la función de producción de la empresa monopolística) y la Curva de Ingreso Marginal (que depende de la Demanda del bien producido por la empresa, demanda que depende de los compradores de ese bien), son tales que, generalmente, cumplen esto: El precio puesto por la empresa es más alto que en los casos en los que no hay monopolio. La cantidad producida por la empresa es también menor que en los casos de no monopolio. La utilidad total percibida por todos los agentes, tanto los compradores como la empresa monopolística, la suma de esas utilidades, suele ser menor también que en los casos de no monopolio. Por todas estas razones, y algunas más, los monopolios son vistos de forma negativa en los mercados (Por ejemplo, recordar las leyes Anti-Monopolio de los U.S.A.). No obstante, existen algunos monopolios inevitables, llamados Monopolios Naturales. En ocasiones se intenta que los problemas de este tipo de monopolios se resuelvan de manera que sea una institución pública (que se supone que no tiene interés en maximizar su propio beneficio, sino el bienestar global) sea quien controle el precio y la producción de ese monopolio o que le permita variarlos en función de los usuarios o compradores del Monopolio. Véanse también: Barrera de entrada y Barreras de salida. Véanse también: Discriminación de precios de primer grado, Discriminación de precios de segundo grado, Discriminación de precios de tercer grado e Hipótesis de eficiencia de los mercados. Oligopolio Artículo principal: Oligopolio En el oligopolio (del griego oligo=pocos, polio=vendedor), se supone que hay varias empresas, pero de tal forma que ninguna de ellas puede imponerse totalmente en el mercado. Hay por ello una constante lucha entre las mismas para poder llevarse la mayor parte de la cuota del mercado en la que las empresas toman decisiones estratégicas continuamente, teniendo en cuenta las fortalezas y debilidades de la estructura empresarial de cada una. El problema se puede plantear en ocasiones usando métodos de la teoría de juegos. Por ejemplo, dada las funciones de costes de cada una de las empresas implicadas, cada una se atreverá a ofrecer a un determinado precio, una cantidad determinada, al mercado. Pero, estas ofertas de las empresas al ser observadas desde el punto de vista de la demanda, tendrán efecto en cuánta cantidad es realmente demandada para cada empresa, y dado el precio que ha puesto cada una, le darán a cada una de ellas un cierto nivel de beneficios. También se puede introducir la idea de que las empresas intenten diferenciar su producto con respecto al producto de las otras, para que no parezcan tan sustitutivos y por ello se puedan considerar como diferentes por los consumidores. Aunque a menudo esas diferencias en producto sean en cosas mínimas como la presentación del producto, su calidad, el envase en el que viene, servicios de post-venta, las redes de distribución, la cercanía del producto al domicilio del consumidor, etcétera (para esto hay que estudiar más las estrategias comerciales de cada empresa en particular). Todo ello puede dar lugar al estudio de diferentes tipos de modelos. Generalmente, cuando se aplica la Teoría de Juegos, se supone que cada empresa puede tomar decisiones en un conjunto de decisiones propio, y que dependiendo de cuales toma esa empresa y las demás, esa empresa y las demás obtendrán un determinado resultado. A veces esto se puede representar como que cada empresa tiene una Curva de Reacción a las acciones de las demás empresas. Por ejemplo, si el resto de las empresas tomaran una serie de decisiones, y nuestra empresa en cuestión conociera (supuesto bastante fuerte, desde luego) qué decisiones han tomado las demás, para poder obtener ella el máximo beneficio debería de tomar ciertas decisiones a su vez, que dependen de las tomadas por las demás. Hipotéticamente, si las curvas de reacción de todas las empresas se cruzaran en algún sitio, ese conjunto de decisiones para todas las empresas implicadas implicaría el Equilibrio del Juego, porque todas las empresas estarían a la vez haciendo lo mejor para sí mismas dado lo que están haciendo el resto de las empresas. Esto es lo que se conoce como Equilibrio de Nash. Nash probó en qué condiciones se puede dar este Equilibrio. Ejemplos de equilibrios en los mercados son el de Cournot, cuando las empresas compiten en cantidades ofertadas, y el de Bertrand, cuando lo hacen en precios. No obstante, un caso común también es que alguna de las empresas sea Líder y las demás Seguidoras. En este caso, en vez de suponerse que se va alcanzar un equilibrio en el que todas las empresas más o menos llegan simultáneamente a esa situación de equilibrio, la ventaja de la empresa Líder (por ejemplo, por tener alguna ventaja empresarial aplastante sobre las otras empresas) le lleva a tomar primero una decisión ante la cual responden, o sea, la tomán después, las empresas Seguidoras. Esto es lo que lleva a la Líder a tener en cuenta, para cada decisión, que las seguidoras van a responder de una determinada manera, por lo que reajusta su forma de decidir teniendo en cuenta cuáles serán las decisiones de las demás, como si en cierto modo también las pudiera controlar a ellas y ponerlas al servicio de su propio beneficio. También es posible que las empresas del oligopolio se pongan de acuerdo para actuar coordinadamente a la hora de ofertar sus bienes y de poner sus precios, con lo que logran mayor beneficio total para cada una de ellas que cuando actúan por separado. Al acuerdo entre empresas para pactar producción o precios se le llama colusión y al grupo de empresas que han coludido se las llama cártel. En estos acuerdos el precio es superior al coste marginal, siendo socialmente ineficiente y produciendo una situación parecida, desde el punto de vista de los consumidores, a la del monopolio. Competencia monopolística Artículo principal: Competencia monopolística La competencia monopolística es una estructura de mercado caracterizada por la presencia de muchas empresas que venden productos heterogéneos, sustitutivos cercanos, pero imperfectos, entre sí. Al tratarse de productos heterogéneos, cada productor tiene un cierto poder de mercado sobre el bien que produce, por lo que la competencia monopolística puede definirse como una estructura de mercado intermedia entre monopolio y competencia perfecta. El modelo clásico de competencia monopolística fue elaborado por economista británico Edward Chamberlin. Chamberlin planteó que, debido al carácter heterogéneo de los bienes y al cierto poder de mercado que posee cada productor sobre los mismos, las empresas creen enfrentarse a una curva de demanda estimada o imaginaria, según la cual las decisiones del resto de productores están dadas. Sin embargo, para el resto de competidores no es óptimo mantener sus decisiones ante una variación unilateral de la producción de la i-ésima empresa. De este modo, existe una curva de demanda real, que recoge las decisiones de todos los productores y que va a determinar el equilibrio de mercado. A corto plazo, el equilibrio de mercado se alcanza cuando las decisiones tomadas por las empresas según la curva de demanda imaginaria son compatibles con la curva de demanda real. Es decir, en el punto en el que ambas se igualan. A largo plazo, bajo el supuesto de libre entrada y salida del mercado, no puede existir beneficio extraordinario, de tal modo que el equilibrio se alcanza en el punto en el que la curva de demanda imaginaria es tangente al coste medio a largo plazo y coincide con la demanda real de mercado. Como resultado se obtiene el Teorema del exceso de capacidad de Chamberlin, según el cual la empresa no alcanza el nivel eficiente de producción a largo plazo (mínimo del coste medio). La clave de los modelos de competencia monopolística es la existencia de productos no homogéneos. Esto se explica habitualmente por la existencia de diferenciación de productos, es decir las empresas producen distintas variedades de un mismo bien, lo que les otorga un cierto poder de mercado sobre el mismo. La diferenciación de productos puede ser: horizontal, los consumidores demanda bienes con diferentes características, o vertical, los consumidores tienen una distinta disposición al pago por una misma característica. El modelo clásico de diferenciación horizontal es el de competencia espacial (Ley de Hotelling) (1929). Véanse también: Competencia de Bertrand, Competencia de Stackelberg, Equilibrio de Stackelberg, Equilibrio de Nash y Competencia de Cournot. Teoría de los juegos Artículo principal: Teoría de los juegos La teoría de juegos es un método fundamental en economía matemática y empresarial para modelar comportamientos competitivos de agentes que interactúan. El término juego implica el estudio de cualquier interacción estratégica entre personas. Sus aplicaciones incluyen una amplia gama de fenómenos y enfoques económicos, como subastas, negociaciones, fijación de precios en fusiones y adquisiciones, reparto equitativo, duopolios, oligopolios, formación de redes sociales, economía computacional basada en agentes, equilibrio general, diseño de mecanismos y sistemas electorales, así como en áreas tan amplias como la economía experimental, la economía del comportamiento, la economía de la información, la organización industrial y la economía política. Evolución reciente Con la teoría de la elección racional y otros enfoques modernos los economistas cada vez intentaron entender fenómenos sociales en los que existían incentivos y desincentivos para las personas en determinadas situaciones. Estos estudios en cierto modo continúan la tradición de la microeconomía clásica."

ksampletext_wikipedia_phil_filosofia: str = "Filosofía. La filosofía (del griego amor a la sabiduría, derivado de amar, sofía, sabiduría; trans. en latín como) es una disciplina académica y un «conjunto de reflexiones sobre la esencia, las propiedades, las causas y los efectos de las cosas naturales, especialmente sobre el hombre y el universo». Bertrand Russell, en su libro Fundamentos de filosofía, comienza diciendo: Quizá espere el lector que comencemos [...] con una definición de la filosofía; pero, con razón o sin ella, no es este mi propósito. Toda definición que se dé a esta palabra variará con la filosofía que se adopte. Por lo tanto, todo lo que podemos decir al empezar es que existen ciertos problemas que interesan a determinadas personas, y que, al menos por ahora, no pertenecen a ninguna ciencia especial. Todos estos problemas son de tal especie que suscitan dudas acerca de lo que pasa comúnmente por conocimiento; y, si estas dudas se han de aclarar, en modo alguno lo harán solo mediante un estudio especial al cual damos el nombre de «filosofía». En consecuencia, el primer paso que puede darse para definir esta palabra consiste en indicar esos problemas y esas dudas, los cuales constituyen asimismo el primer paso en el verdadero estudio de la filosofía. Entre los problemas filosóficos tradicionales hay algunos que no se prestan, según mi parecer, a ningún tratamiento intelectual por sí mismos, ya que trascienden nuestras facultades cognoscitivas; por lo tanto, no trataremos de estos problemas. Hay otros, sin embargo, que, aunque no sean susceptibles de que pueda dárseles solución definitiva por ahora, lo son al menos de que se muestre la dirección que ha de seguirse para lograrla y el género de solución que les conviene, y que tal vez se alcance con el tiempo. La filosofía se origina del esfuerzo inusitadamente obstinado por alcanzar el conocimiento verdadero; lo que en nuestra vida ordinaria pasa por ser conocimiento adolece de tres defectos: es demasiado seguro de sí mismo; es vago; es contradictorio. [...] Esta disciplina aborda una variedad de problemas fundamentales acerca de asuntos como la razón (lógica), el lenguaje y la semántica (filosofía del lenguaje), el ser y la existencia (metafísica, metaontología y ontología), el conocimiento (gnoseología, epistemología y filosofía de la ciencia), la ética (ética o filosofía moral), la belleza (estética), el valor (axiología), la religión (filosofía de la religión), la política (filosofía política) y la mente (fenomenología, filosofía de la mente).  A lo largo de la historia, muchas otras disciplinas han surgido a raíz de la filosofía, y a su vez es considerada la base de todas las ciencias modernas por muchos autores, tanto a nivel de génesis histórica como a nivel de fundamentos teóricos. La disciplina ha existido desde la Antigüedad en Occidente y Oriente, no solo como actividad racional sino también como forma de vida. La historia de la filosofía nos permite comprender su desarrollo, evolución e influencia en las distintas facetas del pensamiento humano. El término probablemente fue acuñado por Pitágoras. Al abordar los problemas, la filosofía se distingue del misticismo, el esoterismo, la mitología y la religión por su énfasis en los argumentos racionales sobre los argumentos de autoridad, y de la ciencia porque generalmente realiza sus investigaciones de una manera no empírica, sea mediante el análisis y la clarificación conceptual, experimentos mentales, como el del gato de Schrödinger, la especulación u otros métodos a priori, aunque sin desconocer la importancia de los datos empíricos. No obstante, la filosofía en ocasiones lleva a término una parte de sus investigaciones de manera interdisciplinar junto con otras áreas de las ciencias cognitivas, del lenguaje, sociales, etc.; especialmente en filosofía de la mente, filosofía del lenguaje o epistemología. Así mismo, el enfoque de la filosofía experimental, aunque problemático por concepto para algunos ,debido a que el enfoque de la filosofía suele ser más abstracto y general que el de las ciencias fácticas particulares ,, trata de ofrecer estudios experimentales para el apoyo de sus desarrollos teóricos. Hoy en día los principales subcampos de la filosofía académica son: la lógica, que estudia las reglas de inferencia o relaciones formales (abstractas) entre proposiciones que permiten deducir conclusiones a partir de premisas verdaderas (lógica formal), o estudiar los argumentos utilizados en su entorno cotidiano en términos de ponderación y validez de las razones (lógica informal); la filosofía del lenguaje, que reflexiona acerca de cuestiones como la naturaleza de las relaciones entre las palabras y sus significados, sus referencias y su verdad, etc.; la epistemología, que estudia la naturaleza del conocimiento y las creencias; la metafísica, que se ocupa de la naturaleza más fundamental de lo que consideramos realidad; la ética, que reflexiona acerca de la vida y el valor, los conceptos del bien y el mal, el buen vivir, la dignidad, los derechos, la virtud y, en su conjunto, la consideración que podemos tener al respecto de las decisiones y acciones. Entre otros subcampos notables se incluyen la filosofía de la ciencia, la filosofía política, la filosofía de la tecnología o la filosofía de la mente. La Conferencia General de la Unesco proclamó el Día Mundial de la Filosofía el tercer jueves del mes de noviembre de cada año.[cita requerida] Definiciones Artículo principal: Definiciones de filosofía Inicialmente el término se refería a cualquier rama de conocimiento. En este sentido la filosofía está estrechamente relacionada con la religión, las matemáticas, las ciencias naturales, la educación y la política. Además los antiguos filósofos no diferenciaban la teoría de la práctica cotidiana, por lo que su discurso filosófico formaba parte integral y preparatoria de su modo de vida, y viceversa. En la sección trece de Vidas, opiniones y sentencias de los filósofos más ilustres, la historia de la filosofía más antigua que se conserva (siglo III), Diógenes Laercio presenta una división en tres partes de la investigación filosófica griega antigua: Filosofía natural (es decir, física, en griego: ta physika, lit. cosas que tienen que ver con physis [naturaleza]) fue el estudio de la constitución y procesos de transformación en el mundo físico. Filosofía moral (es decir, ética, êthika, que tiene que ver con carácter, disposición, modales) fue el estudio de la bondad, el bien y el mal, la justicia y la virtud. Filosofía metafísica (es decir, lógica, de logikós, de o perteneciente a la razón o el habla) fue el estudio de la existencia, causalidad, Dios, lógica, formas, y otros objetos abstractos. (meta ta physika, sobre la Física) En Contra los lógicos el filósofo pirronista Sexto Empírico detalló la variedad de formas en que los filósofos griegos antiguos habían dividido la filosofía, y señaló que Platón, Aristóteles, Jenócrates y los estoicos estuvieron de acuerdo en esta división en tres partes. El filósofo escéptico académico Cicerón también siguió esta división. Para un acercamiento más actualizado y completo a la discusión y definición de la filosofía contemporánea se puede consultar el artículo definiciones de filosofía. Etimología Busto de Pitágoras, a quien se atribuye la invención de la palabra «filosofía». La invención del término «filosofía» se suele atribuir al pensador y matemático griego Pitágoras de Samos, aunque no se conserva ningún escrito suyo que lo confirme. Según la tradición, hacia el año 530 a. C., el general León trató de sabio (σοφóς: sofos) a Pitágoras, el cual respondió que él no era un sabio, sino alguien que aspiraba a ser sabio, que amaba la sabiduría, un φιλο-σοφóς. Admirado León de la novedad del hombre, le preguntó a Pitágoras quiénes eran, pues, los filósofos y qué diferencia había entre ellos y los demás; y Pitágoras respondió que le parecían cosas semejantes la vida del hombre y la feria de los juegos que se celebraba con toda pompa ante el concurso de Grecia entera; pues, igual que allí, unos aspiraban con la destreza de sus cuerpos a la gloria y nombre que da una corona, otros eran atraídos por el lucro y el deseo de comprar y vender. Pero había una clase, y precisamente la formada en mayor proporción de hombres libres, que no buscaban el aplauso ni el lucro, sino que acudían para ver y observaban con afán lo que se hacía y de qué modo se hacía; también nosotros, como para concurrir a una feria desde una ciudad, así habríamos partido para esta vida desde otra vida y naturaleza, los unos para servir a la gloria, los otros al dinero, habiendo unos pocos que, despreciando todo lo demás, consideraban con afán la naturaleza de las cosas, los cuales se llamaban afanosos de sabiduría, esto es, filósofos. Cicerón, Cuestiones Tusculanas, Libro V, capítulos 7 a 11. Según Pitágoras, la vida era comparable a los juegos olímpicos, porque en ellos encontramos tres clases de personas: las que buscan honor y gloria, las que buscan riquezas, y las que simplemente buscan contemplar el espectáculo, que serían los filósofos. Años más tarde, Platón agregó más significado al término cuando contrapuso a los filósofos con los sofistas. Los filósofos eran quienes buscaban la verdad, mientras que los sofistas eran quienes arrogantemente afirmaban poseerla, ocultando su ignorancia detrás de juegos retóricos o adulación, convenciendo a otros de cosas infundadas o falsas, y cobrando además por enseñar a hacer lo mismo. Aristóteles adoptó esta distinción de su maestro, extendiéndola junto con su obra a toda la tradición posterior. El texto más antiguo que se conserva con la palabra «filosofía» se titula Tratado de medicina antigua, y fue escrito hacia el año 440 a. C. Allí se dice que la medicina «moderna» debe orientarse hacia la filosofía, porque solo la filosofía puede responder a la pregunta «¿qué es el hombre?». Ramas Esta sección es un extracto de Anexo:Ramas de la filosofía.[editar] Las ramas y los problemas que componen la filosofía han variado mucho a través de los siglos. Con el tiempo, algunas ramas de la filosofía se han independizado y vuelto ciencias. Por ejemplo, en sus orígenes, la filosofía abarcaba el estudio de los cielos, que hoy llamamos astronomía, así como los problemas que ahora pertenecen a la física. En la actualidad, la lógica está atravesando un proceso similar...[cita requerida] Metafísica Esta sección es un extracto de Metafísica.[editar] ¿Cuáles son los principios y causas del mundo? Grabado de Camille Flammarion: LAtmosphere: Météorologie Populaire (París, 1888). La metafísica (del latín metaphysica, y este del griego μετὰ [τὰ] φυσικά, «después de la naturaleza») es la rama de la filosofía que estudia la estructura, componentes y principios fundamentales de la realidad.  Esto incluye la clarificación e investigación de algunas de las nociones fundamentales con las que comprendemos el mundo, como entidad, ser, existencia, objeto, propiedad, relación, causalidad, tiempo y espacio. Mario Bunge, filósofo de la ciencia afirmaba que “la metafísica es la ciencia general” , así como que “La ciencia, tanto básica como aplicada, trata con conceptos e hipótesis metafísicos: presupone ciertos principios ontológicos ,de tipo heurístico, así como de tipo constitutivo, y es una poderosa fuente de conjeturas metafísicas. De hecho, algunas teorías son a la vez metafísicas y científicas” Junto con la lógica y la epistemología o gnoseología, la metafísica es la rama más básica de la filosofía. Ha sido estudiada por filósofos como Platón, Aristóteles, Agustín de Hipona, Boecio, Tomás de Aquino, Guillermo de Ockham, Descartes, John Locke, Nicolás Malebranche, Spinoza, Leibniz, Hume, Alfred North Whitehead, Martin Heidegger, Kurt Gödel, Karl Popper, Saul Kripke, David Lewis, etc. Antes del advenimiento de la ciencia moderna, muchos de los problemas que hoy pertenecen a las ciencias naturales eran estudiados por la metafísica bajo el título de filosofía natural. Hoy la metafísica estudia aspectos de la realidad que son inaccesibles a la investigación empírica. Esto dará lugar en el siglo XX a la lectura heideggeriana de la metafísica occidental como ontoteología y, por lo tanto, a la necesidad de repensar la cuestión del ser desde el origen mismo de los pensadores presocráticos. Aristóteles designó la metafísica como «filosofía primera». En la química se asume la existencia de la materia y en la biología la existencia de la vida, pero ninguna de las dos ciencias define la materia o la vida; solo la metafísica suministra estas definiciones básicas. La ontología es la parte de la metafísica que se ocupa de investigar qué entidades existen y cuáles no, más allá de las apariencias. La metafísica tiene dos temas principales: el primero es la ontología, que en palabras de Aristóteles es la ciencia que estudia al ser en cuanto tal. El segundo es la teología, que estudia los fines como causa última de la realidad. Aunque esta distinción deriva de la escolástica, quizá especialmente por influencia de Francisco Suárez. A lo largo de su historia, después de -y superando- el tomismo, se realizaron otras distinciones dentro del área. Por ejemplo, Wolff, en el s. XVIII, diferenció la metafísica general -usado indistintamente tanto como ontología que como filosofía primera - de la metafísica específica, sin embargo, incluso esta distinción se considera errónea y superada a día de hoy. Lo común en el panorama actual de la filosofía es usar indistintamente ambos términos para referirse a un conjunto de problemas acerca del ser de las cosas, eventos, entes, procesos, etc, como el problema de la persistencia y la constitución, el problema mente-cuerpo o el problema de la causación. Es difícil encontrar una definición adecuada de metafísica. A lo largo de los siglos muchos filósofos han sostenido, de una manera u otra, que la metafísica es imposible. Esta tesis tiene una versión fuerte y una versión débil. La versión fuerte es que todas las afirmaciones metafísicas carecen de sentido o significado. Esto depende por supuesto de una teoría del significado. Los positivistas lógicos y Ludwig Wittgenstein fueron defensores explícitos de esta posición. Por otra parte, la versión débil es que si bien las afirmaciones metafísicas poseen significado, es imposible saber cuáles son verdaderas y cuáles falsas, pues esto va más allá de las capacidades cognitivas del ser humano. Esta posición es la que sostuvieron, por ejemplo, David Hume e Immanuel Kant. Por otra parte, algunos filósofos han sostenido que el ser humano tiene una predisposición natural hacia la metafísica. Kant la calificó de «necesidad inevitable» y Arthur Schopenhauer incluso definió al ser humano como «animal metafísico». Gnoseología Esta sección es un extracto de Gnoseología.[editar] El experimento mental del cerebro en una cubeta pretende poner a prueba distintas teorías acerca del conocimiento. La gnoseología (del griego γνωσις, gnōsis, «conocimiento» o «facultad de conocer», y λόγος, logos, «razonamiento» o «discurso»), también llamada teoría del conocimiento, es la rama de la filosofía que estudia la posibilidad, el origen o medios, la naturaleza o esencia, y la fenomenología del conocimiento.  La gnoseología no estudia los conocimientos particulares, como pueden ser los conocimientos de la física, de la matemática o del entorno inmediato, sino la naturaleza del conocimiento en general. Muchas ciencias particulares tienen además su propia filosofía, como por ejemplo la filosofía de la física, la filosofía de la matemática, la filosofía de la historia, etc. Otras disciplinas también se ocupan del conocimiento en general, pero desde otros puntos de vista. La psicología estudia los aspectos de la vida mental implícitos en el conocer, la lógica estudia la corrección o incorrección de los razonamientos que pueden implicar nuevos conocimientos, y la ontología o metafísica estudia la naturaleza de los objetos que se pueden conocer. Los problemas en torno al conocimiento son centrales en la filosofía y su consideración se inicia con la filosofía misma, especialmente en el Teeteto de Platón. Prácticamente todos los grandes filósofos han contribuido a la gnoseología. Epistemología Esta sección es un extracto de Epistemología.[editar] «La ciencia (ἐπιστήμη) es un juicio verdadero acompañado de razón (λόγος).» Platón. Teeteto, 202, b-c La epistemología, del griego ἐπιστήμη ─epistḗmē («conocimiento justificado como verdad»)─ y λόγος ─lógos («estudio»)─, es la rama de la filosofía que estudia el conocimiento: su naturaleza, posibilidad, alcance y fundamentos. Algunos autores distinguen a la epistemología, de la gnoseología (estudio del conocimiento en general), al circunscribirla al conocimiento del tipo científico; otros, en cambio, consideran que el término «epistemología» ha ido ampliando su significado y lo utilizan como sinónimo de «teoría del conocimiento», sobre todo en el mundo anglosajón. La epistemología estudia las circunstancias históricas, psicológicas y sociológicas que llevan a la obtención del conocimiento científico y los criterios por los cuales se lo justifica o invalida, así como la definición clara y precisa de los conceptos epistémicos más usuales, tales como verdad, objetividad, realidad o justificación. Algunas de las preguntas que pretende responder la epistemología son ¿Cómo conocemos?, ¿Cuáles son las fuentes del conocimiento?, ¿Cómo diferenciamos lo verdadero de lo falso? y ¿Cuáles son los tipos de conocimiento?. El debate no se centra en un conocimiento específico, sino en la forma en cómo conocemos. Generalmente, los debates en la epistemología se agrupan en torno a cuatro áreas centrales: El análisis filosófico de la naturaleza del conocimiento y las condiciones requeridas para que una creencia haga parte del conocimiento, como la verdad y la justificación. Recursos potenciales del conocimiento y creencias justificadas como la percepción, la razón, la memoria y el testimonio. La estructura del conocimiento o de la creencia justificada incluyendo si todas las creencias justificadas deberían derivarse de creencias originales justificadas o si la justificación requiere solo un conjunto coherente de creencias. Escepticismo filosófico, el cual cuestiona la posibilidad del conocimiento y problemas relacionados como si el escepticismo fuera una amenaza para nuestro conocimiento común y si es posible refutar argumentos escépticos. Las teorías del conocimiento específicas son también consideradas parte de la epistemología, por ejemplo la epistemología de las ciencias físicas, de las ciencias sociales o de las psicología. Lógica Esta sección es un extracto de Lógica.[editar] Este artículo o sección tiene referencias, pero necesita más para complementar su verificabilidad. Busca fuentes: «Filosofía» – noticias · libros · académico · imágenes Este aviso fue puesto el 28 de octubre de 2018. Esquema del modus ponens, una regla de inferencia fundamental de la lógica proposicional. La lógica es una rama de la filosofía   de carácter interdisciplinario, entendida como la ciencia formal que estudia los principios de la demostración y la inferencia válida, las falacias, las paradojas y la noción de verdad. La lógica se divide en varias categorías según su campo de estudio. La lógica filosófica estudia el concepto y la definición, la enunciación o proposición y la argumentación utilizando los métodos y resultados de la lógica moderna para el estudio de problemas filosóficos. La lógica matemática estudia la inferencia mediante sistemas formales como la lógica proposicional, la lógica de primer orden y la lógica modal. La lógica informal se enfoca en el desarrollo lingüístico de los razonamientos y sus falacias. La lógica computacional es la aplicación de la lógica matemática a las ciencias de la computación. Los orígenes de la lógica se remontan a la Edad Antigua, con brotes independientes en China, India y Grecia. Desde entonces, la lógica tradicionalmente se considera una rama de la filosofía, pero en el siglo XX la lógica ha pasado a ser principalmente la lógica matemática, y por lo tanto ahora también se considera parte de las matemáticas, e incluso una ciencia formal independiente. No existe un acuerdo universal sobre la definición exacta o los límites de la lógica.  Sin embargo, el ámbito de la lógica (interpretada en sentido amplio) incluye: La clasificación de los argumentos. El análisis sistemático de las formas lógicas. El estudio sistemático de la validez de las inferencias deductivas. La fuerza de las inferencias inductivas. El estudio de los argumentos defectuosos, como las falacias. El estudio de las paradojas lógicas. El estudio de la sintaxis y la semántica de los lenguajes formales. El estudio de los conceptos de sentido, denotación y verdad. Históricamente, la lógica se ha estudiado principalmente en filosofía desde la antigüedad, en matemáticas desde mediados del siglo XIX y en informática desde mediados del siglo XX. Más recientemente, la lógica también se ha estudiado en lingüística y en ciencias cognitivas. En general, la lógica sigue siendo un área de estudio fuertemente interdisciplinaria. Ética Esta sección es un extracto de Ética.[editar] La ética o filosofía moral es la rama de la filosofía que estudia la conducta humana, lo correcto y lo incorrecto, lo bueno y lo malo, la moral, el buen vivir, la virtud, la felicidad y el deber. La ética contemporánea se suele dividir en tres ramas o niveles: la metaética estudia el origen, naturaleza y significado de los conceptos éticos, la ética normativa busca normas o estándares para regular la conducta humana y la ética aplicada examina controversias éticas específicas.  Ética y moral son conceptos muy relacionados que a veces se usan como sinónimos, pero tradicionalmente se diferencian en que la ética es la disciplina académica que estudia la moral. La ética no inventa los problemas morales, sino que reflexiona sobre ellos. Las acciones relevantes para la ética son las acciones morales, que son aquellas realizadas de manera libre, ya sean privadas, interpersonales o políticas. La ética no se limita a observar y describir esas acciones, sino que busca determinar si son buenas o malas, emitir juicio sobre ellas y así ayudar a encauzar la conducta humana. El estudio de la ética se remonta a los orígenes mismos de la filosofía en la Antigua Grecia, y su desarrollo histórico ha sido amplio y variado. A lo largo de la historia ha habido diversas maneras de entender la ética y distintas propuestas morales orientadoras de la vida humana. Aunque la ética siempre fue una rama de la filosofía, su amplio alcance la conecta con muchas otras disciplinas, incluyendo la antropología, biología, economía, historia, política, sociología y teología. Estética Esta sección es un extracto de Estética.[editar] La estética (del griego αισθητικός, aisthetikós, «susceptible a ser percibido por los sentidos» y este de αισθάνεσθαι, aisthánesthai, «percibir») es la rama de la filosofía que estudia la esencia y la percepción de la belleza y el arte.  Algunos autores definen la estética de manera más amplia, como el estudio de las experiencias estéticas y los juicios estéticos en general, y no solo los relativos a la belleza. Cuando juzgamos algo como «bello», «feo», «sublime» o «elegante» (por dar algunos ejemplos), estamos haciendo juicios estéticos, que a su vez expresan experiencias estéticas. La estética es el dominio de la filosofía, estudiando el arte y cualidades como la belleza; así mismo es el estudio de estas experiencias y juicios que suceden día a día en las actividades que realizamos, produciendo sensaciones y emociones ya sean positivas o negativas en nuestra persona. La estética busca el porqué de algunas cuestiones, por ejemplo, por qué algún objeto, pintura o escultura no resulta atractivo para los espectadores; por lo tanto el arte lleva relación con la estética ya que busca generar sensaciones a través de una expresión. En otra acepción, la estética es el estudio de la percepción en general, sea sensorial o entendida de manera más amplia. Estos campos de investigación pueden coincidir, aunque no necesariamente es lo mismo. La estética estudia las más amplias y vastas historias del conocimiento isabelino, así como las diferentes formas del arte. La estética, así definida, es el campo de la filosofía que estudia el arte y sus cualidades, tales como la belleza, lo eminente, lo feo o la disonancia. Es la rama de la filosofía que estudia el origen del sentimiento puro y su manifestación, que es el arte, se puede decir que es la ciencia cuyo objeto primordial es la reflexión sobre los problemas del arte, la estética analiza filosóficamente los valores que en ella están contenidos. Desde que en 1750 (en su primera edición) y 1758 (segunda edición publicada) Alexander Gottlieb Baumgarten usara la palabra «estética» como ciencia de lo bello, misma a la que se agrega un estudio de la esencia del arte, de las relaciones de esta con la belleza y los demás valores. Algunos autores han pretendido sustituirla por otra denominación: «calología», que atendiendo a su etimología significa ciencia de lo bello (kalos, bello). Al ser la estética también una reflexión filosófica sobre el arte, uno de sus problemas será el valor que se contiene en el arte; y aunque un variado número de ciencias puedan ocuparse de la obra de arte, solo la estética analiza filosóficamente los valores que en ella están contenidos. Por otro lado, filósofos como Mario Bunge consideran que la estética no es una disciplina. Además Elena Oliveras, formada tanto en el campo filosófico como en el artístico, define el concepto de estética como la marca de Modernidad de su momento de la historia donde se realiza su nacimiento, donde se inaugura el principio de subjetividad. Política Esta sección es un extracto de Filosofía política.[editar] La filosofía política de John Locke y otros influyó en la redacción de la Declaración de Independencia de los Estados Unidos, que luego sirvió de modelo para muchos otros países. La filosofía política es una rama de la filosofía que se ocupa de estudiar cómo debería organizarse la relación entre las personas y la sociedad. Aborda cuestiones fundamentales sobre el gobierno, la política, las leyes, la libertad, la igualdad, la justicia, la propiedad, los derechos, el poder político, la legitimidad de la autoridad, y la correcta aplicación de un código legal. Entre sus interrogantes se encuentran: qué hace legítimo a un gobierno, qué derechos y libertades debe proteger y por qué, cuál debería ser su forma y estructura, qué deberes tienen los ciudadanos hacia un gobierno legítimo (si corresponde), y bajo qué condiciones podrían éstos derrocarlo, en caso de ser legítimo hacerlo.  A diferencia de la ciencia política, que se enfoca en el estudio empírico de cómo han sido, son y podrían ser los fenómenos políticos, la filosofía política se centra en cómo deberían ser tales fenómenos, desde un punto de vista normativo.  En un sentido vernacular, el término «filosofía política» a menudo refiere a una perspectiva general, o a una ética, creencia o actitud específica, sobre la política que no necesariamente debe pertenecer a la disciplina técnica de la filosofía. Charles Blattberg, que define la política como «responder a los conflictos con el diálogo», sugiere que las filosofías políticas ofrecen consideraciones filosóficas de ese diálogo. La filosofía política tiene un campo de estudio amplio y se conecta fácilmente con otras ramas y subdisciplinas de la filosofía, como la filosofía del derecho y la filosofía de la economía. Se relaciona fuertemente con la ética en que las preguntas acerca de qué tipo de instituciones políticas son adecuadas para un grupo depende de qué forma de vida se considere adecuada para ese grupo o para los miembros de ese grupo. Las mejores instituciones serán aquellas que promuevan esta forma de vida. En el plano metafísico, la principal controversia divisora de aguas es acerca de si la entidad fundamental sobre la cual deben recaer los derechos y las obligaciones es el individuo, o el grupo. El individualismo considera que la entidad fundamental es el individuo, y por lo tanto promueven el individualismo metodológico. El comunitarismo enfatiza que el individuo es parte de un grupo, y por lo tanto da prioridad al grupo como entidad fundamental y como unidad de análisis. Los fundamentos de la filosofía política han variado a través de la historia. Para los griegos la ciudad era el centro y fin de toda actividad política. En la Edad Media toda actividad política se centraba en las relaciones que debe mantener el ser humano con el orden dado por Dios. A partir del Renacimiento la política adopta un enfoque básicamente antropocéntrico. En el mundo moderno y contemporáneo surgen y conviven muchos modelos, que van desde los totalitarismos hasta los sistemas democráticos participativos (entre los cuales existen muchas variantes). Algunos filósofos políticos influyentes fueron: en el Reino Unido, Thomas Hobbes, John Locke, Jeremy Bentham, John Stuart Mill y John Rawls; en Francia, Montesquieu, Jean-Jacques Rousseau y Voltaire; en Italia, Nicolás Maquiavelo, Cesare Beccaria, Giambattista Vico y Giuseppe Mazzini; y en Alemania, Karl Marx y Friedrich Engels. Lenguaje Esta sección es un extracto de Filosofía del lenguaje.[editar] ¿Cuál de estas figuras es kiki y cuál es bouba? El efecto bouba/kiki sugiere que la relación entre los sonidos y las cosas no siempre es completamente arbitraria. La filosofía del lenguaje es la rama de la filosofía que estudia el lenguaje en sus aspectos más generales y fundamentales, como la naturaleza del significado y de la referencia, la relación entre el lenguaje, el pensamiento y el mundo, el uso del lenguaje (o pragmática), la interpretación, la traducción y los límites del lenguaje. La filosofía del lenguaje se distingue de la lingüística en que se sirve de métodos no-empíricos (como experimentos mentales) para llegar a sus conclusiones. Además, en la filosofía del lenguaje generalmente no se hace diferencia entre el lenguaje hablado, el escrito o cualquiera otra de sus manifestaciones, sino que se estudia aquello que es común a todas ellas. Por último, los lingüistas en general estudian el lenguaje con fines descriptivos, analizando sus formas, niveles y funciones. En cambio, el enfoque de los filósofos del lenguaje es más abstracto y desligado de la descripción práctica de los lenguajes particulares. La semántica es la parte de la filosofía del lenguaje (y de la lingüística) que se ocupa de la relación entre el lenguaje y su significado o sentido. Algunos problemas que caen bajo este campo son el problema de la referencia, la naturaleza de los predicados, de la representación y de la verdad. En el Crátilo, Platón señaló que si la conexión entre las palabras y el mundo es arbitraria o convencional, entonces es difícil entender cómo el lenguaje puede permitir el conocimiento acerca del mundo. Por ejemplo, es evidente que el nombre «Venus» pudo haber designado cualquier cosa, aparte del planeta Venus, y que el planeta Venus pudo haberse llamado de cualquier otra forma. Luego, cuando se dice que «Venus es más grande que Mercurio», la verdad de esta oración es convencional, porque depende de nuestras convenciones acerca de lo que significan «Venus», «Mercurio» y el resto de las palabras involucradas. En otro lenguaje, esas mismas palabras podrían, por alguna coincidencia, significar algo muy distinto y expresar algo falso. Sin embargo, aunque el significado de las palabras es convencional, una vez que se ha fijado su significado, parece que la verdad y la falsedad no dependen de convenciones, sino de cómo es el mundo. A este «fijar el significado» se lo suele llamar interpretación, y es uno de los temas centrales de la semántica. Un problema ulterior en esta dirección es que si una interpretación se da en términos lingüísticos (por ejemplo: «Venus es el nombre del segundo planeta a partir del Sol»), entonces queda la duda de cómo deben interpretarse las palabras de la interpretación. Si se las interpreta por medio de nuevas palabras, entonces el problema resurge, y se hace visible una amenaza de regresión al infinito, de circularidad, o de corte arbitrario en el razonamiento (tal vez en palabras cuyo significado sea supuestamente autoevidente). Pero para algunos este problema invita a pensar en una forma de interpretación no lingüística, como por ejemplo el conductismo o la definición ostensiva. La pragmática, por otra parte, es la parte de la filosofía del lenguaje que se ocupa de la relación entre los usuarios del lenguaje y el lenguaje. Algunas de las cuestiones centrales de la pragmática son la elucidación del proceso de aprendizaje del lenguaje, de las reglas y convenciones que hacen posible la comunicación, y la descripción de los muchos y variados usos que se le da al lenguaje, entre ellos: describir estados de cosas, preguntar, ordenar, bromear, traducir, suplicar, agradecer, maldecir, saludar, rezar, etc. Mente Esta sección es un extracto de Filosofía de la mente.[editar] Representación frenológica de las áreas cerebrales en correspondencia con las funciones mentales. La frenología fue uno de los primeros intentos de relacionar funciones mentales con partes específicas del cerebro. La filosofía de la mente es la rama de la filosofía (en particular de la filosofía analítica) que estudia la mente, incluyendo las percepciones, sensaciones, emociones, fantasías, sueños, pensamientos y creencias. Uno de los problemas centrales de la disciplina es determinar qué hace que todos los elementos de esta lista sean mentales, y otros no. Además de las cuestiones ontológicas acerca de la naturaleza de los estados mentales, la filosofía de la mente estudia cuestiones epistemológicas en torno a la cognoscibilidad de la mente. Tanto para la fenomenología como para la filosofía analítica, un candidato importante para ser una condición necesaria, aunque no suficiente, de todo fenómeno mental es la intencionalidad. La intencionalidad es el poder de la mente de ser acerca de, de representar, o suplir cosas, propiedades o estados de cosas. Por ejemplo, uno no recuerda simplemente, sino que recuerda algo, y tampoco quiere en abstracto, sino que quiere algo determinado. La propuesta de algunos filósofos es que todo lo que sea mental está «dirigido» hacia algún objeto, en el sentido más general de objeto, y que por lo tanto la intencionalidad es una característica necesaria, aunque no suficiente, de lo mental. Otra característica importante y controversial de lo mental son los qualia, o propiedades subjetivas de la experiencia. Cuando uno ve una nube, se pincha un dedo con un alfiler, o huele una rosa, experimenta algo que no se puede observar desde fuera, sino que es completamente subjetivo. A estas experiencias se las llama «qualia». Parte de la importancia de los qualia se debe a las dificultades que suscitan al fisicalismo para acomodarlos dentro de su concepción de lo mental. Algunos neurocientíficos como Antonio Damasio, Gerald Edelman, Vilayanur Ramachandran y Rodolfo Llinás han abordado esta temática de la filosofía de la mente y sostienen que los qualia existen y no son eliminables y reemplazables por otra cosa como conductas o propiedades objetivas del cerebro observadas en imágenes de resonancia magnética.     La filosofía de la mente se relaciona con la ciencia cognitiva de varias maneras. Por un lado, las filosofías más racionalistas pueden considerarse como parte de las ciencias cognitivas. En cambio, otras filosofías más naturalistas que dan énfasis a la biología y neurociencia critican a la ciencia cognitiva por suponer que lo mental es intelectual (lógico) o computacional o por equiparar a los seres vivos a artefactos mecánicos. Por ejemplo, algunos críticos señalan que la ciencia cognitiva descuida muchos factores relevantes para el estudio de lo mental, entre ellos las emociones, la conciencia, el cuerpo y el entorno.  Algunos problemas centrales en la filosofía de la mente son el problema de la relación entre la mente y el cuerpo, el problema de la permanencia de la identidad personal a través del tiempo, y el problema del reconocimiento de otras mentes. Naturaleza Esta sección es un extracto de Filosofía de la naturaleza.[editar] La primera y segunda ley de Newton, en latín, en la edición original de su obra Philosophiæ Naturalis Principia Mathematica. La filosofía de la naturaleza, a veces llamada filosofía natural o cosmología, fue el estudio filosófico de la naturaleza y el universo físico que era dominante antes del desarrollo de la ciencia moderna. Se considera el precursor de lo que hoy conocemos como las ciencias naturales y física hasta mediados del siglo XIX. Problemas como los del determinismo o indeterminismo, causalidad, finalismo, orden y probabilidad, especificidad de la vida, etc., eran considerados argumentos propios de la filosofía de la naturaleza la cual debería ser independiente de la propiamente dichas las ciencias e investigaciones empíricas y teóricas. Similarmente se hablaba de teología natural o racional como la investigación filosófica referida a Dios basados en la razón y la experiencia ordinaria de la naturaleza. La filosofía natural trató cuestiones que pocas ciencias naturales se han planteado, como la existencia de un mundo inmaterial. Por esta cuestión, la cosmología se interpenetra con la psicología como estudio del alma. Desde el mundo antiguo, comenzando con Aristóteles, quien llamaba este estudio física, la filosofía natural era el término común para la práctica de estudiar la naturaleza hasta el siglo XIX. Fue en el siglo XIX que el concepto de ciencia recibió su forma moderna con nuevos títulos emergentes como biología y biólogo, física y físico entre otros campos y títulos técnicos; se fundaron instituciones y comunidades, y se produjeron aplicaciones e interacciones sin precedentes con otros aspectos de la sociedad y la cultura. Así, el conocido tratado de Isaac Newton, Philosophiæ naturalis principia mathematica (1687), cuyo título se traduce como Principios matemáticos de la filosofía natural, refleja el uso actual de las palabras filosofía natural, similar al estudio sistemático de la naturaleza. Ciencia Esta sección es un extracto de Filosofía de la ciencia.[editar] La filosofía de la ciencia es la rama de la filosofía que investiga el conocimiento científico y la práctica científica, se ocupa de examinar y describir la estructura de la ciencia y de los métodos que siguen los científicos para trabajar en ella. Se trata de una disciplina que reflexiona sobre los fundamentos, métodos, límites y alcances de la ciencia, podría decirse que busca responder a la pregunta cómo se hace la ciencia. Se trata de una disciplina que reflexiona sobre los fundamentos, métodos, límites y alcances de la ciencia. La esencia de la filosofía Lo que intenta la filosofía de la ciencia es explicar problemas tales como: Naturaleza y la obtención de las ideas científicas (conceptos, hipótesis, modelos, teorías, paradigma, etc.) Relación de cada una de ellas con la realidad Cómo la ciencia describe, explica, predice y contribuye al control de la naturaleza (esto último en conjunto con la filosofía de la tecnología) Formulación y uso del método científico Tipos de razonamiento utilizados para llegar a conclusiones Implicaciones de los diferentes métodos y modelos de ciencia La filosofía de la ciencia comparte algunos problemas con la gnoseología ,la teoría del conocimiento, que se ocupa de los límites y condiciones de posibilidad de todo conocimiento. Pero, a diferencia de esta, la filosofía de la ciencia restringe su campo de investigación a los problemas que plantea el conocimiento científico; el cual, tradicionalmente, se distingue de otros tipos de conocimiento, como el ético o estético, o las tradiciones culturales. A lo largo de la historia, se han propuesto diversos esquemas para el método científico. No hay un único método científico, algunos de los más importantes son: Método inductivo-deductivo: La ciencia comienza con observaciones individuales, a partir de las cuales se formulan generalizaciones que van más allá de los hechos observados. Estas generalizaciones permiten hacer predicciones, cuya confirmación las fortalece. Aristóteles, Francis Bacon, Galileo, Newton, y muchos otros científicos y filósofos se adhieren a este esquema. Método hipotético-deductivo: Se parte de hipótesis o conjeturas que preceden y guían a las observaciones. La ciencia no se inicia con la experiencia del mundo, sino con ideas propuestas por el investigador. Hume, Whewell, Kant, Popper, y otros se inclinan hacia este método. Método a priori: El conocimiento se alcanza mediante la razón pura, sin necesidad de recurrir a la experiencia. Descartes es un exponente de este método. Anarquismo metodológico: No existe un método científico único y universal. Los científicos utilizan una variedad de métodos y estrategias, y no hay reglas fijas que garanticen el éxito de la investigación. Feyerabend es el principal defensor de esta postura. En la actualidad, muchos científicos consideran que no existe un único método científico, debido a la complejidad y diversidad de las ciencias.  Algunos científicos han mostrado un vivo interés por la filosofía de la ciencia y algunos como Galileo Galilei, Isaac Newton y Albert Einstein, han hecho importantes contribuciones. Numerosos científicos, sin embargo, se han dado por satisfechos dejando la filosofía de la ciencia a los filósofos y han preferido seguir haciendo ciencia en vez de dedicar más tiempo a considerar cómo se hace la ciencia. Dentro de la tradición occidental, entre las figuras más importantes anteriores al siglo XX destacan entre muchos otros Platón, Aristóteles, Epicuro, Arquímedes, Boecio, Alcuino, Averroes, Nicolás de Oresme, Santo Tomas de Aquino, Jean Buridan, Leonardo da Vinci, Raimundo Lulio, Francis Bacon, René Descartes, John Locke, David Hume, Emmanuel Kant y John Stuart Mill. La filosofía de la ciencia no se denominó así hasta la formación del Círculo de Viena, a principios del siglo XX. En la misma época, la ciencia vivió una gran transformación a raíz de la teoría de la relatividad y de la mecánica cuántica. Entre los filósofos de la ciencia más conocidos del siglo XX figuran Karl R. Popper y Thomas Kuhn, Mario Bunge, Paul Feyerabend, Imre Lakatos, Ilya Prigogine, etc. Filosofía de la Religión Esta sección es un extracto de Filosofía de la religión.[editar] La filosofía de la religión es una rama de la filosofía que tiene por objeto de estudio la religión, la espiritualidad, como una manifestación consciente y reflexiva sobre el sentido trascendente de la existencia y el mundo, lo que incluye sus argumentos sobre la naturaleza, la existencia de Dios, el problema del mal, dando cuenta de su universalismo en tanto que ha prevalecido considerablemente en la historia de las culturas humanas, como también sobre la relación entre la religión y otros sistemas de valores como la ciencia. Se advierte la distinción entre la filosofía de la religión y la filosofía religiosa, dado que la última alude a un saber que se considera inspirado y guiada por su Dios y su religión, como pueden ser las filosofías judía, cristiana e islámica.  El término filosofía de la religión no dio comienzo general en Occidente hasta el siglo XIX, cuando se empleó para referirse a la articulación y crítica de la conciencia religiosa de la humanidad y sus expresiones culturales en el pensamiento, el lenguaje, el sentimiento y la práctica.  La filosofía de la religión difiere de la filosofía religiosa en que trata de debatir cuestiones relativas a la naturaleza de la religión en su conjunto, en lugar de examinar los problemas planteados por un sistema de creencias concreto. Puede ser llevada a cabo desapasionadamente por quienes se identifican como creyentes o no creyentes. Métodos La filosofía utiliza varios métodos de investigación. En general se distingue del método científico por ser a priori, es decir que se realiza sin recurrir a la experiencia (aunque también existe la filosofía experimental). Algunos de estos métodos son comunes a la ciencia, como los experimentos mentales y el método axiomático; otros no, como la duda metódica y la mayéutica. El subcampo de la filosofía que estudia estos métodos se denomina metodología filosófica. Argumento Esta sección es un extracto de Argumento.[editar] Duración: 9 minutos y 20 segundos.9:20 Argumento (hablado por voz AI) Un argumento (del latín argumentum) es la expresión oral o escrita de un razonamiento o idea mediante el cual se intenta probar, validar, refutar o incluso justificar una proposición o tesis. Las cualidades fundamentales de un argumento son la consistencia y coherencia; entendiendo por tal el hecho de que el contenido de la expresión, discurso u obra adquiera un sentido o significado que se dirige a un interlocutor con finalidades diferentes: Como contenido de verdad: consistencia y coherencia con otras verdades admitidas, o con referencia a un hecho o situación que haga verdadero o falso dicho contenido. Como esquema lógico-formal: consistencia y coherencia con un sistema que no admite contradicción. Como función lógico-matemática: consistencia y coherencia con el hecho de «ser algo real» frente a una mera posibilidad lógica que define un mundo o una situación posible en un determinado marco teórico que justifica la función. Como discurso dirigido a la persuasión como motivación para promover o proponer una determinada acción. Como finalidad de acción: consistencia o coherencia con otros intereses o motivaciones del individuo o individuos receptores del contenido como motivación a actuar de determinada manera. Es por tanto un discurso dirigido: Al entendimiento, para «convencer» o generar una creencia nueva mediante el conocimiento evidente de nuevas verdades, basándose en una racionalidad común. A la emotividad para «motivar» una acción determinada. En jurisprudencia un argumento se lo conoce como alegato y sirve para llegar a una verdad procesal en los estados de derecho. Falacia Esta sección es un extracto de Falacia.[editar] En lógica, una falacia (del latín fallacia engaño) es una tesis que parece válida, pero no lo es. Algunas falacias se cometen intencionadamente para persuadir o manipular a los demás, mientras que otras se cometen sin intención debido a descuidos o ignorancia. En ocasiones las falacias pueden ser muy sutiles y persuasivas, por lo que se debe poner mucha atención para detectarlas. Que un argumento sea falaz no implica que sus premisas o su conclusión sean falsas ni que sean verdaderas. Un argumento puede tener premisas y conclusión verdaderas y aun así ser falaz. Lo que hace falaz a un argumento es la invalidez del argumento en sí. De hecho, inferir que una proposición es falsa porque el argumento que la contiene por conclusión es falaz es en sí una falacia conocida como argumento ad logicam. El estudio de las falacias se remonta por lo menos hasta Aristóteles, quien en sus Refutaciones sofísticas identificó y clasificó trece clases de falacias. Desde entonces se han agregado a la lista cientos de otras falacias y se han propuesto varios sistemas de clasificación. Las falacias son de interés no solo para la lógica, sino también para la política, la retórica, el derecho, la ciencia, la religión, el periodismo, la mercadotecnia, el cine y, en general, cualquier área en la cual la argumentación y la persuasión sean de especial relevancia. Deducción Esta sección es un extracto de Razonamiento deductivo.[editar] El razonamiento deductivo o deducción es el proceso lógico que consiste en obtener inferencias deductivas. Una inferencia es deductivamente válida si su conclusión se sigue lógicamente de sus premisas, es decir, si es imposible que las premisas sean verdaderas y la conclusión falsa. Por ejemplo, la inferencia de las premisas todos los hombres son mortales y Sócrates es hombre a la conclusión Sócrates es mortal es deductivamente válida. Un argumento es sólido si es válido y todas sus premisas son verdaderas. Algunos teóricos definen la deducción en términos de las intenciones del autor para facilitar la distinción entre el razonamiento deductivo válido y el inválido. La psicología se interesa por el razonamiento deductivo como un proceso psicológico, es decir, por la forma en que las personas realmente sacan inferencias. La lógica, por otro lado, se centra en la relación deductiva de consecuencia lógica entre las premisas y la conclusión o en cómo la gente debe sacar inferencias. Algunos entienden esta relación en términos de los posibles valores de verdad de interpretaciones. Otros, en cambio, se centran en las reglas de inferencia válidas. Una regla de inferencia es un esquema para sacar una conclusión de un conjunto de premisas basándose únicamente en su forma lógica. Hay varias reglas de inferencia, como el modus ponens y el modus tollens. Los argumentos deductivos inválidos, que no siguen una regla de inferencia, se llaman falacias formales. Las reglas de inferencia son reglas definitorias y contrastan con las reglas estratégicas, que especifican qué inferencias hay que sacar para llegar a una conclusión prevista. El razonamiento deductivo contrasta con el razonamiento no deductivo o ampliativo. Para los argumentos ampliativos, como los argumentos inductivos o abductivos, las premisas ofrecen un apoyo más débil a su conclusión: la hacen más probable, pero no garantizan su verdad. Compensan este inconveniente al poder proporcionar información genuinamente nueva que no se encuentra ya en las premisas, a diferencia de los argumentos deductivos. La psicología cognitiva investiga los procesos mentales responsables del razonamiento deductivo. Uno de sus temas se refiere a los factores que determinan si las personas saquen inferencias deductivas válidas o inválidas. Un factor es la forma del argumento: por ejemplo, las personas tienen más éxito con los argumentos de la forma modus ponens que con el modus tollens. Otro es el contenido de los argumentos: es más probable que la gente crea que un argumento es válido si la afirmación hecha en su conclusión es plausible. Un hallazgo general es que las personas tienden a obtener mejores resultados en casos realistas y concretos que en casos abstractos. Las teorías psicológicas del razonamiento deductivo pretenden explicar estos resultados proporcionando una explicación de los procesos psicológicos subyacentes. Las teorías más conocidas son la teoría de la lógica mental, la teoría de los modelos mentales y la teoría del proceso dual. El problema del razonamiento deductivo es relevante para varios campos y cuestiones. La epistemología trata de comprender cómo la justificación se transfiere de la creencia en las premisas a la creencia en la conclusión en el proceso de razonamiento deductivo. La controvertida tesis del deductivismo niega que haya otras formas correctas de inferencia además de la deducción. La deducción natural es un tipo de sistema de prueba basado en reglas de inferencia simples y evidentes. En filosofía, el método geométrico es una forma de filosofar que parte de un pequeño conjunto de axiomas evidentes y trata de construir un sistema lógico integral utilizando el razonamiento deductivo. Inducción Esta sección es un extracto de Razonamiento inductivo.[editar] El razonamiento inductivo o inducción es una forma de razonamiento en que la verdad de las premisas apoyan la conclusión, pero no la garantizan. Abducción Esta sección es un extracto de Razonamiento abductivo.[editar] Charles Sanders Peirce, fundador del pragmatismo. El razonamiento abductivo (del latín abdūctiō y esta palabra de ab, desde lejos, y dūcere, llevar) es un tipo de razonamiento que, a partir de la descripción de un hecho o fenómeno, ofrece o llega a una hipótesis que explica las posibles razones o motivos del hecho mediante las premisas obtenidas. Charles Sanders Peirce la llama una conjetura. Esa conjetura busca ser, a primera vista, la mejor explicación, o la más probable. Sin embargo, la abducción y la inferencia a la mejor explicación son dos tipos de razonamientos distintos, aunque existen autores que lo discuten. Aristóteles investigó los razonamientos abductivos en sus Primeros analíticos (II, 25). Según Aristóteles, los razonamientos abductivos son silogismos en donde las premisas solo brindan cierto grado de probabilidad a la conclusión. Según Peirce, la abducción es algo más que un silogismo: es una de las formas de razonamiento junto a la deducción y la inducción. Analogía La analogía del relojero es un argumento teleológico para la existencia de Dios el cual afirma que el diseño implica un diseñador. Esta sección es un extracto de Analogía.[editar] Una analogía (del griego αναλογíα, ana reiteración o comparación y logos estudio) es una comparación o relación entre varias cosas, razones o conceptos; comparar o relacionar dos o más seres u objetos a través de la razón; señalando características generales y particulares comunes que permiten justificar la existencia de una propiedad en uno, a partir de la existencia de dicha propiedad en los otros. En el aspecto lógico, permite comparar un objeto con otros, en sus semejanzas y en sus diferencias. Una analogía permite la deducción de un término desconocido a partir del análisis de la relación que se establece entre dos términos desconocidos. La analogía es una mezcla entre razonamiento inductivo y deductivo. A pesar de que la analogía se considera inductiva por el hecho de generalizar, va más allá de la inducción porque no se limita a observar y generalizar patrones, sino que también identifica y aprovecha similitudes estructurales entre diferentes situaciones o conceptos. Mientras que la inducción puede llevar a conclusiones generales a partir de la observación de múltiples casos, la analogía proporciona una herramienta poderosa para inferir propiedades o comportamientos de un caso desconocido basándose en su similitud con un caso conocido. Por ejemplo, si conocemos el funcionamiento de un sistema solar (con planetas orbitando una estrella), podemos usar la analogía para entender otros sistemas estelares, incluso sin tener observaciones detalladas de cada uno. Así, la analogía permite avanzar en el conocimiento y la comprensión al aplicar conocimientos previos a nuevas situaciones de manera creativa y extrapolativa. Nos permite intentar representar un pensamiento o experiencia respecto a un objeto a través de una comparación de distintas dinámicas o situaciones; dando a entender que estas comparten similitudes. Experimento mental Esta sección es un extracto de Experimento mental.[editar] Ejemplo: ¿Podría un chimpancé redactar el Quijote tecleando aleatoriamente sobre una máquina de escribir? Realizando un sencillo cálculo de probabilidades, se deduce que no bastaría la edad del universo para que lo consiguiera. Sin embargo, si cada vez que la tecla pulsada fuera la que corresponde al texto buscado, esta pulsación se guardara como correcta y se pasara a la letra siguiente, el chimpancé acabararía escribiendo el Quijote en un tiempo razonable (ejemplo para intentar explicar cómo funciona la evolución darwinista, preservando los aciertos, y eliminando los errores) Un experimento mental es un recurso de la imaginación empleado para investigar la naturaleza de las cosas. En su sentido más amplio es el empleo de un escenario hipotético que nos ayude a comprender cierto razonamiento o algún aspecto de la realidad. Existe una gran variedad de experimentos mentales y se utilizan en campos tan variados como la filosofía, el derecho, la física y la matemática. Sin embargo, todos emplean una metodología racional independiente de consideraciones empíricas, en el sentido de que no se procede por observación o experimentación física (otra forma de realizar la misma distinción sería entre lo a priori y lo a posteriori). En filosofía, los experimentos mentales se utilizan por lo menos desde la Antigüedad clásica, algunos filósofos presocráticos, y eran igualmente bien conocidos en el derecho romano. Varias teorías o posturas filosóficas se fundan en los resultados de experimentos mentales: el dilema del tranvía en ética, la habitación china y la tierra gemela en filosofía del lenguaje, el cerebro en una cubeta y el cuarto de Mary en filosofía de la mente, etc. En física, el siglo XVII fue testigo de experimentos mentales brillantes por parte de Galileo, Descartes, Newton y Leibniz. La creación de la mecánica cuántica y la relatividad son casi impensables sin el papel crucial jugado por los experimentos mentales. Dos ejemplos famosos de experimentos mentales en física son el demonio de Maxwell y el gato de Schrödinger. Especulación Esta sección es un extracto de Especulación (filosofía).[editar] Este artículo o sección necesita referencias que aparezcan en una publicación acreditada. Busca fuentes: «Filosofía» – noticias · libros · académico · imágenes Este aviso fue puesto el 5 de noviembre de 2013. La especulación (del latín speculari, observar) es una forma filosófica de pensar para ganar conocimiento yendo más allá de la experiencia o práctica tradicional y enfocándose en la esencia de las cosas y sus primeros principios. La especulación es la actividad intelectual que permite la resolución dialéctica de las contradicciones en una unidad de orden superior. El término figura en un lugar crucial en la filosofía de Georg Wilhelm Friedrich Hegel, para el cual este procedimiento de resolución (la Aufhebung o superación) constituía la esencia del pensamiento filosófico. El término griego theoría (visión) fue traducido en latín por speculati y significaba contemplatio al mismo tiempo. En De Trinitate (XV, VIII 14, IX 15), San Agustín reinterpretó el término en una demarcación deliberada de la tradición: con referencia a 1 Cor. 13,12 (Ahora vemos a través de un espejo en forma misteriosa, pero luego cara a cara) y 2 Cor. 3,18 lo derivó de speculum (espejo). En la especulación el hombre ve la verdad como en un espejo oscuro. Este espejo está oscurecido por la caída en el pecado, y el hombre mismo, como ser espiritual y como imagen de Dios, representa el espejo que se puede iluminar con un giro fiel a Dios. El término se transforma aquí con elementos de la teoría de la emanación neoplatónica. Mayéutica Esta sección es un extracto de Mayéutica.[editar] La mayéutica (del griego μαιευτικóς, maieutikós, «perito en partos»; μαιευτικη´, maieutiké, «técnica de asistir en los partos» ) es el método aplicado por Sócrates a través del cual el maestro hace que el alumno, por medio de preguntas, descubra conocimientos. Como la partera, Sócrates lleva a cabo tres funciones principales o fundamentales: despierta y apacigua los dolores del parto, conduce bien los partos difíciles y provoca, si es necesario, el aborto; el proceso es doloroso debido a las crueles interrogantes del método socrático, pero esto desencadena la iluminación, en la que la verdad parte desde el mismo individuo. La invención de este método del conocimiento se remonta al siglo IV a. C. y se atribuye por lo general al Sócrates histórico en referencia a la obra Teeteto, de Platón. La mayéutica es la segunda de las fases del método socrático. La primera es la llamada ironía socrática, en la que el maestro simula ignorancia sobre la materia a tratar, ensalzando inicialmente las cualidades de su interlocutor para, después, hacer comprender a este que lo que creía saber en realidad no lo sabe y que su conocimiento estaba basado en prejuicios o costumbres. A continuación vendría la mayéutica, que es la acción pedagógica del método. La técnica consiste en hacer preguntas al interlocutor mediante las cuales este va descubriendo conceptos generales que le ayudan a ver la luz.  Duda metódica Esta sección es un extracto de Duda metódica.[editar] El filósofo René Descartes popularizó el método de la duda metódica. La duda metódica es un método y principio para llegar a una base de conocimiento cierto, desde donde partir y cómo fundamentar otros conocimientos del mundo. René Descartes populariza este método en el siglo XVII. No obstante, son notables y numerosos los escritos y filósofos anteriores que coinciden en formulaciones similares, no solo en su contenido, sino también con evidentes similitudes formales, que sugieren fuertemente que los pudo haber tomado como fuente de consulta e inspiración en su propia filosofía. Descartes expone que su objetivo es encontrar verdades seguras, tangibles y fácticas de las cuales no sea posible dudar en absoluto, verdades evidentes que permitan fundamentar la edificación del conocimiento con absoluta garantía. El primer problema planteado es cómo encontrarlas y, para resolverlo, expone el método de la duda. En este método la cuestión preliminar y fundamental es la de decidir por dónde empezar la búsqueda. La respuesta y el primer momento de este proceso de búsqueda del conocimiento verdadero es la llamada duda metódica. La duda metódica consiste en descartar cualquier supuesto no seguro, del que se pueda dudar. Si esta existe, este supuesto podría ser verdadero o falso. No permitiría construir sobre él el conocimiento. Descartes publica por primera vez esta idea en francés en 1637, «Je pense, donc je suis», (Pienso, luego existo), en su Discurso del método. Luego aparece en latín en su famosa expresión «Cogito, ergo sum», en 1644 en sus Principios de la Filosofía. Temas Ser Esta sección es un extracto de Ser.[editar] Existen dudas o desacuerdos sobre la exactitud de la información en este artículo o sección. Consulta el debate al respecto en la página de discusión. Este aviso fue puesto el 20 de marzo de 2012. Ser es el más general de los términos. Con la palabra «ser» se intenta abarcar el ámbito de lo real en sentido ontológico general, esto es, la realidad por antonomasia, en su sentido más amplio: «realidad radical». El Ser es, por lo tanto, un trascendental, aquello que trasciende y rebasa todos los entes sin ser él mismo un ente, es decir, sin que ningún ente, por muy amplio que sea y se presente, lo agote. Dicho de otro modo: el Ser desborda y supera dialécticamente el mundo de las formas, el mundus asdpectabilis, trasladándose en otro contexto, «más allá del horizonte de las formas», más allá de toda la morfología cósmica. La pregunta por el ser no corresponde solamente a Occidente: ya los filósofos antiguos de China desarrollaron independientemente posiciones acerca del ser. Laozi en el siglo VI a. C. hace la distinción entre ser y no-ser. Luego, las escuelas neo-taoístas (Wang Bi, Guo Xiang, etc.) harán prevalecer el no-ser sobre el ser. La tradición distingue dos tipos de enfoques distintos al concepto de ser: Concepto unívoco de ser: «ser» es la característica más general de diferentes cosas (llamadas entes o entidades), aquello que sigue siendo igual a todos los entes, después de que se han eliminado todas las características individuales a los entes particulares, esto es: el hecho de que «sean», esto es, el hecho de que a todas ellas les corresponda «ser» (cfr. diferencia ontológica). Este concepto de «ser» es la base de la «metafísica de las esencias». Lo opuesto al «ser» viene a ser en este caso la «esencia», a la cual simplemente se le agrega la existencia. En cierto sentido no se diferencia ya mucho del concepto de la nada. Un ejemplo de ello lo dan ciertos textos de la filosofía temprana de Tomás de Aquino (De ente et essentia). Concepto analógico del ser: el «ser» viene a ser aquello que se le puede atribuir a «todo», aunque de distintas maneras (analogía entis). El ser es aquello, en lo que los diferentes objetos coinciden y en lo que, a su vez, se distinguen. Este enfoque del ser es la base de una metafísica (dialéctica) del ser. El concepto opuesto a ser, es aquí la nada, ya que nada puede estar fuera del ser. La filosofía madura de Tomás de Aquino nos brinda un ejemplo de esta comprensión de «ser» (Summa theologica) Causalidad Esta sección es un extracto de Causalidad (filosofía).[editar] La condición de transitividad x≺y,y≺z⇒x≺z se satisface mediante la relación de causalidad ≺ en cualquier espacio-tiempo lorentziano. En filosofía, la causalidad es la relación necesaria existente entre causa y efecto. Se puede hablar de esa relación entre acontecimientos, procesos, regularidad de los fenómenos y la producción de algo. No existe una única definición comúnmente aceptada del término causa. En su acepción más amplia, se dice que algo es causa de un efecto, cuando el último depende del primero tanto lógicamente, como cronológicamente; o, en otras palabras, la causa es aquello que hace que algo lo sea en su efecto. Esto se puede dar de muchos modos diversos y, por ello, no es extraño que a un efecto correspondan multitud de causas (concausa). Dos condiciones necesarias pero no suficientes para que A sea causa de B son: Que A preceda a B en el tiempo. Que A y B estén relativamente próximos en el espacio y en el tiempo. En general, un proceso tiene muchas causas, que también se dice que son factores causales de la misma, y todos se encuentran en su pasado. A su vez, un efecto puede ser causa o factor causal de muchos otros efectos, todos ellos situados en su futuro. Algunos autores han sostenido que la causalidad es metafísicamente anterior a las nociones de espacio-tiempo.  La causalidad es un tema ontológico que indica cómo progresa el mundo. Como concepto tan básico, es más apto como explicación de otros conceptos de progresión que como algo a explicar por otros más básicos. El concepto es como los de agencia y eficacia. Por esta razón, puede ser necesario un salto de intuición para captarlo. En consecuencia, la causalidad está implícita en la lógica y la estructura del lenguaje ordinario, así como explícita en el lenguaje de la notación causal científica. En los estudios ingleses de filosofía aristotélica, la palabra causa se utiliza como un término técnico especializado, la traducción del término αἰτία de Aristóteles, con el que Aristóteles quería decir explicación o respuesta a una pregunta de por qué. Aristóteles categorizó los cuatro tipos de respuestas como causas material, formal, eficiente y final. En este caso, la causa es el explanans del explanandum, y no reconocer que se están considerando diferentes tipos de causa puede llevar a un debate inútil. De los cuatro modos explicativos de Aristóteles, el más cercano a las preocupaciones del presente artículo es el eficiente. David Hume, como parte de su oposición al racionalismo, argumentó que la razón pura por sí sola no puede probar la realidad de la causalidad eficiente; en su lugar, apeló a la costumbre y al hábito mental, observando que todo el conocimiento humano deriva únicamente de la experiencia. El tema de la causalidad sigue siendo un tema básico en la filosofía contemporánea. Verdad Esta sección es un extracto de Verdad.[editar] El Tiempo salvando a la Verdad de la Falsedad y de la Envidia, tela de François Lemoyne, 1737. La verdad (o veridicidad) se entiende, tanto en filosofía como en el uso ordinario, como la propiedad por la cual ciertos contenidos ,las llamadas unidades portadoras de verdad (truth-bearers), guardan acuerdo con los hechos o con la realidad. Dichas unidades suelen contarse entre creencias, proposiciones y oraciones declarativas con significado estable.  Lejos de ser un tecnicismo, la noción articula prácticas sociales y epistémicas de amplio alcance ,desde la investigación científica, el periodismo y el derecho hasta la teología, el arte y la semiótica, y figura como norma de la aserción (según la cual, al afirmar, se pretende decir algo verdadero), aun si otras normas competidoras como la del conocimiento han sido defendidas.  En la literatura contemporánea, el mapa teórico se organiza en varias familias: Las teorías sustantivas, entre ellas las correspondentistas ,que explica la verdad por un ajuste entre contenido y mundo, ya sea postulando hechos/estados de cosas o, de modo más austero, recurriendo a referencia y satisfacción,;  las coherentistas, que la hace depender del encaje de creencias en un sistema holista; y las pragmáticas, que vinculan lo verdadero con el fin autocorrectivo de la indagación o con la utilidad verificada en la práctica.  Las corrientes constructivistas y de consenso, que subrayan la dimensión social e histórica de los criterios de aceptación. Los enfoques deflacionarios/minimalistas, que tratan “es verdadero” como dispositivo lógico-expresivo más que como propiedad robusta.   El pluralismo de la verdad (o pluralismo alético), que sostiene la realización múltiple del rol funcional de lo verdadero según dominios (e.g., correspondencia en lo fáctico y asertabilidad garantizada en lo normativo).  Los enfoques formales, que estudian la verdad con herramientas lógicas y semánticas: la concepción semántica de Alfred Tarski (Convención T, definiciones recursivas por referencia/satisfacción), las lógicas no bivalentes asociadas a oraciones “no fundamentadas”, y la semántica de hacedores de verdad (truthmakers), donde ciertos estados de cosas sirven de verificadores o falsificadores hiperintensionales.     Estas familias se entrecruzan con el debate realismo/antirrealismo, i.e., toda vez que se polemiza sobre cuestiones como la bivalencia, la objetividad de la referencia o la verificabilidad; y con teorías del significado basadas en condiciones de verdad o en su crítica.   En conjunto, el panorama actual concibe la verdad como un nodo donde convergen mundo, lenguaje y práctica, susceptible de modelarse rigurosamente y, a la vez, de modularse según el tipo de discurso en cuestión.  Moral Esta sección es un extracto de Moral.[editar] La moral es el conjunto de costumbres y normas que se consideran «buenas» para dirigir o juzgar el comportamiento de las personas en una comunidad. También es la diferenciación de intenciones, decisiones y acciones entre las que se distinguen como propias (correctas) y las impropias (incorrectas). Se distingue de la ética en que esta es una moral transcultural o universal, aunque suelen confundirse. La moral permite distinguir qué acciones son consideradas buenas o malas según ciertos criterios compartidos por una sociedad. Otra perspectiva la define como el conocimiento de lo que el ser humano debe hacer o evitar para conservar la estabilidad social. El término «moral» tiene un sentido opuesto al de «inmoral» (contra la moral) y «amoral» (sin moral). La existencia de acciones susceptibles de valoración moral está fundamentada en el ser humano, como sujeto de actos voluntarios. Abarca la acción de las personas en todas sus manifestaciones, además de que permite la introducción y referencia de los valores. Los conceptos y creencias sobre la moral llegan a ser considerados y codificados de acuerdo a una cultura, religión, grupo, u otro esquema de ideas, que tienen como función la regulación del comportamiento de sus miembros. La conformidad con dichas codificaciones también puede ser conocida como moralidad, que se distingue del término «moral» en que la anterior hace referencia a la moral aceptada de forma general en un cierto contexto por un cierto colectivo. Esto no quiere decir que todos los sujetos que comparten una cierta moralidad tengan exactaente la misma moral. Sus morales pueden tener diferencias entre sí, pero también tienen importantes elementos en común que constituyen su moralidad. Con todo, se considera que la sociedad depende del uso generalizado de esta para su existencia. En la práctica, suelen ser conductas morales basadas, no en planteamientos religiosos, sino coherentes con un determinada antropología. Pueden llegar a darse situaciones equívocas si se pretende negar valor ético a comportamientos que tengan su origen en la religión. Hay diversas definiciones y concepciones de lo que significa moral, lo que ha sido tema de discusión y debate a través del tiempo. Múltiples opiniones concuerdan en que el término representa aquello que permite distinguir entre el bien y el mal de los actos, mientras que otros dicen que son solo las costumbres las que se evalúan virtuosas o perniciosas. El concepto de la moral se diferencia de la filosofía moral o ética en que esta última reflexiona racionalmente sobre los diversos esquemas morales con la finalidad de encontrar ideas principales racionales que determinen las acciones de la ética correcta y las acciones de las éticas incorrectas, es decir, la ética busca principios absolutos o universales, independientes de la moral de cada cultura. La moral además tiene 2 sentidos aplicables, por una parte descriptivo, el cual la moral se vale de un conjunto de normas, valores, fines o creencias que guían la conducta de una persona o grupo social . Y por otra parte, un sentido normativo, el cual se basa en un conjunto de normas o criterios que se consideran válidos desde un punto de vista ético, por lo que permite evaluar de manera positiva o negativa los diferentes comportamientos o creencias de cada individuo.  En el ámbito de la filosofía moral también se distingue entre moral y moralidades, el primer término se refiere a los valores y normas adoptados por un individuo en concreto, sin embargo, el término moralidades aluye a un conjunto más amplio de principios aceptados de manera general en un contexto social determinado. La moral puede tener distintos fines según a quien se dirijan las normas. Algunos se centran en el propio individuo buscando la mejora de su propia vida y su bienestar, mientras que otros trascienden y buscan el bienestar de manera general (altruismo). Peter Singer explica que la moral humana ha evolucionado para aumentar de manera gradual nuestro círculos de preocupación: desde intereses personales y familiares hasta el bienestar de personas desconocidas o todos los seres sintientes. Esto refleja como la moral puede combinar fines autodirigidos con fines altruistas, siendo este un equilibrio entre el cuidado de uno mismo y el bienestar y responsabilidad general hacia los demás  La moral no solo regula lo que consideramos correcto o incorrecto, refleja nuestra capacidad de empatizar con los demás y anticipar las consecuencias de nuestras acciones, es un espejo que refleja la sociedad en que vivimos pero también un instrumento para mejorarla y cuestionarla. A diferencia de las normas estrictas o leyes, la moral evoluciona con la experiencia, cultura, la reflexión individual y es flexible, lo que nos permite tener un juicio ético propio. Belleza Esta sección es un extracto de Belleza.[editar] La Catedral de Notre Dame (París) de estilo gótico preclásico. Flores en el jardín botánico de Keukenhof. Atardecer en una playa de la provincia de Phuket, Tailandia. La belleza se describe comúnmente como una cualidad de los entes que hace que estos sean placenteros de percibir. Tales entes pueden incluir paisajes, atardeceres o amaneceres, cielos nocturnos, personas, animales, plantas, obras de arte, etc. Belleza es una noción abstracta ligada a numerosos aspectos de la existencia humana. La belleza se estudia dentro de la disciplina filosófica de la estética, además de otras disciplinas como la historia, la sociología y la psicología social. La belleza se define como la característica de una cosa que a través de una experiencia sensorial (percepción) procura una sensación de placer o un sentimiento de satisfacción. Proviene de manifestaciones tales como la forma, el aspecto visual, el movimiento y el sonido, aunque también se la asocia, en menor medida, a los sabores y los olores. En esta línea y haciendo hincapié en el aspecto visual, Tomás de Aquino define lo bello como aquello que agrada a la vista (quae visa placet). La percepción de la «belleza» a menudo implica la interpretación de alguna entidad que está en equilibrio y armonía con la naturaleza, y puede conducir a sentimientos de atracción y bienestar emocional. Debido a que constituye una experiencia subjetiva, a menudo se dice que «la belleza está en el ojo del observador». Aunque tal relativismo es exagerado y suele asociarse a cosmovisiones y modas, lo concreto es que existen objetos y seres que dan la impresión de belleza ya desde su objetividad natural porque se corresponden con los requisitos naturales del homo sapiens, por ejemplo: el sabor dulce es preferido al sabor amargo porque el amargo suele corresponder a tóxicos, lo mismo que la fragancia de muchas flores se prefiere naturalmente en gente psíquicamente sana al hedor pútrido. Una dificultad para entender la belleza se debe al hecho de que tiene aspectos tanto objetivos como subjetivos: es vista como una propiedad de las cosas pero también como dependiente de la respuesta emocional de los observadores. Se ha argumentado que la capacidad del sujeto necesaria para percibir y juzgar la belleza, a veces conocida como el sentido del gusto, puede entrenarse y que los veredictos de los expertos coinciden a largo plazo. Esto sugeriría que los estándares de validez de los juicios de belleza son intersubjetivos, es decir, dependientes de un grupo de jueces, en lugar de completamente subjetivos o completamente objetivos. Las concepciones de la belleza apuntan a captar lo que es esencial en todas las cosas bellas. Las concepciones clásicas definen la belleza en términos de la relación entre el objeto bello como un todo y sus partes: las partes deben estar en la proporción correcta entre sí y así componer un todo armonioso integrado. Las concepciones hedonistas incluyen la relación con el placer en la definición de belleza al sostener que hay una conexión necesaria entre el placer y la belleza, por ejemplo, que para que un objeto sea bello es necesario que cause placer desinteresado. Otras concepciones incluyen definir los objetos bellos en términos de su valor, de una actitud amorosa hacia ellos o de su función. Problemas Categoría principal: Problemas filosóficos Un problema filosófico es una cuestión cuyo planteamiento teórico se presenta sin aparente respuesta o cuya solución es cuestionable. Los problemas tienden a expresar temas de recurrente dominio de la filosofía como: la sabiduría, el hombre, la verdad, el conocimiento, la moral, el arte, la reflexión, la conciencia, la lógica, la realidad, la ciencia, el sentido de la vida, etc. Estos problemas se pueden presentar en dilemas o paradojas. Dilema del tranvía Esta sección es un extracto de Dilema del tranvía.[editar] Este artículo o sección tiene referencias, pero necesita más para complementar su verificabilidad. Busca fuentes: «Filosofía» – noticias · libros · académico · imágenes Este aviso fue puesto el 11 de septiembre de 2016. ¿Deberías desviar el tranvía? El dilema del tranvía es un experimento mental en ética, ideado por Philippa Foot y analizado extensamente por Judith Jarvis Thomson y, más recientemente, Peter Unger. Problemas similares han sido tradicionalmente tratados en derecho penal y, algunas veces, regulados en los códigos penales, también en derecho civil. Un ejemplo clásico de esos problemas es conocido como la tabla de Carnéades, elaborado por Carnéades para atacar la inconsistencia de las teorías morales estoicas. El filósofo del derecho alemán Karl Engisch analizó un dilema similar en su tesis de habilitación de 1930, al igual que el jurista alemán Hans Welzel en un trabajo de 1951.  Dilema de Eutifrón Esta sección es un extracto de Dilema de Eutifrón.[editar] Sócrates y Eutifrón, por Victor Orsel. El dilema de Eutifrón es planteado en el diálogo Eutifrón de Platón. Sócrates pregunta a Eutifrón: «¿Es el piadoso (τὸ ὅσιον) amado por los dioses porque es piadoso, o es piadoso debido a que es amado por los dioses?». Aunque originalmente se aplicó al panteón griego antiguo, el dilema tiene implicaciones para las religiones monoteístas modernas. Gottfried Leibniz preguntó si lo bueno y lo justo es bueno solo porque Dios lo quiere o si Dios lo quiere porque es bueno y justo. Desde la discusión original en Platón, esta cuestión ha presentado un problema para el teísmo, haciendo que surgieran defensas para ambos cuernos del dilema respectivamente o incluso que es un falso dilema. Aún hoy sigue siendo objeto de discusión teológica y filosófica, en gran parte dentro de las tradiciones cristianas, judías e islámicas. Trilema de Münchhausen Esta sección es un extracto de Trilema de Münchhausen.[editar] Este artículo o sección necesita referencias que aparezcan en una publicación acreditada. Busca fuentes: «Filosofía» – noticias · libros · académico · imágenes Este aviso fue puesto el 1 de diciembre de 2015. El barón de Munchausen se saca del lodo tirando de su propio pelo. El trilema de Münchhausen o trilema de Agripa es un ataque a la posibilidad de lograr una justificación última para cualquier proposición, incluso en las ciencias formales como la matemática y la lógica. Un trilema es un problema que admite solo tres soluciones, todas las cuales parecen inaceptables. El argumento discurre así: cualquiera que sea la manera en que se justifique una proposición, si lo que se quiere es certeza absoluta, siempre será necesario justificar los medios de la justificación, y luego los medios de esa nueva justificación, etc. Esta simple observación conduce sin escape a una de las siguientes tres alternativas (los tres cuernos del trilema): Una regresión infinita de justificaciones: A se justifica por B, B se justifica por C, C se justifica por D, etc. La necesidad de remontarse cada vez más en la búsqueda de fundamentos es un proceso sin fin, irrealizable, que no provee ningún fundamento seguro. Un corte arbitrario en el razonamiento: A se justifica por B, B se justifica por C, y C no se justifica. Esta última proposición se puede presentar como de sentido común o como un principio fundamental (postulado o axioma), pero en cualquier caso representaría una suspensión arbitraria del principio de razón suficiente recurriendo a un dogma. Una justificación circular: A se justifica por B, B se justifica por C, y C se justifica por A. En el proceso de justificación se recurre a enunciados que ya antes se habían mostrado como enunciados que requieren justificación y, por lo tanto, sin llegar nunca a una justificación segura por ser lógicamente defectuosa. Problema de Gettier Esta sección es un extracto de Problema de Gettier.[editar] El problema de Gettier es un problema en gnoseología moderna que surge al presentar contraejemplos a la definición clásica de conocimiento como «creencia verdadera justificada» y que obligan a modificar la definición. Desde al menos el Teeteto de Platón, la gnoseología contaba con una definición generalmente satisfactoria del conocimiento proposicional: si S es un sujeto y p una proposición, entonces S sabe que p si y solo si: S cree que p p es verdadera S está justificado en creer que p Por ejemplo, Newton sabe que de alguna manera tiene una manzana si y solo si: Newton cree que tiene una manzana Es verdad que tiene una manzana Newton está justificado en creer que tiene una manzana Sin embargo, en 1963, Edmund Gettier publicó un artículo de tres páginas titulado ¿Es el conocimiento creencia verdadera justificada? en el que argumentó que la definición clásica no es suficiente. Gettier mostró que hay casos en los que una creencia verdadera justificada puede fallar en ser conocimiento. Es decir, hay casos en los que los tres requisitos se cumplen, y sin embargo intuitivamente nos parece que no hay conocimiento. Retomando el ejemplo anterior, podría ser que Newton crea que tiene una manzana y esté justificado en ello (por ejemplo, porque parece una manzana), pero que sin embargo la manzana sea de cera. En ese caso, según la definición clásica, Newton no posee conocimiento, porque falta que sea verdad que tiene una manzana. Pero supongamos también que dentro de la manzana de cera hay otra manzana, más pequeña, pero real. Entonces Newton cumple con los tres requisitos: Newton cree que tiene una manzana; Newton está justificado en su creencia; y de hecho tiene una manzana. Sin embargo, intuitivamente nos parece que Newton no posee conocimiento, sino que solamente tuvo suerte (lo que se llama suerte epistémica). Frente a este problema, muchos filósofos contemporáneos intentaron y aún intentan reparar la definición, dando lugar a nuevas corrientes gnoseológicas. Otros filósofos han propuesto problemas ligeramente diferentes, que se han incorporado a los contraejemplos enunciados por Gettier. El conjunto de estos problemas y el desafío que plantean a la cuestión ¿qué es conocer? recibe el nombre de el problema de Gettier. Aunque se han dedicado cientos de artículos a esta cuestión, no hay consenso respecto a la solución al problema general. El problema de Gettier es uno de los motores principales de la gnoseología contemporánea. Problema de la inducción Esta sección es un extracto de Problema de la inducción.[editar] El hallazgo por Willem de Vlamingh de cisnes negros en Australia, en 1697, obligó a corregir la vieja creencia inductiva de que todos los cisnes eran blancos. Lo mismo podría ocurrir en el futuro con otras generalizaciones científicas. El problema de la inducción radica en si un resultado obtenido mediante inducción está justificado epistemológicamente, es decir, si la inducción produce conocimiento. A partir de la definición de Platón del conocimiento como «creencia verdadera y justificada», disponer de una justificación adecuada para la inducción es requisito indispensable para que tales «creencias» constituyan conocimiento válido o legítimo. La RAE define «inducir» en su acepción filosófica como «extraer, a partir de determinadas observaciones o experiencias particulares, el principio general que en ellas está implícito». Esas «extracciones» son de dos tipos: Generalizaciones acerca de las propiedades de «clases de objetos», efectuadas a partir de una cierta cantidad de observaciones de casos individuales. Por ejemplo: la inferencia que las esmeraldas son verdes a partir de la observación de algunas (quizás muchas) esmeraldas individuales. Presunción de que eventos en el futuro continuarán presentando la misma forma o que las mismas causas ocasionarán los mismos efectos observados en el pasado (ver causalidad). Lo que implica, por ejemplo, la creencia en que las leyes científicas serán válidas en el futuro. John Stuart Mill llamó a esta asunción el principio de la uniformidad de la naturaleza. Dado que ambas son utilizadas, ya sea explícita o implícitamente, en forma generalizada para proponer hipótesis ,ya sea formales o no, a partir de observaciones empíricas, su cuestionamiento pone en duda una gran parte, si es que no la totalidad, del conocimiento humano. El problema adquiere especial relevancia en el ámbito científico, dado que generalmente se pensaba que las asunciones necesarias para formular leyes científicas requieren tanto generalizaciones como expectativas de que eventos en el futuro continuarán exhibiendo los mismos comportamientos que en el pasado. Esto se expresa generalmente como el principio de simetría  o principio de invariancia (véase también principio de Curie y teorema de Noether). Consecuentemente Alfred North Whitehead describió la inducción como «el rompecabezas (the despair) de la filosofía» y el filósofo C. D. Broad sugirió: «La inducción es la gloria de la ciencia, y el escándalo de la filosofía». Según el sentido de la teoría de la justificación la ciencia ha de consistir en proposiciones probadas. El experimento no es una verificación de la teoría que lo sustenta, como mostró Karl Popper desnudando el problema de la inducción. Por otro lado, las inferencias lógicas transmiten la verdad, pero no sirven para descubrir nuevas verdades. Las teorías generales no son directamente contrastables con la experiencia, sino solamente mediante casos particulares, con soluciones específicas mediante teorías específicas, como modelos teoréticos. Cuanto mayor sea la lógica que detente una teoría, menor será la contrastabilidad empírica. Esto quiere decir que teorías tan generales como la teoría de la información, mecánica clásica o mecánica cuántica solo pueden ser contrastadas respecto a modelos teóricos específicos en el marco de dichas teorías, teniendo en cuenta que no siempre es posible saber qué es lo que hay que corregir en el modelo cuando el contraste empírico fracasa o, si por el contrario, es la propia teoría general la que contiene el error, teniendo muy presente la dificultad de poder asegurar que el valor de los datos manejados y obtenidos sean los correctos. Por ello la filosofía de la ciencia adquiere un carácter de investigación científica muy importante.  Problema del ser y el deber ser Esta sección es un extracto de Problema del ser y el deber ser.[editar] David Hume planteó el problema del ser y el deber ser en su Tratado sobre la naturaleza humana. Este artículo o sección necesita referencias que aparezcan en una publicación acreditada. Busca fuentes: «Filosofía» – noticias · libros · académico · imágenes Este aviso fue puesto el 20 de abril de 2024. El problema del ser y el deber ser (también llamado ley de Hume, la guillotina de Hume y a veces confundido con la falacia naturalista) es un problema en metaética sobre la posibilidad de deducir oraciones normativas a partir de oraciones descriptivas. Oraciones descriptivas son aquellas que dicen lo que es el caso (p. ej. «los emperadores son crueles») mientras que oraciones normativas son aquellas que dicen lo que debe ser el caso («los emperadores deben ser crueles»). Claro que así como se puede pedir justificación para las oraciones normativas, se puede pedir justificación para las oraciones descriptivas. Pero esto es otro problema, que puede encontrar otras respuestas. Las oraciones descriptivas se pueden (quizás) justificar a partir de la investigación empírica. Así, por ejemplo, el valor de verdad de la oración «los emperadores son crueles» se puede determinar haciendo una investigación histórica. Sin embargo, no sucede lo mismo con la oración «los emperadores deben ser crueles». La verdad o falsedad de esta oración se debe determinar por otros métodos, y si se descarta la posibilidad de probar su verdad a través de una deducción a partir de premisas verdaderas, entonces vale preguntar si hay algún otro camino. El abismo que separa a los hechos de los deberes no tiene nada que ver con el contenido de las proposiciones descriptivas de las que se parte. Lo mismo da que se trate de proposiciones metafísicas, científicas o de la vida cotidiana. El error se encuentra en el procedimiento, no en el punto de partida. La ambigüedad inadvertida empírico-normativa de ciertos términos conduce a falacias lógicas tales como: «La esencia de la sexualidad es la procreación. Por lo tanto, la anticoncepción no está permitida, porque no refleja la naturaleza de la sexualidad». La dicotomía hechos/valores de Hume, se relaciona con la dicotomía analítico/sintético: las proposiciones analíticas (lógicas) no tienen necesidad de verificación (siempre son verdaderas), mientras que las proposiciones sintéticas se deben verificar con la experiencia y pueden ser verdaderas o falsas, y las proposiciones éticas vienen de la experiencia. Problema mente-cuerpo Esta sección es un extracto de Problema mente-cuerpo.[editar] Diagrama de los diferentes enfoques para resolver el problema mente-cuerpo. En filosofía del espíritu y ciencia cognitiva, el problema mente-cuerpo es el problema de explicar la relación entre la mente (alma para algunos autores) y la materia: cómo es que estados mentales o subjetivos (ej. sensaciones, creencias, decisiones, recuerdos) explican a, interactúan con, o bien supervienen de las sustancias y procesos del mundo de objetos estudiado por la ciencia. Se trata por lo tanto de un problema ontológico; mientras que el problema de otras mentes puede ser entendido como su homólogo epistémico. El problema fue descrito por René Descartes en el siglo XVII, y por los filósofos aristotélicos, en la filosofía de Avicena, y en las anteriores tradiciones asiáticas.  Una variedad de ontologías han sido propuestas; la mayoría de ellas dualistas (como la cartesiana) o monistas. El dualismo sostiene una distinción entre las esferas material y mental; pudiendo llegar a ser esta última algo sobrenatural. El monismo sostiene que existe solo una realidad, sustancia o esencia unificadora en cuyos términos todo puede ser explicado. El problema mente-cuerpo está estrechamente ligado a la intencionalidad, la causalidad mental, el problema difícil de la consciencia, el del libre albedrío, el de la significación de los símbolos, el de la identidad del individuo, el problema de otras mentes, etc. La ausencia de un punto de interacción causal identificable entre la mente no-física y su extensión física ha demostrado ser problemática para el dualismo de sustancias, y muchos filósofos de la mente contemporáneos piensan que la psique no es algo separado del cuerpo. Las posturas no cartesianas y no idealistas también van ganando terreno en círculos científicos. A esto ha ayudado el advenimiento de la sociobiología, la computación, la psicología evolutiva, la revolución cognitiva y las evidencias de la neurociencia que ponen de manifiesto la dependencia de los fenómenos mentales en sustratos corporales.   Aun así, se considera que el problema mente-cuerpo sigue abierto y está lejos de ser sepultado. En efecto, filósofos de corte materialista como David Chalmers y Colin McGinn advierten que algunas de las preguntas planteadas podrían ser inasequibles a la explicación científica o de cualquier otro tipo. Otros como Daniel Dennett dan pronósticos más optimistas, sin dejar de reconocerlo en calidad de problema. Problema del mal Esta sección es un extracto de Problema del mal.[editar] Lactancio atribuye a Epicuro ser el primer exponente del problema del mal en De Ira Dei.  El problema del mal, argumento del mal o también, paradoja de Epicuro, es estudiado en filosofía de la religión y teodicea como el problema que resulta al considerar la compatibilidad entre la presencia del mal y del sufrimiento en el mundo con la existencia de un Dios omnisciente, omnipotente y omnibenevolente. El problema del mal se puede expresar de la siguiente forma:  ¿Es que Dios quiere prevenir el mal, pero no es capaz? Entonces no es omnipotente. ¿Es capaz, pero no desea hacerlo? Entonces es malévolo. ¿Es capaz y desea hacerlo? ¿De dónde surge entonces el mal? ¿Es que no es capaz ni desea hacerlo? Entonces, ¿por qué llamarlo Dios? Paradoja o trilema de Epicuro El argumento del mal afirma que debido a la existencia del mal, o Dios no existe o no tiene alguna de las tres propiedades mencionadas. Los argumentos para sostener lo contrario se conocen tradicionalmente como teodiceas. Además de en la filosofía de la religión, el problema del mal también es importante en los campos de la teología y la ética. Aunque suele atribuirse a Epicuro, como su propio nombre señala, la paradoja no se encuentra en ninguno de sus escritos conocidos. El problema se formula de dos formas: el problema lógico del mal y el problema evidencial del mal. La versión lógica del argumento intenta demostrar deductivamente una imposibilidad lógica en la coexistencia entre Dios y el mal, mientras que la evidencial sostiene inductivamente que dado que existe el mal en el mundo, es improbable que exista un dios omnipotente, omnisciente y perfectamente bueno. Esta versión inductiva del argumento es más popular que su versión deductiva. El problema del mal también se ha extendido a los seres vivos no humanos, incluidos el sufrimiento animal provocado por la naturaleza y la crueldad animal humana. También se diferencian dos tipos de males: mal moral, causado por actos humanos (como el Holocausto); y mal natural, causado por eventos que no tienen que ver con los humanos (como el terremoto de Lisboa de 1755). Existe una amplia variedad de respuestas al problema del mal y se clasifican en: refutaciones, defensas y teodiceas. Hay además muchas discusiones sobre el mal y problemas relacionados en otros campos filosóficos, tales como la ética secular  o la ética evolucionista, pero en el sentido ordinario el problema del mal se trata dentro del contexto teológico.  El problema del mal se aplica intensamente a las religiones monoteístas seguidoras del teísmo clásico, como cristianismo, islam y judaísmo, que creen en un único dios que es omnipotente, omnisciente y omnibenevolente; pero la pregunta ¿Por qué existe el mal? ha sido estudiada en religiones no teístas o politeístas como el budismo, hinduismo y jainismo. El problema del mal también se aplica al politeísmo si algún dios tiene los atributos del mal. Historia Occidente Esta sección es un extracto de Historia de la filosofía occidental.[editar] La historia de la filosofía occidental es la historia de las tradiciones filosóficas desarrolladas y extendidas por Occidente. Se desarrolló de manera relativamente independiente de otras culturas de Eurasia, agrupadas en contraste con la historia de la filosofía oriental. Se remonta a más de 2500 años de antigüedad en Grecia y se puede dividir en cinco grandes periodos: filosofía antigua, medieval, renacentista, moderna y contemporánea, que se corresponden con la periodización de la historia de Europa: Edad Antigua, Edad Media, Renacimiento, Edad Moderna y Edad Contemporánea. La base griega común ha transmitido a la tradición filosófica occidental un método de pensamiento marcado por el antidogmatismo y la sensibilidad hacia una serie de cuestiones ontológicas y éticas que la han caracterizado respecto a otras tradiciones filosóficas. No se puede pasar por alto entonces, como segundo sustrato de la filosofía occidental, la tradición judaico-cristiana, que ya en la antigüedad tardía estableció una compleja relación con el pensamiento secular, introduciendo una serie de conceptos novedosos en el pensamiento filosófico e iniciando esa dialéctica entre fe y razón diversamente resuelta a lo largo de los siglos. Filósofo y alumnos, pintura de Willem van der Vliet (1626) La filosofía occidental ha influido sobre otras ramas del conocimiento humano, por ejemplo, en el ámbito de la ciencia, la religión y la política. Muchos filósofos importantes fueron a la vez grandes científicos, teólogos o políticos y algunas nociones fundamentales de estas disciplinas todavía son objeto de estudio filosófico. Esta superposición entre disciplinas se debe a que la filosofía es una disciplina muy amplia. En el siglo XIX, el crecimiento de las universidades de investigación modernas llevó a la filosofía académica y otras disciplinas a profesionalizarse y especializarse. Desde entonces, varias áreas de investigación que tradicionalmente formaban parte de la filosofía se han convertido en disciplinas académicas separadas como la psicología, la sociología, la biología, la lingüística y la economía. Edad Antigua Esta sección es un extracto de Filosofía antigua.[editar] La filosofía antigua es el período de la historia de la filosofía occidental que corresponde a la Edad Antigua. Comprende la filosofía griega (presocrática y helenística) y la filosofía romana. Duró más de 1100 años, desde alrededor del año 600 a. C. (con Tales de Mileto) hasta el siglo VI d. C., cuando los últimos neoplatónicos estaban activos. Sus principales ubicaciones fueron la antigua Grecia y el Imperio romano. La filosofía de la antigüedad fue limitada geográficamente en el Mediterráneo. Los filósofos de la antigüedad pueden dividirse a grandes rasgos en diferentes grupos. Primero, los filósofos anteriores a Sócrates, llamados «presocráticos» (alrededor del 600 - 400 a. C.) y conocidos por dar «el paso del mito al logos». Luego, el período clásico griego, que comienza con Sócrates (alrededor del 500 - 300 a. C.). Platón, alumno de Sócrates, y Aristóteles, alumno de Platón, se convirtieron en dos de los filósofos más importantes e influyentes, conocidos como los «socráticos mayores». Otros contemporáneos fueron los sofistas y los «socráticos menores» (megáricos, cínicos y cirenaicos). Finalmente, la filosofía del período helenístico siguió al período clásico, seguida por la filosofía de la antigüedad tardía, que incluyen a los epicúreos, los estoicos, los escépticos y los neoplatónicos. Otras tradiciones filosóficas importantes de la antigüedad fueron la filosofía china y la filosofía india, influyentes fueron las culturas del judaísmo, el antiguo Egipto, el Imperio persa y Mesopotamia. En las regiones del Creciente Fértil, Irán y Arabia surgió la literatura filosófica de los libros sapienciales y que hoy domina la cultura islámica. La literatura sapiencial temprana del Creciente Fértil era un género que buscaba instruir a las personas sobre la acción ética, la vida práctica y la virtud a través de historias y proverbios. En el Antiguo Egipto, estos textos eran conocidos como sebayt («enseñanzas») y son fundamentales para nuestra comprensión de la filosofía del Antiguo Egipto. La astronomía babilónica también incluyó muchas especulaciones filosóficas sobre la cosmología que pudieron haber influido en los antiguos griegos. La filosofía judía y la filosofía cristiana son tradiciones religio-filosóficas que se desarrollaron tanto en Oriente Medio como en Europa, que comparten ciertos textos judaicos primitivos (principalmente el Tanaj) y creencias monoteístas. Los pensadores judíos como los Geonim de las Academias Talmúdicas en Babilonia y el filósofo Maimónides estudiaban la filosofía griega e islámica. Más tarde, la filosofía judía estuvo bajo fuertes influencias intelectuales occidentales e incluye las obras de Moisés Mendelssohn, quien marcó el comienzo de la Haskalá (también conocida como la ilustración judía), el existencialismo judío y el judaísmo reformista. La filosofía persa preislámica comienza con el trabajo de Zoroastro, uno de los primeros promotores del monoteísmo y del dualismo entre el bien y el mal. Esta cosmogonía dualista influyó en los desarrollos iraníes posteriores, como el maniqueísmo, el mazdakismo y el zurvanismo. Edad Media Esta sección es un extracto de Filosofía medieval.[editar] La filosofía medieval es todo el conjunto de corrientes de pensamiento y tratados filosóficos medievales que se desarrollaron desde la caída del Imperio romano (476 d. C.) hasta el Renacimiento (siglos XV y XVI). La principal búsqueda de la filosofía medieval era la cohesión de las creencias heredadas de la filosofía clásica con los dogmas del cristianismo, aunque también hubo aportes muy importantes de las creencias judías e islámicas. Renacimiento Esta sección es un extracto de Filosofía renacentista.[editar] El Hombre de Vitruvio, de Leonardo Da Vinci, resume varias ideas del pensamiento renacentista. La filosofía renacentista, o filosofía del Renacimiento, es la filosofía que se desarrolló principalmente entre los siglos XV y XVI, comenzando en Italia y avanzando hacia el resto de Europa. En el Renacimiento, la filosofía todavía era un campo muy amplio que abarcaba los estudios que hoy se asignan a varias ciencias distintas, así como a la teología. Teniendo eso en cuenta, los tres campos de la filosofía que más atención y desarrollo recibieron fueron la filosofía política, el humanismo y la filosofía natural. En la filosofía política, las rivalidades entre los estados nacionales, sus crisis internas y el comienzo de la colonización europea de América renovaron el interés por problemas acerca de la naturaleza y moralidad del poder político, la unidad nacional, la seguridad interna, el poder del Estado y la justicia internacional. En este campo destacaron los trabajos de Nicolás Maquiavelo, Jean Bodin y Francisco de Vitoria. El humanismo fue un movimiento que enfatizó el valor y la importancia de los seres humanos en el universo, en contraste con la filosofía medieval, que siempre puso a Dios y al cristianismo en el centro. Este movimiento fue, en primer lugar, un movimiento moral y literario, protagonizado por figuras como Erasmo de Róterdam, Santo Tomás Moro, Bartolomé de las Casas y Michel de Montaigne. La filosofía de la naturaleza del Renacimiento quebró con la concepción medieval de la naturaleza en términos de fines y ordenamiento divino, y comenzó a pensar en términos de fuerzas, causas físicas y mecanismos. Hubo además un retorno parcial a la autoridad de Platón por sobre Aristóteles, tanto en su filosofía moral, en su estilo literario como en la relevancia dada a la matemática para el estudio de la naturaleza. Nicolás Copérnico, Giordano Bruno, Johannes Kepler, Leonardo da Vinci y Galileo Galilei fueron precursores y protagonistas en esta revolución científica, y Francis Bacon proveyó un fundamento teórico para justificar el método empírico que habría de caracterizar a la revolución. Por otra parte, en la medicina, el trabajo de Andreas Vesalius en anatomía humana revitalizó la disciplina y brindó más apoyo al método empírico. La filosofía de la naturaleza renacentista tal vez se explica mejor por dos proposiciones escritas por Leonardo da Vinci en sus cuadernos: Todo nuestro conocimiento tiene sus orígenes en nuestras percepciones. No hay certeza en la que no se puedan usar ninguna de las ciencias matemáticas ni ninguna de las ciencias derivadas de las ciencias matemáticas. De manera similar, Galileo basó su método científico en experimentos, pero también desarrolló métodos matemáticos para su aplicación a problemas de física, un ejemplo temprano de física matemática. Estas dos formas de concebir el conocimiento humano formaron el fondo para el inicio del empirismo y el racionalismo, respectivamente. Otros filósofos del renacimiento influyentes fueron Pico della Mirandola, Nicolas de Cusa, Michel de Montaigne, Francisco Suárez, Erasmo de Róterdam, Pietro Pomponazzi, Bernardino Telesio, Johannes Reuchlin, Tommaso Campanella, Gerolamo Cardano y Luis Vives. Edad Moderna Esta sección es un extracto de Filosofía moderna.[editar] René Descartes, padre de la filosofía moderna.  La filosofía moderna es aquella filosofía desarrollada durante la Edad Moderna y asociada con la modernidad. No es una doctrina concreta o escuela (por lo que no debe ser confundida con movimientos específicos como el Modernismo), a pesar de que muchos autores de esta era comparten ciertos supuestos comunes, lo cual ayuda para distinguirla de filosofía anterior y posterior. Los siglos XVII y principios del XX marcan aproximadamente el principio y el final de la filosofía moderna. Cuánto del Renacimiento debe incluirse es una cuestión de disputa; del mismo modo, la modernidad puede o no haber terminado en el siglo XX y haber sido reemplazada por la postmodernidad. La forma en que uno decida estas cuestiones determinará el alcance de su uso del término filosofía moderna. Edad Contemporánea Esta sección es un extracto de Filosofía contemporánea.[editar] Friedrich Nietzsche criticó la metafísica y la objetividad del conocimiento y la razón del pensamiento dominante filosófico occidental. Pintura de Edvard Munch. La filosofía contemporánea es el período actual de la historia de la filosofía. Por extensión, se llama también con este nombre a la filosofía producida por filósofos que aún están vivos. Es el período que sigue a la filosofía moderna, y su inicio se suele fijar a finales del siglo XIX o principios del siglo XX. Las tradiciones filosóficas más significativas y abarcadoras del siglo XX fueron la filosofía analítica en el mundo anglosajón, y la filosofía continental en la Europa continental. El siglo XX también vio el surgimiento de nuevas corrientes filosóficas, como el positivismo lógico, la fenomenología, el existencialismo, el postestructuralismo o el actualismo. En este período la mayoría de los filósofos más importantes trabajaron desde las universidades, especialmente en la segunda mitad del siglo. Algunos de los temas más discutidos fueron la relación entre el lenguaje y la filosofía (este hecho a veces es llamado «el giro lingüístico»). Los principales exponentes de este «giro» fueron Martin Heidegger en la tradición continental y Ludwig Wittgenstein en la tradición analítica. Oriente Esta sección es un extracto de Filosofía oriental.[editar] La filosofía oriental o filosofía asiática incluye las diversas filosofías de Asia del Sur y Asia Oriental, incluida la filosofía china, la filosofía india, la filosofía budista (dominante en el Tíbet, Bhután, Sri Lanka y el Sudeste Asiático), la filosofía coreana y la filosofía japonesa.  La categoría de «filosofía oriental» o «filosofía asiática» es un producto de la academia occidental del siglo XIX y no existía en Asia Oriental ni en la India. Esto se debe a que en Asia no existe una sola tradición filosófica unificada con una sola raíz, sino varias tradiciones autóctonas que a veces han estado en contacto. Arthur Schopenhauer fue uno de los primeros pensadores de la filosofía occidental en compartir y afirmar principios importantes de la filosofía oriental, como el ascetismo y la apariencia del mundo. Persia Esta sección es un extracto de Filosofía persa.[editar] La filosofía persa o filosofía iraní   se remonta a tiempos de tradiciones filosóficas y pensamientos que se originaron en la antigua persia con raíces indo-iraníes y fueron influenciadas considerablemente por las enseñanzas de Zoroastro. La cronología de la materia y de la ciencia de la filosofía comienza con los indo-iraníes, que datan este evento a 1500 a. C. La filosofía de Zaratustra ingresó a influir la tradición occidental a través del Judaísmo, y por lo tanto en el platonismo medio. A lo largo de la historia iraní y debido a los cambios políticos y sociales notables tales como conquista musulmana de Persia y las invasiones mongolas de Persia, un amplio espectro de escuelas de pensamiento mostraron una variedad de puntos de vista sobre cuestiones filosóficas que se extienden desde antiguas tradiciones iraníes y sobre todo relacionadas al zoroastrismo, a las escuelas que aparecen en las finales de la era preislámica, como el maniqueísmo y el Mazdakismo, así como varias escuelas post-islámicas. La filosofía iraní después de la invasión árabe de Persia, se caracteriza por diferentes interacciones con la filosofía persa antigua, la filosofía griega y con el desarrollo de la filosofía islámica. La escuela de iluminación y la filosofía trascendente son consideradas como dos de las principales tradiciones filosóficas de la época en Persia. India Esta sección es un extracto de Filosofía india.[editar] La filosofía india es una tradición milenaria que busca la liberación del sufrimiento y el ciclo de reencarnaciones. Se divide en escuelas ortodoxas (como Vedānta, Yoga y Sāṃkhya) que aceptan los Vedas, y heterodoxas (como el budismo y jainismo) que no los aceptan. Sus temas centrales son la naturaleza del ser, la realidad, el conocimiento y la ética. El «om» es uno de los mantras más sagrados de las religiones dhármicas como el hinduismo y el budismo. Estas corrientes no forman una tradición unificada, sino que representan un mosaico diverso de ideas, enfoques y sistemas de pensamiento. El término «filosofía india» ―como la «filosofía oriental» (de toda Asia), abarca doctrinas, visiones del mundo y enseñanzas religiosas y no religiosas. Hablar de «filosofía india» como un todo homogéneo es un error conceptual. En sánscrito (antiguo idioma de la India), las doctrinas se llaman dárshana (darśana: doctrina, visión doctrinal, sistema filosófico, mostrar, ver, mirar, saber, exhibir, enseñar, percibir, inspeccionar, examinar, visitar, experimentar, contemplar, juzgar, discernir, comprender)  Según el téxto épico-mitológico Majabárata (12, 11045), escrito en el III a. C., las tradiciones de la antigua India se clasifican de dos maneras: astika ([Dios] existe, en sánscrito; siendo asti estar o existir) y nastika ([Dios] no existe).</ref> Esa división depende de si las doctrinas aceptan la autoridad del texto épico-mitológico Rig-veda (compuesto a mediados del III milenio a. C.) y si aceptan la doctrina del Brahman (divinidad impersonal hinduista) y la existencia del atma (alma).  Existen seis sistemas ortodoxos (shad-dárshana: seis doctrinas): sankia (de Kapila yoga, niaia (de Gotama), vaisesika (de Kanāda), mimamsa (de Yaimini) y vedanta (de Badaraiana). Las escuelas heterodoxas más comunes son lokaiata (ateísmo, materialismo, enseñado por Charvaka en algún momento del I milenio a. C.) yainismo (desde el VI a. C.) budismo (desde el IV a. C.) agñana ayívika. Algunos de los primeros textos filosófico-religiosos son los Upanishads, que se compusieron en el período posvédico (siglos VII a I a. C. Los conceptos religiosos indios más importantes incluyen dharma (deber religioso), karma (reacción mágica a cada acto virtuoso o pecaminoso), samsara (reencarnación), moksha (liberación del alma del ciclo de reencarnaciones) y ajimsa (no violencia: vegetarianismo). Los pensadores indios desarrollaron un sistema de razonamiento seudoepistemológico (pramana) viciado por la creencia en la autoridad inapelable del Rig-veda y otros textos hinduistas posteriores como fuente última de conocimiento. Bajo esta lupa míticorreligiosa, investigaron temas como la metafísica, la ética, la hermenéutica y la soteriología. La filosofía india también cubrió temas como la filosofía política ―como se ve en el Arta-sastra (del siglo III a. C.)― Las seis escuelas ortodoxas comunes surgieron en los últimos siglos del I milenio a. C. y el período gupta (IV). Estas escuelas hindúes desarrollaron lo que se ha llamado la «síntesis hindú» fusionando elementos brahmánicos y elementos heterodoxos del budismo y el yainismo. El pensamiento hindú también se extendió hacia el este llegando al imperio indonesio Srivijaya y el Imperio jemer camboyano. Estas tradiciones se agruparon más tarde bajo el nombre Hinduismo. El Hinduismo con sus diferentes denominaciones es la religión dominante en Asia del Sur. Así, el hinduismo es una categorización de distintos puntos de vista intelectuales o filosóficos, más que un conjunto de creencias rígidas, y con cerca de mil millones de seguidores es la tercera religión más grande del mundo, después del cristianismo y el islam. Desarrollos posteriores incluyen el desarrollo del Tantra y las influencias islámicas. El budismo desapareció en su mayoría de la India después de la conquista musulmana en el subcontinente indio, sobreviviendo en las regiones del Himalaya y el sur de la India. El período moderno temprano vio el florecimiento de Navya-Nyāya (la «nueva razón») bajo filósofos como Raghunatha Siromani (circa 1460-1540) que fundó la tradición, Jayarama Pancanana, Mahadeva Punatamakara y Yashovijaya (quien formuló una respuesta yaina). En la historia de la filosofía india se pueden distinguir tres grandes períodos. El primer período es el del vedismo, que transcurrió aproximadamente desde el siglo XV a. C. hasta el siglo VIII a. C. Durante este primer período se desarrollaron los primeros textos védicos, en particular el Rig vedá, el Sama vedá, el Iáyur vedá y el Átharva vedá, así como el sistema de castas. El segundo período es aquel del brahmanismo, aproximadamente desde el siglo VIII a. C. hasta el siglo V a. C. En este período se incorporaron los Upanishads al conjunto de textos sagrados. Y el tercer período es el del hinduismo propiamente, que comienza aproximadamente en el siglo V a. C. y aún continúa. La era moderna vio el surgimiento del nacionalismo hindú, los movimientos de reforma hindúes y Neo-Vedanta (o el modernismo hindú) cuyos principales proponentes incluyeron a Vivekananda, Mahatma Gandhi y Aurobindo y que por primera vez promovió la idea de un «hinduismo» unificado. Debido a la influencia del colonialismo británico, gran parte del trabajo filosófico indio moderno surgió en inglés e incluye pensadores como Sarvepalli Radhakrishnan, Krishna Chandra Bhattacharya, Bimal Krishna Matilal y M. Hiriyanna. China Esta sección es un extracto de Filosofía china.[editar] Una de las salas principales del Guozijian (colegio imperial) en el centro de la ciudad de Beijing, la institución más alta de educación superior en la China premoderna. Símbolo del Pa Kua pavimentado en un claro a las afueras de la ciudad da Nanning , provincia de Guangxi, China. La filosofía china es la descripción de la filosofía oriental que comprende la suma de escuelas filosóficas creadas en China. Tiene una historia de varios miles de años y su inicio se suele establecer en el siglo XII a. C. con la escritura del I Ching (El libro de los cambios), un compendio antiguo sobre adivinación que introdujo alguno de los términos fundamentales de la filosofía china. Sin embargo, la tradición oral se remonta a épocas neolíticas. La historia de la filosofía china se puede dividir en cuatro períodos. El primero vio venir las primeras doctrinas de la dinastía Shang acerca de lo cíclico, así como el I Ching (el Libro de los cambios). El segundo período es el de la filosofía china clásica, conocido por la variedad y cantidad de escuelas que se formaron. Entre ellas destacaron el confucianismo, el taoísmo, el moísmo, el legalismo y la Escuela de los Nombres. El tercer período comenzó cuando la dinastía Qin adoptó como filosofía oficial el legismo, persiguiendo además a los confucianistas y moistas. Luego la dinastía Han impuso al confucianismo y taoísmo como doctrinas oficiales, y su influencia continuaría hasta el siglo XX. El último período, el de la modernidad, se caracteriza por la importación e incorporación de la filosofía occidental. Durante la dinastía Zhou occidental y los siguientes períodos después de su caída, florecieron las cien escuelas del pensamiento (siglo VI a. C. a 221 a. C.). Este período se caracterizó por importantes desarrollos intelectuales y culturales y vio el surgimiento de las principales escuelas filosóficas de China: el confucianismo, el legalismo y el taoísmo, así como numerosas otras escuelas menos influyentes. Estas tradiciones filosóficas desarrollaron teorías metafísicas, políticas y éticas como Tao, Yin y yang, Ren y Li que, junto con el budismo chino, influyeron directamente en la filosofía coreana, la filosofía vietnamita y la filosofía japonesa (que también incluye la tradición sintoísta nativa). El budismo comenzó a llegar a China durante la dinastía Han (206 a. C.-220 d. C.) a través de una transmisión gradual a través de la Ruta de la Seda, y mediante influencias nativas desarrollaron distintas formas chinas (como Zen) que se extendieron por toda la esfera cultural de Asia Oriental. Durante las dinastías chinas posteriores, como la dinastía Ming (1368-1644), así como en la dinastía coreana de Joseon (1392-1897), un renacimiento del neoconfucianismo dirigido por pensadores como Wang Yangming (1472-1529) se convirtió en la escuela de pensamiento dominante, y fue promovido por el estado imperial. En la era moderna, los pensadores chinos incorporaron ideas de la filosofía occidental. Gottfried Leibniz fue uno de los primeros intelectuales europeos que reconocieron el valor y la importancia del pensamiento chino. La filosofía marxista china o maoísmo se desarrolló bajo la influencia de Mao Zedong, mientras que el pragmatismo chino bajo el ascenso de Hu Shih y el nuevo confucianismo fue influenciado por Xiong Shili. Japón Esta sección es un extracto de Filosofía japonesa.[editar] Kitarō Nishida, profesor de la Universidad de Kioto, considerado como el precursor de la Escuela de Kioto. La filosofía japonesa es la descripción de la filosofía oriental que se origina a partir del desarrollo cultural de Japón, a través del proceso religioso e histórico que surgió del pensamiento chino, manteniéndose hasta el período Heian, del cual se inicia el pensamiento japonés y al igual que el primero, se orienta a los asuntos de sabiduría práctica. El pensamiento japonés moderno se desarrolló bajo fuertes influencias occidentales, como el estudio de las ciencias occidentales (llamado «Rangaku») y la sociedad intelectual modernista Meirokusha, que se inspiró en el pensamiento europeo. El siglo XX vio el surgimiento del sintoísmo estatal y también el nacionalismo japonés. La Escuela de Kioto, una influyente escuela filosófica japonesa, surgió también influenciada por la fenomenología occidental y la filosofía budista japonesa medieval. Corea Esta sección es un extracto de Filosofía coreana.[editar] La filosofía coreana se enfocó en su totalidad en la cosmovisión. Se integró el contenido emocional del chamanismo, lo impredecible, y algunos aspectos del neoconfucianismo. El pensamiento tradicional coreano ha sido influenciado por un gran número de corrientes de pensamiento filosófico y religiosas a lo largo de los años. Al ser las principales influencias en la vida en Corea, a menudo los movimientos del chamanismo coreano, taoísmo, budismo y confucianismo han moldeado el estilo de vida y pensamiento coreano. Budismo Esta sección es un extracto de Filosofía budista.[editar] La universidad budista Nalanda fue un importante centro de aprendizaje en la India desde el siglo V d. C. hasta el siglo XIII. La filosofía budista es la descripción de la filosofía oriental que comprende la suma de las investigaciones filosóficas de las varias escuelas budistas. La principal preocupación del budismo siempre fue la liberación del sufrimiento (nirvana) y el camino hacia esa liberación, que consiste en acción ética (sīla), meditación y sabiduría (prajña, saber «las cosas como realmente son», sct. yathābhūtaṃ viditvā). Los budistas indios buscaron esta comprensión no solo a partir de las enseñanzas del Buda, sino a través del análisis filosófico y la deliberación racional. Los pensadores budistas en India y posteriormente en Asia oriental han cubierto temas filosóficos tan variados como fenomenología, ética, ontología, epistemología, lógica y filosofía del tiempo en su análisis de este camino. El budismo temprano se basó en evidencia empírica obtenida por los órganos de los sentidos (ayatana) y el Buda parece haber mantenido una distancia escéptica de ciertas preguntas metafísicas, negándose a responderlas porque no eran conducentes a la liberación. Los puntos particulares de la filosofía budista han sido a menudo objeto de disputas entre diferentes campos filosóficos budistas. Estas disputas dieron lugar a varias escuelas llamadas Abhidharma, y a las tradiciones Mahayana de Prajnaparamita (perfección de la sabiduría), Madhyamaka (camino medio) y Yogacara (práctica de yoga). La filosofía budista comienza con el pensamiento de Gautama Buddha (circa siglos VI y IV a. C.) y se conserva en los primeros textos budistas como las Nikayas del Canon Pali. El pensamiento budista es transregional y transcultural. Se originó en la India y luego se extendió a Asia oriental, el Tíbet, Asia central y el Sudeste Asiático, desarrollando tradiciones nuevas y sincréticas en estas diferentes regiones. Las diversas escuelas del pensamiento budistas son la tradición filosófica dominante en el Tíbet y en países del sudeste asiático como Sri Lanka y Birmania. La principal preocupación del budismo es la soteriología, definida como la libertad desde dukkha (inquietud). Debido a que la ignorancia sobre la verdadera naturaleza de las cosas se considera una de las raíces del sufrimiento (dukkha), la filosofía budista se ocupa de la epistemología, la metafísica, la ética y la psicología. Los textos filosóficos budistas también se deben entender dentro del contexto de las prácticas meditativas que se supone que producen ciertos cambios cognitivos. Los conceptos innovadores clave incluyen las Cuatro Nobles Verdades, Anatta (no-yo) una crítica de una identidad personal fija, la transitoriedad (Anicca) de todas las cosas y un cierto escepticismo sobre las preguntas metafísicas. Después de la muerte de Buda, varios grupos comenzaron a sistematizar sus principales enseñanzas y desarrollaron sistemas filosóficos denominados Abhidharma. Los filósofos de Mahayana como Nagarjuna y Vasubandhu desarrollaron las teorías de shunyata (vacuidad de todos los fenómenos) y «vijnapti-matra» (solo apariencia), una forma de fenomenología o idealismo trascendental. La escuela de Dignāga o escuela de pramāṇa promovió una forma de epistemología y lógica. A través del trabajo de Dharmakirti, esta tradición de lógica budista se ha convertido en el principal sistema epistemológico utilizado en la filosofía y el debate de budismo tibetano. Según el profesor de filosofía budista Jan Westerhoff, las principales escuelas indias desde 300 a. C. hasta 1000 d. C. fueron: La tradición Mahāsāṃghika («Gran Comunidad»). Las escuelas Sthavira («Ancianos») que incluyen: Sarvāstivāda, Sautrāntika, Vibhajyavada (más tarde conocida como Theravada en Sri Lanka) y Pudgalavada. Las escuelas Mahayana, principalmente Madhyamaka, Yogachara, Tathāgatagarbha y Tantra. Después de la desaparición del budismo de la India, estas tradiciones filosóficas se extendieron por toda Asia a través de la ruta de la seda y continuaron desarrollándose en el budismo tibetano, el budismo de Asia oriental y las tradiciones budistas Theravada. El período moderno vio el surgimiento del modernismo budista y el humanismo budista bajo influencias occidentales y el desarrollo de un budismo occidental con influencias de la psicología moderna y la filosofía occidental. En el Tíbet, la tradición india continuó desarrollándose bajo pensadores como Sakya Pandita, Tsongkhapa y Ju Mipham. En China, nuevos desarrollos fueron dirigidos por pensadores como Xuangzang, autor de nuevos trabajos sobre Yogacara; Zhiyi, quien fundó la escuela Tiantai y desarrolló una nueva teoría de Madhyamaka y Guifeng Zongmi, que escribió sobre Huayan y Zen. Islam Esta sección es un extracto de Filosofía islámica.[editar] Representación de Sócrates en un manuscrito árabe ilustrado del siglo XIII. La filosofía islámica es filosofía surgida de la tradición islámica. Incluye el conjunto de ideas relacionadas con la vida, el universo, la ética, la sociedad y demás cuestiones fundamentales vinculadas al mundo islámico. Dos términos usados tradicionalmente en el mundo islámico son traducidos a veces como filosofía: falsafa (lit. «filosofía»), que se refiere a la filosofía en su sentido tradicional así como a la lógica, la matemática y la física, y kalama (lit. «habla»), que se refiere a una forma racionalista de teología escolástica islámica, que incluye las escuelas maturidiyah, ashariyyah y mutazila. La filosofía islámica temprana tuvo inicio con Al-Kindi en el siglo II del calendario musulmán (comienzos del siglo IX d. C.) y terminó con Ibn-Rushd (Averroes) siglo VI d. H. (finales del siglo XII d. C.), coincidiendo ampliamente con la Edad de Oro del islam. La muerte de Averroes marcó de manera efectiva el final de una disciplina particular de filosofía islámica llamada usualmente la escuela peripatética islámica, y la actividad filosófica tuvo un importante declive en países islámicos occidentales tales como la Iberia islámica y el Norte de África. La filosofía islámica persistió por mucho más tiempo en países islámicos orientales, en particular la Persia safávida, y los imperios otomano y mogol, donde varias escuelas filosóficas siguieron floreciendo: el avicenismo, el averroísmo, el iluminacionismo, filosofía mística, teosofía trascendente, y filosofía de Isfahán. Ibn Jaldún, en su Muqaddima, hizo contribuciones importantes a la filosofía de la historia. Interés en la filosofía islámica revivió durante el movimiento al-Nahda («despertar») a finales del siglo XIX y comienzos del XX, y continúa hasta la actualidad. La tradición islámica actual combina algunos pensamientos del neoplatonismo y del aristotelismo con otros conceptos que fueron insertados mediante el desarrollo del Islam. Ciertos filósofos de peso como el árabe al-Kindi y los persas al-Farabi y Avicena, así como Ibn Tufail y Averroes, originarios de la península ibérica, precisaron algunas interpretaciones de Aristóteles que fueron después absorbidas por los intelectuales judíos y cristianos. La historia de la filosofía islámica contiene ejemplos significativos de otros filósofos que abordaron un gran número de cuestiones que terminaron por influenciar al escolasticismo medieval de Europa, entre ellos se encuentran Al-Ghazali y Mulla Sadra. Los musulmanes, y en menor medida los cristianos y los judíos, contribuyeron con el folclore arábigo y se distanciaron entre sí de acuerdo a sus dogmas filosóficos más que por sus doctrinas religiosas. Cuando los pueblos árabe y bereber llegaron a la península ibérica, la literatura filosófica arábiga fue traducida a los idiomas hebreo y latín; contribuyendo al desarrollo de la filosofía europea. La filosofía islámica tuvo un gran impacto en la Europa cristiana, donde traducciones al latín de textos filosóficos en árabe «llevaron a la transformación de casi todas las disciplinas filosóficas en el mundo latino medieval», con una influencia particularmente fuerte de filósofos musulmanes en la filosofía natural, la psicología y la metafísica. África La filosofía africana es la filosofía producida por la gente africana, filosofía que presenta visiones del mundo, ideas y temas africanos, o filosofía que utiliza diferentes métodos filosóficos africanos. El pensamiento africano moderno se ha ocupado de la etnofilosofía, con definir el significado mismo de la filosofía africana y sus características únicas y lo que significa ser africano. Durante el siglo XVII, la filosofía etíope desarrolló una sólida tradición literaria como ejemplifica Zera Yacob. Otro de los primeros filósofos africanos fue Anton Wilhelm Amo (c. 1703–1759) que se convirtió en un respetado filósofo en Alemania. Diferentes ideas filosóficas africanas incluyen Ujamaa, la idea Bantú de Fuerza, Negritud, Panafricanismo y Ubuntu. El pensamiento africano contemporáneo también ha visto el desarrollo de la filosofía profesional y de la filosofía africana, la literatura filosófica de la diáspora africana que incluye corrientes como el existencialismo negro de afrodescendientes estadounidenses. Algunos pensadores africanos modernos han sido influenciados por el Marxismo, la literatura afroamericana, teoría crítica, teoría crítica de la raza, postcolonialismo y feminismo. Filosofía indígena americana Un Tlamatini (filósofo azteca) observando las estrellas, parte del Códice Mendoza. El pensamiento filosófico indigenoamericano consiste en una amplia variedad de creencias y tradiciones entre diferentes culturas. Entre algunas de las comunidades de nativos americano de los Estados Unidos, hay una creencia en un principio metafísico llamado el Gran Espíritu (Siux: wakȟáŋ tȟáŋka; Algonquino: gitche manitou). Otro concepto ampliamente compartido fue el de orenda (poder espiritual). Según Whiteley (1998), para los nativos americanos, la mente está críticamente informada por la experiencia trascendental (sueños, visiones, etc.) así como por la razón. La práctica para acceder a estas experiencias trascendentales se denomina chamanismo. Otra característica de las cosmovisiones indígenas estadounidenses fue su extensión de la ética a animales y plantas no humanos.  En Mesoamérica, la filosofía azteca fue una tradición intelectual desarrollada por individuos llamados Tlamatini (los que saben algo) y sus ideas se conservan en varios Códices mexicas. La cosmovisión azteca postuló el concepto de una energía o fuerza universal llamada Ometéotl (Energía Cósmica Dual) que buscaba una forma de vivir en equilibrio en un mundo en cambio constante. La teoría de Téotl puede verse como una forma de panteísmo. Los filósofos aztecas desarrollaron teorías de metafísica, epistemología, valores y estética. La ética azteca se centró en buscar tlamatiliztli (conocimiento, sabiduría) que se basó en la moderación y el equilibrio en todas las acciones como en el proverbio Nahua el bien medio es necesario. La civilización inca también tenía una clase de élite de filósofos-eruditos denominados los Amawtakuna que fueron importantes en el sistema de educación inca como profesores de religión, tradición, historia y ética. Los conceptos clave del pensamiento andino son Yanantin i Masintin que involucran una teoría de “opuestos complementarios” que ve las polaridades (como masculino/femenino, oscuro/claro) como partes interdependientes de un todo armonioso. Progreso filosófico Muchos debates filosóficos que comenzaron en la antigüedad todavía se debaten hoy. El filósofo británico Colin McGinn afirma que no hubo progreso filosófico durante ese intervalo. El filósofo australiano David Chalmers, por el contrario, ve el progreso en la filosofía similar al de la ciencia. Mientras tanto, Talbot Brewer, profesor de filosofía en la Universidad de Virginia, sostiene que el progreso es el estándar equivocado para juzgar la actividad filosófica. Mujeres Esta sección es un extracto de Filósofas.[editar] Retrato idealizado de la antigua filósofa neoplatónica griega Hipatia por Jules Maurice Gaspard (1908). Desde la antigüedad se ha tenido conocimiento de mujeres que se han dedicado a la filosofía a lo largo de la historia, pero mucho de su legado no ha sido tan estudiado hasta nuestros días. Existen testimonios de mujeres filósofas al menos desde la Grecia antigua y un número relativamente pequeño de ellas fueron consideradas como tal en las épocas antigua, medieval, moderna y contemporánea, especialmente durante los siglos XX y XXI, apenas hay mujeres filósofas que hayan entrado en el canon filosófico occidental. La mujer y la filosofía siempre se ha mantenido en un completo tabú y según estudios posteriores algunos filósofos occidentales atribuían al hombre un carácter racional y a la mujer un potencial más emotivo e intuitivo. De esta opinión fueron Aristóteles, Séneca, Tomás de Aquino, Rousseau, Hegel, Schopenhauer y Nietzsche. En la filosofía antigua en Occidente, mientras que la filosofía académica era del dominio de filósofos masculinos como Platón y Aristóteles, filósofas como Hiparquia de Maronea (activa hacia el año 325 a. C.), Areta de Cirene (activa en el siglo V-IV a. C.) y Aspasia de Mileto (470-400 a. C.) mantuvieron también actividad durante este período. Una notable filósofa medieval fue Hipatia (siglo V). Filósofas modernas destacadas fueron Mary Wollstonecraft (1759-1797) y Margaret Fuller (1810-1850). Entre las filósofas contemporáneas influyentes están Ayn Rand (1905-1982), Susanne Langer (1895-1985), Hannah Arendt (1906-1975), Simone de Beauvoir (1908-1986), María Zambrano (1904-1991), Mary Midgley (1919), Mary Warnock (1924-2019), Celia Amorós (1944), Julia Kristeva (1941), Patricia Churchland (nacida en 1943), Susan Haack (nacida en 1945) y Amelia Valcárcel (1950). A principios del siglo XIX, algunas universidades del Reino Unido y Estados Unidos comenzaron a admitir a las mujeres, dando lugar a nuevas generaciones de mujeres académicas. Sin embargo, investigaciones del Departamento de Educación de los Estados Unidos realizados a finales de los años 1990 del siglo XX indicaban que la filosofía era uno de los campos más desiguales en las humanidades con respecto a la presencia de varones y mujeres. Las mujeres constituían apenas el 17% del estudiantado en la Facultad de filosofía. En 2014, Inside Higher Education describió la filosofía ... con una historia propia en la disciplina de la misoginia y acoso sexual de las mujeres estudiantes y profesoras. Jennifer Saul, profesora de filosofía en la Universidad de Sheffield, declaró en 2015 que las mujeres ... están dejando la filosofía después de haber sido acosadas, agredidas o haber sufrido represalias. A principios de los años noventa, la Asociación Filosófica Canadiense afirmó que existe un desequilibrio de género y sesgo de género en el campo académico de la filosofía. En junio de 2013, un profesor de sociología estadounidense declaró que de todas las citas recientes en cuatro prestigiosas revistas de filosofía, las mujeres representan solo el 3,6 % del total. Los editores de la Enciclopedia de Stanford de la Filosofía han trasladado su preocupación sobre la subrepresentación de las mujeres filósofas, y reclaman a editores y escritores garantizar que se incluyan las contribuciones de las mujeres filósofas. Según Eugene Sun Park, la filosofía es predominantemente blanca y predominantemente masculina, esta homogeneidad existe en casi todos los aspectos y en todos los niveles de la disciplina. Susan Price sostiene que el ... canon filosófico sigue dominado por los hombres blancos -la disciplina que ... todavía sigue al mito de que el genio está ligado al género. Según Saul, la filosofía, la más antigua de las humanidades, es también la más masculina (y la más blanca). Si bien otras áreas de las humanidades se acercan a la paridad de género, la filosofía es en realidad más abrumadoramente masculina incluso que las matemáticas. Me fui a hojear al menos tres enciclopedias filosóficas y de todos estos nombres (salvo Hipatia) no encontré ningún rastro. No es que no hayan existido mujeres filósofas. Es que los filósofos han preferido olvidarlas, quizás después de haberse apropiado de sus ideas dice el escritor y filósofo italiano Umberto Eco en Filosofare al femminile recordando la existencia de Diotima, Arete, Nicarete, Ipazia, Astasia, Teodora, Leoncia y Caterina de Siena, a propósito de la publicación en Francia de Histoire des femmes philosophes de Gilles Menage, latinista del siglo XVII, preceptor de Madame de Sévigné y de Madame de Lafayette cuyo libro, aparecido en 1690, se titulaba originalmente Mulierum philosopharum historia. Iconología Personificación de la filosofía, por Eduard Lebiedzki, tras un diseño de Karl Rahl. Alfanio hace a la filosofía hija de la experiencia y de la memoria. Se representa como una mujer de aspecto grave en actitud retórica y con la frente majestuosa ceñida de una preciosa diadema. Está sentada en un sillón de mármol blanco en cuyos brazos hay esculpidas las imágenes de fecunda naturaleza. Esta figura simbólica tiene dos libros, en uno de los cuales se lee naturalis y en el otro moralis. Rafael, autor de esta idea, ha querido con ella indicarnos los cuatro elementos, objeto de las investigaciones filosóficas, valiéndose de los diversos colores que ha dado a los ropajes con que la viste. El manto de color azul que cubre las espaldas, designa el aire; la túnica encarnada, el fuego; el ropaje de azul celeste que cubre sus rodillas, el agua; y el de color amarillo que le llega basta los pies, la tierra. Dos genios que coloca cerca de la figura principal sostienen esta inscripción Causarum cognitio: el conocimiento de las causas. Boecio en el retrato que ha tratado de la filosofía le pone en una mano algunos libros y en la otra un cetro. En el extremo de su ropaje hay una letra griega y en el estómago otra que designan, la primera la teoría y la segunda la práctica, para dar a entender que la filosofía debe ser activa y especulativa. Luego, finge que esta imagen simbólica se le ha presentado bajo los rasgos de una mujer que con rostro radiante y ojos llenos de fuego anuncia algo de divino: que su talla parece igual a la de la especie humana y finalmente, que algunas veces levanta la cabeza hacia los cielos y se oculta a la vista de los débiles mortales. Cochin la representa como una mujer hermosa, reflexiva, vestida sencillamente, con un cetro en una mano y un libro en la otra, la hace trepar un monte áspero y pedregoso, haciéndola apoyar en el freno de la razón. Bernard Picart en un asunto alegórico pinta la armonía de la religión con la filosofía, su figura simbólica tiene diferentes atributos, los cuales caracterizan las cuatro partes. Está coronada de estrellas para designar la física y un cetro que lleva en su mano izquierda indica la moral; dos genios colocados cerca de ella: el uno lleva una serpiente mordiéndose la cola ,símbolo de la eternidad, y esto anuncia la metafísica; el otro, una piedra de toque para expresar la lógica, cuyo objeto es el de distinguir lo verdadero de lo falso."
ksampletext_wikipedia_phil_fenomenologiatrascendental: str = "Fenomenología trascendental. La fenomenología trascendental es una corriente filosófica fundada por Edmund Husserl en un intento de renovar la filosofía como una ciencia estricta. Por ciencia estricta, Husserl se refiere a que la filosofía debía aspirar al alcance de una verdad objetiva en el conocimiento acerca de un fenómeno. Para lograr dicho propósito la filosofía debía cumplir dos requisitos: primero, tener un conocimiento progresivo; segundo, tener un método propio. La fenomenología tiene por objetivo describir cómo el mundo se hace presente en la subjetividad, determinando sus estructuras esenciales. El adjetivo trascendental que acompaña a la fenomenología husserliana se utiliza para distinguir a esta de otras formas de fenomenología. La fenomenología trascendental se inscribe en la tradición de la filosofía trascendental que tiene sus raíces en René Descartes, pero más claramente en Immanuel Kant y el idealismo alemán. Pese a que la fenomenología trascendental se ha referido, prácticamente durante todo el siglo XX, a la filosofía de Edmund Husserl, la exégesis contemporánea sugiere que ciertos periodos del pensamiento de Martin Heidegger podrían adscribirse también a esta corriente. En reacción a la fenomenología trascendental, han emergido muchas de las grandes corrientes de la así llamada filosofía continental del siglo XX, tales como la deconstrucción, el postestructuralismo, el pensamiento de la otredad, la posmodernidad y el existencialismo. Desarrollo histórico de la fenomenología trascendental Pese a que se suele decir que la fenomenología trascendental tiene su origen en el primer tomo de las Ideen zu einer reinen Phänomenologie und phänomenologischen Philosophie (Ideas I), no existe consenso acerca de si la fenomenología de la que Husserl hablaba ya desde 1900 no podría considerarse como trascendental incluso antes de 1913. Lo que sí es un hecho es que en la primera edición de la Logische Untersuchungen de 1900/1901 (que luego recibió una segunda edición con correcciones posterior a la escritura de Ideas I) Husserl caracteriza a la fenomenología como psicología descriptiva, no reconociéndose ella como sucesora de la filosofía trascendental. En las Investigaciones lógicas Husserl presenta una aguda crítica al psicologismo, y desarrolla algunos conceptos heredados de la filosofía de su maestro Brentano, como el de vivencia e intencionalidad, que posteriormente ocuparán un lugar central en la fenomenología trascendental. Por vivencia ha de entenderse la unidad entre un acto intencional de cualquier tipo (percepción, imaginación, recuerdo, deseo, etc.) y el objeto que es alcanzado en esa vivencia, el objeto intencional (lo percibido, lo imaginado, etc.). Por otra parte, con intencionalidad se describe la propiedad de las vivencias de estar referidas siempre a algo. Así, la vida de conciencia es caracterizada como necesariamente intencional, esto es, que todas las vivencias se refieren necesariamente a alguna forma de objeto. A los objetos entendidos como correlatos necesarios de vivencias, Husserl los denomina objetos intencionales. La fenomenología aparece en esta obra como una ciencia que debe de proceder conforme a un método. Este método recibirá luego de las Investigaciones lógicas múltiples correcciones y ampliaciones en los siguientes trabajos de Husserl. Debe observarse, sin embargo, que hasta 1913 no se hablaba aún de la epojé, de la reducción fenomenológica y de la reducción trascendental, que son condición sine qua non para convertir el análisis psicológico de las vivencias en un análisis propiamente filosófico. (Como antecedente, puede observarse que en las lecciones de Die Idee der Phänomenologie se habla de reducción gnoseológica). Otro elemento metódico es la apelación a una mereología, o teoría de los todos y las partes, a partir de la cual se han de distinguir entre partes independientes y partes no independientes de las esencias de los objetos intencionales. Con esto es posible describir las relaciones entre estas partes en términos de fundamentación. Por último, el método supone también una teoría del cumplimiento de las vivencias intencionales. (A este cumplimiento lo denominará más tarde, en Ideas I, evidencia, Evidenz). De acuerdo con esta teoría, la pregunta por el sentido intencional se responde a partir de las vivencias perceptivas en las que se captan objetos reales o ideales. En cuanto a este último punto, Husserl afirma que también en las vivencias que tienen ideas como objetos intencionales es posible distinguir entre vivencias que presentan a sus objetos y vivencias que sólo los mentan de manera vacía (esta mención vacía es una posibilidad que surge con el lenguaje). Por ello cabe pensar en las vivencias en las que se captan o intuyen ideas como vivencias análogas a aquellas en las que se percibe un objeto real. Desde este punto de vista los objetos son inconcebibles sin su referencia a las vivencias en las que se muestran: el postulado de una cosa en sí, independiente de la vida de conciencia, es absurdo. Así pues, en resumen, en las Investigaciones Lógicas la fenomenología ya es concebida una ciencia que estudia las estructuras esenciales de las vivencias y los objetos intencionales, así como relaciones esenciales entre tipos de vivencias y de objetos intencionales. Por otro lado, el propósito de la fenomenología tal y como es propuesta en esta obra consistiría en la aclaración epistemológica de la lógica pura, que comprendería también a la matemática, a partir del cumplimiento de las vivencias intencionales de las objetividades lógicas. Ideas I Ideas relativas a una fenomenología pura y una filosofía fenomenológica, también conocida como Ideas I, es la primera publicación donde aparece el tema de la epojé trascendental: la operación mediante la cual Husserl propone acceder a la subjetividad trascendental o conciencia pura mediante una serie de pasos o reducciones. Una de estas reducciones es la reducción eidética, que consiste en tomar las objetividades que se presentan a la consciencia como meros ejemplares de esencias que se obtienen por variación eidética. Este método ya había sido introducido en las Investigaciones Lógicas. No es este el caso de la reducción trascendental, que aparece por primera vez en esta obra en la forma de una desconexión o puesta entre paréntesis de la creencia en la realidad del mundo. Quien ejecuta esta reducción descubre de manera radical el mundo en tanto que mundo vivido por él, pues con la desconexión de la creencia en la realidad del mundo el fenomenólogo se concentra necesariamente en el campo de la vida de conciencia en la que éste aparece y adquiere incluso su carácter de realidad. Sólo con la desatención del mundo en tanto que mundo real es posible prestar atención al mundo tal y como es vivido por nosotros. Mientras que la reducción trascendental abre el campo de la consciencia pura, la reducción eidética permite captar lo que ahí aparece en términos de esencias y de relaciones esenciales. A la actitud en la que vivimos cotidianamente cuando no hacemos filosofía, Husserl la denomina actitud natural. A la actitud en la que nos mantenemos en la reducción trascendental ,y que, lo sepa o no el filósofo, es propia de toda filosofía, la denomina actitud trascendental. A partir de esta obra toda la fenomenología de Husserl se desarrollará como fenomenología trascendental. Por lo demás, en Ideas I este proyecto adquiere ya claramente el perfil de una filosofía que tiene como tarea esclarecer el sentido que el mundo tiene para nosotros en nuestra vida cotidiana. Esto se debe a que la vida misma sobre la que el fenomenólogo reflexiona tiene un carácter intencional que coincide con una noción ampliada de significación que Husserl denomina sentido (Sinn) en Ideas I. Husserl observa que tras la reducción trascendental toda vivencia intencional sigue teniendo una estructura doble: un lado noético y un lado noemático. Mientras que lo noético se refiere a la forma en que algo es vivido, lo noemático se refiere a aquello a lo que apunta la vivencia como su objeto. En la correlación intencional entre noesis y nóemas las vivencias se entretejen en estructuras sintéticas. Antendiendo a ello se puede hablar de una sintaxis de las vivencias que es análoga a la del lenguaje, pero mucho más fundamental. A las investigaciones fenomenológicas sobre las síntesis en las que el mundo y las cosas en él adquieren su sentido, Husserl las llamó investigaciones constitutivas. En obras posteriores Husserl propondrá varias formas de llevar a cabo las distintas reducciones que conducen a la subjetividad trascendental. Aquí es importante hacer una observación sobre el término reducción, en alemán Reduktion. En su libro Introduction to Phenomenology, Robert Sokolowski propone interpretar este término en el sentido de “re-conducción”, conforme a la etimología latina re-ducere. Esta interpretación del término reducción es congruente con el papel que este concepto juega en la fenomenología trascendental. La fenomenología aparece públicamente por primera vez como fenomenología trascendental en Ideas I. De esta obra en adelante será claro para Husserl que la tarea de la filosofía entendida de esta manera es explicar el origen y el sentido del mundo al reflexionar sobre la experiencia intencional. De acuerdo con ella, el mundo es aquello a lo que se refiere nuestra experiencia y al mismo tiempo el contexto en el que vivimos. Como contexto el mundo es siempre algo implícito. Así que para explicitar el sentido de esto implícito es necesario primero dejar de suponerlo como fundamento de la experiencia y recuperarlo como término objetivo al que ésta se refiere. Esto es lo que pretende la reducción trascendental. Husserl planeó su obra Ideas relativas a una fenomenología pura y una filosofía fenomenológica en tres tomos. El único tomo terminado fue el primero, al que nos hemos referido en este apartado. El segundo y el tercer tomo fueron publicados de manera póstuma. Metodología En lo que se refiere al método, se vale de la reducción eidética, la reducción trascendental y el análisis intencional para explicitar el sentido del mundo en tanto que mundo (o del ser en tanto que ser) y de las cosas en él, así como para exponer las leyes esenciales inherentes a nuestra consciencia del mismo. En todas las obras sistemáticas publicadas por Husserl consta un bosquejo de su programa, y parte de él comprende la fundamentación última de las ciencias empíricas (o ciencias de hechos, como la biología) y de las ciencias eidéticas (o ciencias de esencias, como la geometría), así como la elucidación de lo mentado en sus conceptos. Muchos de los conceptos y las tesis de la fenomenología trascendental sólo pueden ser comprendidos a cabalidad tras la ejecución de la reducción trascendental y tras el esfuerzo por entender a qué se refieren a partir de la experiencia propia. Este es uno de los sentidos del llamado a ir a las cosas mismas, en contraposición a quedarse en meras construcciones de palabras y castillos en el aire. Uno de los conceptos centrales de la fenomenología trascendental es el de intencionalidad, que procede de la tradición escolástica y en última instancia del concepto aristotélico de “logos”. Otro de sus conceptos fundamentales es el de evidencia o intuición, que es una ampliación del concepto de percepción y que se refiere a una verdad más originaria que la proposicional: esta verdad es la de lo que aparece. En la fenomenología trascendental se deshace la oposición entre empirismo y racionalismo, pues en la medida en que llama a dirimir todas las cuestiones sobre la verdad última de las cosas en las experiencias evidentes que tenemos de ellas, puede considerarse una forma radical de empirismo; sin embargo, en la medida en que asume que el orden racional del mundo nace en la experiencia intencional, puede considerarse también una forma de racionalismo. Pocos de los discípulos y de los primeros lectores de Husserl compartieron el espíritu de hacer de la fenomenología un proyecto verdaderamente colectivo. Por el contrario, la historia del movimiento fenomenológico que tiene sus raíces en Husserl parece estar dominada por el deseo de filósofos que aspiran a superarse unos a otros. De ahí que la unidad de lo que se denomina con el título genérico de fenomenología sea la mayoría de las veces superficial, cuando no meramente histórica. Sin embargo, a principios del siglo XXI esta forma colectiva de hacer filosofía y su proyecto pasan por un renacimiento en gran parte del mundo. Meditaciones cartesianas Lógica Formal y Lógica Trascendental, La crisis de las ciencias europeas y la fenomenología trascendental, Ideas II, hace referencia a la maduración de la fenomenología trascendental. De las obras sistemáticas que Husserl alcanzó a terminar en vida, las más maduras son Meditaciones cartesianas (1931) y Lógica Formal y Lógica Trascendental (1929). La lectura de estas dos obras es imprescindible para comprender el proyecto de la fenomenología trascendental frente a las acusaciones de platonismo y solipsismo. Entre otras cosas, en ellas Husserl incorpora sus descubrimientos relativos a la conciencia del tiempo para esclarecer la temporalidad de las objetividades reales e ideales, y sus descubrimientos relativos a la empatía ,la consciencia intencional en la que captamos a los otros en virtud de su aparecer corporal , para esclarecer la forma en que somos conscientes de la intersubjetividad. Durante sus últimos años, Husserl preparaba otra obra sistemática de la cual sólo la primera parte pudo publicarse en un periódico para inmigrantes: La crisis de las ciencias europeas y la fenomenología trascendental (1936). Sin embargo, se publicaron de manera póstuma partes adicionales de este trabajo. Se trata de una introducción a la fenomenología trascendental que recupera críticamente muchos hallazgos históricos de la filosofía y que pretende dar cuenta del sentido de las explicaciones de las ciencias modernas y de su necesidad de fundarse en investigaciones fenomenológicas que tienen como tema al mundo de la vida. Además, en ella Husserl revisa críticamente diversas formas de llevar a cabo la reducción fenomenológica. Mención aparte merece el segundo volumen de sus Ideas relativas a una fenomenología pura y una filosofía fenomenológica, que Husserl se negó a publicar en vida por considerarlo inacabado y problemático. Esta obra, en la que Husserl trabajó entre la década de 1910 y principios de la de 1920, ejerció gran influencia en muchos fenomenólogos que tuvieron acceso a ella, como Martin Heidegger y Maurice Merleau-Ponty. En ella se anticipan problemáticas que cobrarán mucha importancia en sus obras posteriores, como los conceptos de motivación, mundo de la vida y persona, así como la constitución de la corporalidad y de la intersubjetividad. Génesis de la reducción trascendental Husserl introduce explícitamente por primera vez la idea de una “reducción gnoseológica” en La idea de la fenomenología (1907). El desarrollo filosófico de esta idea irá adquiriendo una complejidad cada vez mayor conforme a una cierta maduración en su pensamiento. Por ello la reducción trascendental no debe ser vista como un giro sorprendente y contradictorio en la obra de Husserl respecto de sus Investigaciones lógicas, como se ha pensado a veces. En esta última obra, Husserl habla ya de “una aprehensión fenomenológica pura desconectadora (ausschaltende) de todas las posiciones trascendentes” y de un “yo fenomenológicamente reducido”. Es cierto que estas palabras, como muchas otras de la misma tendencia, fueron introducidas únicamente hasta 1913, año de la segunda edición de las Investigaciones Lógicas y de la publicación de Ideas I. Sin embargo, en aquellas investigaciones podemos ver la predelineación de un radicalismo filosófico que impide hablar de un viraje autocontradictorio. Ya en la primera edición de esa obra (1900-1901) Husserl hablaba de la distinción entre “el yo de las vivencias” (Ichleib) y el yo empírico, de la delimitación del “yo psíquico puro” a su “contenido fenomenológico”, y de cómo a través de esta delimitación el yo “se reduce” (es reduziert sich) a la unidad de la conciencia. Ahora bien, es en sus lecciones de 1907, publicadas con el título de La idea de la fenomenología, donde Husserl expone por primera vez con toda claridad y sistematicidad la idea de la fenomenología en relación con lo que más adelante (1913) sería llamado “reducción trascendental”. Ahí sostiene que “sólo por medio de una reducción –a la que vamos a llamar ya reducción fenomenológica– obtengo un dato absoluto, que ya no ofrece nada de trascendencia”. Desde un punto de vista retrospectivo la obra de Husserl parece implicar, en el camino que va de Investigaciones lógicas a Ideas I, pasando por La idea de la fenomenología, un énfasis creciente en ciertos puntos que tendían desde el principio hacia cierto radicalismo filosófico. Para poner otro ejemplo, en los primeros parágrafos de la primera de las Investigaciones lógicas Husserl hablaba, apuntando ya a desarrollos que más adelante habrían de conducir a la reducción trascendental, del “principio de la falta de supuestos”, entendidos como supuestos ontológicos, y hacía referencia a la fenomenología pura como “terreno de investigaciones [ontológicamente] neutrales”. En concordancia con esto dice también que sus investigaciones “no tenían tema ontológico” y que la fenomenología pura no hace ni la menor afirmación sobre existencias reales (reales Dasein). En fin, las diferencias entre la primera y la segunda edición de las Investigaciones Lógicas ofrecen un campo de estudio para la determinación del progreso en la filosofía de Husserl y para entender su camino hacia la reducción trascendental como el transcurso de un énfasis radicalizador."
ksampletext_wikipedia_phil_metafisica: str = "Metafísica. La metafísica (del latín metaphysica, y este del griego, «después de la naturaleza») es la rama de la filosofía que estudia la estructura, componentes y principios fundamentales de la realidad.  Esto incluye la clarificación e investigación de algunas de las nociones fundamentales con las que comprendemos el mundo, como entidad, ser, existencia, objeto, propiedad, relación, causalidad, tiempo y espacio. Mario Bunge, filósofo de la ciencia afirmaba que “la metafísica es la ciencia general” , así como que “La ciencia, tanto básica como aplicada, trata con conceptos e hipótesis metafísicos: presupone ciertos principios ontológicos ,de tipo heurístico, así como de tipo constitutivo, y es una poderosa fuente de conjeturas metafísicas. De hecho, algunas teorías son a la vez metafísicas y científicas” Junto con la lógica y la epistemología o gnoseología, la metafísica es la rama más básica de la filosofía. Ha sido estudiada por filósofos como Platón, Aristóteles, Agustín de Hipona, Boecio, Tomás de Aquino, Guillermo de Ockham, Descartes, John Locke, Nicolás Malebranche, Spinoza, Leibniz, Hume, Alfred North Whitehead, Martin Heidegger, Kurt Gödel, Karl Popper, Saul Kripke, David Lewis, etc. Antes del advenimiento de la ciencia moderna, muchos de los problemas que hoy pertenecen a las ciencias naturales eran estudiados por la metafísica bajo el título de filosofía natural. Hoy la metafísica estudia aspectos de la realidad que son inaccesibles a la investigación empírica. Esto dará lugar en el siglo XX a la lectura heideggeriana de la metafísica occidental como ontoteología y, por lo tanto, a la necesidad de repensar la cuestión del ser desde el origen mismo de los pensadores presocráticos. Aristóteles designó la metafísica como «filosofía primera». En la química se asume la existencia de la materia y en la biología la existencia de la vida, pero ninguna de las dos ciencias define la materia o la vida; solo la metafísica suministra estas definiciones básicas. La ontología es la parte de la metafísica que se ocupa de investigar qué entidades existen y cuáles no, más allá de las apariencias. La metafísica tiene dos temas principales: el primero es la ontología, que en palabras de Aristóteles es la ciencia que estudia al ser en cuanto tal. El segundo es la teología, que estudia los fines como causa última de la realidad. Aunque esta distinción deriva de la escolástica, quizá especialmente por influencia de Francisco Suárez. A lo largo de su historia, después de -y superando- el tomismo, se realizaron otras distinciones dentro del área. Por ejemplo, Wolff, en el s. XVIII, diferenció la metafísica general -usado indistintamente tanto como ontología que como filosofía primera - de la metafísica específica, sin embargo, incluso esta distinción se considera errónea y superada a día de hoy. Lo común en el panorama actual de la filosofía es usar indistintamente ambos términos para referirse a un conjunto de problemas acerca del ser de las cosas, eventos, entes, procesos, etc, como el problema de la persistencia y la constitución, el problema mente-cuerpo o el problema de la causación. Es difícil encontrar una definición adecuada de metafísica. A lo largo de los siglos muchos filósofos han sostenido, de una manera u otra, que la metafísica es imposible. Esta tesis tiene una versión fuerte y una versión débil. La versión fuerte es que todas las afirmaciones metafísicas carecen de sentido o significado. Esto depende por supuesto de una teoría del significado. Los positivistas lógicos y Ludwig Wittgenstein fueron defensores explícitos de esta posición. Por otra parte, la versión débil es que si bien las afirmaciones metafísicas poseen significado, es imposible saber cuáles son verdaderas y cuáles falsas, pues esto va más allá de las capacidades cognitivas del ser humano. Esta posición es la que sostuvieron, por ejemplo, David Hume e Immanuel Kant. Por otra parte, algunos filósofos han sostenido que el ser humano tiene una predisposición natural hacia la metafísica. Kant la calificó de «necesidad inevitable» y Arthur Schopenhauer incluso definió al ser humano como «animal metafísico». Etimología Platón y Aristóteles en La escuela de Atenas, de Rafael Sanzio. Aristóteles es considerado como el padre de la metafísica; sin embargo, Parménides fue su antecedente. La palabra «metafísica» deriva del griego μετὰ φύσις, que significa «después de la naturaleza o después de la física», proviene del título dado por Andrónico de Rodas (siglo I a. C.) a una colección de escritos de Aristóteles. Esto no implica que la metafísica haya nacido con Aristóteles, sino que es de hecho más antigua, puesto que hay casos de pensamiento metafísico en los filósofos presocráticos. Platón estudió en diversos Diálogos lo que es el ser, con lo que preparó el terreno a Aristóteles de Estagira, el cual elaboró lo que él llamaba «filosofía primera», cuyo principal objetivo era el estudio del ser en cuanto tal, de sus atributos y sus causas. El término «metafísica» proviene de la obra de Aristóteles compuesta por catorce volúmenes (rollos de papiro), independientes entre sí, que se ocupan de diversos temas generales de la filosofía. Estos libros son de carácter esotérico, es decir, Aristóteles nunca los concibió para la publicación. Por el contrario, son un conjunto de apuntes o notas personales sobre temas que pudo haber tratado en sus clases o en otros libros sistemáticos. El peripatético Andrónico de Rodas al publicar la primera edición de las obras de Aristóteles ordenó estos libros a continuación de los ocho libros sobre física (μετὰ [τὰ] φυσικά). De allí surgió el concepto de «metafísica», que en realidad significa «aquello que en el estante está después de la física», pero que también de manera didáctica significa: «aquello que sigue a las explicaciones sobre la naturaleza» o «lo que viene después de la física», entendiendo «física» en su acepción antigua que se refería al estudio de la φύσις, es decir, de la naturaleza y sus fenómenos, no limitados al plano material necesariamente. En la Antigüedad la palabra «metafísica» no denotaba una disciplina particular de la filosofía, sino el compendio de rollos de Aristóteles ya mencionado. Solo es a partir del siglo XIII cuando la metafísica pasa a ser una disciplina filosófica especial que tiene como objeto el ente en cuanto ente. Es hacia ese siglo cuando las teorías aristotélicas se empiezan a conocer en el Occidente latino gracias al influjo de pensadores musulmanes como el persa Avicena y el andalusí Averroes. El significado contemporáneo de lo que conlleva el término latino-medieval Metaphysica se debe a la tradición musulmana, pues tanto el griego clásico como el latín carecían de términos para dicha significación y Europa se encontraba anclada en la filosofía neoplatónica.  A partir de entonces la metafísica pasa a ser la más alta disciplina filosófica, llegando así hasta la Edad Moderna. Con el tiempo la palabra «metafísica» adquirió, como adjetivo, el significado de «difícil» o «sutil» y en algunas circunstancias se utiliza con un carácter peyorativo, pasando a significar «especulativo, dudoso o no científico». En este sentido, también la metafísica es considerada como un modo de reflexionar con demasiada sutileza en cualquier materia que discurra entre lo oscuro y difícil de comprender.[cita requerida] Definiciones y conceptos En la Metafísica de Aristóteles se encuentran diversas definiciones de la metafísica como ciencia. La metafísica considerada como «aiteología» es la ciencia de las causas supremas (A, 1). Como ontología es la ciencia del ser en cuanto ser (G, 1). Como teología es la ciencia de las cosas divinas (E, 1) y como «useología» es la ciencia de la sustancia (Z, 1). A través de la historia las posiciones en cuanto a estas definiciones han sido diversas. De hecho, algunos consideran que en la Metafísica de Aristóteles se encuentran cuatro metafísicas distintas; mientras que otros piensan que las cuatro definiciones se integran para formar una sola metafísica. La metafísica encuentra su unidad de la siguiente manera: la ontología y la useología poseen universalidad de predicación, mientras que la ontología y la useología son universales por causalidad. De esta forma el subiectum de la metafísica sería en el ente en cuanto ente; ahora bien el ente se dice primariamente de la sustancia, por ello el subiectum integra las ciencias universales por predicación. Los principios de la metafísica provienen de las ciencias universales por causalidad. Para Immanuel Kant «la metafísica es un conocimiento especulativo de la razón, enteramente aislado, que se alza por encima de las enseñanzas de la experiencia mediante meros conceptos (no como la matemática, mediante aplicación de los mismos a la intuición), y en donde, por lo tanto, la razón debe ser su propio discípulo». La Real Academia Española define a la metafísica como la «parte de la filosofía que trata del ser en cuanto tal, y de sus propiedades, principios y causas primeras». Objetivos La metafísica pregunta por los últimos fundamentos del mundo y de todo lo existente. Su objetivo es lograr una comprensión teórica del mundo y de los principios últimos generales más elementales de lo que existe, porque tiene como fin conocer la verdad más profunda de las cosas, por qué son lo que son y, aún más, por qué son. Cuatro de las preguntas fundamentales de la metafísica son: ¿Qué es ser? ¿Qué es lo que existe? ¿Por qué existe algo, y no más bien nada? ¿Por qué estoy en este mundo? No solo se pregunta entonces por lo que existe, sino también por qué existe algo. Además aspira a encontrar las características más elementales de todo lo que es: la cuestión planteada es si hay características tales que se le puedan atribuir a todo lo que es y si con ello pueden establecerse ciertas propiedades del ser. Algunos de los conceptos principales de la metafísica son: ser, nada, existencia, esencia, mundo, espacio, tiempo, mente, Dios, libertad, cambio, causalidad y fin. Algunos de los problemas más importantes y tradicionales de la metafísica son: el problema de los universales, el problema de la estructura categorial del mundo, y los problemas ligados al espacio y el tiempo. Métodos La metafísica procede de distintas maneras: Es especulativa, cuando parte de un principio supremo a partir del cual va interpretando la totalidad de la realidad. Un principio de este tipo podría ser la idea, Dios, el ser, la mónada, el espíritu universal o la voluntad. Es inductiva, en su intento de consolidar de manera unificada los resultados asociados a todas las ciencias particulares, configurando una imagen metafísica del mundo. Es reduccionista (ni empírico-inductiva, ni especulativa-deductiva), cuando se la entiende como un mero constructo especulativo a base de presupuestos de los cuales los seres humanos siempre han tenido que partir para poder llegar a conocer y actuar. Conceptos Ser Esta sección es un extracto de Ser.[editar] Existen dudas o desacuerdos sobre la exactitud de la información en este artículo o sección. Consulta el debate al respecto en la página de discusión. Este aviso fue puesto el 20 de marzo de 2012. Ser es el más general de los términos. Con la palabra «ser» se intenta abarcar el ámbito de lo real en sentido ontológico general, esto es, la realidad por antonomasia, en su sentido más amplio: «realidad radical». El Ser es, por lo tanto, un trascendental, aquello que trasciende y rebasa todos los entes sin ser él mismo un ente, es decir, sin que ningún ente, por muy amplio que sea y se presente, lo agote. Dicho de otro modo: el Ser desborda y supera dialécticamente el mundo de las formas, el mundus asdpectabilis, trasladándose en otro contexto, «más allá del horizonte de las formas», más allá de toda la morfología cósmica. La pregunta por el ser no corresponde solamente a Occidente: ya los filósofos antiguos de China desarrollaron independientemente posiciones acerca del ser. Laozi en el siglo VI a. C. hace la distinción entre ser y no-ser. Luego, las escuelas neo-taoístas (Wang Bi, Guo Xiang, etc.) harán prevalecer el no-ser sobre el ser. La tradición distingue dos tipos de enfoques distintos al concepto de ser: Concepto unívoco de ser: «ser» es la característica más general de diferentes cosas (llamadas entes o entidades), aquello que sigue siendo igual a todos los entes, después de que se han eliminado todas las características individuales a los entes particulares, esto es: el hecho de que «sean», esto es, el hecho de que a todas ellas les corresponda «ser» (cfr. diferencia ontológica). Este concepto de «ser» es la base de la «metafísica de las esencias». Lo opuesto al «ser» viene a ser en este caso la «esencia», a la cual simplemente se le agrega la existencia. En cierto sentido no se diferencia ya mucho del concepto de la nada. Un ejemplo de ello lo dan ciertos textos de la filosofía temprana de Tomás de Aquino (De ente et essentia). Concepto analógico del ser: el «ser» viene a ser aquello que se le puede atribuir a «todo», aunque de distintas maneras (analogía entis). El ser es aquello, en lo que los diferentes objetos coinciden y en lo que, a su vez, se distinguen. Este enfoque del ser es la base de una metafísica (dialéctica) del ser. El concepto opuesto a ser, es aquí la nada, ya que nada puede estar fuera del ser. La filosofía madura de Tomás de Aquino nos brinda un ejemplo de esta comprensión de «ser» (Summa theologica) Entidad Esta sección es un extracto de Entidad.[editar] Entidad o ente (del latín: ens) es algo que es o que existe de alguna manera determinada, según la filosofía. De igual forma el término «ente», así como el de ser, es muy general y vago, ya que en la historia de la filosofía occidental se ha usado con diversos sentidos. Algunos autores recientes, entre los que están Martin Heidegger y Étienne Gilson, proponen distinciones entre los conceptos de ser (el acto de ser) y ente (lo que es). Por otro lado, algunos filósofos (como Tomás de Aquino) han utilizado el término entidad para referirse a la esencia de algo, lo que significa la propiedad o característica que define y determina la naturaleza de una cosa. Sustancia Esta sección es un extracto de Sustancia (filosofía).[editar] La sustancia o substancia (del griego: oὐσία, ousía) es un término filosófico utilizado originalmente en la antigua filosofía griega. Existen dos maneras de caracterizar el término: 1. En un sentido genérico, el término, proveniente de una traducción temprana al latín del término griego ousia que habría derivado, a su vez, del verbo einai (ser), vendría a designar a aquellas cosas de las que podemos decir que son; los seres. Desde esta concepción amplia, se entenderían por sustancias aquellas entidades primeras o fundacionales de un sistema filosófico como los átomos para un atomista o las impresiones e ideas para la propuesta de David Hume. 2. El segundo uso es más restrictivo. Se entiende por sustancia aquellas entidades que tienen una realidad independiente y no derivada, frente a las propiedades y eventos. Este segundo uso es aceptado por algunas teorías y por otras no: por ejemplo, Hume no acepta la existencia de sustancias de este tipo. El estado de la cuestión sobre la pertinencia de este segundo uso se mantiene abierto. Este término fue usado por varios filósofos griegos antiguos, como Parménides, Platón y Aristóteles, como una designación primaria para los conceptos filosóficos de esencia. En la filosofía contemporánea es análogo a los conceptos de ser y óntico. En la química acabó por desarrollar el concepto de sustancia química, uno de los más importantes del campo, también en la teología cristiana se desarrolló el concepto de sustancia divina (θεία ουσία) que acabó por ser uno de los conceptos doctrinales más importantes, centrales para el desarrollo de la doctrina trinitaria. El concepto puede clasificarse en variedades monistas, dualistas o pluralistas según la cantidad de sustancias que existen en el mundo. Los monistas sostienen que existe una única sustancia en el mundo (por ejemplo, el panteísmo de Spinoza). El dualismo entiende el mundo como compuesto de dos sustancias fundamentales (por ejemplo, el dualismo cartesiano con la res cogitans y la res extensa). Los pluralistas sostienen la existencia de múltiples sustancias (por ejemplo, la teoría de las formas de Platón, el hilemorfismo de Aristóteles y las mónadas de Leibniz). El término proviene del griego ousía, que se tradujo al latín como essentia o substantia y, por lo tanto, al español como esencia, entidad, sustancia o substancia. Aunque su significado ha experimentado ciertas variaciones a lo largo de su historia etimológica a lo largo de la Historia del pensamiento occidental,   lo más parecido en nuestro lenguaje actual al concepto de sustancia o sustrato en filosofía en general sería el concepto común de «objeto», frente a las propiedades, cualidades o atributos de estos. Categoría Esta sección es un extracto de Categoría.[editar] En lenguaje coloquial, se entiende por categoría el grado de jerarquía dentro de un orden, que puede ser: social: el lugar que ocupa una determinada persona o cargo institucional, generalmente relacionado con el ejercicio del poder en todos sus campos: taxonómico: el nivel de importancia de cualquier cosa respecto a todas las demás. La palabra categoría deriva de la palabra griega katêgoria que significa predicado o atributo. En filosofía, una categoría es una de las nociones más abstractas y generales por las cuales las entidades son reconocidas, diferenciadas y clasificadas. Mediante las categorías, se pretende una clasificación jerárquica de las entidades del mundo. Entidades muy parecidas y con características comunes formarán una categoría, y a su vez varias categorías con características afines formarán una categoría superior. Absoluto Esta sección es un extracto de Absoluto (filosofía).[editar] Retratos de los cuatro principales idealistas alemanes: Kant, Fichte, Schelling y Hegel Lo Absoluto es, en filosofía, un concepto que define a una realidad completa, autosuficiente e independiente de lo demás. La etimología de la palabra añade Ab que significa separación y el verbo latino solvo que significa soltar/desvincular. Cualquier realidad, en tanto que pueda ser considerada como tal, ha de tener una relación de dependencia conocida o desconocida. Lo Absoluto se refiere a lo separado (ab-suelto) de toda existencia o de su posibilidad. En teología se usa también el término el Absoluto para referirse al ser supremo. En otras palabras, la palabra absoluto se refiere a algo que está libre o exento de todo vínculo o conexión con otra cosa.  Ramas Ontología Esta sección es un extracto de Ontología.[editar] La ontología (del griego antiguo ὄν [on] ,genitivo ὄντος, [ontos], ente; y λόγος [lógos] ciencia, estudio, teoría) o metafísica general es la rama de la filosofía que estudia lo que hay, así como las relaciones entre los entes (por ejemplo, la relación entre un universal ,como el rojo, y un particular que lo tiene ,como una manzana,) o la relación entre un acto (como el que Sócrates bebiera la cicuta) y sus participantes (Sócrates y la cicuta). Los ontólogos suelen tratar de determinar cuáles son las categorías o géneros más altos y cómo forman un sistema de categorías que proporciona una clasificación abarcadora de todas las entidades. Las categorías comúnmente propuestas incluyen sustancias, propiedades, relaciones, estados de cosas y eventos. Estas categorías se caracterizan por conceptos ontológicos fundamentales, como particularidad y universalidad, abstracción y concreción o posibilidad y necesidad. De interés especial es el concepto de dependencia ontológica, que determina si las entidades de una categoría existen en el nivel más fundamental. Los desacuerdos dentro de la ontología suelen girar en torno a si las entidades pertenecientes a una determinada categoría existen y, en caso afirmativo, cómo se relacionan con otras entidades. Algunas preguntas ontológicas son: ¿qué es la materia? ¿Qué es un proceso? ¿Qué es el espacio? ¿Qué es el tiempo? ¿Qué es el espacio-tiempo? ¿Hay propiedades emergentes? ¿Se ajustan todos los eventos a alguna(s) ley(es)? ¿Hay especies naturales? ¿Qué hace real a un objeto? ¿Hay causas finales? ¿Es real el azar? Muchas preguntas tradicionales de la filosofía se pueden entender como preguntas ontológicas: ¿Existe Dios? ¿Existen entidades mentales, como pensamientos e ideas? ¿Existen entidades abstractas, como los números? ¿Existen los universales? Cuando se utilizan como sustantivo contable, los términos ontología y ontologías no se refieren a la ciencia del ser, sino a las teorías dentro de la ciencia del ser. Las teorías ontológicas se pueden dividir en varios tipos según sus compromisos teóricos. Las ontologías monocategóricas sostienen que solo hay una categoría básica, lo que es rechazado por las ontologías policategóricas. Las ontologías jerárquicas afirman que algunas entidades existen en un nivel más fundamental y que otras entidades dependen de ellas. Las ontologías planas, en cambio, niegan ese estatus privilegiado a cualquier entidad.[cita requerida] Teleología Esta sección es un extracto de Teleología.[editar] Tanto Platón como Aristóteles, representados en La Escuela de Atenas, desarrollaron argumentos filosóficos que abordan el orden aparente del universo (logos). La teleología (del griego τέλος, telos, fin, propósito, y λογία, logía discurso, tratado o ciencia ) es la rama de la metafísica que se refiere al estudio de los fines o propósitos de algún objeto o algún ser, la capacidad de luchar por una finalidad, o bien, literalmente, la doctrina filosófica de las causas finales. También puede entenderse como una rama de la causalidad que da la razón o la explicación de algo en función de su fin, propósito o meta, en función de su causa. Usos más recientes lo definen simplemente como la atribución de una finalidad, u objetivo, a procesos concretos. Cosmología filosófica Esta sección es un extracto de Filosofía del espacio y el tiempo.[editar] Alegoría del tiempo gobernado con prudencia de Tiziano. La filosofía del espacio y el tiempo, también conocida como cosmología filosófica, es la rama de la filosofía que trata de los aspectos referidos a la ontología, la epistemología y la naturaleza del espacio y el tiempo. Los problemas vinculados al espacio y al tiempo tradicionalmente han sido centrales en los sistemas filosóficos, desde los presocráticos hasta Bergson y Heidegger. La filosofía analítica y el positivismo lógico, en ejercicio de su crítica del método científico y la metafísica tradicionales, los han estudiado con particular interés desde sus comienzos. Teología natural Esta sección es un extracto de Teología natural.[editar] La teología natural es la rama de la filosofía dentro de la corriente escolástica, de gran importancia durante la Edad Media, que trataría la cuestión de la existencia y atributos de Dios desde la filosofía, es decir, sin recurrir a lo que sería considerado como revelación divina. Psicología racional La psicología racional, también llamada filosofía del hombre, psicología metafísica o psicología filosófica, se ocupa del alma o mente del hombre. Se refiere al estudio de conceptos y principios de la psicología a través de la razón y el análisis filosófico. En la metafísica se centra en temas como la inmortalidad, el libre arbitrio del alma humana, la fundamentación de la espiritualidad y el esclarecimiento de la relación entre cuerpo y alma. Una de las principales figuras de la historia de la psicología racional es Christian Wolff, quien abordó temas de psicología en su obra Metafísica Alemana (1720). Historia Edad Antigua Aristóteles consideró a la metafísica como la «primera filosofía». Ya desde los comienzos de la filosofía en Grecia, con los llamados filósofos presocráticos, se aprecian los intentos de entender el universo todo a partir de un principio (originario) único y universal, el αρχη (arjé). Parménides de Elea (siglo VI-V a. C.) es considerado el fundador de la ontología. Es él quien utiliza por primera vez el concepto de ser/ente en forma abstracta. Este saber, metafísico, comenzó cuando el espíritu humano se hizo consciente de que lo real sin más no es lo que nos ofrecen los sentidos, sino lo que se capta con el pensamiento («Lo mismo es pensar y ser»). Es lo que él llama «ser», y que caracteriza a través de una serie de determinaciones conceptuales que están al margen de los datos de los sentidos, como ingénito, incorruptible, inmutable, indivisible, uno, homogéneo, etc. Parménides expone su teoría con tres principios: «el ser (o el ente) es y el no-ser no es», «nada puede pasar del ser al no-ser y viceversa» y «lo mismo es el pensar que el ser» (esto último se refiere a que no puede existir lo que no puede ser pensado). A partir de su afirmación básica («el ser es, el no-ser no es») Parménides deduce que el ser es ilimitado, ya que lo único que podría limitarlo es el no-ser; pero como el no-ser no es, no puede establecer limitación alguna. Por lo tanto, según deducirá Meliso de Samos, el ser es infinito (ilimitado en el espacio) y eterno (ilimitado en el tiempo). La influencia de Parménides es decisiva en la historia de la filosofía y del pensamiento mismo. Hasta Parménides, la pregunta fundamental de la filosofía era: ¿de qué está hecho el mundo? (a lo que algunos filósofos habían respondido que el elemento fundamental era el aire, otros que era el agua, otros un misterioso elemento indeterminado, etc.) Parménides instaló al «ser» (esse) en la escena como objeto principal del discurrir filosófico. El próximo paso decisivo lo dará Sócrates. Sócrates (470-399 a. C.), en cambio, se centra en la moral. Su pregunta fundamental es: ¿qué es el bien? Sócrates creía que si se lograba extraer el concepto del bien se podía enseñar a la gente a ser buena (como se enseña la matemáticas, por ejemplo) y se acabaría así con el mal. Estaba convencido de que la maldad es una forma de ignorancia, doctrina llamada intelectualismo moral. Desarrolló la primera técnica filosófica que se conoce: la mayéutica. Consistía en preguntar y volver a preguntar sobre las respuestas obtenidas una y otra vez, profundizando cada vez más. Con ello pretendía llegar al «logos» o la razón final que hacía que una cosa fuera esa cosa y no otra. Este «logos» es el embrión de la «idea» de Platón, su discípulo. Platón (427-347 a. C.) pone el punto central de la filosofía en la teoría de las Ideas. Platón observó que el logos de Sócrates era una serie de características que percibimos en los objetos (físicos o no) y están asociadas a él. Si a ese logos lo separamos del objeto físico y le damos existencia formal, entonces se llama «idea» (la palabra «idea» la introdujo Platón). En los diálogos platónicos aparece Sócrates preguntando por lo que es justo, valeroso, bueno, etc. La respuesta a estas preguntas presupone la existencia de ideas universales cognoscibles por todos los seres humanos que se expresan en estos conceptos. Es a través de ellas que podemos captar el mundo en constante transformación. Las ideas son el paradigma de las cosas. Su lugar está entre el ser y el no-ser. Son anteriores a las cosas, que participan (methexis) de ellas. En sentido estricto solo ellas son. Las cosas particulares que vemos solo representan copias más o menos exactas de las ideas. La determinación o definición de las ideas se obtiene a través del ejercicio dialógico riguroso, enmarcado en determinado contexto histórico y coyuntural, delimitando aquello en lo que se ha centrado la investigación (la idea). Con la teoría de las Ideas Platón pretende probar la posibilidad del conocimiento científico y del juicio imparcial. El hecho de que todos los seres humanos tengan la posibilidad de acceder a un mismo conocimiento, tanto en el campo de las matemáticas, como en el de la ética, lo explica a través de la teoría del «recuerdo» (ἀνάμνησις), según la cual recordamos las ideas eternas que conocimos antes de nuestro nacimiento. Con ello Platón explica la universalidad de la capacidad racional de todos los seres humanos, enfrentándose a algunos de sus contemporáneos que sostenían la incapacidad de acceder al conocimiento por parte de esclavos o pueblos no-helénicos, entre otros. La tradición postplatónica muchas veces entendió la teoría de las Ideas de Platón, en el sentido de que habría supuesto una existencia de las ideas separada de la existencia de las cosas. Esta teoría de la duplicación de los mundos, en la Edad Media condujo a la polémica sobre los universales. Aristóteles (384-322 a. C.) nunca usó la palabra «metafísica» en su obra conocida como Metafísica. Dicho título se atribuye al primer editor sistemático de la obra del estagirita, Andrónico de Rodas, que supuso que, por su contenido, los catorce libros que agrupó debían ubicarse después de la «física» y por esa razón usó el prefijo «μετὰ» (más allá de... o después de...) La metafísica, según Aristóteles, es en primer lugar, una teoría de los principios generales del pensamiento (que aborda en más detalle en su lógica); y en segundo lugar, una doctrina (logos) del ser (on) en cuanto tal. La metafísica de Aristóteles gira en torno a dos cuestiones fundamentales: la del comienzo y la de la unidad. En su análisis del ente, Aristóteles va más allá de la materia, al estudiar las cualidades y potencialidades de lo existente para acabar hablando del «ser primero», el «motor inmóvil» y generador no movido de todo movimiento, que más tarde sería identificado con Dios. Para Aristóteles la metafísica es la ciencia de la esencia de los entes y de los primeros principios del ser. El ser se dice de muchas maneras y estas reflejan la esencia del ser. En ese sentido elabora ser, independientemente de las características momentáneas, futuras y casuales. La ousía (generalmente traducido como sustancia) es aquello que es independiente de las características (accidentes), mientras que las características son dependientes de la ousía. La ousía es lo que existe en sí, en contraposición al accidente, que existe en otro. Gramaticalmente o categorialmente, se dice que la sustancia es aquello a lo que se adscribe características, es decir, es aquello sobre lo cual se puede afirmar (predicar) algo. Aquello que se afirma sobre las sustancias son los predicados. A la pregunta de qué sería finalmente la esencia que permanece inmutable, la respuesta de Aristóteles viene a ser que la ousía es una forma determinante ,el eidos, es el origen de todo ser, es decir, que por ejemplo en el eidos de Sócrates, lo que en su forma humana, determina su humanidad. Y también la que determina que siendo el hombre por naturaleza libre y no siendo el esclavo libre, determina que el esclavo sea parte constitutiva de su amo, es decir, que no sea solo esclavo de su amo en determinada coyuntura y desde determinada perspectiva, sino que sea esclavo por naturaleza. Edad Media Fresco de Andrea Bonaiuto El Triunfo de Santo Tomás, con la imagen de Tomás de Aquino entronizada y la de Averroes sentada debajo en reposo y pensativa de apoyado posiblemente en algún libro de Aristóteles. En el mundo islámico, la llegada de la filosofía griega no fue directa, sino que tiene que ver con los cenobios cristianos en la península arábiga y los pertenecientes a ideologías consideradas heréticas que utilizaban la filosofía griega no como un fin, sino como un instrumento que les servía para sus especulaciones teológicas (como los monofisistas o los nestorianos). Es por el interés práctico en la medicina griega cuando empiezan a hacerse traducciones al persa que después pasarían tardíamente al árabe. Cabe mencionar que en árabe no existe el verbo «ser» y más difícilmente una construcción como «ser», que es un verbo convertido en sustantivo. La metafísica del mundo islámico quedó influenciada en gran medida por la metafísica de Aristóteles. En el mundo cristiano, después del «redescubrimiento» de la Metafísica de Aristóteles a mediados del siglo XII, muchos escolásticos escribieron comentarios sobre esta obra. El problema de los universales fue uno de los principales problemas tratados durante ese período. Otros temas fueron: Hilomorfismo: desarrollo de la doctrina aristotélica de que las cosas individuales son un compuesto de material y forma Existencia y esencia Causalidad: la discusión sobre la causalidad consistió principalmente en comentarios sobre Aristóteles, principalmente sobre las obras Física, Sobre el cielo y Acerca de la generación y la corrupción. El enfoque de esta área temática era únicamente medieval, la investigación racional del universo se veía como una forma de acercarse a Dios La metafísica pasó a ser considerada la «reina de las ciencias» (Tomás de Aquino), aunque también hubo debate sobre la distinción y orden de jerarquía entre la metafísica y la teología, en especial en la escolástica. La cuestión de la distinción entre metafísica y teología también sería omnipresente en la filosofía moderna. Los escolásticos medievales se propusieron la tarea de conciliar la tradición de la filosofía antigua con la doctrina religiosa (musulmana, cristiana o judía). Con base en el neoplatonismo tardío, la metafísica medieval se propone reconocer el «verdadero ser» y a Dios a partir de la razón pura. Los temas centrales de la metafísica medieval son la diferencia entre el ser terrenal y el ser celestial (analogía entis), la doctrina de los trascendentales y las pruebas de la existencia de Dios. Dios es el fundamento absoluto del mundo, del cual no se puede dudar. Se discute si Dios ha creado el mundo de la nada (creación ex nihilo) y si es posible acceder a su conocimiento a través de la razón o solo a través de la fe. Inspirados en la teoría de la duplicación de los mundos atribuida a Platón su metafísica se manifiesta como una suerte de «dualismo» del «acá» y del «más allá», de la «mera percepción sensible» y del «pensar puro como conocimiento racional», de una «inmanencia» de la vida interior y una «trascendencia» del mundo exterior. René Descartes fue uno de los mayores exponentes de la metafísica de la Edad Media (aunque vivió en la Edad Moderna); construyó un pensamiento metafísico hablando de temas como la existencia, sustancia y Dios; define la metafísica como las raíces del conocimiento si este fuera un árbol, siendo el tronco la física y filosofía natural y las ramas las artes mecánicas; habla acerca de la sustancia como aquello que puede existir por sí mismo sin necesidad de otra cosa; afirma la existencia de la sustancia pensante y la sustancia extensa, siendo este el dualismo sustancial aparte de la existencia de la sustancia infinita (Dios). En su texto llamado Meditaciones metafísicas, Descartes se plantea que solo puede creer lo que sea indudable, en otras palabras, todo de lo que se puede dudar es falso. Partiendo de esa idea: Encuentra que él existe (Cogito ergo sum). Prueba que Dios existe a partir del solipsismo. Propone la existencia de un Genio Maligno que le hace creer ideas falsas. Vuelve a probar la existencia de Dios a partir de un argumento ontológico. Muestra la existencia de lo material y que hay un alma y un cuerpo. Edad Moderna El idealismo trascendental de Immanuel Kant significó un «giro copernicano» para la metafísica. La tradición moderna divide la metafísica en: metafísica general u ontología ,ciencia del ente en tanto ente, y metafísica especial, que se divide en tres ramas: Filosofía de la naturaleza, también llamada cosmología racional o simplemente cosmología Filosofía del hombre, también llamada psicología metafísica, psicología filosófica, psicología racional, antropología metafísica o antropología filosófica Teología natural, también llamada teodicea o teología racional Esta clasificación, que fue propuesta entre otros por Christian Wolff, ha sido posteriormente discutida, pero sigue siendo considerada canónica. El idealismo trascendental de Kant significó un «giro copernicano» para la metafísica. Su posición frente a la metafísica es paradigmática; le atribuye ser un discurso de «palabras huecas» sin contenido real, la acusa de representar las «alucinaciones de un vidente», pero por otra parte recoge de ella la exigencia de universalidad. Kant se propuso fundamentar una metafísica «que se pueda presentar como ciencia». Para ello examinó primero la posibilidad misma de la metafísica. Para Kant las cuestiones últimas y las estructuras generales de la realidad están ligadas a la pregunta por el sujeto. A partir de este presupuesto dedujo que hay que estudiar y juzgar aquello que puede ser conocido por nosotros. A través de su criticismo se diferenció explícitamente de las posiciones filosóficas que tienen como objeto la pregunta sobre qué es el conocimiento. Se alejó así de las tendencias filosóficas imperantes, tales como el empirismo, el racionalismo y el escepticismo. También, a través del criticismo, marcó distancia del dogmatismo de la metafísica que ,según Kant, se había convertido en una serie de afirmaciones sobre temas que van más allá de la experiencia humana. Intentó entonces llevar a cabo un análisis detallado de la facultad humana de conocer, es decir, un examen crítico de la razón pura, de la razón desvinculada de lo sensible (Crítica de la razón pura, 1781-1787). Para ello es decisivo el presupuesto epistemológico de Kant de que al ser humano la realidad no se le presenta tal como es realmente (en sí), sino tal como se le aparece debido a la estructura específica de su facultad de conocimiento. Como el conocimiento científico también depende siempre de la experiencia, el hombre no puede emitir juicios sobre cosas que no están dadas por las sensaciones (tales como «Dios», «alma», «universo», «todo», etc.) Por ello Kant dedujo que la metafísica tradicional no es posible, porque el ser humano no dispone de la facultad de formar un concepto basándose en la experiencia sensible de lo espiritual, que es la única que permitiría la verificación de las hipótesis metafísicas. Como el pensar no dispone de ningún conocimiento de la realidad en este aspecto, estos asuntos siempre permanecerán en el ámbito de lo especulativo-constructivo. Entonces, por principio, no es posible según Kant decidir racionalmente sobre preguntas centrales tales como si Dios existe, si la voluntad es libre o si el alma es inmortal. Las matemáticas y la física pueden formular juicios sintéticos a priori y, por ello, alcanzar un conocimiento universal y necesario, un conocimiento científico. Del idealismo trascendental de Kant surge el idealismo alemán ,representado sobre todo por Fichte, Schelling y Hegel, que considera a la realidad como un acontecimiento espiritual en el que el ser real es superado, siendo integrado en el ser ideal. El idealismo alemán recoge el giro trascendental de Kant, es decir que, en vez de entender la metafísica como la búsqueda de la obtención del conocimiento objetivo, se ocupa de las condiciones subjetivas de posibilidad de tal conocimiento. Así, se plantea hasta qué punto el ser humano puede llegar a reconocer estas evidencias. Sin embargo, rechaza que el conocimiento se limite a la experiencia posible y a los meros fenómenos, y propone una superación de esta posición, volviendo a postulados metafísicos que puedan reclamar validez universal: «conocimiento absoluto» como se decía desde Fichte hasta Hegel. Si aceptamos que los contenidos del conocimiento solo valen en relación con el sujeto ,como suponía Kant, y consideramos que esta perspectiva es absoluta, es decir, es la perspectiva de un sujeto absoluto, entonces el conocimiento válido para este sujeto absoluto también tiene validez absoluta. A partir de este planteamiento el idealismo alemán considera que puede superar la contradicción empírica entre sujeto y objeto para poder captar lo absoluto. Hegel sostiene que de una identidad pura y absoluta no puede surgir o entenderse una diferencia (esa identidad sería como «la noche, en la que todas las vacas son negras»): no explicaría la realidad en toda su diversidad. Por eso «la identidad de lo absoluto» debe entenderse como que está desde su origen ya que contiene en sí la posibilidad y la necesidad de una diferenciación. Esto implica que lo absoluto se realiza en su identidad por el plasmado y la superación de momentos no idénticos, esto es, la identidad dialéctica. A partir de este planteamiento Hegel desarrolla la Ciencia de la lógica, considerada, tal vez, como el último gran sistema de la metafísica occidental. Edad Contemporánea Martin Heidegger afirmó que la metafísica es «el pensamiento occidental en la totalidad de su esencia». Karl Marx y Friedrich Engels adoptaron una actitud antimetafísica con base a su concepción del materialismo dialéctico, que deriva de la dialéctica idealista de Hegel. Según este solo la materia es real, junto con sus cambios. La dialéctica explica estas transformaciones, según la cual todos los procesos naturales y sociales ocurren por contradicción. Por ejemplo, en el análisis de la mercancía en El capital, el objeto mercantil es la “unidad contradictoria” de valor de uso y valor de cambio. Esta explicación pretende ser universal y válida tanto para la naturaleza como para la sociedad y el pensamiento. En su obra Tesis sobre Feuerbach concluye en la célebre tesis 11: «Los filósofos no han hecho más que interpretar de diversos modos el mundo, pero de lo que se trata es de transformarlo». Aunque algunos aspectos del pensamiento marxista pueden interpretarse como metafísica, el marxismo criticaría una determinada manera de hacer metafísica. Friedrich Nietzsche considera que Platón es el iniciador del pensamiento metafísico y le hace responsable de la escisión en el ser, que tendrá luego formas variadas pero constantes. La división entre mundo sensible y mundo inteligible, con su correlato cuerpo-alma, y la preeminencia del segundo asegurada por la teoría de las Ideas sitúa el mundo verdadero más allá de los sentidos. Esto deja fuera del pensar el devenir, aquello no apresable en la división sensible-inteligible por su carácter informe, y que también deja escapar las subsiguientes divisiones aristotélicas, como sustancia-accidente y acto-potencia. Martin Heidegger dijo que nuestra época es la del «cumplimiento de la metafísica», pues desde los inicios del pensamiento occidental se han producido unos determinados resultados que configuran un panorama del que el pensamiento metafísico no puede ya dar cuenta. El propio éxito de la metafísica ha conducido fuera de ella. Ante esto, la potencia del pensamiento consiste precisamente en conocer e intervenir sobre lo conocido. Pero el pensamiento metafísico carece ya de potencia, puesto que ha rendido sus últimos frutos. Heidegger afirmó que la metafísica es «el pensamiento occidental en la totalidad de su esencia». La utilización del término «esencia» en esta definición implica que la técnica para estudiar la metafísica como forma de pensamiento es, o debe ser, la metafísica en el primer sentido antes indicado. Esto quiere decir que los críticos de la metafísica como esencia del pensamiento occidental, son conscientes de que no existe una «tierra de nadie» en que situarse, más allá de esa forma de pensamiento; solo el estudio atento y la modificación consciente y rigurosa de las herramientas proporcionadas por la tradición filosófica pueden ajustar la potencia del pensamiento a las transformaciones operadas en aquello que la metafísica estudiaba: el ser, el tiempo, el mundo, el hombre y su conocimiento. Pero esa modificación supone a su vez un «salto» que toda la tradición del pensamiento ha escenificado, ha fingido o soñado dar a lo largo de su desarrollo. El salto fuera de la metafísica y por tanto, quizá, la revocación de sus consecuencias. Heidegger caracterizó el discurso metafísico por su impotencia para pensar la diferencia óntico-ontológica, es decir, la diferencia entre los entes y el ser. La metafísica refiere al ser el modelo de los entes (las cosas), pero aquel sería irreductible a estos: los entes son, pero el ser de los entes no puede caracterizarse simplemente como estos. El ser es pensado como ente supremo, lo que le identifica con Dios; la pulsión ontoteológica es una constante en el pensamiento occidental. Para Heidegger la metafísica es el «olvido del ser», y la conciencia de este olvido debe abrir una época nueva, enfrentada a la posibilidad de expresar lo dejado al margen del pensamiento. La filosofía analítica fue desde su nacimiento, con autores como Russell y Moore, muy escéptica respecto a la posibilidad de una metafísica sistemática tal y como se había defendido tradicionalmente. Esto se debe a que el nacimiento de la filosofía analítica se debiera principalmente a un intento de rebelión contra el idealismo neohegeliano entonces hegemónico en la universidad británica. Sería a partir de los años veinte cuando el Círculo de Viena ofrecería una crítica total a la metafísica como un conjunto de proposiciones carentes de significado por no cumplir con los criterios verificacionistas del significado. No obstante esta posición es hoy minoritaria en el panorama analítico, donde se ha recuperado el interés por ciertos problemas clásicos de la metafísica como el de los universales, la existencia de Dios y otros de tipo ontológico. El postestructuralismo (Gilles Deleuze, Michel Foucault, Jacques Derrida) retoma la crítica de Nietzsche y argumenta que lo no pensable en la metafísica es precisamente la «diferencia» en tanto tal. La diferencia, en el pensar metafísico, queda subordinada a los entes, entre los que se da algo como una «relación». La pretensión de «inscribir la diferencia en el concepto» transformando este y violentando para ello los límites del pensamiento occidental aparece ya como una pretensión que lleva a la filosofía más allá de la metafísica."
ksampletext_wikipedia_phil_filosofiadelaciencia: str = "Filosofía de la ciencia. La filosofía de la ciencia es la rama de la filosofía que investiga el conocimiento científico y la práctica científica, se ocupa de examinar y describir la estructura de la ciencia y de los métodos que siguen los científicos para trabajar en ella. Se trata de una disciplina que reflexiona sobre los fundamentos, métodos, límites y alcances de la ciencia, podría decirse que busca responder a la pregunta cómo se hace la ciencia. Se trata de una disciplina que reflexiona sobre los fundamentos, métodos, límites y alcances de la ciencia. La esencia de la filosofía Lo que intenta la filosofía de la ciencia es explicar problemas tales como: Naturaleza y la obtención de las ideas científicas (conceptos, hipótesis, modelos, teorías, paradigma, etc.) Relación de cada una de ellas con la realidad Cómo la ciencia describe, explica, predice y contribuye al control de la naturaleza (esto último en conjunto con la filosofía de la tecnología) Formulación y uso del método científico Tipos de razonamiento utilizados para llegar a conclusiones Implicaciones de los diferentes métodos y modelos de ciencia La filosofía de la ciencia comparte algunos problemas con la gnoseología ,la teoría del conocimiento, que se ocupa de los límites y condiciones de posibilidad de todo conocimiento. Pero, a diferencia de esta, la filosofía de la ciencia restringe su campo de investigación a los problemas que plantea el conocimiento científico; el cual, tradicionalmente, se distingue de otros tipos de conocimiento, como el ético o estético, o las tradiciones culturales. A lo largo de la historia, se han propuesto diversos esquemas para el método científico. No hay un único método científico, algunos de los más importantes son: Método inductivo-deductivo: La ciencia comienza con observaciones individuales, a partir de las cuales se formulan generalizaciones que van más allá de los hechos observados. Estas generalizaciones permiten hacer predicciones, cuya confirmación las fortalece. Aristóteles, Francis Bacon, Galileo, Newton, y muchos otros científicos y filósofos se adhieren a este esquema. Método hipotético-deductivo: Se parte de hipótesis o conjeturas que preceden y guían a las observaciones. La ciencia no se inicia con la experiencia del mundo, sino con ideas propuestas por el investigador. Hume, Whewell, Kant, Popper, y otros se inclinan hacia este método. Método a priori: El conocimiento se alcanza mediante la razón pura, sin necesidad de recurrir a la experiencia. Descartes es un exponente de este método. Anarquismo metodológico: No existe un método científico único y universal. Los científicos utilizan una variedad de métodos y estrategias, y no hay reglas fijas que garanticen el éxito de la investigación. Feyerabend es el principal defensor de esta postura. En la actualidad, muchos científicos consideran que no existe un único método científico, debido a la complejidad y diversidad de las ciencias.  Algunos científicos han mostrado un vivo interés por la filosofía de la ciencia y algunos como Galileo Galilei, Isaac Newton y Albert Einstein, han hecho importantes contribuciones. Numerosos científicos, sin embargo, se han dado por satisfechos dejando la filosofía de la ciencia a los filósofos y han preferido seguir haciendo ciencia en vez de dedicar más tiempo a considerar cómo se hace la ciencia. Dentro de la tradición occidental, entre las figuras más importantes anteriores al siglo XX destacan entre muchos otros Platón, Aristóteles, Epicuro, Arquímedes, Boecio, Alcuino, Averroes, Nicolás de Oresme, Santo Tomas de Aquino, Jean Buridan, Leonardo da Vinci, Raimundo Lulio, Francis Bacon, René Descartes, John Locke, David Hume, Emmanuel Kant y John Stuart Mill. La filosofía de la ciencia no se denominó así hasta la formación del Círculo de Viena, a principios del siglo XX. En la misma época, la ciencia vivió una gran transformación a raíz de la teoría de la relatividad y de la mecánica cuántica. Entre los filósofos de la ciencia más conocidos del siglo XX figuran Karl R. Popper y Thomas Kuhn, Mario Bunge, Paul Feyerabend, Imre Lakatos, Ilya Prigogine, etc. Precursores Busto de Aristóteles Para Aristóteles (384 a. C.-322 a. C.) la ciencia era conocimiento cierto por medio de causas. Esta definición (teniendo en cuenta el amplio concepto de ciencia de la antigüedad, diferente del más restrictivo actual) tuvo vigencia en Europa occidental durante siglos, hasta que fue rechazada por la nueva filosofía natural que nacía en los siglos XVII y XVIII. Después de sus conquistas en Europa, partiendo de España, y en Asia hasta la India, los árabes comenzaron a interesarse tanto por las civilizaciones de Occidente como por las de Oriente, a tanto que manifestaron la ambición de heredar la aportación grecorromana. Al-Manzor (712-775 d. C.) fue el primer califa que estimuló esta ambición, pues hizo traducir al árabe todos los libros de los griegos y fundó en Bagdad una especie de universidad, que comprendía una importante biblioteca y un observatorio astronómico. Durante varios siglos, el idioma árabe fue considerado como la lengua de la ciencia, y las gentes de diferentes países de Europa iban desde muy lejos a Bagdad para beber en las fuentes de la ciencia antigua salvaguardada por los árabes. El desarrollo de la ciencia entre los árabes alcanzó su apogeo hacia los siglos IX y X y, como la astronomía gozó siempre de popularidad en Oriente, fue completamente natural que los árabes dedicaran una muy particular atención a esta rama de la ciencia. Sirviéndose del Almagesto, traducción árabe del famoso Tratado de Astronomía, de Ptolomeo, los astrónomos árabes trataron de reducir las teorías a tablas, perfeccionar los instrumentos de medida y multiplicar las observaciones con más precisión. Pronto se dieron cuenta de ciertos errores cometidos por el astrónomo alejandrino, principalmente en lo relativo al tiempo de revolución de la Luna, los límites de los eclipses solares y las posiciones respectivas de Mercurio y Venus con relación al Sol. El descubrimiento más importante hecho por los astrónomos árabes fue la precesión de los equinoccios. Este importante aporte se atribuye a Al-Battani, también llamado Albatenio, gran señor, que vivió entre finales del siglo IX y comienzos del X. La escolástica propuso la regularidad y uniformidad para su aplicación en la ciencia.[cita requerida] René Descartes (1596-1650) pretendía un conocimiento cierto basado en la existencia indudable de un sujeto pensante, así como avanzar gracias a ideas claras y distintas, dejando el papel de la experiencia en segundo plano. No es de extrañar que en el campo de la ciencia, los racionalistas destacaran en matemáticas, como el mismo Descartes o Leibniz, creador junto con Newton del cálculo infinitesimal. La corriente filosófica iniciada por Francis Bacon (1561-1626) proponía un conocimiento de la naturaleza empirista e inductista. Para elegir entre teorías rivales no había que recurrir a la argumentación, sino realizar un experimento crucial (instantia crucis) que permitiese la selección. David Hume (1711-1776), el principal filósofo empirista, subrayó aún más la importancia de los hechos frente a las interpretaciones. Pero el racionalismo y el empirismo clásicos destacaban excesivamente uno de los aspectos de la ciencia (la racionalidad o la experiencia) en detrimento del otro. El idealismo trascendental de Kant (1724-1804) intentó una primera síntesis de ambos sistemas en la que el espacio y el tiempo absolutos de Newton se convirtieron en condiciones que impone la mente para poder aprehender el mundo externo. Dentro de la tradición empirista Auguste Comte (1798-1857) propuso una filosofía, el positivismo, en la que la ciencia se reducía a relacionar fenómenos observables, renunciando al conocimiento de causas. Ernst Mach (1838-1916) ejerció, con su empiriocriticismo, una gran influencia que preparó el nacimiento del Círculo de Viena. Mach desarrolló una filosofía de orientación empirista centrada en los conceptos y métodos de la ciencia. Esta debe estudiar solo las apariencias (los fenómenos), de forma que intentar estudiar algo que no se nos presenta directamente a los sentidos es hacer metafísica. Coherente con sus ideas filosóficas, Mach se opuso hasta el final a la nueva teoría atómica, cuyo objeto es inalcanzable a la experiencia. Pierre Duhem (1861-1916) afirmó que toda ley física es una ley aproximada; por lo tanto, siguiendo la lógica estricta, no puede ser ni verdadera ni falsa; cualquier otra ley que represente las misma experiencias con la misma aproximación puede pretender, con tanto derecho como la primera, el título de ley verdadera, o, para hablar más exactamente, de ley aceptable. Aun así, Duhem opinaba que a medida que la ciencia avanza, se va acercando progresivamente a una descripción más fiel de la naturaleza. Proposiciones lógicas Construcción de Proposiciones Lógicas Algunos filósofos de la ciencia van más allá de la descripción, proponen cómo debería hacerse la filosofía de la ciencia y cómo debería ser el mundo, a partir de ella. Algunas de las proposiciones básicas que permiten construir la ciencia. Por ejemplo: Existe de manera independiente de la mente humana (tesis ontológica de realismo). La naturaleza es regular, al menos en alguna medida (tesis ontológica de legalidad). El ser humano es capaz de comprender la naturaleza (tesis gnoseológica de inteligibilidad). Tomar conciencia de su propia forma de pensar sobre sí misma Si bien estos supuestos metafísicos no son cuestionados por el realismo científico, muchos han planteado serias sospechas respecto del segundo de ellos y numerosos filósofos han puesto en tela de juicio alguno de ellos o los tres. De hecho, las principales sospechas con respecto a la validez de estos supuestos metafísicos son parte de la base para distinguir las diferentes corrientes epistemológicas históricas y actuales. De tal modo, aunque en términos generales el empirismo lógico defiende el segundo principio, opone reparos al tercero y asume una posición fenomenista, es decir, admite que el hombre puede comprender la naturaleza siempre que por naturaleza se entienda los fenómenos (el producto de la experiencia humana) y no la propia realidad. La ciencia como producto de la lógica y la razón Empirismo lógico Esta sección es un extracto de Empirismo lógico.[editar] Este artículo o sección necesita referencias que aparezcan en una publicación acreditada. Busca fuentes: «Filosofía de la ciencia» – noticias · libros · académico · imágenes Este aviso fue puesto el 19 de abril de 2024. El empirismo lógico, también llamado neopositivismo o positivismo lógico, es una corriente en la filosofía de la ciencia que limita la validez del método científico a lo empírico y verificable. Esta limitación, conocida como verificacionismo, prohíbe inducir una regla general a partir de observaciones particulares, lo cual eventualmente despertó críticas sobre la incompatibilidad de esta corriente con muchas ramas de la ciencia fundamentadas en la inducción para construir conocimiento válido. El empirismo lógico o neopositivismo es más estricto aún que el positivismo y su defensa del método científico como única forma válida de conocimiento. El empirismo lógico surgió durante el primer tercio del siglo XX alrededor del grupo de científicos y filósofos que formaron el célebre Círculo de Viena. Falsacionismo Esta sección es un extracto de Falsacionismo.[editar] Sir Karl Raimund Popper. Autor de la obra La lógica de la investigación científica. El falsacionismo o racionalismo crítico es una corriente epistemológica fundada por el filósofo austriaco Karl Popper (1902-1994). Para Popper, contrastar una teoría significa intentar refutarla mediante un contraejemplo. Si no es posible refutarla, dicha teoría queda «corroborada», pudiendo ser aceptada provisionalmente, pero no verificada; es decir, ninguna teoría es absolutamente verdadera, sino a lo sumo «no refutada». El falsacionismo es uno de los pilares del método científico. El filósofo Karl Popper entendió que los filósofos del Círculo de Viena (al cual él mismo estuvo muy vinculado, aunque no como miembro) habían mezclado dos problemas diferentes para los que habían resuelto dar una única solución: el verificacionismo. En contraposición a este punto de vista, Popper remarcó que una teoría podría perfectamente tener significado sin ser científica, y que, como tal, un «criterio de significación» podría no necesariamente coincidir con un «criterio de demarcación». Así pues, ideó su propio sistema, al que se denomina falsacionismo (cabe señalar que Popper no llama a su metodología falsacionismo, sino racionalismo crítico). Este no solo es interpretable como una alternativa al verificacionismo; supone también un acuerdo acerca de la distinción conceptual que habían ignorado las teorías previas. Para Popper ,y a diferencia del Círculo de Viena, la ciencia no es capaz de verificar si una hipótesis es cierta, pero sí puede demostrar si esta es falsa. Por eso no sirve la inducción, porque por mucho que se experimente nunca se podrá examinar todos los casos posibles, y basta con un solo contraejemplo para echar por tierra una teoría. Así pues, frente a la postura verificacionista preponderante hasta ese momento en filosofía de la ciencia, Popper propone el falsacionismo. Aunque Popper era realista no aceptaba la certeza, es decir, nunca se puede saber cuándo nuestro conocimiento es cierto. Popper comenzó describiendo la ciencia, pero en su evolución filosófica acabó siendo prescriptivo (aunque sin llegar al rigor normativo del Círculo), recomendando a la ciencia el método hipotético deductivo. Es decir, la ciencia no elabora enunciados ciertos a partir de datos, sino que propone hipótesis (que aunque se basen en la experiencia suelen ir más allá de esta y predecir experiencias nuevas) que luego somete al filtro experimental para detectar los errores. Popper vio la demarcación como un problema central en la filosofía de la ciencia. Propuso el falsacionismo como una forma de determinar si una teoría es científica o no. Simplificando, según Karl Popper se podría decir que si una teoría es falsable, entonces es científica; si no es falsable, entonces no es ciencia. Para Popper, afirmar que una teoría es científica quiere decir que añade conocimiento racional acerca del mundo empírico, por lo tanto, no puede ser: Tautológica (no añade nada) Contradictoria (va contra la lógica racional) Metafísica (afirma algo que no puede ser comprobado experimentalmente) La falsabilidad fue uno de los criterios utilizados por el Juez William Overton para determinar que el creacionismo no era científico y que no debería enseñarse en los colegios de Arkansas. La falsabilidad es una propiedad de los enunciados y de las teorías, y, en sí misma, es neutral. Como criterio de demarcación, Popper busca tomar esta propiedad como base para afirmar la superioridad de teorías falsables sobre las no falsables, como parte de la ciencia, estableciendo así una posición que podría ser llamada falsacionismo con implicaciones políticas.[cita requerida] Sin embargo, muchas cosas de las que pueden ser consideradas como dotadas de significado y utilidad no son falsables. Con toda certeza, los enunciados no falsables desempeñan una función en las propias teorías científicas. Lo que el criterio Popperiano permite ser llamado científico está abierto a interpretación. Una interpretación estricta concedería muy poco, puesto que no existen teorías científicas de interés que se encuentren completamente libres de anomalías. Del mismo modo, si solo consideramos la falsabilidad de una teoría y no la voluntad de un individuo o de un grupo para obtener o aceptar instancias falsables, entonces permitiríamos casi cualquier teoría. En cualquier caso, es muy útil conocer si un enunciado de una teoría es falsable, aunque solo sea por el hecho de que nos proporciona un conocimiento acerca de las formas con las que alguien podría evaluar una teoría. La tesis de Duhem-Quine argumenta que no es posible probar que un enunciado ha sido falsado; en su lugar, la falsación ocurre cuando la comunidad científica se pone de acuerdo en que ha sido falsado (véase consenso científico). Esta es una crítica importante al falsacionismo, pues cualquier enunciado observacional, por inocente que parezca, presupone ciertas concepciones acerca del mundo, y resulta imposible dejar de preguntarse si esas concepciones son científicas o no. El falsacionismo, en todas y cada una de sus múltiples formas, es una idea interesante, pero insuficiente como para caracterizar qué es lo que es ciencia o para resolver el problema de la demarcación. Sufre de una serie de dificultades lógicas y epistemológicas que deberían hacernos detener si lo que buscamos es obtener una respuesta en cuanto a qué es buena ciencia y qué no. Reacción Hasta la década de los sesenta habían prevalecido las explicaciones lógicas de la ciencia. A partir de la obra de Thomas Kuhn (1922-1996) La estructura de las revoluciones científicas hubo un cambio en la perspectiva y se empezaron a tener en cuenta los aspectos históricos, sociológicos y culturales de la ciencia. La obra de Kuhn fue importante porque posicionó las ideas históricas de la escuela epistemológica francesa (Henri Bergson, Henri Poincaré, Alexandre Koyré, Gaston Bachelard, etc.) en los círculos científicos de los Estados Unidos que después de la Segunda Guerra Mundial se habían mantenido herméticamente cerrados. Ciencia, historia y revolución científica La estructura de las revoluciones científicas se puede clasificar de descriptiva. Apenas dedica espacio a conceptos como verdad o conocimiento, y presenta la ciencia bajo un enfoque histórico y sociológico. Las teorías dominantes bajo las que trabajan los científicos conforman lo que Kuhn llama paradigma. La ciencia normal es el estado habitual de la ciencia en el que el científico no busca criticar, de ninguna manera, el paradigma, sino que da este por asumido y busca la ampliación del mismo. Si el número o la importancia de problemas no resueltos dentro de un paradigma es muy grande, puede sobrevenir una crisis y cuestionarse la validez del paradigma. Entonces la ciencia pasa al estado de ciencia extraordinaria o ciencia revolucionaria en el que los científicos ensayan teorías nuevas. Si se acepta un nuevo paradigma que sustituya al antiguo se ha producido una revolución científica. Así se entra en un periodo nuevo de ciencia normal en el que se intenta conocer todo el alcance del nuevo paradigma. El nuevo paradigma no se admite únicamente por argumentos lógicos, en este proceso intervienen de manera importante aspectos culturales propios de la persona del científico. Según Kuhn, la visión de la naturaleza que acompaña al nuevo paradigma no puede compararse bajo ningún elemento común a la del antiguo; a esto Kuhn llama la inconmensurabilidad de los paradigmas. El nuevo se admite de forma generalizada cuando los científicos del antiguo paradigma van siendo sustituidos. Falsacionismo sofisticado Esta sección es un extracto de Falsacionismo sofisticado.[editar] Este artículo o sección tiene referencias, pero necesita más para complementar su verificabilidad. Busca fuentes: «Filosofía de la ciencia» – noticias · libros · académico · imágenes Este aviso fue puesto el 31 de diciembre de 2024. El falsacionismo sofisticado es el nombre que da Imre Lakatos a su crítica a la epistemología y al falsacionismo, basada en lo que él denomina programas de investigación científica. La metodología de los programas de investigación supone un paso más allá en el falsacionismo ya que resuelve algunos de sus problemas. Debido a esto, Lakatos pasa a denominar falsacionismo ingenuo al defendido por Karl Popper en su libro La lógica de la investigación científica, mientras que llama falsacionismo sofisticado a las sugerencias más tardías de Popper, así como a su propia metodología de los programas de investigación. Lakatos intentó adaptar el sistema de Popper a la nueva situación creada por Thomas Kuhn. La intención de Popper era realizar una reconstrucción racional de la historia de la ciencia que muestre que esta progresa de modo racional. Sin embargo, la historia de la ciencia muestra que la ciencia no avanza solo falsando teorías con hechos, sino que hay que tener en cuenta la competencia entre teorías y la confirmación de teorías. Por eso Lakatos sustituye el falsacionismo ingenuo de Popper por un falsacionismo sofisticado. En la realidad la ciencia no evalúa una teoría aislada, sino un conjunto de teorías que conforman lo que Lakatos llama programa de investigación científica (también llamado paradigma). Un programa de investigación se rechaza al completo cuando se dispone de un sustituto superior que explique todo lo que explicaba el anterior más otros hechos adicionales. Lakatos reconoce que la dificultad de este esquema radica en que, en la práctica, puede costar años llevarlo a cabo, o incluso ser inaplicable en programas de investigación muy complejos. Anarquismo metodológico Esta sección es un extracto de Anarquismo epistemológico.[editar] Paul Feyerabend propuso el anarquismo epistemológico. El anarquismo epistemológico, anarquismo metodológico o dadaísmo epistemológico, es una teoría epistemológica propuesta por el filósofo de la ciencia austríaco Paul Feyerabend, que sostiene que no existen reglas metodológicas útiles y libres de excepciones que rijan el progreso de la ciencia o el crecimiento del conocimiento. Sostiene que la idea del funcionamiento de la ciencia mediante reglas fijas y universales es irreal, perniciosa y perjudicial para la ciencia misma. Según Feyerabend, a lo largo de la historia de la ciencia se han incumplido con éxito todas las reglas metodológicas del método científico. Señala además que no existe tampoco una separación clara entre lo que es ciencia y lo que no, indicando en ello que no sólo la ciencia ha aportado beneficios a la humanidad. Feyerabend pone de ejemplo a las ideologías y, como parte de su razonamiento, señala a la ciencia como una más entre muchas. El uso del término anarquismo en el nombre refleja la prescripción de pluralismo metodológico de la teoría, ya que el supuesto método científico no tiene el monopolio de la verdad ni de los resultados útiles. Feyerabend consideró que la ciencia comenzó como un movimiento liberador, pero con el tiempo se había vuelto cada vez más dogmática y rígida y, por lo tanto, se había convertido cada vez más en una ideología y, a pesar de sus éxitos, la ciencia había comenzado a adquirir algunas características opresivas y no era posible encontrar una forma inequívoca de distinguir la ciencia de la religión, la magia o la mitología. Sentía que el dominio exclusivo de la ciencia como medio para dirigir la sociedad era autoritario e infundado. La promulgación de la teoría le valió a Feyerabend el título de «el peor enemigo de la ciencia» por parte de sus detractores. Constructivismo Esta sección es un extracto de Constructivismo (filosofía).[editar] Este artículo o sección necesita referencias que aparezcan en una publicación acreditada. Busca fuentes: «Filosofía de la ciencia» – noticias · libros · académico · imágenes Este aviso fue puesto el 3 de septiembre de 2008. En filosofía de la ciencia y epistemología, el constructivismo, o constructivismo epistemológico, es una corriente de pensamiento surgida a mediados del siglo XX, de la mano de investigadores de disciplinas muy diversas (filósofos, psiquiatras, antropólogos, físicos, matemáticos, biólogos, psicólogos, sociólogos, lingüistas, etc.), la cual sostiene que la realidad es una construcción en cierto grado «inventada» por quien la observa. Nunca se podrá llegar a conocer la realidad tal como es pues siempre, al conocer algo, ordenamos los datos obtenidos de la realidad (aunque sean percepciones básicas) en un marco teórico o mental. De tal modo, ese objeto o realidad que entendemos «tal» no es tal, no tenemos un «reflejo especular» de lo que está «ahí fuera de nosotros», sino algo que hemos construido con base en nuestras percepciones y datos empíricos. Así, la ciencia y el conocimiento en general ofrecen solamente una aproximación a la verdad, que queda fuera de nuestro alcance. El biólogo estadounidense Gerald M. Edelman ilustra esta idea diciendo que «Cada acto de percepción es en cierto grado un acto de creación y cada acto de memoria es a cierto modo un acto de imaginación». Corrientes actuales Para hablar de una filosofía de la ciencia no basta con tener una visión panorámica de lo que es filosofía y de lo que es ciencia. Tampoco es suficiente el seguimiento histórico de las opiniones y conceptos emitidos por los pensadores del pasado. Es necesario ubicarse en el pensamiento actual de los científicos más avanzados y respetar sus conceptos sobre lo que ellos consideran como ciencia, y es necesario entender que el dominio de la filosofía son los conceptos universales y abstractos que nunca pueden llegar a ser objeto de la ciencia. Es extremadamente complejo (y, posiblemente, todavía falta algo más de perspectiva temporal) presentar un panorama completo de la filosofía de la ciencia de los últimos treinta o treinta y cinco años. Así como todos los autores anteriores ya han muerto, la mayoría de los que vienen a continuación no. Aquí se intentará presentar un bosquejo de la gran variedad de enfoques actuales pero teniendo en mente que, dentro de pocos años, algunas de las corrientes mencionadas pueden haber pasado al olvido, y que destaquen otros pensadores que hoy tienen una repercusión menor. Así como anteriormente se podía hablar de el método de la ciencia, el gran desarrollo de muchas disciplinas científicas ha hecho que los filósofos de la ciencia comiencen a hablar de los métodos, ya que no es posible identificar un método único y universalmente válido. La idea heredada de la física clásica de que todo es reducible a expresiones matemáticas ha cedido terreno ante situaciones nuevas como la teoría del caos o los avances de la biología. Por otro lado han desaparecido cuestiones que llegaron a cubrir cientos de páginas y generaron grandes controversias. Quizás el caso más flagrante sea el del problema de la demarcación, centrado en la distinción (demarcación) entre ciencia y otros conocimientos no científicos. Prácticamente el tema desaparece después de Popper y es seguido en España por Gustavo Bueno en su teoría del cierre categorial. Concepciones estructuralistas y semánticas Frente al intento de los anteriores empiristas lógicos de formalizar las teorías de la física en el lenguaje de la lógica de primer orden, que resultaba un tanto forzado e innecesariamente complicado, Patrick Suppes fue el primero en proponer una concepción semántica y estructural de las teorías, caracterizadas como familias de estructuras conjuntistas identificadas con los modelos de la teoría. Esta manera de presentar las teorías en el lenguaje informal de la teoría de conjuntos resultaba así más intuitiva y familiar. Suppes ha elaborado sus ideas mediante el desarrollo de teorías cada vez más potentes sobre las estructuras teóricas, incluyendo sus importantes teoremas de representación e invariancia. En filosofía de la ciencia se conoce a veces como estructuralismo el programa de reconstrucción de las teorías físicas propuesto por Joseph D. Sneed (1938) en 1971 como una síntesis del aparato formal de Suppes, del racionalismo crítico y del positivismo lógico con la corriente historicista de la ciencia. El estructuralismo fue reelaborado y divulgado por Wolfgang Stegmüller (1923-1991) y Carlos Ulises Moulines (1946). De la consideración de las teorías como estructuras le viene a esta propuesta metodológica el nombre de estructuralismo, que no tiene nada que ver con el estructuralismo lingüístico de Saussure. Junto con las restricciones empíricas, una teoría consta de una estructura conceptual y de un ámbito de aplicación. Puesto que las teorías no se presentan aisladas sino interrelacionadas también es necesario estudiar las relaciones entre teorías, las redes teóricas. Entre estas relaciones encontramos la de reducción, quizá la más destacada por su papel en la unidad de la ciencia. A pesar de las múltiples teorías que puedan coexistir para explicar los mismos hechos, la unidad ontológica de la ciencia puede salvarse si todas ellas son reductibles a una sola teoría (o a unas pocas no inconmensurables entre sí). Esta relación interteorética desempeña un papel fundamental, por ejemplo, en el trabajo de los físicos en su búsqueda de la Teoría del todo. Moulines propone una definición recursiva de la filosofía de la ciencia como teorización sobre teorizaciones, cuya epistemología no es descriptiva ni prescriptiva, sino interpretativa. Las teorías de la ciencia son construcciones culturales, pero ello no implica que la filosofía de la ciencia sea sustituida por una sociología de la ciencia. Aparte del estructuralismo de Sneed y sus seguidores, también otros desarrollos de la filosofía de la ciencia contemporánea han sido influidos por las ideas y métodos conjuntistas y probabilistas introducidos por Suppes. Bas van Fraassen ha aportado su conocida concepción semántica de las teorías, que ha aplicado al análisis de la mecánica cuántica. Jesús Mosterín y Roberto Torretti han hecho contribuciones en esta dirección, que asimismo aflora en el diccionario conjunto de estos dos autores. Filosofía de la ciencia naturalizada Para Ronald N. Giere (1938) el propio estudio de la ciencia debe ser también una ciencia: «La única filosofía de la ciencia viable es una filosofía de la ciencia naturalizada». Esto es así porque la filosofía no dispone de herramientas apropiadas para el estudio de la ciencia en profundidad. Giere sugiere, pues, un reduccionismo en el sentido de que para él la única racionalidad legítima es la de la ciencia. Propone su punto de vista como el inicio de una disciplina nueva, una epistemología naturalista y evolucionista, que sustituirá a la filosofía de la ciencia actual. Larry Laudan (1941) propone sustituir el que él denomina modelo jerárquico de la toma de decisiones por el modelo reticulado de justificación. En el modelo jerárquico los objetivos de la ciencia determinan los métodos que se utilizarán, y estos determinan los resultados y teorías. En el modelo reticulado se tiene en cuenta que cada elemento influye sobre los otros dos, la justificación fluye en todos los sentidos. En este modelo el progreso de la ciencia está siempre relacionado con el cambio de objetivos, la ciencia carece de objetivos estables. Realismo frente a empirismo El debate sobre el realismo de la ciencia no es nuevo, pero en la actualidad aún está abierto. Bas C. Van Fraasen (1941), empirista y uno de los principales oponentes del realismo, opina que todo lo que se requiere para la aceptación de las teorías es su adecuación empírica. La ciencia debe explicar lo observado deduciéndolo de postulados que no necesitan ser verdaderos más que en aquellos puntos que son empíricamente comprobables. Llega a decir que «no hay razón para afirmar siquiera que existe una cosa tal como el mundo real». Es el empirismo constructivo, para el que lo decisivo no es lo real, sino lo observable. Laudan y Giere presentan una postura intermedia entre el realismo y el subjetivismo estrictos. Laudan opina que es falso que solo el realismo explique el éxito de la ciencia. Giere propone que hay ciencias que presentan un alto grado de abstracción, como la mecánica cuántica, y utilizan modelos matemáticos muy abstractos. Estas teorías son poco realistas. Las ciencias que estudian fenómenos naturales muy organizados como la biología molecular, utilizan teorías que son muy realistas. Por ello no se puede utilizar un criterio uniforme de verdad científica. Rom Harré (1927) y su discípulo Roy Bhaskar (1944) desarrollaron el realismo crítico, un cuerpo de pensamiento que quiere ser el heredero de la Ilustración en su lucha contra los irracionalismos y el racionalismo reduccionista. Destacan que el empirismo y el realismo conducen a dos tipos diferentes de investigación científica. La línea empirista busca nuevas concordancias con la teoría, mientras que la línea realista intenta conocer mejor las causas y los efectos. Esto implica que el realismo es más coherente con los conocimientos científicos actuales. Dentro de la corriente racionalista de oposición al neopositivismo se encuentra a Mario Bunge (1919). Analiza los problemas de diversas epistemologías, desde el racionalismo crítico popperiano hasta el empirismo, el subjetivismo o el relativismo. Bunge es realista crítico. Para él la ciencia es falibilista (el conocimiento del mundo es provisional e incierto), pero la realidad existe y es objetiva. Además se presenta como materialista , pero para soslayar los problemas de esta doctrina apostilla que se trata de un materialismo emergentista. Sociología de la ciencia Esta sección es un extracto de Sociología de la ciencia.[editar] Dos científicos dentro del recinto de láser preestabilizado de LIGO, preguntándose qué salió mal. La sociología de la ciencia o sociología del conocimiento científico es el estudio de la ciencia como una actividad social, especialmente destinada a las condiciones sociales y los efectos de la ciencia en las estructuras y procesos de la actividad científica y social. Los sociólogos del conocimiento científico estudian el desarrollo de un campo científico y tratan de identificar puntos de contingencia o flexibilidad interpretativa, donde existen ambigüedades. Tales variaciones pueden estar relacionadas con una serie de factores políticos, históricos, culturales o económicos. Fundamentalmente, el campo no se establece para promover el relativismo o para atacar el proyecto científico; el objetivo del investigador es explicar por qué una interpretación tiene éxito sobre otra debido a las circunstancias históricas y sociales externas. Se trata de un área que surgió a finales de 1960 y principios de 1970 y en un comienzo constaba de una práctica casi exclusivamente británica. Otros centros de educación para el desarrollo del campo se encontraban en Francia, Alemania y los Estados Unidos. Los principales teóricos incluyen Barry Barnes, David Bloor, Sal Restivo, Randall Collins, Gaston Bachelard, Harry Collins, Paul Feyerabend, Steve Fuller, Thomas Kuhn, Martin Kusch, Bruno Latour, Mike Mulkay, Derek J. de Solla Price, Lucy Suchman y Anselm Strauss. Filosofía de la ciencia real Atendiendo a las críticas de Thomas Kuhn y otros historiadores de que la filosofía de la ciencia con frecuencia se ocupa de problemas artificiosos y alejados de la ciencia real, diversos filósofos de la ciencia contemporáneos han tratado de aproximar sus análisis a la problemática actual de la investigación científica. Ello ha tenido como consecuencia tanto la revitalización de la filosofía general de la ciencia como el desarrollo de varias ramas especializadas de la misma: Filosofía de la física Filosofía de la ciencia de la computación Filosofía de las ciencias sociales Filosofía de la economía Filosofía de las matemáticas Filosofía de la psicología Filosofía de la química Filosofía de la biología A ambas tareas han contribuido filósofos como John Earman, Bernulf Kanitscheider, Jesús Mosterín, Lawrence Sklar, Elliott Sober, Roberto Torretti y Bas C. van Fraassen, así como numerosos científicos, como Lee Smolin. Filosofías de las ciencias particulares Matemáticas Esta sección es un extracto de Filosofía de las matemáticas.[editar] Principia Mathematica, una de las obras más importantes sobre filosofía de las matemáticas. La filosofía de las matemáticas es un área de la filosofía teórica que trata de comprender y explicar los requisitos, el objeto, el método y la naturaleza de las matemáticas. Como área de estudio puede ser aproximada desde dos direcciones: el punto de vista de los filósofos y el de los matemáticos. Desde el punto de vista filosófico, el objetivo principal es dilucidar una variedad de aspectos problemáticos en la relación entre las matemáticas y la filosofía. Desde el punto de vista matemático, el interés principal es proveer al conocimiento matemático de fundamentos firmes. Es importante mantener presente que aunque estos dos enfoques pueden implicar diferentes esquemas e intereses, no son opuestos, sino más bien complementarios: «Cuando los matemáticos profesionales se ocupan de los fundamentos de su disciplina, se dice que se dedican a la investigación fundamental (o trabajo fundacional o de fundamentos.- ver Metamatemática). Cuando los filósofos profesionales investigan cuestiones filosóficas relativas a las matemáticas, se dice que contribuyen a la filosofía de las matemáticas. Por supuesto, la distinción entre la filosofía de las matemáticas y los fundamentos de las matemáticas es vaga, y cuanto mayor interacción haya entre los filósofos y los matemáticos que trabajan en cuestiones relativas a la naturaleza de las matemáticas, mejor.». De acuerdo a Jeremy Avigad (profesor de ciencias matemáticas y de filosofía en la Universidad Carnegie Mellon ) “el conocimiento matemático ha sido considerado por mucho tiempo como un paradigma del conocimiento humano con verdades que son a la vez necesarias y ciertas, por lo que dar una explicación del conocimiento matemático es una parte importante de la epistemología. Los objetos matemáticos, tales como los números y los conjuntos, son ejemplos arquetípicos de abstracciones, dado que el tratamiento que reciben en nuestro discurso es el de objetos independientes del tiempo y el espacio. Encontrar un lugar para los objetos de este tipo en un marco más amplio del pensamiento es una tarea central de la ontología, o metafísica. El rigor y la precisión del lenguaje matemático se debe a que está basado en un vocabulario limitado y una gramática muy estructuradas, y las explicaciones semánticas del discurso matemático a menudo sirven como punto de partida de la filosofía del lenguaje. Aunque el pensamiento matemático ha demostrado un alto grado de estabilidad a través de la historia, su práctica también ha evolucionado con el tiempo, y algunos desarrollos han provocado controversia y debate; clarificar los objetivos básicos de esta práctica y los métodos apropiados es, por lo tanto, una tarea metodológica y fundacional importante que sitúa a la filosofía de las matemáticas dentro de la filosofía general de la ciencia. De acuerdo con Bertrand Russell, las matemáticas son una disciplina que, cuando se parte de sus porciones más familiares, puede llevarse a cabo en cualquiera de dos direcciones opuestas (una busca la expansión del conocimiento, la otra darle fundamentos: Nota del traductor). Pero se debe entender que la distinción es una, no en la materia objeto, sino en el estado de la mente del investigador...(...)... así como necesitamos dos tipos de instrumentos, el telescopio y el microscopio, para la ampliación de nuestras capacidades visuales, del mismo modo necesitamos dos tipos de instrumentos para la ampliación de nuestras capacidades lógicas, uno para hacernos avanzar hacia las matemáticas superiores, y el otro que nos lleve hacia atrás, hacia los fundamentos lógicos de aquello que estamos inclinados a dar por sentado en las matemáticas. Veremos que mediante el análisis de las nociones matemáticas ordinarias se adquiere una nueva perspectiva, nuevos poderes, y los medios de llegar a nuevos temas matemáticos completos, mediante la adopción de nuevas líneas de avance, siguiendo nuestro viaje hacia atrás. Como ya se ha sugerido, estas aproximaciones no son conflictivas. En las palabras de Imre Lakatos: «Al discutir los esfuerzos modernos por establecer los fundamentos del conocimiento matemático uno tiende a olvidarse de que se trata solo de un capítulo en el gran esfuerzo de superación del escepticismo estableciendo los fundamentos para el conocimiento en general. El objeto de mi contribución es mostrar que la filosofía matemática moderna está profundamente enraizada en la epistemología general y solamente se puede comprender en ese contexto». (énfasis de Lakatos ). Física Esta sección es un extracto de Filosofía de la física.[editar] La dualidad onda-partícula, en el que se aprecia cómo un mismo fenómeno puede ser percibido de dos modos distintos, fue uno de los problemas filosóficos que planteó la mecánica cuántica. La filosofía de la física se refiere al conjunto de reflexiones filosóficas sobre la interpretación, epistemología y principios rectores de las teorías físicas y la naturaleza de la realidad. Aunque raramente la exposición estándar de las teorías físicas discute los aspectos filosóficos, lo cierto es que las concepciones filosóficas de los científicos han tenido un papel destacado en el desarrollo de dichas teorías. Esto fue notorio a partir de Newton y Kant, llegando a ser muy importante en el siglo XX, cuando la teoría de la relatividad dio lugar a un análisis minucioso de asuntos tradicionalmente objeto de estudio de la filosofía, como la naturaleza del tiempo y el espacio. La filosofía de la física contribuye a través de la crítica de los productos de la física, retroalimentándola. En muchos aspectos, la física proviene de la filosofía griega. Desde el primer intento de Tales de caracterizar la materia, hasta la deducción de Demócrito de que la materia debería reducirse a un estado invariable, la astronomía ptolemaica de un firmamento cristalino, y el libro de Aristóteles Física (un libro temprano de física, que intentaba analizar y definir el movimiento desde un punto de vista filosófico), varios filósofos griegos avanzaron sus propias teorías de la naturaleza. La física se conoció como filosofía natural hasta finales del siglo XVIII.  Para el siglo XIX, la física se realizó como una disciplina distinta de la filosofía y de las demás ciencias. La física, al igual que el resto de la ciencia, se apoya en la filosofía de la ciencia y en su «método científico» para avanzar en el conocimiento del mundo físico. El método científico emplea el razonamiento a priori así como el razonamiento a posteriori y el uso de la Inferencia bayesiana para medir la validez de una teoría determinada. El desarrollo de la física ha respondido a muchas preguntas de los primeros filósofos, pero también ha planteado nuevas preguntas. El estudio de las cuestiones filosóficas que rodean a la física, la filosofía de la física, implica cuestiones como la naturaleza del espacio y del tiempo, el determinismo y las perspectivas metafísicas como el empirismo, el naturalismo y el realismo. Muchos físicos han escrito sobre las implicaciones filosóficas de su trabajo, a partir por ejemplo, de las interpretaciones de la mecánica cuántica; por ejemplo Laplace, que defendió el determinismo causal, y Schrödinger, que escribió sobre la mecánica cuántica. El físico matemático Roger Penrose había sido llamado platonista por Stephen Hawking,, una opinión que Penrose discute en su libro, El camino a la realidad. Hawking se refirió a sí mismo como un «reduccionista desvergonzado» y discrepó de las opiniones de Penrose. Química Esta sección es un extracto de Filosofía de la química.[editar] The Alchemists (c.1757), de Pietro Longhi. Ca Rezzonico, Venecia. La filosofía de la química es una rama de la filosofía de las ciencias, la cual considera como objetos de estudio la metodología y las suposiciones subyacentes a la propia disciplina química (como pueden ser la naturaleza de las sustancias químicas, el atomismo, el enlace químico y la síntesis), así como los aspectos filosóficos generales de la filosofía de las ciencias en los que ésta mantiene estrecha comunicación e pertinencia (como pueden el realismo científico, el reduccionismo, la explicación científica, los ciclos de la investigación científica y la modelización numérica o molecular). Es una de las subdisciplinas más recientes de la filosofía de las ciencias, y sólo hace unas pocas décadas ha conseguido atraer la atención de la comunidad científica.  Biología Esta sección es un extracto de Filosofía de la biología.[editar] La filosofía de la biología es una subdisciplina de la filosofía de la ciencia encargada del estudio de los presupuestos e implicaciones filosóficas (epistemológicos y ontológicos) de la biología. La historia de la biología, la sociobiología y la bioética son campos de investigación estrechamente relacionados con la filosofía de la biología. Algunos temas pasados y presentes de la filosofía de la biología son las unidades de selección, la emergencia, el determinismo biológico, la reductibilidad de la biología, el reduccionismo genético, la epistemología evolucionista, el origen de la vida, el debate externalistas contra internalistas, adaptacionistas contra estructuralistas y vitalistas contra antivitalistas. Psicología Esta sección es un extracto de Filosofía de la psicología.[editar] Socrátes. La filosofía es la madre de todas las ciencias, siendo la psicología parte de ella hasta su independencia alrededor de 1879 con el nacimiento de la psicofísica. Su significado literal es psyché-logos (estudio de la mente). Una rama de la filosofía es la filosofía de la ciencia, que, desde la división hecha por Ferrier en el siglo XIX entre ontología y epistemología, se encarga del análisis del conocimiento científicamente obtenido. Cada ciencia genera su propia epistemología o filosofía especial, con base en las características de su quehacer intrínseco. Otras de las ramas de la filosofía que se relacionan con la psicología y la epistemología es la filosofía de la mente. En el caso de la psicología, según Jacob Robert Kantor, ha habido tres etapas de desarrollo de sus contenidos epistemológicos: una primera ocupándose de entidades aespaciales, como el alma; una segunda en términos de orden organocéntrico-mecanicista, como las variantes estímulo-respuesta y de procesamiento de información; y una tercera, en que se abordan las interacciones complejas entre el individuo y su ambiente. Se ha llegado, pues, aparentemente, a un estudio sistémico del objeto de conocimiento. No obstante, no todas las corrientes de la psicología en vigencia practican ese enfoque sistémico de manera uniforme, debido a que parten de diferentes opciones epistemológicas en pleno debate. Economía Esta sección es un extracto de Filosofía de la economía.[editar] La filosofía de la economía se establece como una rama interdisciplinaria que, desde una perspectiva metateórica, indaga en los fundamentos epistemológicos, ontológicos y metodológicos de la ciencia económica. Examina la naturaleza y justificación del conocimiento económico, la solidez de sus modelos y supuestos, y la interacción entre la construcción teórica y la validación empírica. Este campo desglosa las implicaciones filosóficas presentes en la economía normativa, incluyendo la economía del bienestar y la justicia distributiva, y contrasta las diversas aproximaciones metodológicas que configuran la disciplina, desde el realismo científico hasta el formalismo matemático, el empirismo o el racionalismo. Adicionalmente, la filosofía de la economía se entrelaza con la filosofía de las matemáticas y la lógica (abarcando la metalógica, la lógica matemática y la filosofía del lenguaje), la filosofía de la computación (incluyendo la filosofía de la inteligencia artificial), la filosofía de las ciencias sociales y la filosofía de la mente, delineando su ámbito y sus conexiones con otras esferas esenciales del pensamiento filosófico, como la ética en la economía moral."

ksampletext_wikipedia_spor_deporte: str = "Deporte. El deporte es todo tipo de actividades físicas que, mediante una participación, organizada o de otro tipo, tengan por finalidad la expresión o la mejora de la condición física y psicológica, en desarrollo de las relaciones sociales o el logro de resultados en competiciones de todos los niveles. Se diferencia del juego principalmente en la preparación y capacitación necesarias para su desarrollo, ya que el juego no requiere una preparación específica para su ejecución. No existe un consenso a la hora de establecer qué requisitos debe reunir una actividad física para ser considerada como deporte pero, generalmente, se puede asumir que el deporte es un tipo de actividad física asociado a un alto grado técnico de preparación para la competición, y que requiere de una alta disciplina técnica asociada a una reglamentación institucionalizada mediante federaciones deportivas. La mayoría de las definiciones lo vinculan también con la «actividad física», que aunque normalmente se asocia con el movimiento del cuerpo (ejercicio físico), es un concepto difuso, en el cual se engloba a algún tipo de actividad destinada al mantenimiento del estado de salud del cuerpo físico y psíquico (incluyendo la mente). Por tanto, podemos entender la actividad mental como parte de la actividad física (o actividad del cuerpo humano), no limitando dicho concepto únicamente a practicar ejercicio físico. De acuerdo con el Comité Olímpico Internacional, la práctica del deporte es un derecho humano, y uno de los principios fundamentales del Olimpismo es que «toda persona debe tener la posibilidad de practicar deporte sin discriminación de ningún tipo y dentro del espíritu olímpico, que exige comprensión mutua, solidaridad y espíritu de amistad y de juego limpio». El deporte es una actividad física o mental que se realiza siguiendo ciertas reglas y con fines de competencia, entretenimiento, salud o recreación. Fomenta el desarrollo físico, la disciplina y el trabajo en equipo, y puede ser practicado de forma individual o en grupo. Así mismo, también ayuda a tener una buena salud física. Hacer ejercicio es fundamental para la salud del cuerpo y no deja tener un estado físico corporal Historia Existen utensilios y estructuras que sugieren que los chinos ya realizaban actividades deportivas hace 6900 años, entre 1066-771 a. C. La gimnasia parece haber sido un popular deporte en la Antigua China. Los monumentos a los emperadores indican que una cierta cantidad de deportes, incluyendo la natación y la pesca, fueron ya diseñados y regulados hace miles de años en el Antiguo Egipto. Otros deportes egipcios incluyen el lanzamiento de jabalina, el salto de altura y la lucha. Algunos deportes de la Antigua Persia como el arte marcial iraní de Zourkhaneh están ligados a las habilidades en la batalla. Entre otros deportes originales de Persia están el polo y la justa. Por otra parte, en América las culturas mesoamericanas como los mayas practicaban el llamado juego de pelota, el cual a su vez era un ritual. El futbolista Pelé fue distinguido por el COI como el deportista del siglo XX» Una amplia variedad de deportes estaba ya establecida en la época de la Antigua Grecia, y la cultura militar y el desarrollo de los deportes en Grecia se influyeron mutuamente. Para los griegos el deporte era una parte muy importante de su cultura, por lo que crearon los Juegos Olímpicos, una competición que se disputó desde el año 777 a. C. hasta el año 394 d. C. cada cuatro años en Olimpia, una pequeña población en el Peloponeso griego. En 1896 se celebraron los primeros Juegos Olímpicos de la era moderna, en Atenas, gracias a la iniciativa del barón Pierre de Coubertin de recuperar el espíritu de los antiguos Juegos añadiendo un carácter internacional. Los Juegos Olímpicos modernos, regulados por el Comité Olímpico Internacional (COI), se han convertido en el mayor evento deportivo internacional multidisciplinario, con más de 200 naciones participantes. Los deportes han visto aumentada su capacidad de organización y regulación desde los tiempos de la Antigua Grecia hasta la actualidad. La industrialización ha incrementado el tiempo de ocio de los ciudadanos en los países desarrollados, conduciendo a una mayor dedicación del tiempo a ver competiciones deportivas y más participación en actividades deportivas, facilitada por una mayor accesibilidad a instalaciones deportivas. Estas pautas continúan con la llegada de los medios de comunicación masivos. La profesionalidad en el deporte se convirtió en algo común conforme aumentaba la popularidad de los deportes y el número de aficionados que seguían las hazañas de los atletas profesionales a través de los medios de información. Desde la década de 1920, el fútbol, organizado por la FIFA, ha sido el deporte más practicado en Europa y América Latina. En la actualidad, muchas personas hacen ejercicio para mejorar su salud y modo de vida; el deporte se considera una actividad saludable que ayuda a mantenerse en forma psicológica y físicamente, especialmente en la tercera edad. Significado y uso Etimología La palabra deporte procede del francés antiguo desport que significa ocio, siendo la definición más antigua en inglés, de alrededor de 1300, cualquier cosa que los humanos encuentren divertida o entretenida. Otros significados incluyen los juegos de azar y los eventos organizados con el propósito de apostar; la caza; y los juegos y diversiones, incluyendo los que requieren ejercicio. Rogets define el sustantivo sport como una actividad realizada para la relajación y la diversión con sinónimos que incluyen la diversión y la recreación. Nomenclatura El término singular sport se utiliza en la mayoría de los dialectos ingleses para describir el concepto general (por ejemplo, children taking part in sport), y sports se utiliza para describir múltiples actividades (por ejemplo, football and rugby are the most popular sports in England). El inglés americano utiliza sports para ambos términos. Definición El Comité Olímpico Internacional reconoce algunos juegos de mesa como deportes, incluido el ajedrez. Salto de obstáculos, un deporte de Hípica La definición precisa de lo que separa un deporte de otras actividades de ocio varía según las fuentes. Lo más parecido a un acuerdo internacional sobre una definición lo proporciona SportAccord, que es la asociación de todas las federaciones deportivas internacionales más importantes (incluyendo el fútbol de asociación, el atletismo, el ciclismo, el tenis, la deportes ecuestres, y más), y es, por tanto, el representante de facto del deporte internacional. SportAccord utiliza los siguientes criterios, determinando que un deporte debe: tener un elemento de competición no ser perjudicial para ningún ser vivo no depender del equipamiento suministrado por un único proveedor (excluyendo los juegos patentados como el fútbol playa) no depender de ningún elemento de suerte específicamente diseñado en el deporte. También reconocen que el deporte puede ser principalmente físico (como rugby o atletismo), principalmente mental (como ajedrez o Go), predominantemente motorizado (como Fórmula 1 o motonáutica), principalmente de coordinación (como deportes de billar), o principalmente con animales, como deportes ecuestres. La inclusión de los deportes mentales dentro de las definiciones de deporte no ha sido aceptada universalmente, lo que ha dado lugar a desafíos legales por parte de los órganos de gobierno en lo que respecta a la denegación de la financiación disponible para los deportes. Aunque SportAccord reconoce un pequeño número de deportes mentales, no está abierto a admitir más deportes mentales. Ha aumentado la aplicación del término deporte a un conjunto más amplio de retos no físicos como los videojuegos, también llamados esports (de deportes electrónicos), especialmente debido a la gran escala de participación y competición organizada, pero éstos no están ampliamente reconocidos por las organizaciones deportivas convencionales. Según el Consejo de Europa, Carta Europea del Deporte, artículo 2.i, Se entiende por deporte toda forma de actividad física que, mediante una participación casual u organizada, tiene por objeto expresar o mejorar la aptitud física y el bienestar mental, formar relaciones sociales u obtener resultados en la competición a todos los niveles. Competición El plusmarquista de 100 metros Usain Bolt (de amarillo) y otros corredores, Moscú, 2013. Existen opiniones opuestas sobre la necesidad de la competición como elemento definitorio de un deporte, ya que casi todos los deportes profesionales implican la competición, y los organismos rectores exigen la competición como requisito previo para el reconocimiento por parte del Comité Olímpico Internacional (COI) o SportAccord. Otros organismos abogan por ampliar la definición de deporte para incluir toda la actividad física. Por ejemplo, el Consejo de Europa incluye todas las formas de ejercicio físico, incluidas las que se compiten solo por diversión. Para ampliar la participación y reducir el impacto de la pérdida en los participantes menos capacitados, se ha introducido la actividad física no competitiva en eventos tradicionalmente competitivos como el «día del deporte» escolar, aunque este tipo de medidas suelen ser controvertidas.  En los eventos competitivos, los participantes se califican o clasifican en función de su resultado y a menudo se dividen en grupos de rendimiento comparable, (por ejemplo, sexo, peso y edad). La medición del resultado puede ser objetiva o subjetiva, y se corrige con hándicaps o penalizaciones. En una carrera, por ejemplo, el tiempo para completar el recorrido es una medida objetiva. En gimnasia o submarinismo el resultado lo decide un panel de jueces, y por tanto es subjetivo. Hay muchos matices entre el boxeo y las artes marciales mixtas, donde la victoria es asignada por los jueces si ninguno de los competidores ha perdido al final del tiempo del combate. Deporte profesional Carrera de Fórmula 1 en Austria. El aspecto de entretenimiento del deporte, junto al crecimiento de los medios de comunicación y el incremento del tiempo de ocio, han provocado que se profesionalice el mundo del deporte. Esto ha conducido a cierta polémica, ya que para el deportista profesional puede llegar a ser más importante el dinero o la fama que el propio acto deportivo en sí. Al mismo tiempo, algunos deportes han evolucionado para conseguir mayores beneficios o ser más populares, en ocasiones perdiéndose algunas valiosas tradiciones. El fútbol en Europa y América Latina, o el fútbol americano, el baloncesto y el béisbol en EE. UU., son ejemplos de deportes que mueven al año enormes cantidades de dinero. Esta evolución conduce a un aumento de la competitividad, dado que la lucha por la victoria adquiere otro significado al incluirse también el apartado económico. Este aumento, asimismo, lleva a la aparición de un importante lado negativo de la profesionalidad, incluyendo el uso de diversas argucias o trampas como la práctica del dopaje por parte de los deportistas. El mundo del deporte como espectáculo mueve anualmente una cantidad cercana a los 70 000 millones de euros (datos de 2014), entre venta de entradas, derechos televisivos y patrocinios. Si se incluyen aquellos consumos relacionados con la práctica del deporte, como material y ropa deportivos, equipamientos, y gastos en salud y forma física, la industria del deporte genera cada año a nivel global cerca de 600 000 millones de euros. Según los datos de audiencia, los torneos más seguidos en el mundo son los Juegos Olímpicos de Verano, la Copa Mundial de Fútbol y Copa del Mundo de Rugby, pero anualmente son la Liga de Campeones de la UEFA, la Liga Nacional de Fútbol Americano, Primera División de España, Premier League y la Asociación Nacional de Baloncesto. Arte físico Gimnasta Los deportes comparten un alto grado de afinidad con el arte. Disciplinas como el patinaje artístico sobre hielo o el taichí, son deportes muy cercanos a espectáculos artísticos en sí mismos. Actividades tradicionales como la gimnasia y el yoga, más recientes como el culturismo, y actividades callejeras como el tricking o el street workout también comparten elementos propios del deporte con elementos artísticos[cita requerida]. El hecho de que el arte sea tan cercano al deporte en algunas situaciones está probablemente relacionado con la naturaleza de los deportes. La definición de deporte establece la idea de ejecutar una actividad no solo para el propósito habitual; por ejemplo, no correr solo para llegar a alguna parte, sino correr por propia voluntad, con el fin de mantener el estado físico. Esto es similar a una visión común de la estética, que contempla los objetos más allá de su utilidad. Por ejemplo, valorar un coche no por llevarnos de un sitio a otro, sino por su forma, figura, etc. Del mismo modo, una actividad deportiva como el salto no se valora solo como un modo efectivo de evitar obstáculos; también cuentan la habilidad, la destreza y el estilo. Tecnología Salud: La tecnología se encuentra presente desde la nutrición hasta el tratamiento de lesiones, incrementando el potencial del deportista. Los atletas contemporáneos son capaces de practicar deporte a mayores edades, recuperarse más rápidamente de lesiones y entrenar de forma más efectiva que en generaciones anteriores. Un aspecto negativo de la tecnología aplicada al deporte consiste en el diseño y consumo de sustancias dopantes, las cuales mejoran el rendimiento del deportista hasta muy altos niveles, en ocasiones llegando a afectar seriamente a la salud del mismo, pudiendo ocasionar daños irreversibles en el cuerpo o incluso la muerte. Por esta razón, en un gran número de deportes, dichas sustancias están prohibidas por los distintos órganos reguladores del deporte a nivel profesional, pudiendo significar su consumo la descalificación o la inhabilitación del infractor. Instrucción: Los avances de la tecnología han creado nuevas oportunidades en la investigación deportiva. Ahora es posible analizar aspectos del deporte que antes se encontraban fuera del alcance de nuestra comprensión. Técnicas como la captura de movimientos o las simulaciones por ordenador han incrementado el conocimiento acerca de las acciones de los atletas y el modo en que estas pueden mejorarse. Las mejoras en tecnología también han servido para mejorar los sistemas de entrenamiento, en ocasiones asistidas por máquinas diseñadas para tal efecto. Caso práctico se encuentra en el ciclismo. Equipamiento: En ciertas categorías deportivas, el deportista se vale de diverso instrumental para llevar a cabo la actividad, así como los bates empleados en béisbol o los balones usados en fútbol o baloncesto. Todos ellos han visto cómo sus características han ido variando con el paso de los años para mejorar el rendimiento deportivo, alterándose factores como la dureza o el peso de los mismos. Asimismo, en algunos deportes de contacto físico se hace necesario el uso de equipo protector por parte del deportista, como por ejemplo en fútbol americano. Estas protecciones también han ido evolucionando con el paso de los años y la propia evolución de la tecnología, dirigiéndose hacia elementos más cómodos y seguros para la práctica deportiva. Deporte y sociedad La hípica es conocida como el «deporte de los reyes» El deporte tiene una gran influencia en la sociedad; destaca de manera notable su importancia en la cultura y en la construcción de la identidad nacional. En el ámbito práctico, el deporte tiene efectos tangibles y predominantemente positivos en las esferas de la educación, la economía y la salud pública. La influencia del deporte en nuestra sociedad es enorme. Hoy en día, la práctica deportiva ha establecido gran parte del tiempo de ocio de las personas, tanto si son espectadores como actores del deporte. El deporte es un fenómeno complejo que funciona como instancia de sociabilidad, alimenta el imaginario y las pasiones colectivas, genera sobresaltos de nacionalismo, moviliza ingentes capitales y se presta para la instrumentalización. Refleja las tendencias sociales del momento histórico en cual se enmarca, configurándose y funcionando como un sistema social completo: es un fenómeno tan relevante a nivel social que contiene elementos característicos de la sociedad en sí misma y pone en movimiento la totalidad de las instituciones de la sociedad. En el terreno educativo, el deporte juega un papel de transmisión de valores a niños, adolescentes e incluso adultos. En conjunción con la actividad física se inculcan valores de respeto, responsabilidad, compromiso y dedicación, entre otros, sirviendo a un proceso de socialización y de involucración con las mejoras de las estructuras y actitudes sociales. El deporte contribuye a establecer relaciones sociales entre diferentes personas y diferentes culturas y así contribuye a inculcar la noción de respeto hacia los otros, enseñando cómo competir constructivamente, sin hacer del antagonismo un fin en sí. Otro valor social importante en el deporte es el aprendizaje de cómo ganar y cómo saber reconocer la derrota sin sacrificar las metas y objetivos. En el apartado económico, la influencia del deporte es indudable, debido a la cantidad de personas que practican el deporte así como las que lo disfrutan como espectáculos de masas, haciendo de los deportes importantes negocios que financian a los deportistas, agentes, medios, turismos y también indirectamente, a otros sectores de la economía. Tiende a regirse cada vez más por las leyes del mercado, propias de una sociedad de masas, que influyen de manera trascendente, no solo en el ámbito político, económico y social, sino también en los demás modelos del deporte contemporáneo. La práctica del deporte eleva también el bienestar y la calidad de vida de la sociedad por los efectos beneficiosos de la actividad física, tanto para la salud corporal como la emocional; las personas que practican deporte y otras actividades no sedentarias con regularidad suelen sentirse más satisfechos y experimentan, subjetivamente, un mayor bienestar. La utilización del deporte como mecanismo para lograr desarrollo e inclusión social, está ampliamente difundido en todo el mundo, aun cuando la evidencia rigurosa sobre la efectividad de este tipo de intervenciones es escasa. En este contexto, la Corporación Andina de Fomento ha llevado a cabo una agenda de investigación cuyo objetivo es lograr un mejor entendimiento del potencial de la práctica regular del fútbol como vía para fomentar el desarrollo y la acumulación de habilidades en niños y jóvenes. Para ello, realizaron dos estudios, uno en Colombia y otro en Perú, que contaron ambos con una muestra superior a 1600 jóvenes. Los resultados de ambas evaluaciones permiten concluir que los programas de fútbol para el desarrollo podrían ser beneficiosos, siempre que se ponga atención a la manera en cómo se implementen y en quiénes se focalicen. De lo contrario, pueden ocasionar efectos negativos en los beneficiarios, especialmente problemas de conducta y agresividad. En este sentido, estos programas tienen el potencial de generar cambios positivos sobre dimensiones socioemocionales y cognitivas cuando se implementan bajo entornos de baja competencia. Por último, el máximo potencial de estos programas, en el corto plazo, se obtiene cuando se focalizan en niños de 8 a 13 años. El fenómeno del deporte como representación de la sociedad puede explicar su importancia como espectáculo. En este rol, los encuentros deportivos sirven para afirmar el valor y las aptitudes físicas no solo de los jugadores, sino de la comunidad a la que representan. Es común que los resultados en las competiciones internacionales sean interpretados como una validación de la cultura y hasta del sistema político del país al que representan los deportistas. Este aspecto del deporte puede tener efectos negativos, como estallidos de violencia durante o tras las competiciones. Por otro lado, el deporte es considerado como un medio para disminuir la violencia y delincuencia en la sociedad."
ksampletext_wikipedia_spor_juegosolimpicos: str = "Juegos Olímpicos. Los Juegos Olímpicos (JJ. OO.; Jeux Olympiques en francés y Olympic Games en inglés), Olimpiadas u Olimpíadas son el mayor evento deportivo internacional multidisciplinario en el que participan atletas de más de doscientos países de todo el mundo. Existen dos tipos: los Juegos Olímpicos de Verano y los Juegos Olímpicos de Invierno, que se realizan con un intervalo de dos años, según la Carta Olímpica: «Los Juegos de la Olimpiada se celebran durante el primer año de una Olimpiada y los Juegos Olímpicos de Invierno durante su tercer año». Los Juegos Olímpicos modernos se inspiraron en los Juegos Olímpicos de la antigüedad del siglo VIII a. C. organizados en la antigua Grecia con sede en la ciudad de Olimpia, realizados entre los años 776 a. C. y el 393 de nuestra era. En el siglo XIX, surgió la idea de realizar unos eventos similares a los organizados en la antigüedad, los que se concretarían gracias a las gestiones del noble francés Pierre Frèdy, barón de Coubertin. El barón fundó el Comité Olímpico Internacional (COI) en 1894. Desde entonces, el COI se ha convertido en el órgano coordinador del Movimiento Olímpico, con la Carta Olímpica que define su estructura y autoridad. La primera edición de los Juegos Olímpicos de la era moderna se llevó a cabo en Atenas, capital de Grecia, a partir del 6 de abril de 1896. Desde aquella oportunidad, han sido realizados cada cuatro años en diversas ciudades del mundo, siendo las únicas excepciones las ediciones de 1916, 1940 y 1944, debido al estallido de la Primera y Segunda Guerra Mundial; así como la postergación de la de 2020 para 2021, debido a la pandemia de COVID-19. La evolución del movimiento olímpico durante los siglos XX y XXI ha dado lugar a varias modificaciones en los justa deportiva. El primero de estos ajustes fue la creación de los Juegos Olímpicos de Invierno para deportes propios de esa estación, y que se realizaron por primera vez en Chamonix 1924. Se llevaron a cabo como parte del evento de verano, el COI los consideró como un evento separado retroactivo, y desde esa fecha comenzaron a efectuarse en el mismo año que los originales. Luego, con el fin de potenciar el desarrollo de los invernales, el organismo rector del olimpismo decidió desfasar la realización de los Juegos invernales a partir de Lillehammer 1994. Desde esa fecha, los Juegos Olímpicos de Invierno se realizan en los años pares entre dos Juegos de Verano. Las innovaciones continuaron con la formación de los Juegos Paralímpicos (primera edición en Roma 1960) para atletas con algún tipo de discapacidad y los Juegos Olímpicos de la Juventud para deportistas adolescentes (primera edición en Singapur 2010). Ambos eventos cuentan con su versiones invernales (las primeras en Örnsköldsvik 1976 e Innsbruck 2012, respectivamente). El COI ha tenido que adaptarse a una variedad de avances económicos, políticos y tecnológicos. Como resultado, los Juegos Olímpicos se han alejado del amateurismo puro, según lo previsto por Coubertin, para permitir la participación de los atletas profesionales. La creciente importancia de los medios masivos de comunicación inició el patrocinio de empresas y la comercialización de los Juegos. Grandes boicots se realizaron durante la Guerra Fría en los Juegos de 1980 y 1984. El Movimiento Olímpico consta de Federaciones Internacionales de cada deporte, Comités Olímpicos Nacionales y Comités Organizadores de cada edición. El COI es responsable de la elección de la ciudad sede. Según la Carta Olímpica, la ciudad anfitriona es responsable de la organización y el financiamiento de los Juegos. El programa olímpico, compuesto por los deportes disputados en los Juegos, está determinado por el COI. Existen diversos símbolos y ceremonias olímpicas, como la bandera y la antorcha olímpicas, así como las ceremonias de apertura y clausura. Cerca de 13 000 atletas compiten en los Juegos Olímpicos de Verano e Invierno en 33 deportes y unos 400 eventos. Los ganadores del primer, segundo y tercer lugar en cada evento reciben medallas olímpicas de oro, plata y bronce respectivamente. Juegos Olímpicos de la antigua Grecia Artículo principal: Juegos Olímpicos antiguos Discóbolo. Estatua que representa el lanzamiento de disco Los Juegos Olímpicos modernos tienen su fundamento en los Juegos Olímpicos antiguos ,llamados así por celebrarse en la ciudad de Olimpia, que eran fiestas atléticas celebradas desde el año 776 a. C. (la fecha más aceptada), y cada cuatro años, en el santuario de Zeus en Olimpia, Grecia. En la competencia acudían participantes de varias ciudades-estado y reinos de la antigua Grecia. El período de cuatro años era conocido como Olimpiada, fue utilizado por los griegos como una de sus unidades de medida del tiempo. En total, se realizaron 291 ediciones, de las cuales 194 ocurrieron antes de la era común y 97 con posterioridad. Al periodo entre el 776 a. C. corresponde a la primera olimpiada, y el de 389 d. C. al 393 d. C. a la 292.ª y última. Los juegos fueron parte de un ciclo conocido como Juegos Panhelénicos, que incluía a los Juegos Píticos, los Juegos Nemeos y los Juegos Ístmicos. La modalidad de celebrar competencias y certámenes atléticos tienen un origen más antiguo, con probable datación en la época de la Grecia arcaica, durante el siglo XIII a. C., según se indica en la crónica de Paros, los Juegos Nemeos fueron inaugurados en el 1251 a. C. Origen Durante los juegos, los conflictos entre las ciudades-estado participantes se posponían hasta la finalización de las competiciones deportivas. Este cese de las hostilidades fue conocido como paz o tregua olímpica. El origen de los Juegos Olímpicos está rodeado de misterio y leyenda. Según el relato del historiador griego Pausanias, el Dáctilo Heracles Ideo (no confundir con Heracles el hijo de Zeus) y cuatro de sus hermanos corrieron a Olimpia para entretener al recién nacido Zeus. Al ganar, Heracles se coronó con una corona de olivo y estableció la costumbre de celebrar la serie de eventos deportivos en honor a Zeus, cada cuatro años.   Ruinas del estadio de Olimpia Píndaro, en otro relato, atribuye los Juegos Olímpicos a Heracles, el hijo de Zeus, además persiste la idea de que después de completar sus doce trabajos, construyó el estadio olímpico en honor a Zeus. Tras su finalización, se dirigió en línea recta doscientos pasos, y llamó a esto distancia estadio (en griego: στάδιον), que más tarde se convirtió en una unidad de distancia. Otro mito asocia a los primeros Juegos con el antiguo concepto griego de la tregua olímpica (ἐκεχειρία). Los Juegos Olímpicos tenían una importancia religiosa fundamental, que presentó eventos deportivos, junto con sacrificios rituales en honor a Zeus (cuya estatua, realizada por Fidias, fue colocada en el templo de Olimpia) y a Pélope, héroe divino y rey mítico de Olimpia. Pélope fue famoso por su carrera de carros con el rey Enómao de Pisa (Grecia). Disciplinas atléticas En estos juegos se realizaban diversos eventos deportivos, combates y carreras de cuadrigas. Se celebraron eventos de carreras, pentatlón ,consistente en salto de longitud, lanzamiento de disco (discóbolo), jabalina, carrera pedestre y lucha (boxeo, lucha libre, pancracio) y eventos ecuestres.  El programa de los juegos se limitaba a un día (aunque las celebraciones de artísticas y de culto, así como la tregua olímpica se prolongaban por un mes), se extendió después a cinco en el 468 a. C. El acto inicial era el stadion, o carrera corta de 192.27 m; seguido del diaulos que equivalía a 384.54 m; y el dólichos, carrera larga de 4614.48 m. Juegos Olímpicos está ubicado en Grecia Delfos Nemea Argos Corinto Olimpia Las 5 regiones de Grecia donde se celebraban los juegos Panhelénicos: Olimpia, Corinto, Argos, Delfos y Nemea. Participantes La tradición dice que Corebo, un cocinero de la ciudad de Elis, fue el primer campeón olímpico. Esto se basa en inscripciones en Olimpia, una lista de ganadores de una carrera pedestre celebrada cada cuatro años a partir del año antes mencionado. Los ganadores de los eventos fueron admirados e inmortalizados en poemas y estatuas. En la obra atribuida a Calístenes, se menciona entre los participantes a Alejandro Magno en la competencia de carrera de carros a caballo, hacia el siglo IV a. C. Al final de la disputa, el nombre de cada vencedor y el de su ciudad natal eran proclamados por un heraldo. El triunfador era coronado con una guirnalda de olivo silvestre, laurel o pino; eran recibidos en sus ciudades con himnos y representaciones artísticas de bailes, esculturas o declamaciones. Ocaso y prohibición Los Juegos Olímpicos llegaron a su cénit en los siglos V y VI a. C.; sin embargo, su impacto disminuyó gradualmente a partir del siglo II a. C. tras el aumento de poder de los romanos en Grecia, donde ya varias regiones se encontraban bajo dominio de este imperio. Si bien no hay consenso entre los expertos en cuanto a cuándo finalizaron oficialmente, la fecha más aceptada es el 393 d. C., fecha de inicio del dominio del cristianismo en el Imperio Romano, cuando el emperador Teodosio I decretó que todos los cultos y prácticas paganas serían eliminadas.[Nota 2] Otra fecha comúnmente citada es el 426 d. C., cuando su sucesor, Teodosio II, ordenó la destrucción de todos los templos griegos. Juegos Olímpicos modernos Antecedentes Olympiade de la République el 22 de septiembre de 1796, (Museo de la Revolución francesa). Se ha documentado que, por lo menos, desde el siglo XVII se empleó de diversas formas el término «olímpico» para describir eventos deportivos en la era moderna. El primero fue el Cotswold Olimpick Games, una reunión deportiva anual en las cercanías de Chipping Campden, Inglaterra. Fue organizado por el abogado Robert Dover entre 1612 y 1642, con varias celebraciones hasta la actualidad. La Asociación Olímpica Británica describe a estos juegos como «los primeros estimulantes de los comienzos olímpicos del Reino Unido». Sello griego de los primeros Juegos Olímpicos. LOlympiade de la République, era un festival olímpico nacional celebrado entre 1796 y 1798 en la Francia revolucionaria también trató de emular a los antiguos Juegos Olímpicos. La competición incluyó disciplinas de los Juegos Olímpicos antiguos. El evento de 1796 marcó la introducción del sistema métrico en el deporte.  En 1850, una Clase Olímpica fue iniciada por el Dr. William Penny Brookes en Much Wenlock, Shropshire, Inglaterra. En 1859, Brookes le cambió el nombre por el de Juegos Olímpicos Wenlock. Este festival deportivo anual continúa hasta la actualidad. La Sociedad Olímpica de Wenlock fue fundada por Brookes el 15 de noviembre de 1860. Entre 1862 y 1867, Liverpool celebró el Grand Olympic Festival, un festival anual. Fue diseñado por John Hulley y Charles Melly. Estos juegos fueron los primeros en ser totalmente amateur, sin embargo, solo los «caballeros amateurs» podían competir. El programa de la I Olimpiada, celebrada en Atenas en 1896, fue casi idéntico al de los Juegos Olímpicos de Liverpool. [Nota 3] En 1865, Hulley, Brookes y E. G. Ravenstein fundaron la Asociación Olímpica Nacional en Liverpool, un predecesor de la Asociación Olímpica Británica. Los artículos asentados durante la fundación de la asociación fueron el bosquejo de la Carta Olímpica Internacional. En 1866, se celebraron unos Juegos Olímpicos Nacionales en Reino Unido, organizados en el Crystal Palace de Londres.  El renacimiento de las Olimpiadas Artículo principal: Juegos Olímpicos de verano Barón Pierre de Coubertin. El interés griego de revivir los Juegos Olímpicos comenzó con la Guerra de independencia de Grecia en 1821, cuando los griegos lucharon contra el Imperio otomano. En 1833 el poeta y editor Panagiotis Soutsos propuso restablecer los Juegos Olímpicos de la Antigüedad. En 1856 Evangelos Zappas, un rico filántropo griego, escribió al rey Otón I de Grecia ofreciéndose a financiar el renacimiento permanente de los Juegos Olímpicos. En 1859 Zappas patrocinó los primeros Juegos Olímpicos, que fueron celebrados en una plaza de la ciudad de Atenas. Los participantes eran originarios de Grecia y el Imperio otomano. Zappas financió la restauración del antiguo Estadio Panathinaiko para que acogiera futuras ediciones. El estadio fue sede en 1870 y 1875. Treinta mil espectadores asistieron a la edición de 1870; sin embargo, no hay registros oficiales de asistencia del evento de 1875. En 1890, después de asistir a los Juegos de la Sociedad Olímpica de Wenlock, el barón Pierre de Coubertin se inspiró para fundar el Comité Olímpico Internacional (COI). Coubertin basó sus ideas en los trabajos de Brookes y Zappas, con el objetivo de establecer unos Juegos Olímpicos internacionales que se celebraran cada cuatro años. Presentó estas ideas durante el primer Congreso Olímpico. Esta reunión se celebró del 16 al 23 de junio de 1894 en la Universidad de París. El 23 de junio se adoptó de manera unánime una resolución que definió el renacimiento de los Juegos Olímpicos; además se estableció que la primera edición se celebrara en Atenas dos años después. También se asentaron las bases para la fundación del COI. El COI eligió al escritor griego Dimitrios Vikelas como su primer presidente. Dos años más tarde, Coubertin sustituyó a Vikelas como presidente. El COI se estableció con representantes de doce países: Argentina (José Benjamín Zubiaur) Austria-Bohemia (Jiri Guth-Jarkovsky) Bélgica (Maxime de Bousies) Estados Unidos (William Sloane) Francia (Ernest F. Callot y Pierre de Coubertin) Grecia (Demetrius Vikelas) Hungría (Ferenc Kemény) Italia (Mario Luccesi Palli y Andria Carafa) Nueva Zelanda (Leonard A. Cuff) Reino Unido (C. Herbert Ampfhill y Charles Herbert) Rusia (General Alexei de Boutowsky) Suecia (General Viktor Balck). Atenas 1896 Artículo principal: Juegos Olímpicos de Atenas 1896 El atleta estadounidense James Connolly (triple salto y salto de altura) fue el primer campeón olímpico de la historia moderna. Miembros del Comité Organizador de la I Olimpiada. Estadio Panathinaikó, primer estadio olímpico en el que se realizó la primera Olimpiada (Atenas 1896). Los primeros Juegos Olímpicos se celebraron bajo los auspicios del COI en el Estadio Panathinaiko en Atenas entre el 6 y el 15 de abril de 1896. Participaron 241 atletas de 14 países que compitieron en 43 eventos de nueve deportes. Zappas y su primo Konstantinos Zappas habían dejado al gobierno griego un fideicomiso para financiar futuros Juegos Olímpicos. El fideicomiso se empleó en el financiamiento de Atenas 1896. George Averoff contribuyó a la remodelación del estadio. El gobierno griego aportó fondos que esperaban recuperar con la venta de entradas y de estampillas conmemorativas. Los funcionarios griegos y el público estaban entusiasmados con la experiencia de albergar unos Juegos Olímpicos. Este sentimiento fue compartido por muchos de los atletas, que exigieron que Atenas fuera la ciudad sede permanente del evento. Sin embargo, el COI buscó rotar a diversas ciudades del mundo la sede, de esta manera se eligió a París como ciudad sede de la segunda edición de los Juegos Olímpicos. Cambios y adaptaciones Artículo principal: Juegos Olímpicos de Verano El Campo Francis de la Universidad Washington en San Luis durante los Juegos Olímpicos de 1904. Tras el éxito de Atenas 1896, los Juegos Olímpicos entraron en un período de estancamiento que amenazó su supervivencia. Los Juegos Olímpicos celebrados de forma paralela a la Exposición universal de París en 1900 y en la Exposición Universal de San Luis en 1904 fueron atracciones secundarias. En París 1900 la mujer hizo su aparición en las olimpiadas. En San Luis 1904 participaron unos 650 atletas, pero 580 eran de Estados Unidos. El carácter homogéneo de estas celebraciones fue un punto en contra para el Movimiento Olímpico. Los Juegos se recuperaron en 1906 cuando se celebraron los primeros y únicos Juegos Intercalados ,llamados así porque fueron los segundos celebrados en la tercera Olimpiada, en Atenas. Estos no están reconocidos por el COI. Atrajeron a un amplio campo internacional de participantes y generaron un gran interés. Esto marcó el inicio y constante aumento tanto de popularidad como de tamaño de los Juegos Olímpicos. Gillis Grafström en los Juegos Olímpicos de Chamonix 1924. Olimpíadas de Invierno Artículo principal: Juegos Olímpicos de Invierno Los Juegos Olímpicos de Invierno fueron creados para presentar deportes invernales, los cuales eran de una logística imposible durante el Verano. Se realizaron competiciones de patinaje artístico (en 1908 y 1920) y hockey sobre hielo (en 1920) durante la edición de verano, sin embargo, el COI buscó ampliar esta lista de deportes para abarcar otras actividades invernales. En la 19.ª Sesión del Comité Olímpico Internacional de 1921, celebrada en Lausana, se decidió llevar a cabo una versión de invierno de los Juegos Olímpicos. Una semana ,11 días, de juegos se celebró en 1924 en Chamonix, Francia, en el marco de los Juegos de París celebrados tres meses después, este evento serían los primeros Juegos Olímpicos de Invierno.  El COI estableció que los Juegos de Invierno se celebraran cada cuatro años en el año que su homólogo de verano. Esta tradición se mantuvo hasta los Juegos Olímpicos de Albertville 1992; tras esta edición se decidió que la edición invernal tuviera lugar dos años después de los juegos de verano. Así, en 1994, se realizaron los Juegos Olímpicos de Lillehammer, fue la primera ocasión en que unos Juegos Olímpicos fueron celebrados en un periodo menor a cuatro años. Juegos Paralímpicos Artículo principal: Juegos Paralímpicos En 1948, sir Ludwig Guttmann declaró estar resuelto a promover la rehabilitación de los soldados tras la Segunda Guerra Mundial y organizó un evento deportivo entre varios hospitales, coincidiendo con los Juegos Olímpicos de Londres 1948. El evento de Guttmann fue conocido como los Juegos de Stoke Mandeville, los cuales se convirtieron en un festival deportivo anual. Durante los siguientes doce años, Guttmann y otros continuaron sus esfuerzos por utilizar el deporte como una vía para la curación. Para los Juegos Olímpicos de Roma 1960, Guttmann reunió 400 atletas para competir en la Olimpiada paralela, convirtiéndose en los primeros Juegos Paralímpicos. Desde entonces, los Juegos Paralímpicos se celebran cada año olímpico. Desde los Juegos Olímpicos de Seúl 1988, la ciudad sede de los Juegos Olímpicos también es sede de los Juegos Paralímpicos. En 2001, el Comité Olímpico Internacional (COI) y el Comité Paralímpico Internacional (CPI) firmaron un acuerdo en el cual se garantizaba que las ciudades anfitrionas se comprometían a administrar tanto los Juegos Olímpicos como los Paralímpicos. El acuerdo entró en vigor en 2008 (verano) en los Juegos Olímpicos de Pekín y en 2010 (invierno) en los Juegos Olímpicos de Vancouver. El presidente del Comité Organizador de los Juegos Olímpicos y Paralímpicos de Londres 2012, Sebastian Coe, habló sobre los Juegos Paralímpicos de 2012: Queremos cambiar las actitudes del público hacia la discapacidad, celebrar la excelencia del deporte paralímpico y consagrar el principio de que los dos Juegos forman parte de un todo. Sebastian Coe Juegos de la Juventud Artículo principal: Juegos Olímpicos de la Juventud Un reloj de cuenta regresiva muestra 60 días restantes para los primeros Juegos Olímpicos de la Juventud que fueron celebrados en Singapur en 2010. Los Juegos Olímpicos de la Juventud se celebran cada cuatro años y en ellos participan atletas de entre 14 y 18 años. Fueron concebidos por Jacques Rogge en 2001 y aprobados en la 119.ª Sesión del Comité Olímpico Internacional.  Los primeros Juegos Olímpicos de la Juventud de Verano se celebraron en Singapur del 14 al 26 de agosto de 2010, mientras que los primeros Juegos Olímpicos de la Juventud de Invierno tuvieron lugar en Innsbruck, dos años más tarde. Estos juegos son más cortos que los sénior, la versión de verano tiene una duración de doce días; la de invierno, nueve. Cerca de 3500 atletas y 875 oficiales participaron en la edición de verano, mientras que 970 atletas y 580 oficiales hicieron lo propio en la edición de invierno. Los deportes disputados coinciden con los previstos para los Juegos sénior, sin embargo, hay variaciones en las disciplinas y pruebas. Juegos recientes En 1896, 241 atletas en representación de 14 naciones participaron en los Juegos de la I Olimpiada disputando 43 pruebas de 9 deportes; mientras que cerca de 10 500 competidores de 206 delegaciones participaron en los Juegos de la XXXIII Olimpiada en 2024 compitiendo en 329 pruebas de 32 deportes. El alcance y la escala de los Juegos Olímpicos de Invierno es menor (Pekín acogió 2834 atletas de 91 naciones que compitieron en 109 eventos durante los Juegos Olímpicos de Invierno de 2022). La mayoría de los atletas y oficiales se alojan en la Villa Olímpica, destinada a ser un alojamiento independiente para todos los participantes, cuenta con cafeterías, clínicas y lugares para la expresión religiosa. Impacto económico y social en los países y ciudades sedes Diversos economistas se muestran escépticos sobre los beneficios económicos de la organización de los Juegos Olímpicos, hacen hincapié en que tales mega eventos a menudo tienen grandes costos mientras producen pocos beneficios tangibles a largo plazo. Sin embargo, los Juegos Olímpicos parecen aumentar las exportaciones del país sede, ya que, como tal ,o solo como candidato,, envía una señal de apertura comercial. Las investigaciones sugieren que la celebración de unos Juegos Olímpicos de Verano tiene un fuerte efecto positivo sobre las contribuciones filantrópicas de las empresas con sede en la ciudad anfitriona, que parece beneficiar al sector filantrópico local. Este efecto positivo se inicia en los años previos a la celebración y puede persistir durante varios años después, aunque no permanentes. Este hallazgo sugiere que albergar los Juegos Olímpicos podría crear oportunidades para las ciudades de influir en las corporaciones locales en formas que beneficien al sector filantrópico local y a la sociedad civil. Los Juegos también han tenido importantes efectos negativos en las ciudades sede, por ejemplo, el Centro por el Derecho a la Vivienda y contra los Desalojos informa que los Juegos Olímpicos han desplazados más de dos millones de personas en más de dos décadas, a menudo afectando de forma desproporcionada a los grupos desfavorecidos. Comité Olímpico Internacional Artículo principal: Comité Olímpico Internacional Sede del COI en Lausana, Suiza. El Movimiento Olímpico abarca un gran número de organizaciones nacionales e internacionales, federaciones deportivas, medios de comunicación, así como atletas, funcionarios, jueces y cualquier otra persona e institución que esté de acuerdo en cumplir las normas de la Carta Olímpica. La organización paraguas del Movimiento Olímpico, el Comité Olímpico Internacional, es responsable de elegir la ciudad sede de cada edición de Juegos Olímpicos, la supervisión de la planificación de los mismos, así como la actualización y aprobación del programa deportivo y la negociación de los derechos de patrocinio y radiodifusión. El Movimiento Olímpico se compone de tres elementos: Federaciones internacionales (FI): son los órganos reguladores a nivel internacional de un deporte. Por ejemplo, la Federación Internacional de Fútbol Asociado (FIFA) es la FI del fútbol. Hay 40 federaciones en el Movimiento Olímpico, en representación de cada uno de los deportes olímpicos. Además de 36 con reconocimiento parcial, pues no integran el programa olímpico oficial; y cinco federaciones provisionales cuyos deportes están a prueba la justa veraniega. Comités olímpicos nacionales (CON): representan y regulan el Movimiento Olímpico dentro de cada país. Por ejemplo, el Comité Olímpico de Suecia es el CON de Suecia. Hay 206 comités olímpicos nacionales reconocidos por el COI. Estos se agrupan de forma regional en las Asociaciones de Comités Olímpicos Nacionales, una por cada uno de los continentes habitados y con el auspicio de la ACNO. Comités Organizadores de los Juegos Olímpicos: son comisiones temporales encargadas de la organización de cada uno de los Juegos Olímpicos. Estos comités se disuelven después de cada edición una vez que el informe final es entregado al COI. El francés y el inglés son los idiomas oficiales del Movimiento Olímpico. El idioma utilizado en cada edición de los Juegos es el idioma del país sede ,idiomas, si el país tiene más de un idioma oficial,. Cada proclamación se realiza en esos tres ,o más, idiomas, o en los dos principales si el país sede tiene por idioma oficial el inglés o el francés. Comités olímpicos nacionales Artículos principales: Comité olímpico nacional y Asociación de Comités Olímpicos Nacionales. El COI permite la formación de comités olímpicos nacionales que representan a cada país, sin verse obligadas a cumplir con estrictos requisitos relacionados con la soberanía política como lo demandan otras organizaciones internacionales. Como resultado, a colonias y dependencias se les permite competir en los Juegos Olímpicos. Ejemplos de esto incluyen territorios como Puerto Rico, Bermudas, Aruba, las Islas Caimán, las Islas Cook, las Islas Vírgenes Británicas, las Islas Vírgenes de los Estados Unidos, Samoa Americana, Hong Kong y Guam, los cuales compiten como naciones independientes a pesar de ser parte de otro país. A ello se añade la condición especial que guarda la representación de China Taipéi, nombre con el que COI y China, permiten y reconocen la actuación de Taiwán. La versión actual de la Carta Olímpica permite la creación de nuevos Comités Olímpicos Nacionales que representen a naciones que califican como un «Estado independiente reconocido por la comunidad internacional». Por lo tanto, el COI no permite la formación de Comités Nacionales de Sint Maarten y Curazao a pesar de haber ganado el mismo estatuto constitucional que Aruba en 2010, aunque el COI ha reconocido al Comité Olímpico de Aruba desde 1986.  Críticas Bandera olímpica. El COI ha sido a menudo criticado por ser una organización cerrada, con varios miembros vitalicios, y sin representación de todas las naciones integrantes. Las presidencias de Avery Brundage y Juan Antonio Samaranch fueron muy controvertidas. Brundage fue presidente durante más de dos décadas y durante su gestión protegió a los Juegos Olímpicos de la participación política y la influencia de la publicidad. Se le acusó de racismo, por su forma de manejar la cuestión del apartheid con la delegación de Sudáfrica, y de antisemitismo. Bajo la presidencia de Samaranch, el COI fue acusado de nepotismo y corrupción. Las relaciones de Samaranch con el régimen franquista de España fueron una fuente de críticas. En 1998 se descubrió que miembros del COI habían aceptado sobornos del Comité de la Candidatura de Salt Lake City para celebrar allí de los Juegos Olímpicos de Invierno de 2002. El COI realizó una investigación que llevó a la renuncia de cuatro miembros y la expulsión de otros seis. El escándalo impulsó reformas que cambiaron la forma de elegir las sedes, para evitar casos similares. Un documental de la BBC titulado Panorama: La compra de los Juegos, emitido en agosto de 2004, contenía una investigación sobre la aceptación de sobornos durante el proceso de elección de la ciudad sede de los Juegos Olímpicos de 2012. El documental establecía que era posible sobornar a los miembros del COI para que estos votaran por una ciudad candidata en particular. Después de ser derrotado por poco en su candidatura, el alcalde de París Bertrand Delanoë acusó al primer ministro británico, Tony Blair, y al Comité de la Candidatura de Londres ,encabezado por Sebastian Coe, de romper las reglas. Citó al presidente francés, Jacques Chirac, como testigo. Sin embargo, Chirac fue precavido en sus respuestas. En julio de 2012, la Liga Antidifamación calificó la persistente negativa del COI de realizar un minuto de silencio en las ceremonias de apertura en honor a los once atletas israelíes asesinados por terroristas palestinos en Múnich 1972 como «una terca y cruel insensibilidad a la memoria de los atletas israelíes asesinados». Comercialización El COI se resistió a permitir financiamiento de patrocinadores corporativos. No fue sino hasta el retiro de Avery Brundage en 1972 cuando el COI empezó a explorar el potencial del medio televisivo y los mercados lucrativos de publicidad disponibles para ellos. Bajo la presidencia de Juan Antonio Samaranch, los Juegos comenzaron a buscar patrocinadores internacionales que trataron de vincular sus productos con la marca olímpica. Siendo el punto de inflexión la organización de los Juegos Olímpicos de Los Ángeles 1984. Presupuesto Durante la primera mitad del siglo XX, el COI contó con un pequeño presupuesto. Como presidente del COI de 1952 a 1972, Avery Brundage rechazó todos los intentos de vincular las olimpiadas con el interés comercial. Brundage creía que la presión de los intereses corporativos afectaría la toma de decisiones del COI. La resistencia de Brundage a este flujo de ingresos provocó que el COI dejara a los comités organizadores negociar sus contratos de patrocinio y empleo de los símbolos olímpicos. Cuando Brundage se retiró, el COI contaba con dos millones de dólares en activos; ocho años después, el COI contaba con 45 millones. Esto se debió a un cambio en la ideología hacia la expansión de los Juegos a través del patrocinio corporativo y la venta de derechos televisivos. Cuando Juan Antonio Samaranch fue elegido presidente del COI en 1980, expresó su deseo de hacer que el COI fuera independiente en lo financiero. En Los Ángeles 1984, gracias a la venta de derechos exclusivos de patrocinio, difusión y mercadeo, el comité organizador generó un superávit presupuestario. Más tarde, el COI buscó controlar estos derechos de patrocinio. Por esta razón, en 1985 estableció el Programa Olímpico (TOP por sus siglas en inglés). Los miembros de este programa reciben derechos exclusivos de publicidad y uso del símbolo olímpico ,los anillos olímpicos, en sus publicaciones y anuncios. El costo de incorporarse a él es de unos 50 millones de dólares por una olimpiada ,cuatro años,. Televisión La prensa mundial en los Juegos Olímpicos de Londres 2012. Los Juegos Olímpicos de Berlín 1936 fueron los primeros Juegos transmitidos por televisión, aunque solo a un público local. Los Juegos Olímpicos de Invierno de 1956 fueron los primeros con transmisión internacional. Los siguientes Juegos de Invierno ya contaban con derechos de transmisión vendidos por primera vez a redes de difusión televisiva. CBS pagó 394 000 dólares para los derechos estadounidenses mientras que la Unión Europea de Radiodifusión (UER) pagó 660 000 dólares. En las décadas siguientes los Juegos Olímpicos se convirtieron en uno de los frentes ideológicos de la Guerra Fría. Las superpotencias competían por la supremacía política y el COI quería aprovechar este creciente interés por medio de la transmisión de los Juegos. La venta de derechos de transmisión permitió al COI aumentar la exposición de las olimpiadas, lo que generó más interés. Este ciclo permitió al Comité cobrar cada vez más por esos derechos televisivos. Por ejemplo, la CBS pagó 375 millones de dólares por los derechos de transmisión de los Juegos Olímpicos de Nagano 1998, mientras que la NBC gastó 3500 millones de dólares por los derechos de transmisión de todos los Juegos Olímpicos del periodo 2000-2012. La audiencia incrementó de manera exponencial desde la década de 1960 hasta finales de siglo. Esto se debió al empleo de satélites para la difusión de televisión en vivo al mundo en Tokio 1964 y la introducción de la televisión a color en México 1968. Las estimaciones globales de audiencia de México 1968 fue de 600 millones, mientras que, en Los Ángeles 1984, la audiencia aumentó a 900 millones, para Barcelona 1992, la audiencia fue de 3500 millones de personas. Sin embargo, en Sídney 2000, NBC obtuvo la cuota de pantalla más baja para cualquier edición de los Juegos Olímpicos desde 1968. Camarógrafa de la OBS durante las competiciones de atletismo en Londres 2012. Esto se atribuyó a dos factores: por un lado, el aumento de la competencia de los canales de cable y por otro lado, el internet, medio capaz de mostrar resultados y videos en tiempo real. Las empresas de televisión seguían confiando en el contenido diferido, que se estaba volviendo una tecnología obsoleta en la era de la información. Una caída en las cuotas de pantalla de los estudios televisivos significó regalar tiempo de publicidad. Con estos altos costos cobrados por transmitir los Juegos, la presión añadida de la internet y la creciente competencia de cable, las televisiones demandaron al COI concesiones para buscar aumentar la audiencia. El COI respondió implementando una serie de cambios en el programa olímpico. En la edición de verano, la competición de gimnasia se amplió de siete a nueve días y se agregó una Gala de Campeones buscando obtener un mayor interés del público. El COI también amplió los programas de natación y clavados, ambos deportes populares con una amplia cantidad de televidentes. En mayo de 2001, el Comité creó los Olympic Broadcasting Services (OBS), establecidos para asegurar los altos estándares de emisión en los sucesivos Juegos Olímpicos. La OBS es el organismo de difusión olímpica y principal responsable de proveer imágenes de los Juegos a todas las organizaciones de radiodifusión que hayan comprado los derechos de televisión para los Juegos. Las labores de la OBS son: Producir las señales internacionales de radio y televisión. Diseñar, construir, instalar, operar y desmantelar el Centro Internacional de Radiodifusión. Diseñar, construir, instalar, operar y retirar todas las instalaciones y equipos en las sedes de competición. Coordinar y proporcionar diversas instalaciones y servicios a las emisoras que cuenten con los derechos de radiodifusión. Representar las necesidades de esas emisoras al Comité Organizador en relación con las instalaciones y servicios. Asistir al Comité Organizador en el diseño y construcción de infraestructura necesaria para dar cabida a las necesidades de la OBS y las diversas emisoras. En 2003, el COI decide que la concesión en Europa se haga una puja por los derechos televisivos, dejando de tener uso la asignación directa a la UER (quien los llevaba poseyendo desde Roma 1960) y sus cadenas, aunque mantiene los juegos de Vancouver y Londres a cambio de 614 millones de euros. A partir de los derechos de Sochi 2014 los acuerdos en Europa pasan a hacerse con independencia de cada país y no hay acuerdos para todo el continente debido a que el EBU rechazó ofertar. En 2015 vendió los derechos de los Juegos Olímpicos de 2018 hasta 2024 a Discovery Communications por 1300 millones de euros. Controversias La venta de la marca olímpica ha sido motivo de controversia. El argumento es que los Juegos se han convertido en algo indistinguible de cualquier otro espectáculo deportivo comercializado. Durante Atlanta 1996 y Sídney 2000, las ciudades estaban inundadas de empresas y comerciantes que trataban de vender mercancías relacionadas con las olimpiadas. El COI indicó que trataría de arreglar esta situación para evitar la sobre-comercialización de las ediciones futuras. Otra crítica sobre los Juegos es que son financiados por los países y ciudades anfitrionas, por lo cual el COI no incurre en los costos, sin embargo, controla todos los derechos y beneficios de los símbolos olímpicos. El COI también tiene un porcentaje de todos los ingresos de patrocinio y difusión. Las ciudades candidatas compiten mucho por organizar los Juegos, aunque no hay certeza de que las ciudades vayan a recuperar esa inversión. Sin embargo, hay investigaciones que han demostrado que el comercio es un 30 % mayor en países que han albergado unos Juegos Olímpicos. Costo El costo de los Juegos Olímpicos (verano e invierno) ha sido estudiado por los académicos Flyvbjerg Bent y Stewart Allison, de la Universidad de Oxford. Descubrieron que en los últimos 50 años, los Juegos más costosos han sido: Londres 2012 (14 800 millones), Barcelona 1992 (11 400 millones) y Montreal 1976 (6000 millones). Pekín 2008 puede clasificar entre los tres Juegos más costosos, sin embargo, las autoridades chinas no han dado a conocer sus datos. Estos costos solo incluyen los relacionados con el deporte y la seguridad; no incluyen otros gastos públicos, como la construcción y mantenimiento de carreteras, ferrocarriles, aeropuertos, infraestructura, además de los costos privados, como mejoras en hoteles u otras inversiones empresariales generadas a partir de la preparación de los Juegos, que son sustanciales, pero que varían mucho de una ciudad a otra y son difíciles de comparar. Flyvbjerg y Stewart además descubrieron que el sobrecosto es un problema persistente en los Juegos Olímpicos: Los Juegos Olímpicos sobrepasan el presupuesto con una regularidad del 100 %, son el único megaproyecto así de predecible. Los sobrecostos en los Juegos han sido muy mayores que en otro tipo de megaproyectos. Los mayores sobrecostos se han realizado durante Montreal 1976 (796 %), Barcelona 1992 (417 %) y Lake Placid 1980 (321 %). Los datos muestran que para una ciudad y país tomar la decisión de albergar unos Juegos Olímpicos es tomar uno de los riesgos económicos más grandes. Por ejemplo, el sobrecosto y la deuda de Atenas 2004 se agravó por la crisis económica de Grecia en el periodo de 2008 a 2013. A Montreal le tomó 30 años pagar la deuda de los Juegos de 1976.  Por último, Flyvbjerg y Stewart encontraron que en la última década, el sobrecoste en los Juegos ha descendido a niveles más regulares de este tipo de megaproyectos. Para el período de 2000 a 2010 el sobrecosto promedio fue de 47 %, mientras que antes el rebasamiento promedio fue de hasta 258 %. Sin embargo, Londres 2012 ha invertido esta tendencia, con un sobrecoste del 101 %. Flyvbjerg y Stewart concluyen en que el reto para los planificadores y administradores de los Juegos será lograr mantener los costos bajo control y de ser posible reducirlos. Símbolos Artículos principales: Anillos olímpicos, Símbolos olímpicos, Llama olímpica y Juramento Olímpico. Wenlock y Mandeville, mascotas de Londres 2012. El Movimiento Olímpico emplea diversos símbolos para representar los ideales consagrados en la Carta Olímpica. El símbolo más conocido son los anillos olímpicos: cinco entrelazados que representan la unión de los cinco continentes habitados ,África, América, Asia, Europa y Oceanía,. La versión a color de los anillos los representa en azul, amarillo, negro, verde y rojo sobre un fondo blanco, que forma la bandera olímpica. Estos colores fueron elegidos porque cada nación tiene al menos uno de ellos en su bandera. Fue adoptada en 1914, pero se izó por primera vez en los Juegos Olímpicos de Amberes 1920. Desde entonces, ha sido izada en cada celebración de los Juegos. El lema olímpico es Citius altius fortius, locución latina que significa «más rápido, más alto, más fuerte». Este fue tomado por parte de Coubertin de la inscripción que hizo en la bandera del club deportivo escolar de Arcueil. Los ideales de Coubertin se expresan en la siguiente frase: The most important thing in the Olympic Games is not to win but to take part, just as the most important thing in life is not the triumph but the struggle. The essential thing is not to have conquered but to have fought well. Lo más importante en los Juegos Olímpicos no es ganar sino participar, al igual que la cosa más importante en la vida no es el triunfo sino la lucha. Lo esencial no es haber vencido sino haber luchado bien. Pierre de Coubertin Meses antes de cada edición de los Juegos, la llama olímpica se enciende en Olimpia, en una ceremonia que refleja los antiguos ritos griegos. Una artista femenina, como sacerdotisa, enciende una antorcha, la coloca dentro de un espejo parabólico que concentra los rayos del sol, entonces se enciende la primera antorcha, inicia un recorrido de la antorcha olímpica que finalizará en la ceremonia de apertura (encendido del pebetero) de esa edición de los Juegos. Aunque el fuego ha sido un símbolo olímpico desde 1928, el recorrido de la antorcha se introdujo en los Juegos Olímpicos de Berlín 1936, como parte de una tentativa del gobierno alemán para promover su ideología nacionalsocialista. La mascota olímpica, un animal o una figura humana que representa el patrimonio cultural del país anfitrión, fue introducido en 1968; la primera mascota como tal fue Waldi, un perro salchicha, en Múnich 1972. La mascota olímpica ha jugado un papel importante en la promoción de la identidad de los Juegos desde Moscú 1980, cuando el oso ruso Misha alcanzó fama internacional. Las mascotas de Londres 2012, Wenlock y Mandeville, surgen «de las gotas de acero que formaron el Estadio Olímpico». Ceremonias Artículo principal: Ceremonial de los Juegos Olímpicos Ceremonia de apertura de los Juegos Olímpicos de Londres 2012. Apertura Según lo dispuesto en la Carta Olímpica, varios elementos conforman la ceremonia de apertura de los Juegos Olímpicos. La mayor parte de estos elementos se establecieron en los Juegos Olímpicos de Amberes 1920. La ceremonia suele comenzar con representaciones artísticas que dan lugar al conteo regresivo para el inicio formal del evento. Se recibe al jefe de Estado anfitrión y al presidente del Comité Olímpico Internacional, quienes antes de tomar su lugar central en el palco de honor, dan paso al izamiento de la bandera del país donde se realizan los Juegos, así como la interpretación del himno nacional.  Se presentan manifestaciones de música, canto, danza, teatro, etc., de la cultura de ese país. Las presentaciones artísticas han crecido en tamaño y complejidad a medida que las sedes han buscado ofrecer una ceremonia que perdure más que la de su predecesor. La ceremonia de apertura de Pekín 2008 costó 100 millones de dólares, con gran parte del presupuesto gastado en el segmento artístico. Después de la parte artística de la ceremonia, se realiza el desfile de los atletas, agrupados por país, en el Estadio Olímpico. El contingente de Grecia es el primero en entrar al estadio con el fin de honrar los orígenes de los Juegos. Enseguida, las naciones ingresan al estadio en orden alfabético, de acuerdo con el idioma hablado en la sede de esa edición, el contingente del país anfitrión es el último en entrar. En los Juegos Olímpicos de Atenas 2004, la bandera griega entró primero al estadio, sin embargo, la delegación de Grecia entró última. Los discursos dan formal apertura a los Juegos, en este orden: el presidente del comité organizador local, el presidente del COI y el jefe de Estado anfitrión, quien solo realiza la declaratoria oficial de inauguración. Un deportista y juez deportivo del país local realizan los juramentos correspondientes acompañados del estandarte olímpico (más pequeño que la bandera y usado para este juramento y el pase de estafeta entre ciudades en la clausura). Por último, la antorcha olímpica ingresa al estadio en una última serie de relevos y se enciende el pebetero olímpico. Lo habitual es que sea realizado por un deportista o personalidad local significativa.  Clausura Ceremonia de clausura de los Juegos Olímpicos de Londres 2012 Presentación de Río de Janeiro 2016 Apagado del pebetero Video externo Presentación de Río 2016 (transmitida por la OBS el 12 de agosto de 2012) en el canal oficial de Youtube del Comité Olímpico Internacional. Atención: este archivo está alojado en un sitio externo, fuera del control de la Fundación Wikimedia. La ceremonia de clausura de los Juegos Olímpicos se produce después de que todos los eventos deportivos han concluido. Los abanderados de cada país participante entran al estadio, seguidos por los atletas que entran juntos sin distinción nacional. Tres banderas se izan mientras que se reproducen sus himnos: la bandera del país sede, la de Grecia y la del país sede siguiente. El presidente del Comité Organizador y el presidente del COI realizan discursos. Por tradición, el presidente del COI declara los Juegos clausurados y llama a la «juventud del mundo a reunirse» cuatro años después en la siguiente justa. En lo que se conoce como la Ceremonia de Amberes, el alcalde de la ciudad que organizó los Juegos Olímpicos transfiere un estandarte al presidente del COI, quien luego se la entrega al alcalde de la ciudad anfitriona de la siguiente edición de las olimpiadas. Una vez que los Juegos están clausurados, la llama olímpica se apaga. A continuación, la siguiente sede introduce exhibiciones artísticas representativas de su cultura. Ceremonia de premiación La ceremonia de premiación se lleva a cabo después de cada evento olímpico. Los ganadores del primer, segundo y tercer lugar se suben a un podio de tres niveles en el cual se les entregan sus medallas. Después de que las medallas se han otorgado por un miembro del COI, las banderas de los tres medallistas se izan mientras se escucha el himno nacional del ganador del oro. Deportes Artículo principal: Deportes olímpicos El programa olímpico de verano consta de 36 deportes, 51 disciplinas y 339 competiciones. Por ejemplo, la lucha es un deporte olímpico que se divide en dos disciplinas: grecorromana y libre. Se divide en 14 competiciones masculinas y 4 femeninas. El programa de los Juegos Olímpicos de Verano incluye 36 deportes, mientras que los de Invierno cuenta con 15 deportes. El atletismo, la natación, la esgrima y la gimnasia artística son los únicos deportes de todas las ediciones. Por otro lado, el esquí de fondo, el patinaje artístico sobre hielo, el hockey sobre hielo, la combinada nórdica, el salto de esquí y el patinaje de velocidad sobre hielo son los deportes invernales presentes desde su creación en 1924. El bádminton, el baloncesto y el voleibol, aparecieron por primera vez en el programa como deportes de demostración, más tarde se convirtieron en deportes plenos olímpicos. Algunos deportes de los primeros Juegos fueron retirados. Los deportes olímpicos se rigen por federaciones internacionales reconocidas por el COI, cumpliendo las funciones de supervisores globales de esos deportes. Hay 40 federaciones representadas en el COI. Hay deportes reconocidos por el COI no incluidos en el programa. Estos deportes no se consideran olímpicos, pero pueden ser promovidos a este estatus durante una revisión del programa que se presenta en la primera sesión del COI tras una celebración de Juegos Olímpicos. Durante estas revisiones, los deportes pueden ser incluidos o excluidos del programa si esa decisión la toma una mayoría de dos tercios de los miembros del COI. Hay deportes reconocidos que nunca han estado en un programa olímpico, entre ellos el ajedrez. Entre octubre y noviembre de 2004, el COI estableció una Comisión del Programa Olímpico, que fue la encargada de la revisión de los deportes en el programa olímpico, así como de los deportes no olímpicos reconocidos. El objetivo fue aplicar un enfoque sistemático para establecer el programa olímpico de cada edición de los Juegos. La comisión formuló siete criterios para juzgar si un deporte debía o no ser incluido en el programa olímpico. Estos criterios son: la historia y la tradición de este deporte, la universalidad, la popularidad de este deporte, la imagen, la salud de los atletas, el desarrollo de la Federación Internacional que rige el deporte y los costos de la celebración de este deporte. A partir de este estudio surgieron cinco deportes reconocidos como candidatos para su inclusión en los Juegos Olímpicos de Londres: golf, karate, rugby, patinaje y squash. Estos deportes fueron examinados por la Comisión Ejecutiva del COI y luego el tema se discutió en la Sesión General de julio de 2005, celebrada en Singapur. De los cinco deportes candidatos para inclusión solo dos fueron seleccionados como finalistas: karate y squash. Ninguno de esos deportes alcanzó la mayoría de dos tercios de los votos y por consiguiente no fueron promovidos. En octubre de 2009, el COI agregó el golf y el rugby al programa olímpico de los Juegos Olímpicos de Verano de 2016 y 2020. La 114.ª Sesión del COI, celebrada en 2002, limitó el programa de los Juegos de Verano a un máximo de 28 deportes, 301 eventos y 10 500 atletas; Tres años más tarde, en la 117.ª Sesión del COI, la primera revisión importante del programa se llevó a cabo, lo que dio lugar a la exclusión del béisbol y el softball del programa oficial de Londres 2012, que debutaron en Tokio 2020. Como no hubo acuerdo sobre la promoción de otros dos deportes, el programa de 2012 incluyó solo 26. Río de Janeiro 2016 tuvo un total de 28 deportes, gracias a la adición del rugby y el golf la edición de Tokio 2020 aumentó hasta los 33 deportes, con debut del surf, el skateboarding (en modalidad street y park), la escalada deportiva, el kárate, el beisbol y el softbol. En las próximas ediciones de Verano, harán su debut otros deportes o retornarán al programa otros. Para París 2024, se incluyó el breakdance, en detrimento del kárate, el béisbol y el softbol, sumando 31 deportes. En Los Ángeles 2028, regresarán el béisbol y el softbol, junto al críquet (cuya única presencia tuvo lugar en 1900) y el lacrosse (excluido desde 1908, sin considerar su presencia como deporte de exhibición en 1928, 1932 y 1948), y debutarán el squash y el flag football. El breakdance se excluye del programa. Competirán hasta 36 deportes, el mayor número hasta la fecha. El remo sufrirá variaciones en su programa de la edición de 2028. Las disciplinas de peso ligero (hasta 59 kg en mujeres y 72,5 kg en hombres) serán retiradas, en favor del remo de mar en su modalidad beach sprint o remo esprint de playa. Por logística, la distancia reglamentaria de 2000 metros se reducirá a 1500 metros, de forma excepcional. Amateurismo y profesionalismo Véase también: Amateurismo El juego de la soga fue un deporte incluido en el programa olímpico de 1900 a 1920. Amberes vio la última competencia olímpica de este deporte. El ethos de la aristocracia, ejemplificada en la escuela pública inglesa, influenció fuerte a Pierre de Coubertin. Las escuelas públicas mantenían la creencia de que el deporte era una parte importante de la educación, una actitud resumida en la frase mens sana in corpore sano. En este ethos, un caballero se convertía en un polifacético, no en el mejor de algo específico. Hubo un concepto en el cual entrenar se consideró equivalente a hacer trampa. A aquellos que practicaban de forma profesional se les consideraba con una ventaja injusta sobre aquellos que lo practican como pasatiempo. La exclusión de los profesionales causó controversias. El campeón de pentatlón y decatlón de los Juegos Olímpicos de Estocolmo 1912, Jim Thorpe, fue despojado de sus medallas cuando se descubrió que había jugado béisbol semiprofesional antes de los Juegos Olímpicos. El COI le restauró sus medallas póstumas en 1983. Esquiadores suizos y austríacos boicotearon los Juegos Olímpicos de Invierno de 1936 en apoyo de sus profesores de esquí, a los cuales no se les permitió competir puesto que ganaban dinero gracias a su deporte y por lo tanto fueron considerados profesionales. La estructura de las clases evolucionó a lo largo del siglo XX y la definición del deportista amateur como un caballero aristócrata se convirtió en obsoleta. La llegada del patrocinio del Estado a atletas amateur de tiempo completo de los países del bloque del Este erosionó aún más la ideología del amateurismo puro, ya que puso a los atletas amateur con financiamiento propio de los países occidentales en desventaja. Sin embargo, el COI mantuvo las reglas tradicionales sobre el amateurismo. A principios de la década de 1970, el amateurismo fue eliminado de modo gradual de la Carta Olímpica. Después de los Seúl 1988, el COI decidió permitir a profesionales participar en los Juegos Olímpicos, sujetos a la aprobación de las Federaciones Internacionales. Controversias Boicots Países que boicotearon los Juegos Olímpicos de 1976 (amarillo), 1980 (azul) y 1984 (rojo), países que no boicotearon ninguna edición (gris). Desde la creación de los Juegos Olímpicos modernos en 1896, solo los contingentes de Grecia, Australia, Francia, Reino Unido y Suiza se han hecho presentes en todas la ediciones.[Nota 4] Si bien algunos países no participan por falta de atletas clasificados, o por no estar afiliados al COI, algunos optan por el boicot. El Consejo Olímpico de Irlanda boicoteó Berlín 1936, debido a que el COI insistió en que su contingente debía unirse al del Estado Libre de Irlanda para representar en uno solo a toda la isla de Irlanda. Diversos países boicotearon los Juegos Olímpicos de Melbourne 1956: Países Bajos, España y Suiza se negaron a asistir a causa de la represión soviética de la revolución húngara de 1956 ,sin embargo, estos tres países enviaron competidores a los eventos ecuestres celebrados en Estocolmo,. Camboya, Egipto, Irak y Líbano boicotearon debido a la Guerra del Sinaí; China (República Popular China) hizo lo propio porque a Taiwán (República de China) se le permitió competir en esa edición. En 1976, 24 países africanos boicotearon los Juegos de Montreal en forma de protesta, pues habían solicitado, sin éxito, que se excluyera a Nueva Zelanda por disputar encuentros de rugby contra equipos de Sudáfrica, país excluido por sus políticas racistas. Los países africanos cumplieron su amenaza, a ellos se les unieron Guyana e Irak en un boicot dirigido por Tanzania, después de que algunos atletas ya habían competido. Taiwán decidió boicotear los Juegos, debido a la presión ejercida por la República Popular China al Comité Organizador en relación con el nombre de la República de China. La República de China rechazó una propuesta que le hubiera permitido usar la bandera y el himno de Taiwán, siempre y cuando se modificara el nombre con el que participaría. Taiwán no participó en los Olímpicos hasta 1992, cuando volvió con el nombre de China Taipéi, con una bandera y un himno especiales. En 1980 y 1984, oponentes de la Guerra Fría realizaron grandes boicots. 65 países se negaron a competir en Moscú 1980 debido a la invasión soviética de Afganistán. Esto redujo los participantes a 81 países, la cifra más baja desde 1956. La Unión Soviética y 14 de sus aliados del bloque del Este ,excepto Rumania, boicotearon la siguiente, en Los Ángeles 1984, alegando que no podían garantizar la seguridad de sus atletas. Funcionarios soviéticos defendieron su decisión de retirarse diciendo que «Estados Unidos estaba siendo azotado por sentimientos chauvinistas y una histeria antisoviética». Las naciones que boicotearon los Juegos organizaron su evento alternativo, los Juegos de la Amistad, en agosto.  En Seúl 1988, Corea del Norte presionó al COI para realizar una edición conjunta entre ambas Coreas, sin embargo, ante las reglas de la Carta Olímpica, el COI se negó. A cambio, el COI ofreció la realización de algunas pruebas en Corea del Norte siempre y cuando ese país aceptara algunas condiciones, entre ellas abrir la frontera intercoreana, condiciones que no fueron aceptadas. Corea del Norte llamó a boicotear los Juegos y fue apoyada por Cuba, Nicaragua, Albania y Etiopía. En protesta a la situación de los derechos humanos en la República Popular China, las protestas del Tíbet y el conflicto de Darfur se llamó a boicotear Pekín 2008. Ninguna nación apoyó el boicot. En agosto de 2008, el gobierno de Georgia llamó a un boicot de los Juegos Olímpicos de Invierno de 2014, en respuesta a la participación rusa en la Guerra de Osetia del Sur de 2008.  Política Jesse Owens en Berlín 1936. Los Juegos Olímpicos se han utilizado como plataforma para promover ideologías. La Alemania nazi quiso retratar al Partido Nacional Socialista como benévolo y amante de la paz cuando Berlín fue sede de los Juegos de 1936, a pesar de que los utilizaron para mostrar la supuesta superioridad aria. Alemania fue el país más exitoso en esa edición, lo que hizo mucho para respaldar sus alegaciones de la supremacía aria, pero las victorias notables de afroamericanos como Jesse Owens, que ganó cuatro medallas de oro, y judíos como la húngara Ibolya Csák, arruinaron el mensaje. En los Juegos Olímpicos de Helsinki 1952, la Unión Soviética participó por primera vez en unos Juegos Olímpicos. Sin embargo, a partir de 1928, los soviéticos organizaron un evento deportivo internacional llamado Espartaquiada. Durante el período de entreguerras (1920-1930), las organizaciones comunistas y socialistas de varios países trataron de contrarrestar las Olimpiadas burguesas creando las Olimpiadas Obreras. Para 1956, los soviéticos se habían convertido en una superpotencia deportiva y al hacerlo se aprovecharon de la publicidad obtenida al ganar en los Juegos Olímpicos. En Melbourne 1956, Alemania Federal y Alemania Oriental participaron en la ceremonia de apertura con una misma bandera, pero compitieron por separado. Debido a las leyes australianas de cuarentena y de protección animal, en Melbourne 1956 los deportes ecuestres tuvieron que realizarse en Estocolmo, siendo la primera vez que los JJ. OO. se realizan en dos países diferentes (en dos continentes). También han buscado promover la política. En México 1968, dos atletas estadounidenses, Tommie Smith y John Carlos, primer y tercer lugar en los 200 metros, realizaron el saludo del Poder Negro durante la ceremonia de premiación. El segundo lugar, el australiano Peter Norman, aceptó la invitación a portar la insignia del Proyecto Olímpico para los Derechos Humanos en apoyo a Smith y Carlos. En respuesta a la protesta, el presidente del COI, Avery Brundage, dio dos opciones al Comité Olímpico Estadounidense (USOC): enviar a casa a los dos atletas o retirar a todo el equipo de atletismo. El USOC optó por retirar a Smith y Carlos. El gobierno iraní ha tomado medidas para evitar competencias entre atletas de Irán versus Israel. El yudoca iraní Arash Miresmaeili no participó contra un israelí en los Juegos Olímpicos de Atenas 2004. A pesar de que fue descalificado por tener exceso de peso, Miresmaeli fue galardonado con 125 000 dólares por el gobierno iraní, una cantidad pagada a todos los iraníes ganadores de medallas de oro. Fue absuelto de evitar la pelea de forma intencionada, pero el premio monetario levantó sospechas. En Sídney 2000 y Atenas 2004, Corea del Norte y Corea del Sur desfilaron en las ceremonias de apertura de ambas ediciones con una sola bandera, aunque compitieron por separado. Lo hicieron como una forma de unificarse, pese a las ideologías opuestas. Dopaje Artículo principal: Dopaje en los Juegos Olímpicos Thomas Hicks durante el maratón de los Juegos Olímpicos de San Luis 1904. En San Luis 1904 se presentó el primer caso documentado de dopaje. Thomas Hicks, medallista de oro en el maratón, recibió estricnina de su entrenador. En Londres 1908, el atleta italiano Dorando Pietri se inyectó estricnina; acusado de usar estas sustancias para mejorar el rendimiento, fue descalificado. Se lo veía correr desorientado. La única muerte ocasionada por el dopaje, relacionada con los Juegos Olímpicos ocurrió en Roma 1960. El danés Knud Jensen Enemark cayó de su bicicleta y murió tiempo después. Una investigación forense le descubrió rastros de anfetaminas. A mediados de esa década, las federaciones deportivas empezaron a prohibir el uso de drogas que mejoraban el rendimiento; en 1967, el COI hizo lo mismo. El COI creó la Comisión Médica que inició los controles antidopaje en los Juegos Olímpicos de México 1968. El primer atleta olímpico en dar positivo en una prueba de dopaje fue Hans-Gunnar Liljenwall, un sueco que dio positivo por alcohol en México 1968, por lo que le quitaron su medalla de bronce.  El dopaje más publicitado fue el del canadiense Ben Johnson, quien ganó los 100 metros en Seúl 1988; dio positivo por estanozolol y fue despojado de su oro. A finales de 1990, el COI tomó la iniciativa en una batalla más organizada contra el dopaje, mediante la formación de la Agencia Mundial Antidopaje en 1999. Hubo un fuerte aumento en las pruebas positivas de dopaje en los Juegos Olímpicos de Sídney 2000 y 2002. Varios medallistas en halterofilia y esquí de fondo fueron descalificados por dopaje. Durante los Juegos Olímpicos de Invierno de 2006, solo un atleta dio positivo en dopaje. El régimen de pruebas antidopaje creado por el COI ha establecido un punto de referencia mundial que otras federaciones deportivas del mundo intentan emular. Durante los Juegos de Pekín 2008, 3667 atletas fueron evaluados por el COI bajo los auspicios de la Agencia Mundial Antidopaje. Se hicieron pruebas de orina y sangre para detectar sustancias prohibidas. Varios atletas fueron excluidos por Comités Olímpicos Nacionales de las competencias antes de los Juegos. Solo tres atletas dieron positivo.  Para Río de Janeiro 2016, el gobierno de Rusia había planeado hacer que sus deportistas fueran con dopaje, pero fueron descubiertos. Dicho truco había sido utilizado en la Universiada de 2013, en el Campeonato Mundial de Atletismo de 2013 y en Sochi 2014. Debido a esto, se exigió que Rusia fuera excluida de los Juegos Olímpicos; sin embargo, el COI anunció que Rusia participaría en Río de Janeiro 2016. No obstante, la sanción evitó que en Tokio 2020 los rusos pudieran usar su bandera e himno. La mujer en los JJ. OO. Esta sección es un extracto de Mujeres en los Juegos Olímpicos.[editar] Las mujeres en los Juegos Olímpicos participaron por primera vez en la segunda edición en 1900 con 22 mujeres (el 2,2% del total de competidores) de manera no oficial. Las mujeres participaron por primera vez de manera oficial en los Juegos Olímpicos de Verano de Ámsterdam en 1928. En 1968 por primera vez una mujer encendió la Llama Olímpica. La proporción de participantes femeninas fue aumentando lentamente hasta que se consiguió paridad plena en 2024. La representación de las mujeres en el Comité Olímpico Internacional (COI) fue también históricamente muy inferior a la tasa de participación femenina. Las dos primeras mujeres miembros del COI asumieron en 1981 y recién en 2023, se logró que el 41,1% de los miembros sean mujeres. Los Juegos Olímpicos de París 2024 se destacaron por ser los primeros en tener como objetivo lograr la paridad de género entre hombres y mujeres.  Terrorismo y violencia Tres olimpiadas transcurrieron sin celebración de Juegos Olímpicos debido a la guerra: Berlín 1916 ,cancelada a causa de la Primera Guerra Mundial,; Helsinki 1940, Londres 1944, Garmisch-Partenkirchen 1940 y Cortina dAmpezzo 1944 ,por la Segunda Guerra Mundial,. La guerra de Osetia del Sur entre Georgia y Rusia estalló en la jornada inaugural de los Juegos Olímpicos de Pekín 2008. George W. Bush y Vladímir Putin asistían a los Juegos Olímpicos en ese momento y discutieron sobre ese conflicto durante un almuerzo ofrecido por el presidente chino, Hu Jintao. Cuando Nino Salukvadze de Georgia ganó la medalla de bronce en la competencia de pistola de aire 10 m, compartió podio con Natalia Paderina, tiradora rusa ganadora de la plata. En lo que se convirtió en un evento muy publicitado, Salukvadze y Paderina se abrazaron en el podio después de la finalización de la premiación.   El terrorismo ha afectado dos ediciones de Juegos Olímpicos, en particular Múnich 1972. Cuando los Juegos Olímpicos de Verano se celebraron en la ciudad alemana, once miembros del equipo olímpico israelí fueron secuestrados por el grupo terrorista palestino Septiembre Negro. En la denominada «masacre de Múnich», asesinaron a dos atletas poco después de secuestrarlos, e hicieron lo mismo con el resto tras un intento fallido de liberación, en ello, un oficial de la policía alemana y cinco terroristas fallecieron. Avery Brundage, presidente del COI, se negó a suspender los Juegos y 34 horas después de la masacre los reanudó con una polémica frase: «los Juegos deben continuar».   El 29 de noviembre de 1987 Corea del Norte intentó boicotear los Juegos Olímpicos de Seúl 1988 con un atentado terrorista el cual consistió en hacer explotar un avión Boeing 707 de la aerolínea surcoreana Korean Air en el cual murieron 115 personas (todos los ocupantes del avión). El ataque fue obra de dos agentes secretos norcoreanos que intentaron suicidarse al ser detenidos; la única sobreviviente, Kim Hyon-hui, aseguró uno de los objetivos se pretendía sabotear el turismo durante los Juegos de Seúl 1988, y asustar a las delegaciones participantes para que no asistan. Sin embargo, el plan norcoreano no resultó y los Juegos de Seúl 1988 se llevaron con total normalidad. (Véase Vuelo 858 de Korean Air). Durante Atlanta 1996, una bomba detonada en el Centennial Olympic Park mató a dos personas e hirió a 111. Eric Rudolph, un terrorista estadounidense, fue condenado a cadena perpetua. Los Juegos Olímpicos de Invierno de 2002, tuvieron lugar cinco meses después de los atentados del 11 de septiembre, con un aumento en el nivel seguridad. La ceremonia de apertura contó con signos de ese día: la bandera que ondeaba en la zona cero: Daniel Rodríguez, oficial de la Policía de Nueva York, cantando God Bless America y guardias de honor de los miembros del Departamento de Bomberos de la Ciudad de Nueva York y del Departamento de Policía de Nueva York. Los acontecimientos provocaron una creciente preocupación en relación con la seguridad en los Juegos Olímpicos e intentos más grandes de evitar un ataque terrorista a gran escala. Racismo Si bien en la actualidad el COI prohíbe cualquier tipo de eventual manifestación de racismo en los Juegos Olímpicos, éstos no fueron ajenos, por parte de países o atletas. En San Luis 1904, se llevó a cabo la que está considerada la mayor mancha racista en la historia del deporte; fue la primera vez que el racismo se hacía presente en los Juegos Olímpicos. En esta edición participaron deportistas de diferentes razas, y era la primera vez que deportistas de raza negra participaban en Juegos Olímpicos. El estadounidense George Poage se convertiría en el primer afrodescendiente en ganar una medalla olímpica. Sin embargo, lo que en verdad originó el racismo fueron los Días Antropológicos que se realizaron en paralelo y que fueron una parodia de los Juegos Olímpicos para demostrar la superioridad blanca ante afrodescendientes e indígenas. Sobre Berlín 1936, existieron mitos y leyendas en los que se cree que Adolf Hitler intentó utilizar los juegos para demostrar la superioridad de la raza aria. Se dice que Hitler se negó a darle la mano a Jesse Owens (ganador de medallas de oro) por ser de raza negra, y que según el propio Owens, solo recibió una felicitación por escrito por parte del gobierno alemán. También se dice que estos juegos fueron una humillación para los nazis, debido a que un gran número de deportistas negros lograron ganar medallas de oro; pero en realidad, fue Alemania quien ganó más medallas, con lo que Hitler se mostró satisfecho. La atleta Gretel Bergmann, a pesar de igualar un récord nacional en salto de altura un mes antes de los juegos, fue excluida del equipo alemán por ser judía. Bandera utilizada por Sudáfrica en Barcelona 1992, luego de su readmisión en los Juegos Olímpicos. Sudáfrica debutó en los Juegos Olímpicos en Londres 1908 y participó con normalidad hasta Roma 1960. Sin embargo, como en ese país existía la política segregacionista del apartheid, la cual discriminaba a personas de raza negra, el COI decidió expulsarla de forma permanente mientras no cambiara su proceder. Ante esto, el gobierno sudafricano intentó convencer al COI de volver a participar en la justa deportiva, apelando a la neutralidad del evento, lo que significaba no tener pretensiones de eliminar el segregacionismo. Cuando Sudáfrica, por fin, eliminó la política del apartheid en 1990, se le permitió volver a los Juegos Olímpicos; y fue así que en Barcelona 1992, aunque lo hizo con una bandera provisional para la ocasión, recién en Atlanta 1996, pudo utilizar otra vez su bandera. En Londres 2012, dos deportistas fueron expulsados, debido a publicaciones en sus cuentas de Twitter, que fueron consideradas racistas. La atleta griega Paraskevi Voula Papajristu fue expulsada luego de declarar que: Con tantos africanos en Grecia, al menos los mosquitos del Nilo Occidental serán nutridos de comida casera. El futbolista suizo Michel Morganella, luego de que su selección perdiera 2 a 1 contra Corea del Sur fue expulsado luego de declarar que: Yo rompo a todos los coreanos, váyanse todos a incendiarse. Ahahahhahahaah, banda de trisómicos. ¡Voy a dar una paliza a todos los coreanos del sur! Menudos retrasados mentales. Luego, ambos deportistas cerraron sus cuentas de Twitter y ofrecieron disculpas, pidieron no ser expulsados, pero sus delegaciones optaron por echarlos. El racismo en los Juegos Olímpicos también puede verse en la exclusividad que han tenido los deportes occidentales sobre otras tradiciones de competición. Por ejemplo, los países con pasado indígena han practicado desde antaño juegos y deportes cuyo registro no cabe en el modelo occidental promovido por el Comité Olímpico Internacional. Muertes En Estocolmo 1912, el atleta portugués Francisco Lázaro se convertiría en el primer deportista en morir en unos Juegos Olímpicos luego de haber corrido 30 km de la maratón. Se creyó que había muerto por deshidratación debido a las altas temperaturas. Luego se descubrió que se había colocado cera en la piel para evitar las quemaduras solares y mejorar su velocidad y ligereza. La cera limitó su sudoración natural, lo que le llevó a un desequilibro electrolítico de los fluidos del cuerpo que le produjo la muerte. Antes de la carrera, habría dicho: o gano o muero. Había sido el primer abanderado de su país. En Roma 1960, el ciclista danés Knud Jensen Enemark cayó de su bicicleta y murió tiempo después. Una investigación forense le descubrió rastros de anfetaminas. Maltrato animal En París 2024, la campeona olímpica ecuestre la jinete británica Charlotte Dujardin se retiró debido a que apareció un video en donde ella maltrata a un caballo. La Federación Ecuestre Internacional (FEI) anunció la suspensión provisional de la campeona debido a que su conducta es contraria a los principios del bienestar de los caballos. Nacionalidad Banderas ondeando durante los Juegos Olímpicos de Sochi 2014. Reglas del Comité Olímpico Internacional sobre la nacionalidad La Carta Olímpica establece que un atleta sea nacional de un país para competir en su representación. Deportistas con doble nacionalidad pueden competir por uno u otro país, siempre y cuando hayan transcurrido tres años desde el momento en que compitió por su país anterior. Sin embargo, si los Comités Olímpicos Nacionales y la Federaciones Internacionales llegan a un acuerdo, la comisión ejecutiva del COI tiene la capacidad de reducir o cancelar este período. Razones para cambio de nacionalidad En ocasiones, los atletas se convierten en ciudadanos de otra nación con el único fin de competir en los Juegos Olímpicos. Esto suele ocurrir ya porque las personas se sienten atraídas por los patrocinios y las instalaciones de entrenamiento o porque un atleta no cumple con los requisitos de su país. Esto suele ser debido a que hay muchos atletas cualificados del país de origen de un atleta y este quiere ser capaz de participar y ayudar al equipo de su nuevo país. Entre 1992 y 2008, cerca de cincuenta atletas habían emigrado a los Estados Unidos para competir en el equipo olímpico de ese país, después de haber competido para otra nación. Un caso conocido de elección por doble nacionalidad es el del atleta Félix Sánchez, quien nació en Estados Unidos, pero representó el país de sus padres, República Dominicana. Cambios de nacionalidad y disputas Uno de los casos más famosos de cambio de nacionalidad para los Juegos Olímpicos fue Zola Budd, una corredora sudafricana que emigró al Reino Unido, ya que a su país de origen no se le permitía participar por sus políticas de apartheid. Budd era elegible para la ciudadanía británica porque su abuelo nació allí, pero ciudadanos británicos acusaron al gobierno de acelerar el proceso de ciudadanía para ella. Otros casos notables incluyen al del corredor keniano Bernard Lagat que se convirtió en un ciudadano de Estados Unidos en mayo de 2004. La Constitución de Kenia requiere que un ciudadano renuncie a la ciudadanía keniana cuando se convierte en ciudadano de otra nación. Lagat compitió por Kenia en los Juegos Olímpicos de Atenas 2004, a pesar de que ya se había convertido en ciudadano de Estados Unidos. Sin embargo, para Kenia, él ya no era un ciudadano keniano, dejando así su medalla de plata en peligro. Lagat dijo que comenzó el proceso de ciudadanía a finales de 2003 y no esperaba convertirse en un ciudadano estadounidense hasta después de los Juegos de Atenas. La jugadora de baloncesto Becky Hammon no estaba considerada para el equipo olímpico de Estados Unidos. Sin embargo, ella quería jugar en los Olímpicos, por lo que emigró a Rusia, donde jugó en una liga nacional durante el receso de temporada de la WNBA. Hammon recibió críticas por parte de algunos estadounidenses, entre ellos el entrenador del equipo nacional de Estados Unidos e incluso fue llamada antipatriota. Medallistas Paavo Nurmi (1897-1973), un corredor finlandés de media y larga distancia, apodado el finlandés volador o el finlandés fantasma, estableció 22 récords mundiales oficiales en distancias entre 1500 y 20 km, ganando doce medallas en su carrera, en los Juegos Olímpicos. Paavo Nurmi en los Juegos Olímpicos de París 1924. Véanse también: Anexo:Medallas olímpicas, Medallero de los Juegos Olímpicos y Máximos medallistas olímpicos. Los atletas o equipos que obtengan el primer, segundo o tercer lugar en cada evento recibirán medallas. Los ganadores ,primer lugar, las recibían de oro macizo hasta 1912, luego de plata cubierta de oro y ahora de plata chapada en oro. Sin embargo, cada medalla de oro debe contener por lo menos seis gramos de oro puro. Los subcampeones recibirán medallas de plata y los ganadores del tercer lugar obtendrán medallas de bronce. En las pruebas de combate disputadas en un torneo de eliminación directa ,en especial el boxeo,, el tercer lugar no puede ser determinado y los dos perdedores de las semifinales recibirán medallas de bronce. En Atenas 1896 solo los dos primeros lugares recibieron medallas, plata y bronce. La entrega de las tres medallas fue introducida en los Juegos de San Luis 1904. De 1948 en adelante los atletas ganadores del cuarto, quinto y sexto lugar han recibido certificados o diplomas olímpicos. En 1984, se añadieron diplomas para el séptimo y octavo lugar. En Atenas 2004, los medallistas también recibieron coronas de olivo. Sedes de los Juegos Olímpicos Artículo principal: Anexo:Elección de la sede de los Juegos Olímpicos Sedes de los Juegos Olímpicos de Verano. En verde se muestran las sedes en una edición, en azul se muestran las sedes en dos o más ediciones.[actualizar] Sedes de los Juegos Olímpicos de Invierno. En verde se muestran las sedes en una edición, en azul se muestran las sedes en dos o más ediciones. La ciudad sede de los Juegos Olímpicos suele ser elegida siete años antes de la celebración. El proceso de selección se lleva a cabo en dos fases que abarcan un período de dos años. La ciudad que busca la candidatura lo solicita a su Comité Olímpico Nacional, pero si más de una ciudad de un país presenta una propuesta a su CON, este tiene un proceso interno de selección, ya que solo una ciudad por CON puede presentarse a la evaluación del Comité Olímpico Internacional. Una vez que el plazo para la presentación de propuestas por parte de los CON se alcanza, la primera fase (Solicitud) comienza con las ciudades aspirantes que solicitaron se les aplicara el cuestionario sobre varios criterios clave relacionados con la organización de los Juegos Olímpicos. De esta forma, los solicitantes deben dar garantías de que cumplirán con la Carta Olímpica y con las demás normas establecidas por el Comité Ejecutivo del COI. La evaluación de los cuestionarios contestados es realizada por un grupo especializado del COI. Los cuestionarios ofrecen una visión general de los proyectos de cada solicitante y su potencial para albergar los Juegos. Sobre la base de esta evaluación técnica, el Comité Ejecutivo del COI elige a los candidatos que pasarán a la fase de candidatura. Una vez que las ciudades candidatas son seleccionadas, deberán realizar una presentación más grande y más detallada de su proyecto como parte de un expediente de candidatura. Cada ciudad es analizada por una comisión de evaluación. Esta comisión visitará las ciudades candidatas, entrevistando a funcionarios locales e inspeccionando las instalaciones potenciales, para presentar un informe sobre sus conclusiones un mes antes de la decisión final del COI. Durante el proceso de entrevista, la ciudad también debe garantizar que será capaz de financiar los Juegos. Después del trabajo del comité evaluador, la lista de candidatos se presenta en la Sesión General del COI, que debe reunirse en un país que no cuente con una ciudad candidata. Los miembros del COI reunidos en la sesión deben elegir una ciudad sede. Una vez elegida, el Comité de Candidatura de esa ciudad (junto con el CON del país) firma un Contrato de Ciudad Sede con el COI, así se convierte en ciudad sede oficial de los Juegos Olímpicos. Para 2024, los Juegos Olímpicos (en sus versiones de verano e invierno) han tenido 44 ciudades sede de 23 países en cuatro continentes; sin embargo, de un total de cincuenta y cuatro ediciones realizadas, solo en diez ocasiones fuera de Europa y América del Norte. Desde Seúl 1988, los Juegos Olímpicos se celebraron siete veces en Asia u Oceanía, un fuerte aumento en comparación con los primeros 92 años de historia olímpica moderna. Los Juegos Olímpicos de Río de Janeiro 2016 fueron los primeros celebrados en América del Sur y los segundos en América Latina. Ninguna edición de los Juegos se ha celebrado en África. Pekín es la única ciudad que albergó las dos versiones de los justa olímpica (verano en 2008 e invierno en 2022). No obstante, el hecho de que los Juegos Olímpicos son concedidos a ciudades anfitrionas, no a países, los Estados Unidos han organizado ocho Juegos Olímpicos ,cuatro de verano y cuatro de invierno, más que cualquier otra nación. Londres y París han albergado tres Juegos Olímpicos, todos en su edición de verano, más que cualquier otra ciudad. Alemania, Australia y Grecia han organizado dos Juegos Olímpicos de Verano, al igual que Los Ángeles, Atenas y Tokio.[Nota 5] Francia ha organizado tres ediciones invernales, mientras que Suiza, Austria, Noruega, Canadá e Italia lo han hecho dos veces. Solo Lake Placid, Innsbruck y Sankt Moritz han celebrado Olimpiadas de Invierno en dos ocasiones. Los Juegos Olímpicos de Verano más recientes fueron los de París 2024 y los próximos serán en Los Ángeles 2028. En el caso de los Juegos de Invierno, Pekín 2022 fue la última edición de esta modalidad del evento y Milán-Cortina dAmpezzo 2026 será la siguiente."
ksampletext_wikipedia_spor_tenis: str = "Tenis. El tenis es un deporte que se disputa entre dos jugadores (individuales) o entre dos parejas (dobles). El objetivo principal del juego es lanzar una pelota golpeándola con la raqueta de modo que rebote en el otro lado pasando la red dentro de los límites permitidos del campo del rival, procurando que este no pueda devolverla para conseguir un segundo rebote en el suelo y darle un punto. Etimología La palabra española «tenis» proviene del inglés «tennis», que a su vez tiene su origen en el francés «tenez». Cuando el jugador de tenis (llamado «tenista») ponía la pelota en juego, exclamaba «¡tenuz!» («¡ahí va!», en francés). Historia Pista de césped en los Estados Unidos, 1887. Final de dobles masculinos en los Juegos Olímpicos de 1896. Artículo principal: Historia del tenis Las primeras referencias del tenis tienen lugar en Francia, nombrado «le Paume» (juego de palmas), dado que al principio se golpeaba la pelota con la mano. Más tarde se empezaron a utilizar raquetas. Tenis sobre hierba en Canadá, c. 1900. El tenis original se jugaba en pistas de hierba natural. Se originó en Europa a finales del siglo XVIII[cita requerida] y se expandió en un principio por los países angloparlantes, especialmente entre sus clases altas. En la actualidad, el tenis se ha globalizado y es practicado principalmente en los países de América y Europa. Desde 1926, con la creación del primer tour, es un deporte profesional. Es además, un deporte olímpico desde los Juegos Olímpicos de Atenas 1896. Reglamento del tenis La pelota puede ser golpeada por el jugador solo una vez cuando la pelota pasa a su lado de la cancha. Pero si gana un punto puede volver a golpear si está en su turno de saque. Los puntos se cuentan de la siguiente forma: el primer punto se cuenta como 15 puntos, el segundo como 30 puntos y el tercero como 40 puntos. Si se empata en 40 puntos, se considera empate y los jugadores tienen que anotar dos puntos seguidos para ganar el juego. El partido se gana con 2 sets o con 3 sets dependiendo si se juega al mejor de 3 o al mejor de 5. La pelota solo puede rebotar una vez antes de ser impactada por el jugador. Ningún jugador podrá tocar la red; esto se considera punto perdido para el mismo. Los jugadores van cambiando de lado de pista a lo largo del partido, en los juegos determinados, al final del primero, tercero, quinto juego y a veces hay sexto juego, esto se debe a la puntuación del partido. Medidas de la cancha Medidas de la cancha de tenis Artículo principal: Cancha de tenis El tenis se juega en una cancha (llamada pista en España) de forma rectangular, de 23,77 metros (78 pies) de longitud por 8,23 m (27 pies) de ancho. Para el partido de dobles la cancha será de 10,97 m (36 pies) de ancho. Las líneas que limitan los extremos de la pista se denominan líneas de fondo y las líneas que limitan los costados de la pista se denominan líneas laterales. A cada lado de la red y paralela a ella, se trazan dos líneas entre las líneas laterales a una distancia de 6,40 m a partir de la red. Estas líneas se llaman líneas de saque o de servicio. A cada lado de la red, el área entre la línea de servicio y la red queda dividida por una línea central de servicio en dos partes iguales llamadas cuadros de servicio. La línea central de servicio se traza paralelamente a las líneas laterales de individuales y equidistante a ellas. Cada línea de fondo se divide en dos por una marca central de 10 cm de longitud, que se traza dentro de la pista y es paralela a las líneas laterales de individuales. La línea central de servicio y la marca central son de 5 cm de anchura. Las otras líneas de la pista son de entre 2,5 y 5 cm de anchura, excepto las líneas de fondo que pueden ser de hasta 10 cm de anchura. Todas las medidas de la pista se toman por la parte exterior de las líneas. Todas las líneas de la pista tienen que ser del mismo color para que contrasten claramente con el color de la superficie. El tenis puede ser practicado en distintas superficies; ya conocemos la primera en que se comenzó a jugar el tenis, hierba natural. Existen también otras que con el paso del tiempo se han ido popularizando, como son las pistas duras, tierra batida, tenis quick... Estas últimas son elegidas para la apertura de clubes, ya que son las más económicas. Actualmente no figura ninguna competición profesional que se realiza en dicha superficie. Las pistas de tenis que tienen un mayor coste económico en su mantenimiento son las de tierra batida, seguidas de las de hierba natural. La red del tenis La pista está dividida en su mitad por una red suspendida de una cuerda o un cable metálico, cuyos extremos están fijados a la parte superior de dos postes o pasan sobre la parte superior de dos postes a una altura de 1,07 metros. La red está totalmente extendida, de manera que llena completamente el espacio entre los dos postes de la red, y la malla es de un entramado lo suficientemente pequeño para que no pase la pelota de tenis. La altura en el centro de la red es de 0,914 m, en donde está sostenida mediante una faja. Hay una banda cubriendo la cuerda o el campo metálico y la parte superior de la red. La faja y la banda son blancas por todas partes. El diámetro máximo de la cuerda o cable metálico es de 8 mm. La anchura máxima de la faja es de 5 cm. La banda es de entre 8 y 10,35 cm de anchura a cada lado. Puntuación El marcador de un partido de tenis. Un partido de tenis está compuesto por parciales (sets en inglés). El primero en ganar un número determinado de sets es el ganador. Cada set está integrado por juegos. En cada juego hay un jugador que saca, el cual se va alternando. A su vez, los juegos están compuestos de puntos, que son 15, 30, 40 y el punto de juego. La cuenta de los puntos es bastante particular: cuando un jugador gana su primer punto, su tanteador es 15, cuando gana 2 puntos, 30, y cuando gana 3 puntos, 40. Por ejemplo, si el sacador de ese juego lleva ganados 3 puntos y el receptor 1 punto, el marcador es de 40-15. Siempre se nombra en primer lugar la puntuación del sacador. Cuando ambos jugadores empatan a 40, se dice que hay deuce o «iguales». El primer jugador o equipo que gane un punto después del deuce, logra una «ventaja» y, en caso de ganar el siguiente punto, se lleva el juego. De lo contrario se vuelve a estar en deuce hasta que se logre la diferencia de dos puntos. El primer jugador que supera los 40 puntos gana el juego. El primero en ganar 6 juegos, con una diferencia mínima de 2 con respecto a su rival, es el ganador del set; en caso de que ninguno de los dos jugadores o equipos tenga una ventaja de dos juegos al llegar a seis (6-5), se juega un juego más para conseguir la diferencia de 2 juegos (7-5). De darse el empate (6-6), se jugará un juego de «desempate» o tie-break. Tie break Si el reglamento del torneo establece un tope de juego, o sea si hay un empate entre dos jugadores en un set, entonces habrá que jugar un juego especial denominado Tie break, «decisivo» o «desempate», en el cual el resultado se decide mediante puntos correlativos (uno-uno, dos-dos, tres-tres, etc.) hasta llegar a 7 tantos, con diferencia de 2. Si se llega a 7 puntos sin diferencia de 2 (por ejemplo: 7-6), el juego se prolongará hasta que uno de los dos jugadores obtenga dicha diferencia y consiga la victoria. La anotación de un set que se ha decidido en el tie break será 7-6, acompañada abreviadamente por el número de puntos obtenidos por el perdedor del mismo entre paréntesis. P. ej., si el jugador perdió el juego decisivo por 7-3, la anotación del set será: 7-6 (3). Origen del tanteo La inusual y exclusiva forma de anotar el tanteo en tenis (y otros deportes de raqueta inspirados en él) proviene del sistema sexagesimal. Al parecer, antiguamente el tanteo de cada juego se llevaba con un reloj y por cada punto obtenido se movía la aguja un cuarto de vuelta. Así, con el primer punto la aguja se desplazaba al 15, con el segundo al 30, con el tercero al 45 y con el cuarto se cerraba el círculo y se concluía el juego. Con el tiempo y por economía del lenguaje, el parcial 45 se convirtió en 40, dando origen al actual modo de llevar el tanteo: 15, 30, 40 y juego. Históricamente, esa puntuación de 15-30-40-juego y luego seis juegos para un set viene de la astronomía antigua en la que se usaba un sextante para medir la elevación del sol. El sextante se divide en 4 partes (15°-30°-45°-60°), y es la sexta parte de una circunferencia de 360° (6 juegos = 1 Set = 360°). La puntuación corresponde a las dichas mediciones que eran en esa época tan usuales como para nosotros el sistema decimal. Luego el 45, que en inglés es forty five, se dejó en 40 (forty) para comodidad del árbitro. Técnica y golpes Técnica de frenado en tenis. El tenis es un deporte que requiere que los jugadores dominen técnicas como son: golpes, empuñaduras, efectos, posiciones corporales y desplazamientos corporales, además de necesitar resistencia física para aguantar peloteos largos o fuertes. Durante el partido se utilizan muchos tipos de golpes, cada uno con sus respectivas técnicas; los golpes son: el saque, la derecha (forehand), el revés, el globo, la volea, el slice, la dejada y el remate (smash) y dentro de cada uno de ellos hay gran variedad de diferentes tipos de golpes detallándose la dirección del golpe, la posición en la pista, la potencia del golpe, etc. Saque Ivo Karlović sacando con gran potencia. El saque es el golpe más importante del tenis, ya que este da comienzo al punto, y su correcta aplicación puede permitir a la persona que saca quedar en una posición de ventaja tras la devolución o bien lograr un saque ganador o ace (punto ganado sin que el rival impacte la pelota), o que tras el impacto del adversario la pelota no llegue a pasar la red o ésta se vaya fuera de los límites de los ejes (en cuyo caso no se denomina ace). Al tener buen saque, el tenista aprende a acabar mejor los golpes efectuados sin que la pelota toque suelo y pudiendo dificultarle al contrincante marcarle un punto después de que le hagan una cortada. El segundo saque suele realizarse buscando mayor seguridad en el resultado. Para ello suelen hacerse saques liftados, cortados o con kick (que es lo mismo que liftado) para provocar la mayor dificultad al rival, ya que esos saques suelen ser peligrosos al cambiar la dirección de la pelota o la rapidez después del bote. Uno de los cambios de cómo se hace cada saque es que el cortado, intenta hacer que la pelota corra por las cuerdas de un lado al otro de la raqueta en posición vertical, y el liftado es igual pero en forma horizontal. Entre los mejores sacadores de la denominada Era Abierta se encuentran los croatas Goran Ivanišević e Ivo Karlović, los estadounidenses Pete Sampras, Andy Roddick y John Isner, el neerlandés Richard Krajicek, el canadiense Milos Raonic y el suizo Roger Federer. Derecha o drive Juan Martín del Potro preparando una derecha. El drive o derecha es el golpe básico. Consiste en golpear la pelota después del bote, de forma directa, del mismo lado del brazo hábil del jugador. Para la mayoría de los jugadores es el arma principal para ganar un punto y el de mayor control. Para realizar un correcto forehand, se debe estar perfilado a la pelota; en el caso de un diestro, el golpe empieza en el lado derecho del cuerpo, continuando a través del mismo hasta impactar la pelota y terminando en la parte izquierda del cuerpo. El impacto debe darse en la zona comprendida entre hombro y cadera, y el movimiento se realiza de abajo arriba. Una vez que la pelota impacta en la raqueta, el tenista pasa el brazo derecho adelante cerrando el golpe. En el momento que llega la pelota en altura, el tenista toma la decisión de dar un golpe potente o cruzarla a algún lado. Es el golpe más fácil de aprender, al ser también el más natural. Hay un error común en países de habla hispana de llamar drive al forehand o derecha. En los diccionarios de tenis en inglés drive es el golpe que se hace desde el fondo de la cancha con potencia, luego del bote. Por lo tanto hay drive de derecha y drive de revés. Entre los mejores golpeadores de derecha ya sea por potencia, precisión, o ambas, se encuentran Pete Sampras, Roger Federer, Ivan Lendl, Juan Martín del Potro y Fernando González. Volea Stefan Edberg, uno de los más grandes intérpretes del saque y volea, donde el saque es esencial. La volea o golpe de aire es el golpe que se realiza antes que la pelota rebote en el suelo. Es ejecutado normalmente cerca de la red para definir un punto. A su vez podemos encontrarnos dos tipos de voleas, profundas y cortas. Cada una de ellas dependerá del lugar en el que se produzca el bote de la pelotaː las voleas cortas se producen cuando la pelota bota a la altura del cuadro de saque y las profundas, cuando el bote se produce detrás del cuadro de saque. Debido a la mayor cercanía del jugador al contrincante, es un golpe que requiere ser realizado con gran velocidad y reflejo. La raqueta debe encontrarse en todo momento al frente y alto. El golpe se realiza llevando adelante el pie opuesto al lado donde se va a impactar la pelota, simultáneamente con el perfilado del cuerpo, de modo que la raqueta pueda hacer un breve movimiento atrás para impactar la pelota adelante y de arriba abajo, aprovechando la fuerza que la propia pelota trae, en lo posible sin aplicar energía extra y sin flexionar la muñeca. El golpe que se utiliza para llegar a la red en una jugada se denomina approach según la trayectoria del golpe se realizará sin dificultad. Entre los mejores voleadores de la historia se encuentran Stefan Edberg, John McEnroe, Boris Becker, Patrick Rafter, Pete Sampras, Roger Federer, Pat Cash y Radek Štěpánek. Revés Revés a una mano. El revés o backhand es el golpe al lado opuesto al forehand o derecha. A pesar de ser un golpe de mecánica natural, suele ser uno de los que más cuesta llegar a dominar cuando se empieza a jugar al tenis. Es muy importante la posición del cuerpo, que debe ser colocado de perfil, utilizándose como técnica para ello bajar el hombro para apuntarlo en dirección a la red, mientras que el brazo derecho en los diestros e izquierdo en los zurdos, pasa sin ser flexionado por debajo del mentón, para ubicarse atrás antes de retornar para impactar la pelota, siempre delante del cuerpo, al igual que en el forehand o derecha, que el peso del cuerpo se traslade de atrás adelante en el momento de impactar la pelota. Décadas atrás, el golpe de revés se enseñaba a impactarlo tomando la raqueta con una sola mano (unos grandes exponentes de esta técnica fueron Ivan Lendl, Gustavo Kuerten, Ken Rosewall, Guillermo Vilas, Gastón Gaudio, Stefan Edberg, Pete Sampras y Boris Becker. En la actualidad lo son Stanislas Wawrinka, Roger Federer y Richard Gasquet. Hoy en día el revés a dos manos está ganando cada vez más terreno: jugadores como Rafael Nadal, Juan Martín del Potro, Novak Djokovic y los ya retirados David Nalbandián y André Agassi hacen uso de este golpe. Vale la pena recordar a Jimmy Connors y Björn Borg, cuyos golpes de revés a dos manos inspiraron la popularización que actualmente tiene esta forma de golpeo. Dejada La dejada o drop shot (del inglés drop, dejar caer) es un golpe en el que se le resta potencia a la pelota con la intención de que caiga lo más cerca posible de la red, del lado contrario. Se realiza habitualmente de revés o backhand, aunque es posible hacerlo también de derecha o forehand. La preparación del golpe es similar a la preparación del forehand (o revés), debiendo hacerse en el último momento, para sorprender al contrincante, que espera un tiro al fondo. Al momento del impacto, en lugar de realizarse el swing amplio, la raqueta debe caer de manera perpendicular a la pelota, con un giro de muñeca para producir el efecto de goteo que hará a la pelota caer y pasar bien la red. Se utiliza generalmente cuando el tenista rival se encuentra muy por detrás del fleje del fondo de la pista, y no es un golpe que se deba usar con mucha regularidad, ya que el objetivo es sorprender al rival. Resulta vital que el golpe sea bajo y corto, para así evitar que el contrincante llegue a la pelota antes del segundo bote, porque de lo contrario le quedará un golpe fácil cerca de la red. Simultáneamente, el jugador puede acercarse a la red para prevenir una contradejada. La dejada fue especialmente utilizada a principios de la Era Abierta, especialmente la versión de volea. Esto se ve explicado por el menor desarrollo físico de los tenistas a mediados del siglo XX, lo cual les hacía más difícil llegar desde el fondo de la pista, a lo que hay que sumar el auge de la estrategia saque-red en aquellos tiempos. Destacan aquí jugadores como los australianos Rod Laver o Ken Rosewall, el sueco Björn Borg, o el estadounidense John McEnroe. A partir de las décadas de los 80 y 90, se aprecia un progresivo descenso en el uso de este golpe, debido a la mejora de las condiciones físicas de los tenistas, y el cambio a raquetas de materiales compuestos, que redujeron la popularidad del saque-red en favor de un estilo más centrado en la línea de fondo. Sin embargo, los últimos años han visto un renacimiento de la dejada en el tenis profesional, principalmente debido a la tendencia de los jugadores a jugar notablemente detrás de la línea de fondo, haciendo que el golpe recupere parte de su sentido perdido y permita ahora romper el ritmo en los puntos de forma más efectiva. Aunque jugadores de la talla de Roger Federer, Rafael Nadal, o Stan Wawrinka dominaron el golpe durante sus carreras, la llegada de Carlos Alcaraz a principios de la década de 2020 es la que revolucionó verdaderamente el juego, provocando un aumento radical en el uso de la dejada en todo el circuito. Contradejada La contradejada es la respuesta a una dejada, a la que el jugador llega poco antes del segundo rebote. Como habitualmente la pelota se encuentra muy baja y cerca de la red, no es posible recurrir a un golpe potente, por lo cual, el jugador solo tiene la opción de dar un golpe suave sobre el fondo, es decir, una nueva dejada de respuesta, esta vez hecha desde cerca de la red. Remate o smash El Smash de Rafael Nadal. El smash o remate es un golpe que se efectúa sobre la cabeza con un movimiento similar al saque. En general se puede golpear con gran fuerza de manera relativamente segura y es a menudo un tiro definitorio. La mayoría son realizados cerca de la red o a mitad de la pista antes del pique de la pelota. Suele ser la respuesta a un globo realizado por el oponente que no tuvo la suficiente altura. También puede hacerse desde la línea de base tras el pique, aunque es menos definitorio. Es un golpe alto, realizado de arriba abajo, antes de que la pelota bote, o después de que lo haga, pero únicamente en caso de que este lleve una parábola más vertical que horizontal. Para que sea efectivo, es indispensable que sea muy potente y que no dé oportunidad de respuesta al contrario, ya que se trata siempre de un golpe de definición. Se realiza cuando la pelota viene muy alta, a la altura del brazo extendido del jugador. El golpe se prepara perfilando el cuerpo, llevando la raqueta atrás y colocándola detrás de la nuca, mientras la mano libre apunta a lo alto, hacia la pelota. En el momento del impacto, el pie trasero pasa adelante, al mismo tiempo que la raqueta sale de atrás del cuerpo en un movimiento similar al del saque. Al momento de impactar la pelota, la muñeca debe flexionarse abajo, terminando el golpe de manera parecida al saque. La pelota tiene que rebotar antes de que el contrario la devuelva. Globo El globo es un golpe sencillo que se utiliza para pasar la pelota por encima del jugador contrario. Se ejecuta tanto de derecha como de revés. Incluso existe (su uso no es tan frecuente) la volea globeada. Su ejecución consiste en impactar arriba la pelota (a diferencia de las demás ejecuciones que se hacen adelante); con esto se logra pasar a un jugador que está parado en la zona de la volea o bien hacer un juego defensivo de fondo. Efectos sobre la pelota Ejecución de una «gran Willy» El desarrollo de la técnica ha llevado a aplicar con diferentes modos de golpe, diferentes efectos sobre la pelota, de tal modo que dichos efectos y las consiguientes variaciones de trayectoria y velocidad dificulten la devolución del rival. Los mismos se obtienen con diferentes empuñaduras y formas de impactar la pelota. Cada uno de los golpes, con sus respectivas trayectorias según el efecto aplicado sobre la pelota y sus respectivos tipos de bote contra la superficie, se explican a continuación. Golpe liftado Pelota arrastrada de abajo arriba, para producir el liftado. El golpe liftado o con top spin (literalmente en inglés «efecto desde arriba») se ejecuta mediante la aplicación de una trayectoria de la raqueta anterior y posterior al golpeo (swing) de abajo arriba. Antes del impacto con la pelota, la cabeza de la raqueta está por debajo de la trayectoria de la misma y, tras el impacto, el swing finaliza por encima de esa altura. Este golpe impone a la pelota un efecto de rotación adelante, que hace que tras el bote, esta salga despedida arriba y adelante, obligando al rival a golpear bien a una altura superior a la normal ,muchas veces por encima del hombro,, lo cual le impide ejecutar un golpe agresivo, o bien le obliga a «atacar» la pelota en su trayecto ascendente tras el bote, lo que supone un mayor riesgo de error. Este tipo de golpe es el más usado, pues tiene, además, la ventaja para el jugador que lo ejecuta de ofrecerle un «margen de seguridad» más amplio, puesto que la trayectoria que se le impone a la pelota es más elíptica y esta pasa a una mayor altura sobre la red que con el golpeo plano o el cortado. El motivo por el cual logramos imprimir una velocidad angular es la fricción que existe (durante un período muy pequeño) entre la pelota y las cuerdas de la raqueta, estas últimas colisionan casi tangencialmente respecto de la pelota. Esto último es lo que produce un momento de la fuerza sobre la pelota, la cual pasa a tener energía cinética de traslación más energía cinética de rotación. Dicha velocidad angular supone una fricción aerodinámica, de modo que la pelota va empujando (por su parte posterior) constantemente el aire que se le presenta como obstáculo arriba (la fluidodinámica de Bernouilli demuestra este suceso), por la tercera ley de Newton (acción-reacción), la pelota recibe la misma respuesta por parte del viento pero de sentido contrario. Por esto mismo, la pelota adoptará una trayectoria parabólica mucho más pronunciada que en el caso de una colisión plana con las cuerdas. Finalmente, la velocidad angular que aún conserve la pelota al colisionar con la superficie, se verá reducida considerablemente por la fricción estática con el suelo, haciendo una fuerza atrás, y de nuevo por la tercera ley de Newton, el suelo se la devolverá en sentido pitido. Golpe cortado Pelota «cortada» de arriba abajo, para producir el cortado. El cortado (en inglés slice, cortar en rebanadas, o backspin, rotar atrás) es el efecto inverso al liftado: la pelota adquiere una rotación «atrás» que la lleva a adoptar una trayectoria más baja al botar, obligando al contrario a tener que impactarla más bajo. El efecto se obtiene impactando a la pelota desde arriba y estirando el brazo como si se atravesara la pelota y se la siguiera en su recorrido. Esto hace que la pelota rote de arriba abajo vista desde atrás (nótese que esto no cambia el sentido de rotación de la pelota si el oponente puso efecto liftado a la suya). Debido al efecto Magnus, que en este caso imprime una fuerza neta a la pelota dirigida arriba, un golpe cortado hace que la pelota parezca «flotar» y vuele más lenta. La menor velocidad de la pelota hace que uno de los usos del cortado sea para tener más tiempo de volver a ponerse en posición o acercarse a la red. Golpe plano El golpe plano es aquel que se realiza sin imprimirle ningún efecto a la pelota. En general es muy efectivo cuando se realiza desde una altura mayor a la red, de arriba abajo. Los golpes que pueden ejecutarse de esta manera son el forehand o derecha, el revés, el saque y la volea (esta debe ejecutarse de manera plana «siempre», exceptuando las veces que la volea se quiera muy suave, es decir, haciendo un drop shot de volea. Clasificaciones El tenista estadounidense Pete Sampras, ex-numero uno del mundo. En la actualidad los jugadores están ordenados en un tipo de clasificación llamado «Sistema de entradas» (Entry Ranking), en el cual se suman los puntos obtenidos por los jugadores en las últimas 52 semanas, o sea, aproximadamente un año. Por lo tanto, al finalizar cada semana, se le restan a cada jugador los puntos obtenidos en esa misma semana del año anterior, y se le suman los ganados en la semana actual. ATP Artículo principal: Ranking ATP La ATP (Asociación de Tenistas Profesionales) es el organismo directivo del circuito masculino de tenis profesional a nivel mundial. El circuito tiene 66 torneos en 32 países que reparten entre 20 millones y 325 000 dólares en premios. La ATP también organiza los Torneos Challenger, donde muchos de los jóvenes jugadores ganan sus primeros partidos y torneos. Cada año se organizan alrededor de 90 eventos a nivel mundial y sus premios en metálico varían entre 50 000 y 125 000 dólares. Los puntos se consiguen en función de la categoría del torneo y la posición resultante en él. Los torneos más valorados son los cuatro Grand Slams, seguido de la Barclays ATP World Tour Finals y las ATP Master Series. La siguiente tabla resume las puntuaciones otorgadas en 2009: El nuevo sistema de puntuación otorga estos puntos: 1) Grand Slams: Australian Open, Roland Garros, Wimbledon, US Open. Puntuación: 2000 puntos (ganador) 1200 (finalista), 720 (semifinalista) 360 (cuartofinalista) 180 (octavofinalista) 90 (3.ª ronda) 45 (2.ª ronda) 10 (1.ª ronda). 2) ATP Serie 1000: Indian Wells, Miami, MonteCarlo (no es obligatorio jugarlo), Roma, Madrid, Canadá, Cincinnati, Shanghái, París. Puntuación: 1000 puntos (ganador), 600 (finalista), 360 (semifinalista), 180 (cuartofinalista), 90 (octavofinalista), 45 (2.ª ronda), 10 (1.ª ronda). En Miami e Indian Wells al haber una ronda más la 1.ª ronda son 10, la 2.ª 25 y la 3.ª 45. 3) ATP Serie 500: Róterdam, Dubái, Acapulco, Memphis, Barcelona, Hamburgo, Washington, Pekín, Tokio, Basilea, Valencia. Puntuación: 500 puntos (ganador), 300 (finalista), 180 (semifinalista), 90 (Cuartofinalista), 45 (octavofinalista), 0 (1.ª ronda). Sería obligatorio para los jugadores top jugar cuatro de estos torneos. 4) ATP Serie 250: EJ. Viña del Mar. Puntuación: 250 (ganador), 150 (finalista), 90 (semifinalista), 45 (cuartofinalista), 20 (octavofinalista), 0 (1.ª ronda). La copa Davis repartirá puntos como un torneo ATP Serie 250. El ranking resultará de la suma de los siguientes torneos: los cuatro Grand Slam, ocho de los Masters 1000, cuatro mejores torneos 500 y dos mejores 250 o challengers. Puntuaciones otorgadas en 2009[cita requerida] Categoría del torneo Número de torneos Puntos para el ganador Grand Slam 4 2000 ATP World Tour Finals 1 Hasta 1500* ATP Masters 1000 9 1000 ATP World Tour 500 11 500 ATP World Tour 250 41 250 ATP Challenger Series 115 75-125 Torneos Future 420 17-33 Nota: Si acaba invicto, se obtiene 200 por cada triunfo en partido del round robin, +400 por victoria en semifinales, +500 por victoria en la final. Clasificación de individuales masculino El suizo Roger Federer fue el número uno del mundo desde el 3 de febrero de 2004 hasta el 17 de agosto de 2008, estableciendo un récord de 237 semanas consecutivas al frente del ATP Ranking. El español Rafael Nadal, que escoltó al suizo durante 160 semanas desde el 25 de julio de 2005, cuando ocupó el número dos, se convirtió en el número uno del mundo desde el 18 de agosto de 2008 hasta el 5 de julio de 2009, fecha en que Roger Federer volvió a ocupar el primer lugar tras ganar su sexto Wimbledon en 2009. Rafael Nadal recuperó el número uno tras ganar el trofeo de Roland Garros de 2010 a Robin Söderling quedando Roger Federer en el número dos. Después del US Open 2010, Roger Federer pasa a ocupar el puesto tres y Novak Đoković sube hasta el puesto número dos. Nadal se mantuvo en el primer lugar hasta Wimbledon de 2011, en donde Djokovic derrotó en la final a Rafa y le arrebató el número uno del ranking mundial. Después de ganar la final contra Andy Murray en Wimbledon 2012, Federer recupera el número uno después de dos años, superando a Pete Sampras en semanas como número uno. Después de noviembre de 2012, Novak Đoković vuelve a ocupar el primer lugar hasta finales del año 2013, donde Rafael Nadal volvió a ser número uno. Posteriormente en julio de 2014 Novak Đoković recuperó el número uno luego de disputarse la final de Wimbledon con Roger Federer, quien se quedó con el número tres, dejando a Rafael Nadal en segundo lugar. Roger Federer en un movimiento de saque. Novak Djokovic Rafael Nadal Carrera de Campeones Desde enero de 2000 existe una clasificación paralela, llamada Carrera de Campeones (ATP Champions Race), en la cual se suman los puntos conseguidos en los torneos del año en curso, sin contar puntos del año anterior, y se utilizaba para completar la lista de clasificados a la ATP World Tour Finals (los ganadores de los cuatro torneos de Grand Slam se clasifican automáticamente). Para aquellos tenistas ubicados en el Top 30, se suman los puntos obtenidos en los cuatro torneos de Grand Slam, los nueve torneos de la Serie Masters y cinco torneos adicionales, incluido la participación en Copa Davis de aquellos jugadores que participen en el Grupo Mundial o en los playoffs. En caso de no haber actuado en algún torneo de Grand Slam o de la Serie Masters, el jugador sumaba los puntos obtenidos en algún torneo de la Serie Internacional. La Carrera de Campeones refleja el desempeño de los jugadores en el año que está en curso. A final de temporada las clasificaciones de ambos rankings son muy similares especialmente en los primeros puestos, encontrándose algunas diferencias ya que los torneos Challenger y Future no suman puntos en la Carrera de Campeones pero si en el Ranking ATP. Desde 2009 cambia el nombre para la clasificación de dobles, denominándose ATP Doubles Team Rankings. WTA Artículo principal: Ranking WTA La WTA (Asociación Femenina de Tenis) (en inglés, Womens Tennis Association), es la organización que rige los torneos y el circuito profesional del tenis femenino a nivel mundial. A modo comparativo, la WTA es al tenis femenino lo que la ATP al tenis masculino. La Asociación organiza el calendario y designa las sedes oficiales de los torneos del circuito femenino, también llamado como WTA Tour. En 2005 la Asociación Femenina de Tenis cambió el nombre del WTA Tour por el de The Sony Ericsson WTA Tour debido a un contrato firmado de patrocinio con la firma nipona-sueca de teléfonos móviles y accesorios, Sony Ericsson. Clasificación de individuales femenino Martina Navratilova resalta por ser una de las mejores tenistas femeninas de la historia, llegando a acumular 128 550 puntos en el sistema de clasificación histórica hasta su retiro en 2006. Martina superó las marcas de Chris Evert y Steffi Graf, quienes compitieron hasta 1989 y 1999 respectivamente. Otras tenistas destacadas en el ranking femenino en la historia han sido: Margaret Smith Court, Billie Jean King, Helen Wills Moody, Suzanne Lenglen, Evonne Goolagong, Serena Williams, Martina Hingis y Mónica Seles. Marcas Las principales marcas del tenis son: El partido más largo de la historia fue el encuentro disputado en Campeonato de Wimbledon 2010 entre el francés Nicolás Mahut y el estadounidense John Isner con una duración de once horas, cinco minutos y veintitrés segundos. John Isner se llevó el partido de tenis más largo de la historia (6-4, 3-6, 6-7, 7-6 y 70-68). Isner y Mahut siguieron la misma tónica durante los tres días que duró el encuentro. Muy sólidos con su saque, se plantaron del 59-59 hasta el 68-68 sin conceder ningún punto de quiebre. La marca del saque más rápido masculino la sustenta el australiano Samuel Groth con una velocidad de 263 km/h, en el Challenger de Busan 2012. Georgina García Pérez mantiene la marca femenina del saque más rápido con 136 mph (220 km/h) realizado en Budapest en 2018. Guillermo Vilas mantiene la marca masculina de la mayor cantidad de victorias consecutivas. Desde 1977 son 46. Helen Wills Moody (1905-1998) mantiene la marca femenina con 158 victorias consecutivas, obtenidas entre 1927 y 1933. La final más larga de un Grand Slam se dio en el Abierto de Australia 2012, la cual duró cinco horas con cincuenta y tres minutos. La final la disputaron el serbio Novak Djokovic y el español Rafael Nadal. La victoria fue para el serbio en cinco mangas. Novak Djokovic es el tenista con mayor cantidad de títulos de Grand Slam ganados en la categoría masculina (24). Margaret Court es la tenista con mayor cantidad de títulos de Grand Slam ganados en la categoría femenina, igualada con Djokovic en masculino (veinticuatro en total). Serena Williams es la segunda tenista con mayor cantidad de títulos de Grand Slam ganados (veintitrés). Novak Djokovic ostenta el mayor tiempo como número uno del Ranking Mundial ATP con 418 semanas acumuladas. Roger Federer es el tenista que más semanas consecutivas ha estado como número 1 de la clasificación con 237 semanas. Steffi Graf ostenta el mayor tiempo como número uno del Ranking Mundial WTA con 377 semanas acumuladas. Además está empatada con Serena Williams como las tenistas con más semanas consecutivas como número 1 con 186 semanas. Novak Djokovic es el tenista que más veces ha finalizado el año como número 1 con un total de 8. Steffi Graf es la tenista que más veces ha finalizado el año como número 1 con un total de 8. Novak Djoković ostenta el récord de haber ganado el Masters Tennis Cup (Copa de Maestros) ,actualmente conocido como ATP World Tour Finals, proclamándose campeón en siete ocasiones. Martina Navrátilová ostenta el récord de haber ganado la WTA Tour Championships (Copa de Maestras) ,actualmente conocido como WTA Finals, proclamándose campeona en ocho ocasiones. Rafael Nadal es el único tenista de la historia (tanto en hombres como en mujeres) en ganar más de 10 torneos del mismo Grand Slam (catorce títulos en Roland Garros 2005-2008, 2010-2014, 2017-2020, 2022). Steffi Graf es la única tenista en obtener un Golden Slam en un año, en 1988. Novak Djoković es el tenista con más títulos Master 1000 (40), y el único que los ha ganado todos al menos 2 veces. Novak Djoković es el único tenista que ha ganado al menos 3 veces cada Grand Slam y el único en la era abierta en ganar 4 seguidos, aunque en año no natural. Novak Djoković es el tenista con mayores ingresos producto de su premiación en torneos en toda la historia."
ksampletext_wikipedia_spor_futbol: str = "Fútbol. El fútbol, futbol o balompié (del inglés británico football) es un deporte de equipo jugado entre dos conjuntos de once jugadores cada uno, mientras los árbitros se ocupan de que las normas se cumplan correctamente. Es, ampliamente, considerado el deporte más popular del mundo, pues lo practican unas 270 millones de personas. También se le conoce como fútbol 11 por el número de jugadores de un equipo o fútbol asociación, nombre derivado de la «Asociación del Fútbol» (The Football Association), primera federación oficial del mundo en este deporte y que utilizó ese nombre para distinguirlo de otros deportes que incluyen la palabra «fútbol» o «futbol». En algunos países de habla inglesa, también se lo conoce como soccer, abreviatura de association, puesto que el nombre de football en esos países se asocia mayoritariamente a otros deportes con esa denominación (principalmente en Estados Unidos, donde el nombre football aplica para el fútbol americano, un deporte totalmente distinto). El terreno de juego es rectangular de césped natural o artificial, con una portería o arco a cada lado del campo. Se juega mediante una pelota que se debe desplazar a través del campo con cualquier parte del cuerpo que no sean los brazos o las manos, y mayoritariamente con los pies (de ahí su nombre). El objetivo es introducirla dentro de la portería o arco contrario, acción que se denomina marcar un gol. El equipo que logre más goles al cabo del partido, de una duración de 90 minutos, es el que resulta ganador del encuentro. El juego moderno fue reinventado en Inglaterra tras la formación de la Football Association, cuyas reglas de 1863 son la base del deporte en la actualidad. El organismo rector del fútbol es la Federación Internacional de Fútbol Asociación, más conocida por su acrónimo FIFA. La competición internacional de balompié más prestigiosa es la Copa Mundial, organizada cada cuatro años por dicho organismo. Este evento es el más famoso y el que cuenta con mayor cantidad de espectadores del mundo, doblando la audiencia de los Juegos Olímpicos. Naturaleza del juego Un partido de fútbol en el HSH Nordbank Arena, en Hamburgo, Alemania. El fútbol se juega siguiendo una serie de reglas, llamadas oficialmente reglas de juego. Este deporte se practica con una pelota esférica (de cuero u otro material con una circunferencia no mayor a 70 cm y no inferior a 68 cm, y un peso no superior a 450 g y no inferior a 410 g al comienzo del partido), donde dos equipos de once jugadores cada uno (diez jugadores de campo y un guardameta) compiten por encajar la misma en la portería rival, marcando así un gol. El equipo que más goles haya marcado al final del partido es el ganador; si ambos equipos no marcan, o marcan la misma cantidad de tantos, entonces se declara un empate. Puede haber excepciones a esta regla; véase Duración y resultado más abajo. La regla principal es que los jugadores, excepto los guardametas, no pueden tocar intencionalmente la pelota con sus brazos o manos durante el juego, aunque deben usar sus manos para los saques de banda. En un juego típico, los jugadores intentan llevar la pelota hasta la portería rival, lo que se denomina gol, a través del control individual de la misma, conocido como regate, o de pases a compañeros o tiros a la portería, la cual está protegida por un guardameta. Los jugadores rivales intentan recuperar el control de la pelota interceptando los pases o quitándole la pelota al jugador que la lleva; sin embargo, el contacto físico está limitado. El juego en el fútbol fluye libremente, y se detiene solamente cuando la pelota sale del terreno de juego o cuando el árbitro decide que debe detenerse. Luego de cada pausa, se reinicia el juego con una jugada específica. Al final del partido, el árbitro compensa el tiempo total en minutos que se suspendió el juego en diferentes momentos. Daniel Souček realizando un saque de banda en un partido clasificatorio de Eurocopa Sub-21 entre la República Checa y Grecia. A nivel profesional, en la mayoría de los partidos se marcan solo unos pocos goles. Por ejemplo, durante la temporada 2006/07 de la Primera División de España, la liga de fútbol española, se marcó un promedio de 2,48 goles por partido. Las reglas no especifican ninguna otra posición de los jugadores aparte de la del guardameta, portero o arquero, pero con el paso del tiempo se han desarrollado una serie de posiciones en el resto del campo. A grandes rasgos, se identifican tres categorías principales: los delanteros, cuya tarea principal es marcar los goles; los defensas o defensores, ubicados cerca de su portería, quienes intentan frenar a los delanteros rivales; y los centrocampistas, mediocampistas o volantes, que manejan la pelota entre las posiciones anteriores. A estos jugadores se los conoce como jugadores de campo, para diferenciarlos del guardameta. A su vez, estas posiciones se subdividen en los lados del campo en que los jugadores se desempeñan la mayor parte del tiempo. Así, por ejemplo pueden existir centrocampistas derechos, centrales (de contención) e izquierdos. Los diez jugadores de campo pueden distribuirse en cualquier combinación. Por ejemplo, puede haber cuatro defensas, cuatro centrocampistas y dos delanteros (4-4-2); o tres defensas, cuatro centrocampistas y tres delanteros (3-4-3), y la cantidad de jugadores en cada posición determina el estilo de juego del equipo: más delanteros y menos defensas (por ejemplo 3-3-4) creará un juego más agresivo y ofensivo, mientras que lo contrario (por ejemplo 5-3-2) generará un juego más lento y defensivo. Aunque los jugadores suelen mantenerse durante la mayoría del tiempo en una posición, hay pocas restricciones acerca de su movimiento en el campo. El esquema de los jugadores en el terreno de juego se llama la formación del equipo, y esta, junto con la táctica, es trabajo del entrenador. Posición táctica de los jugadores Guardameta, arquero o portero Artículo principal: Guardameta (fútbol) El guardameta, también conocido como portero, arquero o golero, es el jugador cuyo principal objetivo es evitar que la pelota entre a su meta durante el juego, acto conocido como gol. El guardameta es el único jugador que puede tocar la pelota con las manos durante el juego activo, aunque solo dentro de su propia área. Cada equipo debe presentar un único guardameta en su alineación. En caso de que el jugador deba abandonar el terreno de juego por cualquier motivo, deberá ser sustituido por otro futbolista, ya sea uno que se encuentre jugando o un sustituto. Este tipo de jugadores deben llevar una vestimenta diferente a la de sus compañeros, sus rivales (incluido el guardameta) y el cuerpo arbitral. Por lo general suelen llevar el número 1 estampado sobre la camiseta. Defensa Artículo principal: Defensa (fútbol) El defensa, también conocido como defensor, es el jugador ubicado una línea delante del guardameta y una por detrás de los centrocampistas, cuyo principal objetivo es detener los ataques del equipo rival. Generalmente esta línea de jugadores se encuentra en forma arqueada, quedando algunos defensas más cerca del guardameta que los demás. Si es solo un jugador el ubicado más atrás, recibe el nombre de líbero; si son dos o más, reciben el nombre de zagueros. Los defensores posicionados en los costados del terreno son llamados laterales o stoppers (en el caso de que haya un líbero), y debido a su colocación (más cerca de los centrocampistas) estos pueden avanzar más en el terreno si lo desean. Para nombrarlos se agrega la zona a la palabra defensa: por ejemplo, un defensa que juega por la derecha (mirando hacia la meta rival) sería un lateral derecho. También el arquero debe proteger y dar instrucciones a los defensas. Centrocampista Artículo principal: Centrocampista El centrocampista, mediocampista o volante es la persona que juega en el mediocampo en un campo de fútbol. Entre sus funciones se encuentran las de recuperar balones, propiciar la creación de jugadas y explotar el juego ofensivo. De acuerdo a estas funciones podemos distinguir: los volantes carrileros (los que juegan más cerca de la línea de banda); los de contención, que juegan casi a la misma altura que los defensores laterales para contribuir a la defensa y pueden ser uno o dos jugadores (el apodo de Cinco que se da a estos jugadores se debe a que es el número que suelen llevar en la camiseta); y los de creación o enganches, que se sitúan entre la línea de los carrileros y delanteros (son el cerebro del ataque y se caracterizan por su habilidad). Delantero Artículo principal: Delantero Un delantero o atacante es un jugador de un equipo de fútbol que se destaca en la posición de ataque, la más cercana a la portería del equipo rival, y es por ello el principal responsable de marcar los goles. Es muy importante estar en movimiento y buscar siempre pase, es decir, desmarcarse para que le sea más fácil al que lleva la pelota pasársela. La velocidad es esencial. Actividad física El fútbol incluye una actividad física muy importante. Durante un partido de balompié profesional de 90 minutos, un jugador, dependiendo de su posición y de las dimensiones del campo, recorre entre 12 y 15 km.  También durante un partido de similares características, un futbolista pierde alrededor de 2 kg de líquidos, parte de los cuales son recuperados durante el tiempo de descanso. En partidos que se juegan con altas temperaturas, los árbitros tienen el derecho de detener el encuentro, generalmente a mediados de un período, para que los jugadores y el cuerpo arbitral se hidraten. Pero al mismo tiempo, el fútbol es uno de los deportes con mayor número de lesiones, aunque la mayoría de ellas no son de gravedad. Las lesiones más comunes ocurren en las rodillas y los tobillos, debido a los movimientos rotativos a los que son sometidos. Las roturas de meniscos y ligamentos cruzados junto a los desgarros musculares, son lesiones habituales dentro del fútbol. Las probabilidades de lesión aumentan cuando el jugador no recibe una preparación física adecuada, particularmente en un deportista aficionado, y cuando el juego se desarrolla sobre un terreno irregular. Para futbolistas profesionales o semiprofesionales es de vital importancia la presencia de un preparador físico que regule el tipo de ejercicio físico, así como la duración y regularidad del mismo. El trabajo del preparador físico se debe complementar con una correcta alimentación, donde también es recomendable la presencia de un profesional en la materia. Recepción Los jugadores de fútbol deben ser capaces de controlar los balones que reciben. La forma más fácil suele ser parar la pelota; en este caso, el balón debería ser situado en la posición ideal para el próximo toque. Los jugadores avanzados pueden usar el primer toque para hacer que el balón se mueva de forma rápida hacia donde tienen planeado correr. También pueden utilizar el primer toque para pasar el balón. Evitar el balón en vez de recibirlo puede despistar a los jugadores oponentes. Es una acción muy utilizada durante un partido, con el objetivo de recibir el balón, controlarlo y ponerlo a su servicio para desarrollar en buenas condiciones una acción posterior. La recepción en parada es aquella en la que se consigue controlar totalmente el balón en los pies, perdiendo el componente de velocidad pero aumentando la precisión en el manejo posterior del esférico. La recepción en semiparada es para conseguir que el balón pierda parte de su valor inicial. La recepción de amortiguamiento se utiliza para controlar balones aéreos, con trayectoria descendente, y se produce una amortiguación de la aceleración con el que llega la pelota. El control del balón puede ser al ras de suelo, o bien con la cara interna de la bota, o con la planta del pie, con la punta hacia arriba y encajando la pelota entre la planta y el suelo. En los balones altos, se debe conseguir llevar el balón al suelo con posibilidades de ser jugado adecuadamente. Pase Para mantener la posesión del balón es esencial tener capacidad para pasarlo en corto entre los jugadores cercanos de forma precisa y a tiempo. Los pases largos precisos permiten una mayor variedad de situaciones y un juego más directo. Generalmente, los pases que crean ocasiones de gol se les llama asistencia, aunque cualquier jugador en el campo podría hacer este tipo de jugadas, generalmente es un mediocampista ofensivo quien lo hace, es trabajo usual de los jugadores marcados con el 10 o el 8 en sus camisetas. Tiro Los jugadores deben tener un equilibrio a la hora de tirar a puerta: ni hacerlo en demasiadas ocasiones ni tampoco dejar de intentarlo cuando tienen ocasión. Los tiros deberían ser precisos y potentes, aunque generalmente no se logra esta precisión y potencia al mismo tiempo. Elegir precisión o potencia depende de la situación y de las características del jugador. La elección del lugar de la portería al cual disparar es un tema controvertido y depende de cuántos jugadores estén cubriéndola. Cuando el jugador encara solo al guardameta, los tiros deberían situarse cerca de uno de los postes. De forma ideal, el tiro debería ir dirigido a la escuadra, pero es menos difícil y también efectivo hacerlo a ras de suelo. Cuando el guardameta está demasiado adelantado, se puede intentar un globo. Historia Artículo principal: Historia del fútbol Orígenes Artículos principales: Soule, Fútbol de carnaval, Calcio florentino y Episkyros. Relieve de la Antigua Grecia (siglo IV a. C.). El episkyros es reconocido por la FIFA como el fútbol de la antigüedad. La Copa Europea de Fútbol lleva grabada esta imagen en el trofeo. La humanidad ha realizado a lo largo de su historia diversos juegos de pelota, desde la Antigüedad. Se conoce que este entretenimiento existía tanto en la cultura del Mar Mediterráneo como en América. El hallazgo más antiguo y revelador se remonta a un relieve de la Antigua Grecia (400 a. C.), donde un hombre domina una pelota sobre su muslo. Este deporte era el episkyros, que se jugaba con una pelota de cuero pintada con colores brillantes y dos equipos de 12 a 14 jugadores. La FIFA lo reconoció como una de las formas más antiguas del actual balompié. Más tarde, en Roma, el mismo recibió el nombre de harpastum. En Europa, ya a finales de la Edad Media y siglos posteriores, se desarrollaron en las islas británicas y zonas aledañas distintos juegos de equipo, a los cuales se los conocía como códigos de fútbol. Estos códigos se fueron unificando con el paso del tiempo, pero fue en la segunda mitad del siglo XVII cuando se dieron las primeras grandes unificaciones del fútbol, que más tarde dieron origen al fútbol de rugby, al fútbol americano, al fútbol australiano y al deporte que hoy se conoce en gran parte del mundo como fútbol a secas.[cita requerida] En otras zonas del mundo también se practicaban juegos en los que una pelota era impulsada con los pies. Entre ellas pueden mencionarse las Reducciones Jesuíticas de la zona guaraní, más específicamente en la de San Ignacio Miní en el siglo XVII, en la región que ahora se conoce como Misiones. El jesuita español José Manuel Peramás escribió en su libro De vita et moribus tredecim virorum paraguaycorum: «Solían también jugar con un balón, que, aún siendo de goma llena, era tan ligero y rápido que, cada vez que lo golpeaban, seguía rebotando algún tiempo, sin pararse, impulsado por su propio peso. No lanzaban la pelota con la mano, como nosotros, sino con la parte superior del pie desnudo, pasándola y recibiéndola con gran agilidad y precisión». “Sí, los Guaraníes jugaban un juego de pelota con los pies. Lo describe el padre jesuita José Cardiel, en el libro Las misiones del Paraguay” (Madrid, Historia 16, 1989, p. 135): Los Guaraníes no juegan a la pelota como nosotros con la mano, sino que la envían y la vuelven a recibir con la parte superior del pie descalzo con gran rapidez y mucha destreza” (José Manuel Peramás, Platón y los Guaraníes. Asunción, CEPAG, 2004, p. 97). Los primeros códigos británicos se caracterizaban por tener pocas reglas y por su extrema violencia. Uno de los más populares fue el fútbol de carnaval. Por dicha razón el fútbol de carnaval fue prohibido en Inglaterra por decreto del rey Eduardo III y permaneció prohibido durante 500 años. El fútbol de carnaval no fue el único código de la época; de hecho existieron otros códigos más organizados, menos violentos e incluso que se desarrollaron fuera de las islas británicas. Uno de los juegos más conocidos fue el calcio florentino, originario de la ciudad de Florencia (Italia). Este deporte influenció en varios aspectos al fútbol actual, no solo por sus reglas, sino también por el ambiente de fiesta en que se jugaban estos encuentros. Unificaciones del siglo XIX Una representación del calcio florentino durante el siglo XVII. Los colegios británicos se dividieron frente al Código Rugby; mientras varios decidieron seguirlo, otros decidieron rechazarlo debido a que en ellos la práctica era no tocar el balón con la mano. Entre estos últimos colegios se encontraban los de Eton, Harrow, Winchester, Charterhouse y Westminster. A mediados del siglo XIX se dieron los primeros pasos para unificar todos los códigos del fútbol en uno. El primer intento fue en 1848, cuando en la Universidad de Cambridge, Henry de Winton y John Charles Thring hicieron un llamamiento a miembros de otras escuelas para reglamentar un nuevo código, el Código Cambridge, también conocido como las Reglas de Cambridge. Las reglas presentaban un importante parecido a las reglas del fútbol actual. Quizás el más importante de todos fue la limitación de las manos para tocar la pelota, pasando la responsabilidad de trasladar la misma a los pies. El objetivo del juego era hacer pasar una pelota entre dos postes verticales y por debajo de una cinta que los unía, y el equipo que marcaba más goles era el ganador. Incluso se creó una regla de fuera de juego similar a la actual. Los documentos originales de 1848 se perdieron, pero se conserva una copia de las reglas de 1856. Entre 1857 y 1878 se utilizó un código del fútbol que también aportaría características al fútbol moderno: el Código Sheffield, también conocido como las Reglas de Sheffield. El código, creado por Nathaniel Creswick y William Prest, adoptó reglas que se ven reflejadas en el fútbol actual, como el uso de un travesaño (poste horizontal) de material rígido, en lugar de la cinta que se usaba hasta el momento. También se adoptó la utilización de tiros libres, saques de esquina y saques de banda como métodos de reanudación del juego. Si bien con estas unificaciones del fútbol se lograron varios avances para la creación del balompié moderno, se considera que el día de su nacimiento es el 26 de octubre de 1863, cuando The Football Association se reunió por primera vez. Ese día, Ebenezer Cobb Morley inició una serie de seis reuniones entre doce clubes de distintas escuelas londinenses en la Taberna Freemasons, con el objetivo de crear un código de fútbol universal y definitivo, que tuviera la aceptación de la mayoría. Escocia 0-0 Inglaterra, primer partido oficial entre selecciones (30 de noviembre de 1872). Finalizadas las reuniones, el 8 de diciembre, once de los doce clubes lograron el consenso para establecer 14 reglas del nuevo código, el cual recibiría el nombre de fútbol asociación (association football en inglés), para diferenciarlo de otros códigos del fútbol de la época. Solo el club Blackheath se negó a la creación de estas reglas, que más tarde se convertiría en uno de los creadores de otro famoso deporte, el rugby. El reglamento utilizado como base para el fútbol fue el Código Cambridge, excepto por dos puntos del mismo, los cuales eran considerados de mucha importancia para los códigos actuales: el uso de las manos para trasladar el balón y el uso de los tackles (contacto físico brusco para quitarle la pelota al rival) contra los adversarios. Este fue el motivo del abandono del club Blackheath. Junto a la creación del nuevo código se creó la Asociación Inglesa de Fútbol (FA por sus siglas en inglés), órgano que rige hasta la actualidad el balompié en Inglaterra. En esos momentos, los estudiantes de las escuelas inglesas desarrollaron las abreviaturas rugger y soccer (derivado de association), para designar a ambos deportes: el rugby y el fútbol, respectivamente. Con este último término es mayoritariamente conocido el fútbol en los Estados Unidos. Primeros eventos Ya con el fútbol bien definido, se comenzaron a disputar los primeros encuentros con este nuevo código. El 20 de julio de 1871, un periódico británico anunció la creación de un torneo organizado por la FA, el primer paso para la creación de la Copa de Inglaterra. Ese año, la FA estaba compuesta por 50 equipos, pero solo 12 decidieron participar en la primera edición del torneo, 1871-1872, que fue ganada por el Wanderers FC. El 30 de noviembre de 1872, Escocia e Inglaterra disputaron el primer partido oficial entre selecciones nacionales, encuentro que concluyó en empate sin goles. El partido se disputó en el Hamilton Crescent, actual campo de críquet, en Partick (Escocia). Entre enero y marzo de 1884, se disputó la primera edición del British Home Championship, que hasta su desaparición en 1984 fue el torneo de selecciones más antiguo de la historia. El primer título correspondió a Escocia. La primera competición de liga llegó en la temporada 1888-1889 con la creación de la Football League. Participaron 12 equipos afiliados a la FA, y cada uno jugó un total de 22 encuentros. Dicho torneo quedó en manos del Preston North End Football Club, que además lo logró sin ser vencido. Expansión internacional La selección británica obtuvo el primer campeonato internacional de selecciones, en los Juegos Olímpicos de 1908. Balón de fútbol de 1924. Museo del Sevilla FC. Con el pasar de los años, el fútbol se expandió rápidamente en las islas británicas, donde se crearon nuevas asociaciones balompédicas, aparte de la inglesa (fundada en 1863), que representaban a las cuatro regiones del por entonces Reino Unido de Gran Bretaña e Irlanda: la Asociación Escocesa de Fútbol (Escocia, 1873), la Asociación de Fútbol de Gales (Gales, 1876) y la Asociación Irlandesa de Fútbol (Irlanda, 1880). A finales de los años 1880 el fútbol comenzó a expandirse rápidamente fuera del Reino Unido, principalmente debido a la influencia internacional del Imperio británico. Los primeros países en fundar sus asociaciones balompédicas fueron Dinamarca y los Países Bajos (1889), a los que luego se sumaron las asociaciones de Nueva Zelanda (1891), Argentina (1893), Bélgica, Chile y Suiza (1895), Italia (1898), Alemania y Uruguay (1900), Hungría (1901), Noruega (1902), Suecia (1904) y España (1913).  El auge del fútbol a nivel mundial motivó la creación de la FIFA el 21 de mayo de 1904. Las asociaciones fundadoras fueron Bélgica, España (representada por el Madrid F. C.), Dinamarca, Francia, Países Bajos, Suecia y Suiza. Las cuatro asociaciones de fútbol del Reino Unido, las denominadas Home Nations, se opusieron a la creación de dicho órgano. Debido al crecimiento del balompié, la FIFA había anunciado la primera competición internacional de selecciones para 1906, pero por problemas internos de varias asociaciones la misma no se desarrolló. El fútbol ya se había presentado al mundo por medio de una serie de encuentros de exhibición durante los Juegos Olímpicos de 1900, 1904, 1906 (juegos intercalados), todos a nivel de clubes, hasta que la edición de 1908 recibió por primera vez una competición de selecciones. La medalla de oro quedó en manos de la selección de fútbol del Reino Unido. En 1916 se fundó la Confederación Sudamericana de Fútbol (Conmebol), que ese mismo año organizó la primera edición del Campeonato Sudamericano de Fútbol, actual Copa América ,dicho torneo se mantiene como el más antiguo de la historia del fútbol a nivel de selecciones, de los que todavía existen,. En esa primera edición participaron Argentina, Brasil, Chile y Uruguay, resultando campeón este último. La Primera Guerra Mundial hizo retroceder el desarrollo del fútbol, pero las ediciones de 1924 y 1928 de los Juegos Olímpicos revitalizaron el deporte, en particular las actuaciones de la selección uruguaya. Este crecimiento nuevo del fútbol motivó que la FIFA confirmara el 28 de mayo de 1928 en Ámsterdam la realización de un campeonato mundial de selecciones, cuya sede sería confirmada el 18 de mayo de 1929 en el congreso de Barcelona. Uruguay sería sede de la primera edición de la Copa Mundial de Fútbol, que se celebraría en conjunto con el centésimo aniversario de la primera Constitución uruguaya. La selección uruguaya se quedó con el primer título de la historia de la competición. La segunda edición del torneo se realizó en 1934 en Italia, y fue utilizada por el dictador Benito Mussolini como propaganda de su régimen. La competición se vio deslucida debido a la intervención de Mussolini, que hizo todo para que su selección italiana obtuviera el torneo, incluso con amenazas a los árbitros de la final. La tercera edición del torneo también se vio deslucida debido a Mussolini, que antes de la final entre Italia y Hungría envió un telegrama a su selección amenazándolos de muerte. Finalmente la selección azzurra, que vistió un uniforme completamente negro representando al Partido Nacional Fascista, se impuso en la final por 4 tantos a 2. La Segunda Guerra Mundial también tuvo un efecto similar sobre el fútbol. En 1946 las Home Nations, que se habían desafiliado de la FIFA tras la Primera Guerra Mundial, volvieron al órgano internacional. El 10 de mayo de 1947 se considera una fecha de vital importancia para el resurgimiento de la FIFA y del fútbol mundial, gracias a la realización del encuentro amistoso entre la selección británica y un combinado de futbolistas europeos, el Resto de Europa XI, en el denominado Partido del Siglo. El encuentro se disputó en Hampden Park (Glasgow, Escocia), ante 135 000 espectadores. El conjunto británico ganó el partido con un marcador de 6 goles a 1, y la recaudación del partido fue donada a la FIFA para ayudarla en su relanzamiento. La primera edición de la Copa Mundial posterior a la Segunda Guerra Mundial se desarrolló en Brasil durante 1950. El triunfo de la selección uruguaya en el recordado Maracanazo coronó el resurgimiento de la FIFA y del balompié mundial. Denominación: de football a fútbol La castellanización de la palabra se produce a principios del siglo XX. [página requerida] El juego de la pelota o balompié, que ya es bastante popular en Inglaterra, aparece por primera vez en una revista gráfica de España en 1868. La revista El Panorama, publicada en Valencia entre 1867 y 1869, incluyó un pequeño reportaje de curiosidades hablando por primera vez del football inglés en su número del 30 de abril de 1868, e incorporando una ilustración gráfica del mismo. A partir de entonces se empieza a extender el juego del football en España y otros países hispanoamericanos (Argentina, Paraguay) y son muchas las referencias a este juego, utilizando la denominación original. Para entonces los primeros clubes y equipos en España y Argentina incorporan la palabra inglesa a sus denominaciones originales. Son muy comunes las denominaciones Sport Club, Sporting Club, Racing Club, Athletic Club, Recreation Club o Foot-ball Club que acompañaban a la localidad que representaban. En 1908, Mariano de Cavia propone que se denomine al nuevo juego balompié y en septiembre de ese mismo año ya algunos nuevos equipos lo incorporan, como el recién creado Sevilla Balompié, antecedente inmediato del Real Betis Balompié. En 1922, la Real Academia de la Lengua Española, a instancias de Gabriel Maura y Gamazo, quien fuera presidente de la Real Federación Española de Football, incorpora la palabra futbol sin tilde, aunque se seguía utilizando asiduamente la denominación inglesa. Con la llegada de la República, en 1931, la Federación Española incorpora definitivamente la palabra fútbol a su denominación y empieza a utilizarla asiduamente, oficializándola. Al término de la Guerra civil hubo una política clara de castellanización y depuración del franquismo de todas las palabras y vocablos, incluidos los extranjeros. Mediante la Orden del Ministerio de la Gobernación de 16 de mayo de 1940, se prohibió el empleo de vocablos genéricos extranjeros en todo tipo de rótulos, carteles y espacios públicos, lo que llevó al cambio definitivo hacia la denominación futbol tanto de los clubes como de su comunicación, utilizándose de facto todas aquellas palabras del léxico que ya habían sido incorporadas por el diccionario de la Real Academia Española. El Consejo Nacional de Deportes, bajo cuyo mando quedó la Federación Española de Fútbol, impuso a todas las federaciones nacionales una limpieza idiomática, unificando la terminología del fútbol mediante una decisión tomada el 20 de diciembre de 1940. La Federación trasladó la voluntad del gobierno a sus asociados mediante la circular federativa 3. 149, comunicada el 21 de diciembre de 1940, por la que se obligaba a todos los clubes a cambiar su denominación, lo que debía hacerse antes del 1 de febrero de 1941. Así fue como el Real Madrid F.C. pasó a ser Real Madrid C. F., el F. C. Barcelona a C.F. Barcelona, el Valencia F.C. a Valencia C. F., el Athletic Club a Atlético de Bilbao y el Athletic-Aviación Club a Club Atlético-Aviación, mientras que otros, como el Real Sporting de Gijón o el Real Santander Racing Club quedaron en Real Gijón y Real Santander SD. A partir de ese momento se unificó totalmente el léxico futbolístico adoptándose por los medios de comunicación y las instituciones del fútbol gran parte de la terminología que ha llegado hasta el siglo XXI. Consolidación Encuentro entre Alemania y Portugal en la Copa Mundial de Fútbol de 2006. Ryan Valentine anota para el Wrexham durante un encuentro de la Football League Two ante Boston United. La segunda mitad del siglo XX sería la época de mayor crecimiento del fútbol. El balompié sudamericano ya se encontraba organizado desde 1916, año en que se fundó la Confederación Sudamericana de Fútbol, pero el deporte en otras zonas se comenzaría a agrupar en los años 1950 y 60. En 1954 el fútbol europeo y asiático se organizaría en la Unión de Asociaciones Europeas de Fútbol y la Confederación Asiática de Fútbol respectivamente. En Europa se consolidan los clubes con mayores presupuestos, fundados en su mayoría a finales del siglo XIX, pero que alcanzan grandes cotas de éxito en la segunda mitad del siglo XX. Entre ellos destaca el club español Real Madrid Club de Fútbol, declarado por FIFA Mejor Club del Mundo en el siglo XX. En África se fundaría la Confederación Africana de Fútbol en 1957; en América del Norte, Central y el Caribe, la Concacaf en 1961; y por último en Oceanía, la Confederación de Fútbol de Oceanía en 1966. Dichas organizaciones se afiliarían a la FIFA bajo el nombre de confederaciones. En paralelo con las creaciones de las nuevas confederaciones se comenzaron a disputar los primeros torneos regionales de selecciones, excepto por la Confederación Sudamericana de Fútbol, que ya disputaba su Campeonato Sudamericano de Selecciones desde 1916. En 1956 la Confederación Asiática de Fútbol realizó la primera edición de la Copa Asiática, y al año siguiente la Confederación Africana de Fútbol organizó la Copa Africana de Naciones. En 1960 se crea la Eurocopa, que agrupa a las selecciones de la UEFA. Por su parte, la Concacaf disputaría por primera vez la Copa Concacaf en 1963, que más tarde sería reemplazada por la Copa de Oro. La Confederación de Fútbol de Oceanía sería la última en crear su propio torneo, la Copa de las Naciones de la OFC, celebrada por primera vez en 1973. Debido a la creación de las confederaciones se comenzaron a disputar los primeros campeonatos internacionales a nivel de clubes, siendo la primera de su tipo la Campeonato Sudamericano de Campeones, que reunió a siete equipos de Sudamérica. Luego apareció la Liga de Campeones de la UEFA, que reúne a los distintos campeones de las principales ligas de los países de la UEFA a partir de 1955. Cinco años más tarde se inició la Copa Libertadores de América, máximo evento para clubes afiliados a la CONMEBOL, que se disputó por primera vez en 1960. Ese mismo año se disputó la primera edición de la Copa Intercontinental, que reunió a los campeones de los torneos continentales de Sudamérica y Europa. Este torneo se reemplazó en 2005 por la Copa Mundial de Clubes de la FIFA, que además recibie a los campeones de las demás confederaciones. Mientras tanto, la Copa Mundial de Fútbol se consolidó como el evento deportivo de mayor importancia en el mundo entero, incluso superando en audiencia a los propios Juegos Olímpicos. Desarrollo en el mundo Artículo principal: Cultura futbolística Popularidad Mapa comparativo de la popularidad del fútbol a nivel mundial, medido por el número de jugadores activos por cada 1000 habitantes. Los países que figuran en color verde corresponden a los lugares en los que el fútbol es el deporte más popular. Según una encuesta realizada por la FIFA en el año 2006, aproximadamente 270 millones de personas en el mundo están activamente involucradas en el fútbol, incluyendo a balompedistas, árbitros y directivos. De estas, 265 millones juegan al fútbol regularmente de manera profesional, semiprofesional o amateur, considerando tanto a hombres, mujeres, jóvenes y niños. Dicha cifra representa alrededor del 4 % de la población mundial. La confederación con mayor porcentaje de personas activamente involucradas con el fútbol es la Concacaf, con el 8,53 % de la población. Su contraparte se da en la zona de la AFC, donde el porcentaje es de solo un 2,22 %. La UEFA tiene un porcentaje de participación del 7,59 %; la CONMEBOL de 7,47 %; la OFC de 4,68 %; y la Confederación Africana de Fútbol del 5,16 %. Existen más de 1,7 millones de equipos en el mundo y aproximadamente 301 000 clubes. El país con más futbolistas que se desempeñan regularmente (excepto niños) es China, que posee 26,1 millones. Lo siguen: Estados Unidos (24,4 millones), India (20,5), Alemania (16,3), Brasil (13,1), Colombia (9,2) y México (8,4). Por otro lado, las federaciones con menor cantidad de futbolistas regulares (excepto niños) son Montserrat, con 300, las Islas Vírgenes Británicas (658), Anguila (760) y las Islas Turcas y Caicos (950). Fútbol femenino Artículo principal: Fútbol femenino Un encuentro de fútbol femenino. El fútbol femenino ha tenido un crecimiento lento en el fútbol moderno, principalmente por obstáculos sociales y culturales que no permiten el ingreso pleno de la mujer al deporte. El primer encuentro femenino bajo las reglas del fútbol asociación del cual se tienen registros sucedió en 1892 en Glasgow, Escocia. A finales de 1921 el fútbol femenino fue prohibido en Inglaterra, hecho que no le permitió expandirse al resto del mundo. En 1969 el balompié femenino se volvió a organizar en Inglaterra, motivo por el cual comenzó a expandirse fuera de su territorio. El primer encuentro internacional de selecciones de fútbol femenino ocurrió en 1972, casualmente 100 años después del primer encuentro masculino, donde Inglaterra venció a Escocia por 3 a 2. Los primeros torneos mundiales comenzaron a disputarse en los años 1990: la Copa Mundial Femenina de Fútbol a partir de 1991 y como deporte de los Juegos Olímpicos desde 1996. Según una encuesta realizada por la FIFA, existen alrededor de 26 millones de jugadoras en el mundo. En promedio, por cada 10 futbolistas (de ambos sexos) existe una mujer futbolista en el mundo. Economía Objetos personalizados como esta pelota de Brasil son claros ejemplos de merchandising. Según estimaciones de la FIFA, durante el período 2003-2006 dicho organismo tuvo ingresos por 3238 millones de francos suizos (CHF) y gastos por 2422 millones de CHF, lo cual da un superávit de 816 millones de CHF. El 92 % (2986 millones de CHF) de los ingresos están relacionados con las competiciones internacionales, particularmente la retransmisión por televisión de la Copa Mundial de Fútbol de 2006, que comprende 1660 millones de CHF de ese valor. El resto de los ingresos se dividen en partes iguales entre ingresos financieros y otros ingresos de explotación. Del total de los ingresos, 714 millones de CHF se consiguen por concepto de derechos de merchandising. Muchos de estos ingresos se dan en puntos de venta en los alrededores de los estadios de la Copa Mundial de Fútbol. En cuanto a los gastos, el 69 % (1682 millones de CHF) de los mismos están dedicados a la organización de campeonatos y al desarrollo del deporte: un 46 % de los gastos totales (1.125 millones de CHF) y un 23 % (557 millones de CHF) respectivamente. El 26 % (622 millones de CHF) se dedicó a gastos operativos, como lo son el transporte, alquileres, gastos jurídicos, comunicaciones, entre otros. El otro 5 % (118 millones de CHF) corre por efectos del cambio de divisas e intereses. Los presupuestos de los clubes de fútbol se pueden encontrar en diferentes valores dependiendo de la zona del mundo donde se encuentren. Los mayores presupuestos se pueden encontrar en Europa, particularmente en las 5 grandes ligas: Alemania, España, Italia, Inglaterra y Francia. En gran parte de América del Sur los mayores ingresos se deben a la transferencia de jugadores a las ligas europeas, los fondos aportados por las transmisiones de la televisión y la publicidad en las camisetas. Por el lado de los europeos, los derechos televisivos, la publicidad, la venta de entradas y el merchandising cubren gran parte del presupuesto.  El fútbol también cumple un rol solidario. Uno de los principales aportes de la FIFA al desarrollo del deporte en áreas donde esto se hace difícil por falta de materiales y técnicas de desarrollo es el Programa Goal. Por otro lado, la FIFA trabaja con UNICEF desde 1999, brindando material de trabajo relacionado con el fútbol para que este sea repartido por esta organización de las Naciones Unidas. Regularmente se realizan en todo el mundo encuentros amistosos con propósitos benéficos, cuyos promovedores suelen ser estrellas del fútbol mundial.  Se entiende por fichaje o pase, a la transferencia de un futbolista de un club a otro a cambio de una suma económica que paga el club que recibe al jugador al que lo entrega. Algunos de estos fichajes pueden alcanzar precios muy altos. Los diez fichajes más caros en la historia del fútbol son los siguientes: En negrita jugadores activos en su club de destino # Año Futbolista Nacionalidad Club de origen Club destino Valor en millones (€) 1 2017 Neymar Bandera de Brasil Brasil Bandera de España Barcelona Bandera de Francia PSG 222 2 2017 Kylian Mbappé Bandera de Francia Francia Bandera de Francia Mónaco Bandera de Francia PSG 180 3 2019 Eden Hazard Bélgica Bandera de Inglaterra Chelsea Bandera de España Real Madrid 160 4 2025 Alexander Isak Suecia Bandera de Inglaterra Newcastle Bandera de Inglaterra Liverpool 144 5 2025 Florian Wirtz Alemania Bandera de Alemania Bayer Leverkusen Bandera de Inglaterra Liverpool 137 6 2017 Ousmane Dembélé Bandera de Francia Francia Bandera de Alemania Borussia Dortmund Bandera de España Barcelona 135 2018 Philippe Coutinho Bandera de Brasil Brasil Bandera de Inglaterra Liverpool Bandera de España Barcelona 8 2023 Moisés Caicedo Bandera de Ecuador Ecuador Bandera de Inglaterra Brighton Bandera de Inglaterra Chelsea 133 9 2019 João Félix Bandera de Portugal Portugal Bandera de Portugal Benfica Bandera de España Atlético de Madrid 126 10 2023 Declan Rice Bandera de Inglaterra Inglaterra Bandera de Inglaterra West Ham Bandera de Inglaterra Arsenal 122 Dopaje La lucha contra el dopaje en el fútbol se remonta a 1966, cuando la FIFA se convirtió en una de las primeras federaciones internacionales en crear reglas para controlar al mismo. Anualmente se hacen 20 000 pruebas antidopaje a futbolistas de todo el mundo, de las cuales entre 80 y 90 dan positivas, mayoritariamente por el uso de cocaína o cannabis. Se entiende por dopaje positivo lo siguiente: Sólo debe hablarse de resultado de dopaje positivo si el laboratorio identifica la presencia de una sustancia prohibida, sus metabolitos, marcadores o evidencias del uso de un método prohibido que no haya sido aprobado mediante una EUT (Exención por Uso Terapéutico) o cuando no se originen naturalmente en el organismo. El procedimiento para una prueba de dopaje se inicia en el descanso de un partido, cuando se sortean dos jugadores por equipo. Quince minutos antes de la finalización del encuentro se le entrega un sobre a él o los delegados en el terreno, identificados por un chaleco blanco con una cruz verde, los cuales eligen al azar dos de ellos (uno por equipo). Una vez terminado el encuentro informarán a los dos jugadores elegidos que deben dirigirse a la zona de pruebas para ser controlados. Los jugadores expulsados durante el encuentro también pueden ser citados. Más tarde los jugadores seleccionados deberán dar una muestra de orina. Previamente a esto, los seleccionados no pueden ingresar a sus vestuarios, pero si a una sala donde pueden beber bebidas analcohólicas e incluso tomar una ducha. La muestra tomada por un profesional del mismo sexo que el del jugador envía la misma a un laboratorio para su análisis. En caso de que la muestra dé positiva, el informe sobre la misma se envía la Subcomisión del Control de Dopaje de la FIFA, la cual investiga la veracidad del estudio y, una vez aprobado, pasa a manos del director de control de dopaje de la FIFA, el cual verificará la información para autorizar su envío a la Comisión Disciplinaria, a la Comisión de Medicina Deportiva y a la asociación a la cual pertenezca el jugador. La Comisión Disciplinaria decidirá qué pena se le aplica al jugador. Quizás el caso más conocido de dopaje en el fútbol fue el de Diego Armando Maradona, que tras un partido por la Copa Mundial de Fútbol de 1994 en Estados Unidos fue seleccionado para la realización de la prueba. Su muestra dio positiva y se le aplicó una pena de 15 meses. Violencia Cartel ubicado en el Camp Nou de Barcelona que prohíbe el ingreso con elementos que podrían usarse en forma violenta. La violencia en el fútbol es casi tan antigua como el deporte mismo. Sus orígenes se remontan a los encuentros de fútbol de carnaval durante la Edad Media, los cuales se caracterizaban por no tener reglas y por el uso desmedido de la violencia. En 1314, se realizó la primera prohibición de este deporte, para evitar la creciente ola de violencia que producía. La primera aparición de la violencia en el fútbol moderno, de 1863 en adelante, ocurrió en 1885, cuando un encuentro entre los equipos ingleses de Preston North End y Aston Villa terminó con una brutal pelea entre jugadores de ambos equipos. Los grupos violentos de una hinchada reciben varios nombres, pero se destacan algunos como barra bravas, hooligans o ultras. Por ejemplo, en Argentina ha habido más de doscientas muertes relacionadas con incidentes en campos de fútbol y sus alrededores. En Italia, la violencia de los denominados ultras italianos se caracteriza por sus insultos racistas e incluso por la fabricación de armamento para las batallas que efectúan antes, durante y después de los encuentros. La violencia en el fútbol ha generado cosas aún peores, tal como lo fue la denominada Guerra del Fútbol. Dicho conflicto se agudizó por la serie de encuentros que disputaron las selecciones de fútbol de El Salvador y Honduras. Los encuentros eran válidos por la clasificación para la Copa Mundial de Fútbol de 1970, a disputarse en México. Para evitar estos males del fútbol, la FIFA promueve una campaña llamada «juego limpio» o fair play en inglés, la cual invita a los participantes de este deporte a mostrar valores que hacen crecer al fútbol. Anualmente, el mismo organismo entrega uno o más premios a personas, clubes, asociaciones o entidades de cualquier tipo que transmiten los valores de la deportividad. Fuera de la cancha, también se han dado casos: el árbitro mexicano Adalid Maganda fue víctima de actitudes de racismo por parte del público, de los jugadores, del arbitraje (Comisión de Arbitraje) y de la Federación Mexicana de Fútbol. Cultura La visión de un artista plástico sobre el calzado deportivo en el Walk of Ideas. El Fußball Globus en Núremberg. La cultura es el conjunto de expresiones de una sociedad, y como tal el fútbol no está exento de la misma. Uno de los institutos que promueven el fútbol como cultura a nivel internacional es el Goethe-Institut, que ha realizado exposiciones alrededor del mundo cuyos temas eran el fútbol. Una importante muestra de cultura se dio durante la Copa Mundial de Fútbol de 2006 disputada en Alemania. El Walk of Ideas fue una muestra exhibida en ese país durante ese año, la cual constaba de una serie de obras plásticas que representaban a Alemania, siendo una de ellas un par de zapatos de fútbol. El diseño gráfico aporta su parte a la cultura de este deporte, ya que el diseño de los afiches y demás elementos gráficos deben representar al país donde se realiza la competición a la cual le fueron asignados dichos trabajos. También valen destacar trabajos como los del humorista gráfico y escritor argentino Roberto Fontanarrosa, ya que muchos de los mismos estaban relacionados con el fútbol. La relación con la literatura fue más problemática. Habiendo sido popularmente rechazada por los escritores desde sus inicios, recién a partir de los años 1960 y 1970 los literatos se acercan al mundo del fútbol. Este acercamiento puede relacionarse con el auge de los estudios semióticos, que revalorizaron las manifestaciones culturales de carácter masivo y popular. En el cine y la televisión se suele ver contenido relacionado con este deporte. En cuando al primero, existen muchas películas dedicadas a este juego, aunque pocas creadas con el apoyo de la FIFA. Dentro de las más conocidas se destacan la trilogía Goal!: Goal! (2005), Goal II: Living the Dream (2007) y Goal! 3: Taking on the World (2008), todas con el apoyo de FIFA. La FIFA también lanzó una película relacionada con la Copa Mundial de Fútbol de 2006. En televisión se pueden destacar series animadas como el anime Captain Tsubasa, que prácticamente popularizó el fútbol en dicho país; también se pueden mencionar producciones mexicanas y argentinas que lo toman como ingrediente principal o secundario. En cuanto a la música, Voices from the FIFA World Cup contiene una serie de canciones de autores conocidos. Este fue el álbum musical realizado para la celebración de la Copa Mundial de Fútbol de 2006 en Alemania. La relación entre los videojuegos y el fútbol es muy fructífera. Uno de los primeros videojuegos futbolísticos exitosos fue el Peles Soccer de la consola Atari 2600. Muchas ediciones se volvieron clásicos, como el FIFA Series, el Pro Evolution Soccer/Winning Eleven y el PC Fútbol. Reglas del juego Artículo principal: Reglas del fútbol Solo el guardameta puede tocar el balón con sus manos, siempre y cuando esté dentro de zona delimitada (área). El fútbol está reglamentado por 17 leyes o reglas, las cuales se utilizan universalmente, aunque dentro de las mismas se permiten ciertas modificaciones para facilitar el desarrollo de las modalidades femenina, infantil y veterana. Si bien las reglas están claramente definidas, existen ciertas diferencias en las aplicaciones de las mismas que se deben a varios aspectos. Un aspecto importante es la región futbolística donde se desarrolla el juego. Por ejemplo, en Europa, particularmente en Inglaterra, los árbitros se destacan por ser más permisivos con las faltas e infracciones, reduciendo de esta manera las amonestaciones y expulsiones, mientras que en otros lugares, por ejemplo en Sudamérica, las faltas son penadas con tarjetas más a menudo. Las reglas del juego están definidas por la International Football Association Board, organismo integrado por la FIFA y las cuatro asociaciones del Reino Unido. Para aprobarse una modificación a las mismas se deben tener por lo menos los votos de la FIFA y de 2 de los 4 votos de las asociaciones del Reino Unido. Campo de juego Campo de juego. El fútbol infantil es un claro ejemplo de los cambios de reglas, donde se destaca el tamaño del campo y el número de jugadores por equipo. El fútbol se juega en un terreno de césped natural o artificial de forma rectangular. Las medidas permitidas del terreno son de 90 a 120 metros de largo y de 45 a 90 metros de ancho, pero para partidos internacionales se recomiendan las siguientes medidas: entre 100 y 110 metros de largo, y entre 64 y 75 metros de ancho. Las dos líneas ubicadas a lo largo del terreno reciben el nombre de líneas laterales o de banda, mientras que las otras son llamadas líneas de meta o finales. Los puntos medios de cada línea de banda son unidos por otra línea, la línea media. Sobre el centro de cada línea de meta y adentrándose en el terreno, se ubican las áreas penales, las áreas de meta y las metas o porterías. Las llamadas metas, también conocidas como porterías o arcos, constan de dos postes verticales (conocidos como palos o verticales) de 2,44 metros de alto ubicados a 7,32 metros de separación y sobre el centro de cada línea de meta. Las partes superiores de los postes son unidas por otro poste horizontal, conocido como travesaño o larguero. Las áreas penales son áreas rectangulares ubicadas en el centro de las metas y adentrándose en el terreno. Estas se trazan a 16,5 metros de los postes verticales, adentrándose también 16,5 metros hacia el interior del terreno, y luego uniéndose por otra línea mayor. El trazado del área de meta es igual, pero utilizando una medida de 5,5 metros. Inicio del juego Cada uno de los dos equipos consta de un máximo de 11 jugadores y un mínimo de 7. Durante el partido se podrán cambiar a estos jugadores por otros, los denominados suplentes o sustitutos. Uno de los jugadores titulares deberá ser el guardameta. Está permitido que un guardameta y otro jugador del equipo se intercambien de posición durante el encuentro, siempre y cuando sea durante una interrupción con el consentimiento del árbitro. Cada jugador deberá tener una indumentaria básica, la cual consta de una camiseta o jersey con mangas, unos pantalones cortos, medias, canilleras o espinilleras y un calzado adecuado. Los colores de la indumentaria de ambos equipos y la de ambos guardametas deben ser claramente diferenciables para la vista. Los capitanes (jugadores representantes de cada equipo) deberán llevar alguna marca identificadora para ser llamados por el árbitro cuando sea necesario, que generalmente es un brazalete. El fútbol se juega con un balón o pelota de forma esférica. Deberá ser de cuero u otro material adecuado. Su circunferencia será de entre 68 y 70 centímetros, su masa de entre 410 y 450 gramos y su presión de entre 0,6 y 1,1 atmósferas al nivel del mar. Los jugadores pueden tocar y mover el balón con cualquier parte de su cuerpo excepto los brazos. El guardameta tiene la ventaja de poder utilizar cualquier parte de su cuerpo para esto, pero solo dentro de su área penal. Cada encuentro será controlado por un árbitro principal designado por la organización de la competición en cuestión, quien será la autoridad máxima del partido y el encargado de hacer cumplir las reglas del juego. Todas las decisiones del árbitro son definitivas. Solo él puede modificar una decisión, siempre que no haya reanudado el juego o el partido haya finalizado. Además tendrá a su disposición 2 árbitros asistentes o lineman (hombre de línea) para ayudarlo en la toma de decisiones. Posee también un cuarto árbitro a su disposición quien es el que lo corrobora, y además controla a los suplentes y cuerpo técnico. El cuarto árbitro además indica las sustituciones y el aumento del tiempo reglamentario. Para iniciar el encuentro, uno o más jugadores de un equipo moverán el balón hacia la portería rival desde el punto medio de la línea media, momento donde empezará a correr el tiempo reglamentario. Esta situación se da con el equipo contrario al comienzo de la segunda mitad. También ocurre luego de cada gol, donde el equipo que lo recibió ejecuta el saque. Duración y resultado La duración de un partido, especificada en la Regla 7 del reglamento, será de dos tiempos iguales de 45 minutos, con un periodo intermedio de descanso que no podrá exceder los 15 minutos, debiendo establecer su duración el reglamento de cada competición. La duración de cada mitad de tiempo solo podrá alterarse si lo permiten las reglas de la competición, y si existe acuerdo entre el árbitro y los dos equipos participantes antes de iniciarse el partido. El tiempo perdido durante la disputa del partido deberá recuperarse al final de cada periodo, quedando a criterio del árbitro principal la duración de esos periodos de recuperación. El objetivo del deporte es marcar más goles que el rival. Se considera que un equipo ha marcado un gol cuando el balón rebase por completo la línea de meta entre los postes verticales y por debajo del poste horizontal de la portería rival, siempre y cuando no se haya cometido una infracción a las reglas del juego previamente. El gol es la única forma de marcar en el fútbol, cosa que no sucede en otros códigos del fútbol. Si ambos equipos marcan la misma cantidad de tantos, el partido se considera empatado. La meta o portería del fútbol, donde se marcan los goles. En muchos casos, cuando el partido termina en empate, se debe buscar alguna forma de que uno de los dos equipos sea considerado el ganador del encuentro, y para lograr esto existen varias formas. Si el partido termina empatado, se puede jugar una prórroga o tiempo extra, la cual consta de dos tiempos, generalmente, de 15 minutos cada uno, donde se continua el partido inicial. Además, existen dos formas de que la prórroga culmine antes de tiempo: el gol de oro y el gol de plata, aunque estas formas han sido dejadas de lado en los últimos años. Si persiste la igualdad, se ejecutará una serie de tiros desde el punto penal. La misma consiste en que cada equipo lance penaltis de forma alternada hasta totalizar 5 cada uno; si al cabo de los 10 penaltis o penales ejecutados persiste la igualdad, se continuará ejecutando un penalti por equipo hasta que se defina un ganador. La utilización de la prórroga y los tiros penales son un formato muy utilizado en el fútbol moderno, siendo el principal exponente de esto las fases eliminatorias de la fase final de la Copa Mundial de Fútbol. En algunas competiciones se pasa a la ejecución de penales directamente después de la culminación del partido inicial, sin utilizar la prórroga. Un claro ejemplo de este sistema son las fases eliminatorias de la Copa América. En todos estos ejemplos se jugaba un único encuentro, pero existen otros torneos donde las fases eliminatorias se juegan a dos encuentros, los denominados partidos de ida y vuelta. Para determinar si la llave (ambos partidos) terminó en empate, se suman los goles a favor de ambos equipos en los dos partidos, y si dan lo mismo, se dice que la llave terminó en empate. En algunos casos, si la llave terminó empatada, se utiliza un sistema de desempate con prórroga o tiros penales, los cuales se ejecutan al finalizar el segundo partido de la llave. En algunas fases eliminatorias se considera otra forma de desempate previo a la prórroga o los penaltis: los goles de visitantes. Si al cabo de ambos partidos ningún equipo superó al otro en goles a favor, se contarán la cantidad de tantos convertidos por cada equipo en el partido que jugó como visitante. Si un equipo marcó más goles de visitante al cabo de ambos partidos, será el ganador de la llave, pero si persiste la igualdad también en los tantos de visitante, se procederá con la prórroga o los penales. Un ejemplo de este sistema son las fases eliminatorias de la Copa Libertadores de América y de la Liga de Campeones de la UEFA. Faltas y reanudación del juego El árbitro amonestará a un jugador cuando este cometa una infracción merecedora de dicha amonestación. Las faltas en el área se sancionan con tiro penal. Cada vez que un jugador intente golpear o golpee a otro, lo empuje, lo retenga para sacar una ventaja, lo escupa o toque el balón con sus manos (excepto el guardameta), el árbitro marcará un tiro libre directo a favor del equipo que no cometió la infracción, que se ejecutará desde el lugar de la infracción. Si ocurrió dentro del área penal propia, independientemente de la posición del balón y si el balón está en juego, se marcará un tiro penal en contra del equipo infractor. Si un jugador juega de forma peligrosa, obstaculiza a un adversario o impide al guardameta sacar el balón con sus manos, se marcará un tiro libre indirecto a favor del equipo que no cometió la infracción, que se ejecutará desde el lugar de la infracción. Además se marcará un tiro libre indirecto si el guardameta mantiene la pelota en sus manos por más de 6 segundos o toca el balón luego de haberlo tocado anteriormente, lo recibe de un compañero o directamente de un saque de banda. Un jugador podrá ser merecedor de recibir una tarjeta amarilla (amonestación) o roja (expulsión), si comete alguna infracción de las especificadas en el reglamento. Si un jugador recibe una tarjeta roja, será expulsado del terreno y no podrá ser reemplazado por otro. Si un jugador recibe dos tarjetas amarillas en un mismo partido, recibirá una tarjeta roja y será expulsado. Las tarjetas son una forma de hacer cumplir las reglas del juego por parte de los jugadores. Si el balón abandona el terreno de juego por una línea de meta luego de ser tocado por un jugador defensa, se concederá un saque de esquina al equipo rival. Si es tocado por última vez por un atacante, se concederá un saque de meta al equipo defensor. Si el balón abandona el campo por una de las líneas de banda, se concederá un saque de banda al equipo que no tocó el balón por última vez. Variantes Principales Categoría principal: Variantes del fútbol Junto a la variante principal de fútbol –conocida también como fútbol 11 (por su número de jugadores por equipo) o fútbol asociación (por la Asociación del Fútbol originaria)–, existen muchas otras variantes. Las principales son: Fútbol sala Artículo principal: Fútbol sala El fútbol sala o futsal se juega entre dos equipos de 5 jugadores cada uno, siendo uno de ellos el guardameta. Se juegan dos tiempos de 20 minutos cada uno. Cada encuentro se juega sobre una superficie de material sólido de unos 40 por 20 metros. El resto de las reglas son prácticamente iguales a las del fútbol tradicional, con algunas diferencias, como la falta del fuera de juego y el uso de los pies para efectuar los saques de banda. Desde 1989 se juega el Campeonato Mundial de Futsal, el equivalente de la Copa Mundial de Fútbol para este deporte, que también es organizado por la FIFA. Fútbol playa Artículo principal: Fútbol playa Al igual que el fútbol sala o fútbol de salón, el fútbol playa contiene grandes similitudes con el fútbol tradicional. Participan dos equipos de cinco jugadores cada uno, siendo uno de ellos el guardameta. Se juega en un campo de unos 35 por 25 metros, el cual está cubierto íntegramente por arena. Cada partido consta de tres tiempos de 12 minutos cada uno y a diferencia de otras variantes del fútbol (fútbol sala por ejemplo), el tiempo se detiene cuando el árbitro marca un tiro libre, marca un tiro penal o consta que un jugador está haciendo pasar el tiempo de forma inapropiada. Todos los tiros libres son directos y sin barrera del equipo rival. Si un jugador recibe dos tarjetas amarillas, recibirá una tarjeta azul y deberá salir del campo de juego por 2 minutos sin poder ser reemplazado por otro jugador. Si un jugador recibe una tarjeta roja o tres amarillas, será expulsado y no podrá ser reemplazado por otro. Los saques de banda pueden ejecutarse con los pies. El resto de las reglas son prácticamente iguales a las del fútbol tradicional. La competición más importante en la actualidad es la Copa Mundial de Fútbol Playa de FIFA, que se disputa desde 1995, aunque solo desde 2005 bajo el mandato de la FIFA. Fútbol para discapacitados En la actualidad, particularmente en los Juegos Paralímpicos, existen dos versiones del balompié adaptadas a personas con algún tipo de discapacidad: el Fútbol 5, para personas que sufren de ceguera, y el Fútbol 7, para personas con parálisis cerebral. Se utilizan reglas similares a las del fútbol tradicional y a las del futsal, pero con modificaciones para adaptarlas a la discapacidad en cuestión. El fútbol 5 se juega entre dos equipos de 5 jugadores cada uno, donde uno de ellos, el guardameta, no debe sufrir de ceguera total (ceguera B-1). Para evitar trampas, los 4 jugadores de campo llevan una venda sobre sus ojos, independientemente de su nivel de ceguera. Se juegan dos tiempos de 25 minutos cada uno. El terreno es de forma rectangular, su largo es de entre 38 y 42 metros y su ancho entre 18 y 22 metros. Posee una meta o portería, un área penal y otras características similares a la del fútbol tradicional. Se utiliza un balón que al girar sobre sí mismo emite un sonido claramente identificable por los jugadores. El objetivo del juego es marcar más goles que el rival y para esto se debe hacer pasar el balón por la portería rival utilizando cualquier parte del cuerpo, excepto los brazos. Al igual que en el fútbol tradicional, cada encuentro es controlado por varios árbitros, los cuales están encargados de hacer respetar las reglas y sancionar a los jugadores de ser necesario. El fútbol 7 es prácticamente igual al fútbol tradicional. Las diferencias más notorias son el menor tamaño del terreno de juego y las porterías, el número de jugadores por equipo (7 en lugar de 11), la inexistencia del fuera de juego y la libertad para efectuar un saque de banda de la forma que el jugador lo desee. Solo pueden participar jugadores que sufran parálisis cerebral de clase 5 a 8. Showbol Artículo principal: Showbol El showbol es parecido al fútbol sala, jugado en una cancha con las dimensiones de una pista de hockey, muchas veces en pasto sintético, con paredes laterales que rodean la cancha contra las cuales se puede rebotar el balón sin que esté fuera del juego. En Sudamérica han creado muchos aficionados los exjugadores Diego Armando Maradona (argentino) e Iván Zamorano (chileno). Una variante de cinco jugadores se juega en España bajo el nombre de fútbol indoor. Otras variantes Un futbolista efectúa una chilena o chalaca durante un encuentro de bossaball en España. Aparte de las principales variantes mencionadas más arriba, existen otros deportes que comparten grandes similitudes con el fútbol tradicional o que incluso combinan aspectos de otros deportes, aunque las reglas de los mismos varían de acuerdo al lugar donde se juegue y a los elementos disponibles. El juego denominado bossaball combina aspectos del fútbol tradicional y del voleibol. Basado en las reglas del voleibol, se juega sobre una superficie de colchones hinchables y camas elásticas, lo cual permite mayor número de toques y mucha más espectacularidad en los saltos. Se juega entre dos equipos de hasta 5 jugadores, los cuales deben pasar la pelota por encima de una red utilizando cualquier parte de su cuerpo, aunque con un número limitado de toques. Por su parte el fútbol tenis, como lo indica su nombre, combina aspectos del fútbol tradicional y el tenis. Se juega sobre un terreno similar o incluso igual al campo de tenis, donde cada uno de los dos equipos debe pasar la pelota por encima de la red utilizando la cabeza y los pies. La altura de la red puede variar, con lo cual el terreno podría ser reemplazado por un campo de voleibol. Un nuevo deporte es el padbol, que es una variante del fútbol-tenis pero con el agregado de las paredes del pádel y nuevas reglas para hacerlo más dinámico. Juegos de mesa y otros Jóvenes jugando al fútbol de mesa en Nueva York. Uno de los juegos de mesa más tradicionales relacionados con el fútbol es el fútbol de mesa (junto con su variante española, el futbolín). El mismo consta de una mesa con una réplica de un terreno de juego en su interior, la cual posee varios muñecos representando a los futbolistas. El ancho de la mesa es atravesado por una serie de barras rotantes donde se encastran los jugadores. Cada uno de los equipos tiene sus barras de forma alternada. Dichas barras se giran con las manos gracias a gomas en sus extremos. Si bien este juego es de carácter informal, particularmente por sus reglas y el formato de la mesa, existe una organización llamada International Table Soccer Federation que estipula reglas y organiza campeonatos de forma regular. También vale destacar al subbuteo, un juego de mesa similar al fútbol de mesa donde la principal diferencia es que los futbolistas no se encuentran encastrados en una barra, sino que se apoyan individualmente sobre una peana de forma cóncava. Para que un jugador pueda mover la pelota se le debe pegar con el dedo a uno de los muñecos para que este golpee el esférico. El fútbol también tiene una gran importancia a nivel de programación de videojuegos y robots. A nivel de videojuegos se destacan simuladores como FIFA y Pro Evolution Soccer (también conocido como Winning Eleven), los cuales permiten emular encuentros de fútbol controlando los movimientos de los futbolistas, y otros como Football Manager, el cual permite controlar equipos desde el punto de vista dirigencial. También existen competiciones como la Robocup, las cuales simulan encuentros de fútbol utilizando pequeños robots. Organización Confederaciones de la FIFA. color #ffb6c1 Confederación Asiática de Fútbol (AFC) en Asia y Australia (excepto Transcaucasia, Rusia, Turquía, Chipre e Israel). color #deb887 Confederación Africana de Fútbol (CAF) en África. color #db7093 Confederación de Fútbol Asociación de Norte, Centro América y el Caribe (CONCACAF) en América del Norte, Central, el Caribe y Las Guayanas. color #8fbc8f Confederación Sudamericana de Fútbol (CONMEBOL o CSF) en América del Sur (excepto Las Guayanas). color #ffd700 Confederación de Fútbol de Oceanía (OFC) en Oceanía (excepto Australia). color #4682b4 Unión de Asociaciones de Fútbol Europeas (UEFA) en Europa, Transcaucasia, Rusia, Turquía, Chipre e Israel. El ente rector del fútbol a nivel internacional es la Fédération Internationale de Football Association, más conocida por su acrónimo FIFA, con sede en Zúrich, Suiza. Dicho organismo se plantea 5 puntos principales para garantizar el buen desarrollo del deporte: mejorar el fútbol desde su carácter universal, educativo y cultural, así como mejorar los valores humanos que plantea el mismo; organizar competiciones del deporte; elaborar un reglamento para mantener el espíritu del juego; controlar las distintas formas del fútbol, adoptando medidas para mejorar las mismas; e impedir ciertas prácticas que afecten la esencia del deporte. La FIFA no se centra únicamente en los detalles organizativos del deporte, sino que también promueve mejoras en la infraestructura futbolística de cada país, en particular en los más pobres, por medio del Programa Goal. El mismo inculca aspectos tácticos, técnicos, de salud y organizativos a las poblaciones de dichos países, ayudándolos a crecer futbolísticamente. El programa no financia la construcción de estadios, pero sí lo hace con terrenos de entrenamiento, material para entrenar y elementos de oficina para las asociaciones. En la actualidad, 185 asociaciones nacionales se benefician del programa, el cual ha completado o está trabajando en un total de 292 proyectos. Debido al constante crecimiento de la FIFA, se han creado a lo largo de la historia seis confederaciones regionales, cuyos objetivos son similares a los de la FIFA. Las mismas están encargadas de coordinar todos los aspectos del deporte en cada región. Para que una asociación sea miembro de una confederación, no necesariamente debe serlo de la FIFA. A su vez, dentro de cada confederación hay asociaciones de fútbol, las cuales representan a un país y, en algunas ocasiones, un territorio o estado no reconocido internacionalmente. Salvo casos excepcionales, hay una sola asociación por país o territorio, y en caso de existir más de una, solo una puede estar afiliada a su confederación. En algunos casos la asociación principal del país tiene afiliadas otras sub asociaciones para ayudar en la organización del fútbol. Cada asociación organiza el fútbol de su país independientemente de su confederación, pero en algunos casos, por ejemplo para clasificar clubes a torneos internacionales, dichos clubes deben estar avalados por la asociación ante la confederación. En algunos casos, un equipo puede estar afiliado directa o indirectamente a una asociación sin estar afiliado a una confederación. También hay que mencionar a la NF-Board, organización que agrupa a las asociaciones no afiliadas ni a la FIFA, ni a ninguna de sus confederaciones. La gran mayoría de sus asociaciones pertenecen a territorios y estados no reconocidos políticamente a nivel internacional. La única competición de dicho organismo es la Copa Mundial VIVA. Campeonatos más importantes Selecciones A nivel de selecciones nacionales, el torneo más importante es la Copa Mundial de Fútbol ,que se disputa desde 1930,; previo a la creación del mismo ,en particular durante los años 1920, la competición de fútbol de los Juegos Olímpicos era considerada la máxima competición del deporte, aunque en la actualidad se mantiene como un torneo secundario donde se permiten jugadores menores de 23 años, con hasta 3 jugadores que sobrepasen ese límite de edad por equipo. A nivel femenino, el equivalente de la Copa Mundial es la Copa Mundial Femenina de Fútbol. A nivel de las confederaciones de FIFA, los torneos más importantes son la Copa América (América del Sur) y la Eurocopa (Europa); y, ubicándose a un nivel inferior, se encuentran la Copa Africana de Naciones (África), la Copa de Oro de la Concacaf (América del Norte, central y el Caribe), la Copa Asiática (Asia) y la Copa de las Naciones de la OFC (Oceanía). El torneo más importante para jugadores jóvenes es la Copa Mundial de Fútbol Sub-20, la cual recibe a equipos clasificados desde el Campeonato Sudamericano Sub-20 (América del Sur), el Campeonato Europeo de la UEFA Sub-19 (Europa), el Campeonato Juvenil de la CAF (África), el Campeonato Sub-20 de la Concacaf (América del Norte, central y el Caribe), el Campeonato Juvenil de la AFC (Asia) y el Campeonato Sub-20 de la OFC (Oceanía). Clubes En cada país los clubes de fútbol suelen federarse en asociaciones o ligas que organizan torneos oficiales entre ellos, de donde surgen los campeones de cada país y los equipos que participarán en torneos internacionales. No existe un sistema único de torneos y cada liga nacional los organiza de acuerdo a sus tradiciones. En general, la mayoría de los países tienen dos torneos principales al año: Colombia (Apertura y Clausura) (la temporada arranca según el calendario europeo), México (ídem, pero en el orden inverso), Chile (Torneo Nacional y Copa Chile), España (Liga y Copa del Rey), Italia (Serie A y Copa Italia), etc. En algunos casos los campeones de ambos torneos disputan supercopas anuales y recopas entre sí. Inglaterra tiene un sistema de campeonato principal (la Premier League) y luego varias copas en las que participan equipos de distintas divisiones. Brasil tiene un sistema de campeonatos por Estado (paulista, carioca, mineiro, gaúcho, etc.), además de tener un Campeonato Nacional (Brasileirão) y la Copa de Brasil. A nivel mundial la competición más importante es la Copa Mundial de Clubes de la FIFA que se disputa desde el año 2000 y en la que participan los campeones de las diferentes conferencias que conforman la FIFA. A nivel continental las competiciones más importantes son la Copa Libertadores de América (América del Sur), la Liga de Campeones de la UEFA (Europa), la Liga de Campeones de la CAF (África), la Concacaf Liga Campeones (América del Norte, central y el Caribe), la Liga de Campeones de la AFC (Asia) y la Liga de Campeones de la OFC (Oceanía). También a nivel continental se destacan las competiciones secundarias de América del Sur y Europa: la Copa Sudamericana y la Europa League (ex Copa UEFA), respectivamente. En algunos casos clubes de otras confederaciones son invitados a certámenes de una confederación determinada. Por ejemplo en el caso de la participación de equipos mexicanos afiliados a la CONCACAF en la Copa Libertadores de América, organizada por la CONMEBOL. Nombres del juego Un anuncio londinense utilizando la palabra soccer. Véase también: Fútbol (homonimia) Cuando se creó la Football Association en 1863, junto a la creación del deporte, se eligió el nombre de fútbol asociación (association football en inglés) para este nuevo juego. Se optó por ese nombre para diferenciarlo de otros códigos de fútbol de la época, principalmente del rugby fútbol, uno de los más populares del momento. El significado de la palabra fútbol varía de acuerdo al lugar donde se mencione. En gran parte de los países hispanohablantes el término fútbol hace referencia al fútbol asociación. En otros países, donde el fútbol asociación no es el código más popular, el término fútbol es asociado con otros códigos del fútbol. En dichos países se suele utilizar el vocablo soccer (derivado de association football en los años 1880) para referirse al fútbol asociación. En los países de habla española también son aceptados los términos balompié, utilizado en algunas zonas de España y en los nombres de varios clubes de dicho país, futbol o fútbol/futbol soccer (nótese la palabra aguda) en México, América Central (a excepción de Belice que lo traduce como football) y las Grandes Antillas y simplemente fútbol en América del Sur (a excepción de Brasil que lo traduce como futebol, Surinam como voetbal y Guyana y Guayana Francesa como football). En España se documenta el uso de foot-ball en 1878 y la grafía fútbol en 1915. Sin embargo, ya en 1908 Mariano de Cavia había propuesto el calco balompié, aunque algunos lo rechazaron por considerarlo un galicismo. Azorín incluso trajo el término esferomaquia, usado por los romanos."

ksampletext_wikipedia_vide_videojuego: str = "Videojuego. Un videojuego o juego de video es un software o juego electrónico en el que uno o más jugadores interactúan por medio de diversos dispositivos (teléfonos, consolas, computadoras), los cuales muestran imágenes de video. Estos dispositivos, conocidos genéricamente como «plataforma», puede ser una computadora, una máquina de arcade, una consola de videojuegos o un dispositivo portátil, como por ejemplo un teléfono móvil, teléfono inteligente, tableta o una consola de videojuegos portátil. La industria de los videojuegos es un sector en constante crecimiento y se ha convertido en una forma de entretenimiento muy popular a nivel mundial. Los jugadores interactúan con los videojuegos a través de dispositivos de entrada a los que se conoce como controladores o mandos. Mediante estos dispositivos, los jugadores controlan los movimientos y acciones de los personajes del juego y varía dependiendo de la plataforma. Por ejemplo, un controlador podría únicamente consistir de un botón y una palanca de mando o joystick, mientras otro podría presentar una docena de botones y una o más palancas, lo que llamamos mando. Los primeros juegos informáticos solían hacer uso de un teclado para llevar a cabo la interacción, o bien requerían que el usuario adquiriera un mando con un botón como mínimo. Muchos juegos de computadora modernos permiten o exigen que el usuario utilice un teclado y un ratón de forma simultánea. Generalmente los videojuegos hacen uso de otros medios, además de la imagen, de proveer los reflejos e interacción al jugador. El audio es casi universal, usándose dispositivos de reproducción de sonido, tales como altavoces y auriculares. Otro tipo de realimentación se hace a través de periféricos hápticos que producen vibración o retroalimentación de fuerza. Historia Artículo principal: Historia de los videojuegos Copia de Tennis for Two (1958), uno de los primeros videojuegos de la historia, para computadora analógica; el juego se mostraba en un osciloscopio. Máquina recreativa del videojuego Pong (1972). Los orígenes del videojuego se remontan a la década de 1950, cuando poco después de la aparición de las primeras computadoras electrónicas tras el fin de la Segunda Guerra Mundial, se llevaron a cabo los primeros intentos por implementar programas de carácter lúdico. Así, fueron creados el Nimrod (1951) o el Oxo (1952), juegos electrónicos que aún no pueden ser denominados videojuegos, y el Tennis for Two (1958) o el Spacewar! (1962), auténticos pioneros del género. Todos ellos eran todavía prototipos, juegos muy simples y de carácter experimental que no llegaron a comercializarse, entre otras cosas, porque funcionaban en unas máquinas que solo estaban disponibles en universidades o en institutos de investigación. La consola Magnavox Odyssey (1972). No sería hasta la década de los 70 en que, con la bajada de los costes de fabricación, aparecieron las primeras máquinas recreativas y los primeros videojuegos dirigidos al gran público. Títulos como Computer Space (1971) o Pong (1972), de Atari, inauguraron las primeras máquinas recreativas construidas al efecto, que funcionaban con monedas. Poco después llegarían los videojuegos a los hogares gracias a las consolas domésticas, la primera de las cuales fue la Magnavox Odyssey (1972), y más tarde la exitosa Atari 2600 o VCS (de 1977), con su sistema de cartuchos intercambiables. Por aquel entonces las máquinas de arcade empezaron a hacerse comunes en bares y salones recreativos, una expansión debida en parte a un matamarcianos que alcanzó gran popularidad, el Space Invaders (1978). Otros juegos que marcaron esta primera época fueron Galaxian (1979), Asteroids (1979) o Pac-Man (1980). La consola portátil Game Boy (1989). En los años 1980, la empresa norteamericana Atari tuvo que compartir su dominio en la industria del videojuego con dos compañías llegadas de Japón: Nintendo (con su famosa consola NES) y SEGA (con la Master System). Paralelamente, surgió una generación de computadoras, personales asequibles y con capacidades gráficas que llegaron a los hogares de millones de familias, como fueron el Spectrum, el Amstrad CPC, el Commodore 64 o el MSX. A partir de entonces, los videojuegos empezaron a convertirse en una poderosa industria. Fue además una época muy creativa para los desarrolladores de videojuegos; muchos de los principales géneros que existen hoy en día (conducción, lucha, plataformas, estrategia, aventura...) tomaron forma en esta década. Por otra parte, aparecieron también las primeras consolas de bolsillo, también conocidas como «maquinitas», que aunque hasta la llegada de la Game Boy de Nintendo (1989) solo ejecutaban un juego cada una, alcanzaron gran popularidad entre los más jóvenes. Los años 1990 traen el salto a la tecnología de 16-bit (como la SNES y la Mega Drive), lo que traía consigo importantes mejoras gráficas. Entra en escena el gigante Sony con su primera PlayStation (1994), mientras Nintendo y Sega actualizan sus máquinas (Nintendo 64 y Sega Saturn). En cuanto a las computadoras personales, el progreso de los PC termina por barrer del mapa a los demás sistemas salvo el de Apple. Aparecen entonces juegos cada vez más avanzados tecnológicamente como los juegos de disparos en primera persona o FPS, que recrean espacios en 3D. En el año 2002 entra Microsoft en el sector de las videoconsolas con su Xbox, mientras Nintendo lanza en 2001 la Gamecube y en el 2006 su innovadora Wii. Entretanto, Sony actualiza su exitosa PlayStation (versiones II y III), y en los PC, gracias a la expansión del Internet, cobran protagonismo los juegos en línea y multijugador. Por último, en la década de 2010 emergen como plataformas de juegos los dispositivos táctiles portátiles, como los teléfonos inteligentes y las tabletas, llegando a un público muy amplio. Por otro lado, varias empresas tecnológicas empiezan a desarrollar cascos de realidad virtual, que prometen traer nuevas experiencias al mundo del entretenimiento electrónico. Generalidades Típicamente, los videojuegos recrean entornos y situaciones virtuales en los que el videojugador puede controlar a uno o varios personajes (o cualquier otro elemento de dicho entorno), para conseguir uno o varios objetivos dentro de unas reglas determinadas. Dependiendo del videojuego, una partida puede disputarla una sola persona contra la máquina, dos o más personas en la misma máquina, o bien múltiples jugadores a través de una red LAN o en línea vía Internet, compitiendo colaborativamente contra la máquina o entre sí. Existen videojuegos de muchos tipos. Algunos de los géneros más representativos son los videojuegos de acción, rol, estrategia, simulación, deportes, aventura o mundo abierto. Tecnología Un videojuego se ejecuta gracias a un programa de software (el videojuego en sí) que es procesado por una máquina (el hardware) que cuenta con dispositivos de entrada y de salida. El programa de software o soporte lógico contiene toda la información, instrucciones, imágenes y audio que componen el videojuego. Va grabado en cartuchos, discos ópticos, discos magnéticos, tarjetas de memoria especiales para videojuegos, o bien se descarga directamente al aparato a través de Internet. En la década de 1980 el soporte habitual para el software era el cartucho en las videoconsolas, y el disco magnético o la cinta de casete en las computadoras. Posteriormente se usó el CD-ROM, pues tenía más capacidad que los cartuchos ya que estos habían llegado a su tope tecnológico y además resultaba más económico para producir en masa. Actualmente se usa el sistema DVD de alta capacidad y, en las consolas de sobremesa como PlayStation 4 y Xbox One, el Blu-Ray, de capacidad muy alta. Sin embargo desde hace unos años está creciendo la descarga desde Internet, al ser una tecnología extendida masivamente, de fácil acceso y menos costosa que la distribución física de discos, aparte de las ventajas de seguridad al evitar pérdidas por daños o extravío de discos (ya que el videojuego estará virtualmente siempre disponible). Un mando, dispositivo de entrada típico en las consolas. Los dispositivos de entrada son los que permiten al jugador manejar el juego. Si bien es habitual el uso de un dispositivo de entrada externo ,como son los clásicos teclado y ratón, el mando, o el joystick,, las plataformas portátiles de hoy en día (smartphones, tablets, videoconsolas de bolsillo...) permiten jugar mediante su pantalla táctil o mediante el movimiento del propio aparato (gracias al uso de giroscopios y acelerómetros). Otros dispositivos de entrada pueden ser los detectores de movimiento, entre los que destacan los dispositivos de mano (por ejemplo el Wiimote de Wii), los de presión (alfombras o soportes con sensores), los de dispositivos de realidad virtual como la PlayStation VR y los de captura de imágenes, caso del Kinect de Xbox. También se puede emplear la voz en aquellos videojuegos que la soporten a través de procesadores de voz. Los dispositivos de salida son aquellos que muestran las imágenes y los sonidos del videojuego: un televisor, un monitor o un proyector para el vídeo, y unos altavoces o auriculares para el audio. Los equipos más modernos utilizan sonido digital con Dolby Surround con efectos EAX y efectos visuales modernos por medio de las últimas tecnologías en motores de videojuego y unidades de procesamiento gráfico. La pieza central del hardware lo constituye la CPU o unidad central de procesamiento, que interpreta las instrucciones contenidas en los programas y procesa los datos. Su capacidad de procesamiento, mayor en cada nueva generación de dispositivos, marca el límite de las posibilidades técnicas y gráficas de los videojuegos. Todos estos dispositivos (de entrada, de salida, de procesamiento...) pueden constituir unidades físicamente separadas pero conectadas entre sí ,como es el caso de las PC o las videoconsolas de sobremesa,, o bien estar integradas en un solo aparato ,como sucede en los teléfonos celulares y otros dispositivos portátiles,. Plataformas Artículos principales: Videoconsola, Computadora personal, Arcade y Videoconsola portátil. Los distintos tipos de dispositivo en los que se ejecutan los videojuegos se conocen como plataformas. Los cuatro tipos de plataformas más populares son la PC, las videoconsolas, los dispositivos portátiles y las máquinas de arcade. La PlayStation 2 es, con más de 155 millones de unidades, la consola de videojuegos más vendida de la historia. Las videoconsolas o consolas de videojuegos son aparatos electrónicos domésticos destinados exclusivamente a reproducir videojuegos. Creadas por diversas empresas desde los años 70, han generado un inmenso negocio de trascendencia histórica en la industria del entretenimiento. La videoconsola por antonomasia es un aparato de sobremesa que se conecta a un televisor para la visualización de sus imágenes, pero existen también modelos de bolsillo con pantalla incluida, conocidos como videoconsolas portátiles. El PC u computador personal es también una plataforma de videojuegos, pero como su función no es solo ejecutar videojuegos, no se considera como videoconsola. La PC no entra en ninguna generación, ya que cada pocos meses salen nuevas piezas que modifican sus prestaciones. La PC por regla general puede ser mucho más potente que cualquier consola del mercado. Las más potentes soportan modos gráficos con resoluciones y efectos de postprocesamiento gráfico muy superiores a cualquier consola. Las máquinas recreativas de videojuegos están disponibles en lugares públicos de diversión, centros comerciales, restaurantes, bares, o salones recreativos especializados. En los años 1980 y 90 del siglo pasado disfrutaron de un alto grado de popularidad al ser entonces el tipo de plataforma más avanzado técnicamente. Los progresos tecnológicos en las plataformas domésticas han supuesto a comienzos del siglo XXI una cierta decadencia en el uso de las máquinas arcade. Las videoconsolas portátiles y otros aparatos de bolsillo cuentan con la capacidad para reproducir videojuegos. Entre estos últimos destacan hoy en día los teléfonos celulares (en particular los teléfonos inteligentes) que, sin ser los videojuegos su función primaria, los han ido incorporando a medida que se han ido incrementando sus prestaciones técnicas. Otros dispositivos portátiles de creciente popularidad en los últimos años son las tabletas. Géneros Artículo principal: Género de videojuegos Los videojuegos de conducción son uno de los géneros más populares. Los videojuegos se pueden clasificar en géneros dependiendo de factores como el sistema de juego, el tipo de interactividad con el jugador, sus objetivos, etc. La evolución de los videojuegos desde sus comienzos ha dado lugar a una variedad creciente y cambiante de géneros, muchas veces en relación con lo que los avances en la tecnología han ido haciendo posible. Entre los géneros de videojuegos más populares están los de acción, estrategia, rol, aventura, rompecabezas, simulación, deportes o carreras, cada uno de ellos con varios subgéneros. Por otro lado, hoy en día son habituales los videojuegos que toman elementos de más de un género, lo que ha dado lugar a géneros mixtos (por ejemplo rol-acción, acción-aventura, mundo abierto, etc.). Junto a los géneros, existen otras formas de clasificar o caracterizar los juegos como puede ser por su temática (fantástico-medieval, futurista, de guerra...), su complejidad (juegos hardcore, juegos casuales...), su finalidad (educativos, promocionales, artísticos...), tipo de desarrollo (producciones AAA, independientes), etc. Por otra parte, también se distingue a unos juegos de otros, incluso dentro de un mismo género, por la perspectiva visual que adoptan (o dicho de otra manera, la posición de la cámara). Así, hay juegos con perspectiva 2D (ya sea con proyección paralela, vista lateral o vista cenital), 2.5D (mediante proyección isométrica, oblicua, o parallax scrolling, entre otras), y 3D (en perspectiva fija, en primera persona, o en tercera persona). Multijugador Artículo principal: Videojuego multijugador En muchos juegos se puede encontrar la opción de multijugador, es decir, que varias personas puedan participar simultáneamente en la misma partida, ya sea empleando todos la misma máquina (como suele ocurrir con las videoconsolas) o bien usando cada uno su propio dispositivo (el caso habitual en las PC o dispositivos portátiles, en lo que se conoce como videojuegos en línea). Existen juegos en los que un jugador humano se enfrenta contra otros jugadores controlados por la máquina mediante inteligencia artificial (PNJ), pero en este caso no se considera que sea un videojuego multijugador. Por último, hay videojuegos que están pensados para congregar a un gran número de jugadores de todo el mundo conectados a través de Internet, conocidos como videojuegos MMO (del inglés massive multiplayer online). Industria del videojuego Artículo principal: Industria de los videojuegos Nace con la aparición de la primera máquina recreativa a monedas en 1971, la industria del videojuego ha pasado en unas pocas décadas de ser una mera curiosidad tecnológica a convertirse en una de las mayores industrias del entretenimiento por volumen de facturación. Se estima este en 1 dólar en 2014 a nivel mundial, llegando a duplicar el de la industria del cine en el mismo año. Los ingresos proceden fundamentalmente de la venta de videojuegos, de videoconsolas, de accesorios y de máquinas recreativas. Los principales países en ingresos por videojuegos son EE. UU., China y Japón, seguidos de Alemania y Reino Unido. España se sitúa en la décima posición, facturando anualmente cerca de 1 500 000 000 de dólares. La industria del videojuego da trabajo a más de 100 000 personas en todo el mundo, gente de muy diversas disciplinas entre las que se incluye la programación, el diseño, la ingeniería, la interpretación, las finanzas, la mercadotecnia, la música, la comunicación o el comercio. La cadena de valor en la industria del videojuego se puede dividir en 6 partes: los inversores, los desarrolladores de videojuegos, los creadores del software empleado por los desarrolladores, los fabricantes de hardware, las distribuidoras de videojuegos, y los consumidores. Los costes de desarrollo de un videojuego comercial varían enormemente desde los pocos miles de dólares que puede representar un título pequeño, desarrollado por una sola persona, hasta los más de 100 000 000 de dólares de algunos videojuegos AAA, en los que intervienen equipos de hasta un centenar de trabajadores. El videojuego con mayor coste de desarrollo hasta la fecha es Grand Theft Auto V, de la desarrolladora Rockstar Games, con 167 millones de dólares, seguido por Destiny, de Bungie, con 154 millones de dólares. Las cifras son aún bastante mayores si se suma la inversión en mercadotecnia. Las ferias de videojuegos constituyen uno de los principales escaparates donde la industria presenta sus más recientes creaciones cada año. Las más conocidas mundialmente son la E3 en Los Ángeles (EE. UU., cancelada en 2023), la Gamescom en Colonia (Alemania) y la Tokyo Games Show (Japón). En Francia la feria más importante es la Paris Games Week y en España destacan la Madrid Games Week, GameLab y GameFest. En Chile destaca sobre todo la Festigame, siendo la más importante de Iberoamérica. Los consumidores se informan de las novedades del sector principalmente a través de medios de comunicación especializados. Entre los pertenecientes al ámbito hispano se pueden destacar revistas en papel como Micromanía, New Superjuegos o Hobby Consolas, y revistas en Internet como Meristation, Vandal, Eurogamer, Gamercafe, LagZero o Niubie. Existen también numerosos blogs y canales de Youtube centrados esta temática. Es menor sin embargo su presencia en televisión o en radio. La venta de videojuegos se ha realizado tradicionalmente en grandes almacenes o en tiendas físicas especializadas; en España las dos principales cadenas de tiendas de videojuegos son Game y, hasta su cierre en 2014, Gamestop. En Chile las más grandes son Zmart, MicroPlay y TodoJuegos. Sin embargo, la tendencia en estos últimos años en todo el mundo es hacia la venta por internet mediante descarga, tanto en PC como en consolas. En dispositivos móviles, la venta por internet ,a través de las tiendas de aplicaciones, es de hecho el único canal disponible. Las principales asociaciones sectoriales en España son AEVI (Asociación Española de Videojuegos), que engloba a compañías que facturan el 90 % de los ingresos totales del sector, y DEV (Desarrollo Español de Videojuegos), que agrupa a las principales compañías desarrolladoras del país. En Chile está el grupo VGChile, donde se agrupan los desarrolladores chilenos. Algunas de las más importantes desarrolladoras de videojuegos a nivel internacional son: Blizzard Entertainment, Valve, Rockstar North, Bungie, Electronic Arts, Microsoft, Nintendo, BioWare, Sega, Sierra Entertainment o Zynga, a las cuales hay que agregar los estudios internos (a menudo homónimos) de las principales distribuidoras. Videojuegos indie Artículo principal: Videojuego independiente En un mercado dominado por las grandes distribuidoras de videojuegos, cabe destacar en esta última década el auge de los videojuegos independientes (también conocidos como indie), que han llegado a constituir un apartado propio en tiendas y en plataformas de distribución como Steam o Itch.io. Estos juegos son desarrollados por pequeños grupos de no más de 20 personas, sin la ayuda financiera de ninguna distribuidora. Se caracterizan habitualmente por un desarrollo artístico variado y particular, tanto en gráficos como en banda sonora, con historias de tramas innovadoras y que generalmente no tienen una continuidad o no están diseñados para crear una saga. La originalidad de sus planteamientos, tanto visual como mecánicamente, alejándose muchas veces de los estereotipos establecidos, les ha valido el interés de una parte de los aficionados. Desarrollo de videojuegos Artículo principal: Desarrollo de videojuegos Los desarrolladores usan varias herramientas para crear videojuegos. La creación de videojuegos es una actividad llevada a cabo por las desarrolladoras de videojuegos. Estas se encargan de diseñar y programar el videojuego, desde el concepto inicial hasta el videojuego en su versión final. Esta es una actividad multidisciplinaria, que involucra profesionales de la informática, el diseño, el sonido, la actuación, etc. El proceso es similar a la creación de software en general, aunque difiere en la gran cantidad de aportes creativos (música, historia, diseño de personajes, niveles, etc.) necesarios. El desarrollo también varía en función de la plataforma objetivo (PC, móviles, consolas), el género (estrategia en tiempo real, RPG, aventura gráfica, plataformas, etc.) y la forma de visualización (2d, 2.5d y 3d). La comercialización de los juegos creados por las desarrolladoras es labor de las distribuidoras de videojuegos. Estas se ocupan de su distribución (ya sea a través de tiendas físicas o por internet), publicidad, presentación, traducción... pero también ejercen a menudo un papel fundamental antes y durante el desarrollo del juego, como es su concepción, su financiación, los estudios de mercado, el control de calidad, etc. Muchas distribuidoras tienen uno o varios estudios de desarrollo propio, al margen de que puedan o no trabajar con desarrolladoras externas. Ludología Artículo principal: Ludología La ciencia que estudia los videojuegos se llama «ludología» o estudio de los juegos. Uno de los aspectos principales estudiados por esta ciencia y la información que recolecta son los impactos positivos y negativos de los videojuegos en las personas. Esta ciencia se ocupa del estudio crítico de los juegos, de su diseño, de los jugadores y de la interacción entre ambos, así como su papel en la sociedad y la cultura. Los métodos usados para recolectar información van desde encuestas e investigaciones etnográficas, hasta experimentos controlados de laboratorio.[cita requerida] Impacto social Demografía Una encuesta en línea realizada por la ISFE entre europeos con edades comprendidas entre 16 y 64 años reveló que el 48 % de ellos juega a videojuegos, ya sea de manera habitual (1 o más veces por semana, 25 %) u ocasional (23 %). En España dicho porcentaje se situaría en el 40 %, y en EE. UU. (según la ESA) en el 59 %. Por edades, el 51 % de los videojugadores europeos son menores de 35 años y el 49 % mayores, situándose la edad promedio en 34 años (el estudio realizado en EE. UU la sitúa en 31 años). Por sexos, el 55 % son hombres, el 45 % mujeres. Según un estudio de 2015, los datos en los EE. UU. cambian; 50% de los hombres juegan a videojuegos mientras que un 48% de mujeres lo hace; de estos, solo un 15% de hombres se denominarían Gamers contra un 6% en las mujeres. Cabe señalar que la demografía de los videojuegos no ha sido siempre la misma. En el pasado los videojuegos eran un tipo de entretenimiento casi exclusivamente para personas jóvenes, pero esta situación ha ido cambiando con el paso de los años como demuestran diversos estudios recientes. Además de los ya señalados, un estudio de Parra, David et al. (2009) en el que se realizaron 974 encuestas a españoles mayores de 35 años concluyó que los videojuegos se están implantando con singular intensidad en el conjunto de la población española. Más de la mitad de los españoles mayores de 35 años (53,5 por ciento) juega con videojuegos (bien de manera esporádica o bien de manera habitual). Efectos psicológicos Artículo principal: Controversia en los videojuegos Los efectos que pueda tener el uso habitual de videojuegos en las personas, y en especial en los niños, han sido objeto de interés y de controversia. Jóvenes jugando a videojuegos. Entre los efectos positivos que se les atribuyen están capacidades tales como: «coordinación ojos-manos, capacidad lógica, capacidad espacial, resolución de problemas, desarrollo de estrategias, concentración, atención, colaboración, cooperación y selección de información relevante, estimulación auditiva, entre otras». Según un estudio, el niño desarrolla habilidades mentales y su capacidad de razonamiento es más activa en comparación a un niño de hace 20 años que no contaba con esta tecnología. En adultos pueden funcionar como un liberador de estrés, contribuyendo a una buena salud. Otros afirman que mejoran la salud visual e incluso ciertas habilidades como por ejemplo las necesarias para práctica de la cirugía. Hay que señalar también que los efectos varían según el tipo de juego. Un catedrático de la Universidad de Nottingham también ha afirmado que pueden tener el efecto de atenuar el dolor. Según un estudio, la exposición a corto plazo tiene un efecto positivo en la atención en unos niños sin problemas psiquiátricos, estos obtuvieron una mejor puntuación en la prueba de Stroop después de estar expuestos durante una hora a un videojuego que jugaron por primera vez. En cuanto a los aspectos negativos de los videojuegos, cabe señalar factores que se encuentran bajo escrutinio como la adicción. El fácil acceso a computadoras, smartphones y consolas, junto a la falta de control por parte de los padres o el ambiente de un hogar disfuncional, podría dar lugar a que niños o adolescentes hagan un uso abusivo de los videojuegos. Ello podría conllevar efectos negativos como ser más propensos a la agresividad, falta de asertividad y bajo rendimiento académico. Expertos en medicina y salud mental han mostrado preocupación por ello y la Organización Mundial de la Salud llegó a incluirlo como «juego patológico» en la undécima revisión de su Clasificación Internacional de Enfermedades entre otras adicciones a nuevas tecnologías. Sin embargo; otros expertos en la materia, entre los que se incluye la Asociación Estadounidense de Psiquiatría, han afirmado que no hay evidencias suficientes de que los videojuegos puedan producir tendencias violentas o inducir a un comportamiento adictivo, aunque están de acuerdo que los videojuegos típicamente hacen uso del «bucle de compulsión» en su diseño el cual puede crear dopamina y esta puede ayudar a reforzar el deseo de continuar jugando a través de ese bucle de compulsión y potencialmente llevar a comportamientos violentos o adictivos.  Otros medios han puesto de manifiesto que la inclusión es algo cuestionable dado que no todos los juegos presentan un diseño que incluya mecanismos etiológicos como sí lo hacen los juegos que incluyen cajas de botín por su similitud con los juegos de azar. Otro aspecto controvertido de los videojuegos en los niños es que pueden frenar algunos aspectos de su desarrollo motriz, y conducir a una falta de socialización, aunque esto último está rebatido por otros estudios que apuntan a todo lo contrario, a que los videojuegos aumentan su sociabilidad. Los videojuegos son un entretenimiento que se adecua bien a la realidad del niño nacido en la era de la informática ya que suponen una socialización en la cultura de simulación que caracteriza a las sociedades avanzadas contemporáneas” (Turkle 1997). En un estudio se asocia la salud mental y jugar videojuegos, se encontró que los jugadores que jugaban de forma moderada tenían la mejor salud mental, los que jugaban de forma excesiva tenían un leve incremento en comportamientos problemáticos y los que no jugaban videojuegos tuvieron la peor salud mental. Estudios científicos demuestran que, en general, los videojuegos enriquecen la vida del jugador, le enseñan a resolver problemas técnicos, y estimulan sus habilidades neuro-cinéticas, reflejos visuales y enfoque de múltiples puntos de visión (objetivos). Incluso mejoran la comunicación cuando se juega en familia o en línea. Como herramientas educativas En algunos colegios han sido implementados algunos videojuegos con el fin de expandir la metodología didáctica y hacer las clases más amenas. Diversos expertos han señalado el valor de los videojuegos como herramientas para inculcar conocimientos. Gros, B. y sus colaboradores (1997) escriben: «Pensamos que los juegos de computadora constituyen un material informático de gran valor pedagógico», y enumeran una serie de características: Se trata de materiales con una capacidad de motivación muy alta. Mejoran los aspectos procedimentales del trabajo de los estudiantes. Son muy flexibles dado que se pueden utilizar en diferentes asignaturas y de manera transversal. Proporcionan elementos para mejorar la autoestima de los alumnos. Es un material que está a disposición tanto de los alumnos como del profesorado. Los juegos educativos se presentan en los últimos tiempos como una alternativa a los videojuegos violentos. Incluso existe una colección de juegos cuya carátula versa «la alternativa inteligente a los videojuegos violentos».  A pesar de las cuestiones positivas, se deben tener en cuenta todos aquellos aspectos negativos, como el uso ilimitado y no vigilado, así como la falta de compromiso, responsabilidad o esfuerzo con actividades que no estén relacionadas con el juego. Por ello, lo ideal es no perder de vista que aunque los videojuegos están en función del entretenimiento, son utilizados como herramientas para posibilitar o potencializar el aprendizaje, lo cual se logrará siempre y cuando exista un buen uso y control por parte de los usuarios o los responsables de estos. Gee (2004) establece que “la teoría de aprendizaje incorporada a los buenos videojuegos se acerca más a lo que yo creo que son las mejores teorías del aprendizaje planteadas por la ciencia cognitiva”. De esta forma, establece una similitud entre las teorías del aprendizaje que se utilizan diariamente en el ámbito educativo con las teorías incorporadas en los videojuegos, encontrando entre ellas pocos puntos en los que diverjan. Críticas Los videojuegos, como otras formas de expresión audiovisual, han despertado controversia entre personas o colectivos que consideran que tienen o pueden tener efectos perniciosos sobre los jugadores. Entre estos se arguyen por ejemplo los efectos que puede tener en el desarrollo emocional el hecho de pasar demasiado tiempo ante la pantalla e inhibirse por completo en un universo de fantasía. Existen asimismo casos de ludopatía y de ciber-adicción.[cita requerida] También se argumenta[¿quién?] un posible fomento de la violencia y lujuria, gráficamente presentes en muchos videojuegos. Por otro lado, se ha comprobado que la rapidez con que se mueven los gráficos puede provocar ataques en las personas que padecen diversos tipos de epilepsia.  Influencia en el rendimiento académico Diferentes estudios con niños y adolescentes (Castells y Bofarull, 2002; Bringas, Rodríguez y Herrero, 2008) demuestran que el rendimiento escolar puede verse afectado debido al uso de los videojuegos. Ahora bien, existen diferentes artículos que establecen que niveles moderados de juego no se asocian con un bajo rendimiento escolar (Ferguson, 2011); incluso, podrían relacionarse con un mejor rendimiento (Llorca, Bueno, Villar y Díez, 2010). Esto se debe a que los jugadores que utilizan videojuegos adquieren mejores estrategias de conocimiento, estrategias de resolución de problemas, y sus capacidades espaciales, su precisión y capacidad de reacción se ven mejoradas gradualmente (McFarlane, 2002). Aspectos jurídicos en España En España, la imposibilidad de registrar el videojuego como tal, obedece a que no está reconocido jurídicamente por no estar contemplado en el marco del artículo 145.1 del Real Decreto Legislativo 1/1996, de 12 de abril (en adelante, Ley de Propiedad Intelectual o LPI), como creación intelectual original y unitaria, obliga a sus creadores y titulares a desistir a registrarlo como videojuego, al tener que pasar por separar cada obra de acuerdo a su propia naturaleza, artística, científica o literaria, lo que se evidencia como un obstáculo ante la falta de una regulación específica que permita a la industria proteger, exportar e internacionalizar estos activos intangibles en constante crecimiento socioeconómico, bajo un marco legal estable en el ámbito de la propiedad intelectual de sus creadores. En el año 2009, la «Proposición No de Ley (PNL)» del Congreso de los Diputados, fue aprobada por unanimidad con dos enmiendas del Partido Popular y CiU: «La Comisión de Cultura del Congreso de los Diputados establece que el videojuego constituye un ámbito fundamental de la creación y la industria cultural de España. En consecuencia, insta al Gobierno a reconocer a sus creadores y emprendedores como protagonistas de nuestra cultura. Asimismo, en el marco de sus competencias y en coordinación con las administraciones autonómicas competentes, se insta al Gobierno a facilitar su acceso a todas las ayudas factibles para la promoción de su actividad, la financiación como industria cultural y la internacionalización de sus iniciativas». Lo más relevante de la PNL, es que se acuerda por unanimidad que «no debe ser válido contemplarlos como obra audiovisual y, por otro, tampoco se les debe asimilar simplemente a creadores de software». Origins Tennis for Two (1958), uno de los primeros juegos de computadora analógicos que utilizaba un osciloscopio como pantalla. Spacewar! (1962), uno de los primeros juegos de computadora mainframe, mostrado ejecutándose en un computador PDP-1 Pong (1972), uno de los primeros videojuegos arcade Los primeros videojuegos utilizaban dispositivos electrónicos interactivos con varios formatos de visualización. El primer ejemplo data de 1947: un dispositivo de entretenimiento de tubo de rayos catódicos fue patentado el 25 de enero de 1947 por el pionero de la televisión Thomas T. Goldsmith Jr. y Estle Ray Mann, y fue emitidó el 14 de diciembre de 1948 como la patente estadounidense número 2455992. Inspirado en la tecnología de visualización por radar, este consistía en un dispositivo analógico que permitía al usuario controlar el arco parabólico de un punto en la pantalla para simular el disparo de un misil contra los objetivos, los cuales eran dibujos de papel fijados a la pantalla. Otros de los primeros ejemplos incluyen una versión del juego de mesa de damas del informático teórico británico Christopher Strachey, la computadora Nimrod en el Festival de Gran Bretaña de 1951; OXO, un juego de computadora de tres en raya (triquis, ceros y curces o tres en línea) desarrollado por Alexander S. Douglas para la EDSAC en 1952; Tennis for Two, un juego electrónico interactivo diseñado por el físico William Higginbotham en 1958; y Spacewar!, escrito por los estudiantes del Instituto Tecnológico de Massachusetts Martin Graetz, Steve Russell (programador) y Wayne Wiitanen en una computadora DEC modelo PDP-1 en 1962. Cada juego tenía diferentes medios de visualización: La NIMROD tenía instalado un panel de luces para jugar al videojuego Nim, por ejemplo, El OXO contaba con un monitor gráfico para jugar su versión de tres en línea, Tennis for Two poseía un osciloscopio el cual permitía mostrar una vista lateral de una cancha de tenis, y por último Spacewar! tenía el monitor vectorial DEC modelo PDP-1 para tener dos naves espaciales que lucharan entre sí. Ralph H. Baer (Izq, en 2009) y Nolan Bushnell (Der, en 2013) han sido descritos como los Padres de los Videojuegos por su trabajo en el campo Estos inventos sentaron las bases de los videojuegos modernos. En 1966, mientras trabajaba en Sanders Associates,Inc como ingeniero, Ralph H. Baer ideó un sistema para jugar a un simple juego de tenis de mesa en una pantalla de televisión. Con la aprobación de la empresa, Baer creó el prototipo llamado Brown Box. La Corporación Sanders patentó las innovaciones de Baer y las licenció a la compañía electrónica Magnavox, la cual comercializó dicha tecnología como la primera consola de videojuegos doméstica, bautizada como la Magnavox Odyssey, lanzada en el año 1972. Por otro lado, el emprendedor Nolan Bushnell y el ingeniero Ted Dabney, al ser inspirados después de ver Spacewar! funcionando en la Universidad de Stanford, idearon una versión similar que funcionaba en una máquina recreativa más pequeña (o máquina arcade) que funcionaba por medio de monedas y utilizando una computadora un poco más económico. Este se estrenó como Computer Space, el primer juego arcade en 1971. Bushnell y Dabney fundaron la compañía Atari, Inc. y, junto con Allan Alcorn, crearon su segundo juego arcade en 1972, el exitoso videojuego titulado Pong, un juego al estilo ping pong inspirado de forma muy directa en el juego de tenis de mesa de la consola de videojuegos Odyssey. Atari creó una versión doméstica de Pong, lanzado en la Navidad de 1975. El gran éxito de la Odyssey y Pong, como juego arcade y como consola doméstica, impulsó la creación y desarrollo de la industria de los videojuegos.  Tanto Baer como Bushnell han sido titulados y apodados como los Padres de los Videojuegos gracias a sus contribuciones."
ksampletext_wikipedia_vide_videoconsola: str = "Videoconsola. Una videoconsola, o simplemente una consola, es un sistema electrónico de entretenimiento que ejecuta videojuegos contenidos en cartuchos, discos ópticos, discos magnéticos, tarjetas de memoria o en cualquier dispositivo de almacenamiento. Los primeros sistemas de videoconsolas fueron diseñados únicamente para jugar videojuegos pero a partir de la quinta generación de videoconsolas han sido incorporadas características importantes de multimedia, internet, tiendas virtuales y servicio en línea como: Nintendo Switch Online, PlayStation Network, y Xbox Network. Una videoconsola es un pequeño sistema electrónico que está diseñado para ejecutar principalmente juegos desarrollados en una computadora o servidor. Al igual que las computadoras, pueden adoptar diferentes formas y tamaños; de este modo, pueden ser de sobremesa, es decir, requieren ser conectadas a un televisor para la visualización del videojuego, y a la red eléctrica para su alimentación, en la cual suelen consumir 12 voltios, o bien el dispositivo electrónico videoconsola portátil, que cuenta con una pantalla de visualización integrada y una fuente de alimentación propia (baterías o pilas). Los videojuegos pueden presentarse en forma de cartuchos de plástico que protegen una placa con chips en los que está almacenado el software, o también en disquete (como en Commodore 64), tarjeta de memorias, disco compactos (como en el caso de PlayStation, Sega Saturn), discos GD-ROM (en el caso de Sega Dreamcast), discos GOD (en el caso de Nintendo GameCube), DVD (como en PlayStation 2, Wii, Xbox, Xbox 360), Blu-ray (en el caso de la PlayStation 3, Xbox One, PlayStation 4), o Blu-ray Ultra HD (en el caso de PlayStation 5, Xbox Series X). El DVD-ROM y el BD-ROM son los que se han impuesto como estándar en las videoconsolas de séptima generación. El formato cartucho se utilizaba básicamente para videoconsolas portátiles o en generaciones pasadas de videojuegos, siendo las últimas más destacables la Nintendo 64 y Game Boy Advance. A la fecha septiembre de 2021, PlayStation Portable usa UMD, formato propietario de Sony, Nintendo DS, Nintendo 3DS y Nintendo Switch utilizan dispositivos portátiles de tarjetas SD. Historia En la industria de los videojuegos, las videoconsolas han sido clasificadas en distintas generaciones. Esta clasificación la determina su tiempo de lanzamiento y la tecnología existente en ese momento. Las empresas fabricantes lanzan una nueva consola en determinado tiempo (que puede variar entre 5 o 6 años). Por otro lado, algunas generaciones están señaladas por un número determinado de bits, los cuales determinan el ancho de bus del procesador, (de la segunda generación hasta la sexta generación). Los primeros fabricantes ya presentaban equipos de 16 bits. A partir de esta cantidad, se fueron realizando las siguientes generaciones de consolas. Una consola de generación superior no tiene que poseer necesariamente un procesador de ancho de bus de datos de más bits, al contrario que la creencia popular que piensa que en cada generación se dobla el número de la anterior, ya que la potencia de un procesador está determinada además de por su ancho de bus por su estructura y velocidad. En las videoconsolas de reciente generación ya no solo depende la potencia de la unidad CPU sino también del procesador gráfico GPU que es el procesador encargado del manejo de gráficos en la consola. Cada componente tiene una determinada cantidad de bits y velocidad. Primera generación Artículo principal: Videoconsolas de primera generación Si bien los primeros juegos de computadora aparecieron en la década de los 1960, éstos utilizaban pantallas vectoriales, no de vídeo analógico. No fue hasta 1972 cuando se lanzó la primera videoconsola de sobremesa por la compañía electrónica Magnavox. La Magnavox Odyssey, fue creada por Ralph Baer, considerado como el padre de los videojuegos. La Odyssey tuvo un moderado éxito, sin embargo, con el lanzamiento del juego arcade Pong de Atari, comenzaron a popularizarse los videojuegos, el público comenzó a mostrar interés ante la nueva industria. En el otoño de 1975, la compañía Magnavox, cede ante la popularidad del Pong, se cancela el proyecto Odyssey, ya que el público solo jugaba al Pong y Hockey en la Odyssey 100. Una posterior actualización de la consola Odyssey 100, la 200, llevaba incorporada una pantalla de puntuación, permitía hasta 4 jugadores, y se vendía junto con un tercer juego: Smash. Casi simultáneamente, la cadena de centros comerciales Sears compró los derechos del sistema Atari Pong y lo introdujeron en el mercado de consumo bajo el nombre de Sears-Telegames. Al igual que en el mercado arcade, el mercado pronto fue inundado por consolas clones de Pong y juegos derivados. Atari Pong de Atari Atari Pong de Atari Tele-Games Pong de Sears Licenciado por Atari Tele-Games Pong de Sears Licenciado por Atari Coleco Telstar de Coleco Coleco Telstar de Coleco Magnavox Odyssey de Magnavox y Philips Magnavox Odyssey de Magnavox y Philips Magnavox Odyssey 200 de Magnavox y Philips Magnavox Odyssey 200 de Magnavox y Philips Color TV-Game de Nintendo Color TV-Game de Nintendo APF TV Fun de APF Electronics Inc APF TV Fun de APF Electronics Inc Color TV Game System de Ricochet Electronic Color TV Game System de Ricochet Electronic Volley VI de Roberts Volley VI de Roberts Serie de consolas PC-50x Serie de consolas PC-50x Serie Philips Tele-Spiel de Philips Serie Philips Tele-Spiel de Philips Overkal de Inter Electrónica Overkal de Inter Electrónica Segunda generación Artículo principal: Videoconsolas de segunda generación En esta generación resaltaron Atari 2600, Colecovision, Mattel Intellivision y la Atari 5200. El dominio absoluto fue de Atari, aunque tuvo al menos dos rivales destacables. Colecovision con el doble de colores que la 2600 e Intellivision de Mattel que por primera vez en la historia incluye una CPU de 16 bits.(1976-1983) Atari 2600 de Atari Atari 2600 de Atari ColecoVision de Coleco ColecoVision de Coleco Intellivision de Mattel Intellivision de Mattel Magnavox Odyssey² de Magnavox y Philips Magnavox Odyssey² de Magnavox y Philips Atari 5200 de Atari Atari 5200 de Atari Game & Watch de Nintendo Game & Watch de Nintendo Juegos Elektronika IM de Elektronika Juegos Elektronika IM de Elektronika Vectrex de Milton Bradley Company Vectrex de Milton Bradley Company Arcadia 2001 de Arcadia Corporation Arcadia 2001 de Arcadia Corporation Epoch Cassette Vision de Epoch Epoch Cassette Vision de Epoch RCA Studio II de RCA RCA Studio II de RCA Fairchild Channel F de Fairchild Semiconductor Fairchild Channel F de Fairchild Semiconductor Palmtex Portable Videogame System de Palmtex Palmtex Portable Videogame System de Palmtex Commodore MAX Machine de Commodore Commodore MAX Machine de Commodore Milton Bradley Microvision de Milton Bradley Company Milton Bradley Microvision de Milton Bradley Company Bally Astrocade de Bally Manufacturing Bally Astrocade de Bally Manufacturing Tercera generación Artículo principal: Videoconsolas de tercera generación Tras la crisis de los videojuegos, el mundo de las consolas prácticamente es un monopolio japonés. En esta generación las consolas como la NES (Nintendo Entertainment System), Famicom (así se llamaba la NES en Japón) o Hyundai Comboy (llamada así en Corea del Sur) y la Sega Master System tenían 8 Bits. La NES domina prácticamente sola hasta la llegada de Mega Drive en 1988. SG-1000 de Sega SG-1000 de Sega SG-1000 Mark III Sega Mark III de Sega Versión Japonesa de la Master System SG-1000 Mark III Sega Mark III de Sega Versión Japonesa de la Master System Master System de Sega Master System de Sega Famicom de Nintendo Versión Japonesa de la NES Famicom de Nintendo Versión Japonesa de la NES Nintendo Entertainment System de Nintendo Nintendo Entertainment System de Nintendo Twin Famicom de Sharp Corporation Licenciado por Nintendo Twin Famicom de Sharp Corporation Licenciado por Nintendo Sharp Nintendo Television de Nintendo y Sharp Sharp Nintendo Television de Nintendo y Sharp Dendy de Steepler Clon Taiwanesa de la NES Dendy de Steepler Clon Taiwanesa de la NES Atari 7800 de Atari Atari 7800 de Atari Amstrad GX4000 de Amstrad Amstrad GX4000 de Amstrad Atari XE Game System de familia Atari de 8 bits Atari XE Game System de familia Atari de 8 bits Videopac + G7400 de Philips Videopac + G7400 de Philips Super Cassette Vision de Epoch Co. Super Cassette Vision de Epoch Co. Dina de Bit Corporation Dina de Bit Corporation Commodore 64 Games System de Commodore Commodore 64 Games System de Commodore VTech Socrates de VTech VTech Socrates de VTech Action Max de Worlds of Wonder Action Max de Worlds of Wonder Cuarta generación Artículo principal: Videoconsolas de cuarta generación En 1987 NEC y Hudson, ponen la consola PC Engine en Japón o Turbografx en el resto del mundo, que tienen una CPU de 8 bits pero un chip gráfico de 16 bits. En 1988 Sega presenta su consola con una CPU de 16 bits conocida como Sega Genesis en América y Sega Mega Drive en Europa y Asia. En 1990 Nintendo saca su consola de 16bits Super Nintendo y este mismo año, la productora de arcades SNK saca Neo-Geo, la consola más potente de esta generación llamada el Rolls Royce de las consolas por su elevado precio. Esta generación destaca, por los chips gráficos añadidos al cartucho, como el Super FX y SVP y las ampliaciones de hardware de Mega Drive: Mega CD y Sega 32X. Aparecen conceptos como multitarea, multimedia, gráficos vectoriales, etc. Super Nintendo es la consola más vendida con 49 millones de unidades, aunque la más vendida en Europa es Mega Drive. Sega Mega Drive de Sega Versión Europea de la Sega Genesis. Sega Mega Drive de Sega Versión Europea de la Sega Genesis. Sega Genesis de Sega Sega Genesis de Sega Super Nintendo Entertainment System de Nintendo Super Nintendo Entertainment System de Nintendo Super Famicom de Nintendo Versión Japonesa de la SNES. Super Famicom de Nintendo Versión Japonesa de la SNES. PC Engine de NEC Versión japonesa de la TurboGrafx-16. PC Engine de NEC Versión japonesa de la TurboGrafx-16. TurboGrafx-16 de NEC TurboGrafx-16 de NEC Neo-Geo de SNK. Neo-Geo de SNK. Sega Mega-CD de Sega Sega Mega-CD de Sega Sega Game Gear de Sega Sega Game Gear de Sega PC Engine SuperGrafx de NEC PC Engine SuperGrafx de NEC CD-i de Philips CD-i de Philips Atari Lynx de Atari Atari Lynx de Atari TurboExpress de NEC TurboExpress de NEC PC Engine Duo de NEC PC Engine Duo de NEC Watara Supervision de Supervision Watara Supervision de Supervision Game Boy de Nintendo Game Boy de Nintendo Game Boy Pocket de Nintendo Game Boy Pocket de Nintendo Game Boy Light de Nintendo Game Boy Light de Nintendo Quinta generación Artículo principal: Videoconsolas de quinta generación En el período de la quinta generación existen muchos fabricantes de juegos que presentaron diversos equipos con características parecidas a las de un computadora. Estos fabricantes comenzaron a presentar títulos en un entorno 3D, aprovechando la mayor capacidad de hardware de los equipos. A esta generación se la conoce como la era de los 32 bits, aunque ocasionalmente algunas personas se refieren a ella como la era de los 64 bits puesto que Nintendo lanzaría dos años más tarde un sistema que rompería este apodo. Se trata de la consola Nintendo 64, a la que raramente se llama también la era 3D. Se trata de una generación que supuso el paso de los 2D a los entornos tridimensionales 3D, que comenzó en el año 1994 cuando Sega lanzó su Sega Saturn y Sony su PlayStation, la cual supuso la irrupción de esta compañía en el mundo de los videojuegos. Básicamente el mercado estaba dominado por tres consolas, Nintendo 64 (1996), la cual contó con algunos de los videojuegos más emblemáticos de la época y de la historia del ocio electrónico, como el inolvidable Super Mario 64 (juego de lanzamiento que, en consecuencia, cumple también 25 años), F-Zero X, Perfect Dark o The Legend of Zelda: Ocarina of Time, entre otros. Del mismo modo, vio nacer sagas como Super Smash Bros. que siguen siendo grandes éxitos de ventas en la actualidad. Sega Saturn (1994), la consola más controvertida de SEGA, que sin embargo gozó de un gran éxito en Japón, fue la primera en lanzar los juegos en soporte de CD y es para muchos una gran desconocida, aunque atesora una larga lista de grandes juegos. Por su parte Sony, con la consola PlayStation (1994). En el transcurso de una década, PlayStation se convirtió en la primera consola de juegos de la historia en vender más de 100 millones de unidades en todo el mundo y, a lo largo de su vida, llegó a contar con casi 8 000 juegos. La demografía en las ventas de consolas varió considerablemente, pero estas consolas definieron la guerra de consolas de esta era. La 3DO Interactive Multiplayer y la Atari Jaguar fueron también parte de esta era, pero su marketing fue pobre y fallaron a la hora de crear impacto. Esta era también vio una versión actualizada de la Game Boy de Nintendo: la Game Boy Color. FM Towns Marty de Fujitsu. FM Towns Marty de Fujitsu. Pioneer LaserActive de Pioneer. Pioneer LaserActive de Pioneer. Atari Jaguar de Atari. Atari Jaguar de Atari. Atari Jaguar CD de Atari Atari Jaguar CD de Atari 32X de Sega. 32X de Sega. Sega Saturn de Sega. Sega Saturn de Sega. PlayStation de Sony. PlayStation de Sony. Nintendo 64 de Nintendo. Nintendo 64 de Nintendo. Apple Pippin de Apple. Apple Pippin de Apple. Casio Loopy de Casio. Casio Loopy de Casio. AmigaCD32 de Commodore International. AmigaCD32 de Commodore International. 3DO Interactive Multiplayer de Panasonic, Sanyo y Goldstar (hoy llamada LG Electronics). 3DO Interactive Multiplayer de Panasonic, Sanyo y Goldstar (hoy llamada LG Electronics). Game Boy Color de Nintendo Game Boy Color de Nintendo Sega Multi-Mega de Sega Sega Multi-Mega de Sega Sega Nomad de Sega Sega Nomad de Sega Virtual Boy de Nintendo Virtual Boy de Nintendo PocketStation de Sony PocketStation de Sony PC-FX de NEC PC-FX de NEC WonderSwan de Bandai WonderSwan de Bandai Game.com de Tiger Electronics Game.com de Tiger Electronics Neo Geo Pocket de SNK Neo Geo Pocket de SNK R-Zone de Tiger Electronics R-Zone de Tiger Electronics Sexta generación Artículo principal: Videoconsolas de sexta generación En esta generación, se produjeron equipos con similitudes a la arquitectura de una computadora personal, no obstante, las consolas de sobremesa prescindieron de los cartuchos y utilizan medios de almacenamiento de gran capacidad como el DVD, GD-ROM, GOD. Lo cual hizo que los juegos fuesen más largos y visualmente más atractivos. Además, esta generación también experimenta el videojuego en línea en las consolas y la aplicación de sistemas almacenamiento internos en los equipos como memoria flash y disco duros que son utilizados para guardar datos del videojuego. PlayStation 2 de Sony. PlayStation 2 de Sony. Xbox (consola) de Microsoft. Xbox (consola) de Microsoft. Nintendo GameCube de Nintendo. Nintendo GameCube de Nintendo. Panasonic Q de Nintendo y Matsushita Electric (Actualmente conocido como Panasonic). Panasonic Q de Nintendo y Matsushita Electric (Actualmente conocido como Panasonic). Sega Dreamcast de Sega. Sega Dreamcast de Sega. Game Boy Advance SP de Nintendo Game Boy Advance SP de Nintendo Game Boy Advance de Nintendo Game Boy Advance de Nintendo Game Boy Micro de Nintendo Game Boy Micro de Nintendo Pokémon mini de Nintendo Pokémon mini de Nintendo Nintendo iQue de Nintendo Nintendo iQue de Nintendo Neo Geo Pocket Color de SNK Neo Geo Pocket Color de SNK N-Gage de Nokia N-Gage de Nokia N-Gage QD de Nokia N-Gage QD de Nokia Tapwave Zodiac de Tapwave Tapwave Zodiac de Tapwave GP32 de Game Park GP32 de Game Park La Dreamcast fue la primera consola de esta generación, y la última consola de videojuegos de Sega, también fue la primera en cesar su producción en esta generación. Sega implementó un tipo especial de soporte óptico llamado GD-ROM. Estos discos fueron creados con el fin de evitar la piratería de software, relativamente fácil en las consolas de la generación anterior, ya que coincidió con la salida al mercado de las primeras grabadoras de CD-ROM, sin embargo, este formato fue instantáneamente violado. En 2001, se suspende la producción de este sistema, Sega se enfoca únicamente al desarrollo de software. Sin embargo, la compañía continuó dando soporte a las consolas que fueron vendidas y al formato GD-ROM, hasta el 2007. El equipo fue el primero en disponer de un módem de 33.6 Kb, con el cual se podía acceder a Internet y jugar algunos títulos en línea como Phantasy Star Online. La PlayStation 2 de Sony continuó el mismo éxito de la PlayStation, y fue la primera videoconsola casera en incluir un reproductor de DVD, que permitía reproducir películas en el sistema. Además existía la posibilidad de poner un disco duro interno, en combinación con el adaptador de red. Al igual que su predecesor, también dispuso de un modelo pequeño que fue lanzado en el 2004. La consola fue retirada en 2013. La Nintendo GameCube fue la cuarta videoconsola de sobremesa de Nintendo, y el primer sistema de la compañía que prescinde de los cartuchos. Este sistema utiliza un formato de disco similar al DVD, denominado GOD (Gamecube Optical Disc), cuyo tamaño es de 8 cm. Fue retirada en 2008. La Xbox fue la última consola que salió en esta generación y la primera de Microsoft. Se apoyó como lo hizo Sega Dreamcast en el juego en línea e innovó al proporcionar a la consola un disco duro integrado; utiliza el formato DVD y da la posibilidad de guardar música desde un CD de audio a la consola utilizándolo juegos como GTA San Andreas. Tuvo una buena aceptación, aunque una corta vida. La V Smile también fue la penúltima consola que salía de primera de Vtech que era para menores de 0 a 7 años y se usaban cartuchos compatibles en vez y su memoria era de 644 mb. y su tamaño es de 2.2 cm. Séptima generación Artículo principal: Videoconsolas de séptima generación Esta generación se caracteriza por la introducción de la tecnología multinúcleo en la unidad central de procesamiento. También está marcada por la integración del formato de disco óptico Blu-ray y los controladores inalámbricos y la detección de movimiento que han desplazado el clásico controlador por cable. Otro aspecto importante es la distribución de juegos vía Internet, gracias a la aparición del servicio de banda ancha a nivel mundial. Algunos de los servicios de Internet que dan soporte técnico a los juegos multijugador es la Xbox Live de Microsoft, la PlayStation Network de Sony y la Nintendo WiFi Connection de Nintendo. Otro aspecto importante que caracteriza esta generación a las otras, es la inclusión de chips gráficos sofisticados que ayudan a procesar imágenes reales tal es el caso del procesador digital GPU. En esta generación Sega dejó de competir con las principales videoconsolas para dirigirse a un mercado de menor escala con el lanzamiento de su miniconsola Sega Zone que al igual que las demás consolas de la generación, está equipada con sensores de movimiento. En este mismo mercado aparece la miniconsola Zeebo que ofrecía juegos en línea y que más tarde anunciaría el cese de su producción. Vtech llegó a la séptima generación con su V Smile Moniton, una consola que paso en 2008 que Zeebo en precederlo al 1. Únicamente tres compañías se disputan el mercado a gran escala: Nintendo, Sony y Microsoft. Nintendo y Sony son empresas de origen de Japón, mientras que Microsoft es la única empresa de origen de los Estados Unidos que disputa el mercado de videoconsolas a gran escala. A finales del año 2005, la Xbox 360 de Microsoft fue la primera en aparecer en esta generación. En noviembre de 2006, aparecen la Wii de Nintendo y la PlayStation 3 de Sony. Respecto a las ventas, la compañía Nintendo recupera el mercado, gracias al nuevo enfoque con el cual se diseñó la Wii, para así posicionarse en el primer lugar en las ventas de videoconsolas de sobremesa. En 2017 salió Nintendo Switch, siendo la sucesora de la Wii U; PlayStation 3 de Sony. PlayStation 3 de Sony. Xbox 360 de Microsoft. Xbox 360 de Microsoft. Wii de Nintendo. Wii de Nintendo. Wii Mini de Nintendo Wii Mini de Nintendo Wii Family Edition de Nintendo Wii Family Edition de Nintendo Zeebo de Tectoy y Qualcomm. Zeebo de Tectoy y Qualcomm. PlayStation Portable de Sony PlayStation Portable de Sony Nintendo DS Lite de Nintendo Nintendo DS Lite de Nintendo Nintendo DSi de Nintendo Nintendo DSi de Nintendo Nintendo DS de Nintendo Nintendo DS de Nintendo Nintendo DSi XL de Nintendo Nintendo DSi XL de Nintendo PlayStation Portable Go de Sony PlayStation Portable Go de Sony PlayStation Portable Slim & Lite de Sony PlayStation Portable Slim & Lite de Sony Gizmondo de Tiger Telematic Gizmondo de Tiger Telematic GP2X de Gamepark Holdings. GP2X de Gamepark Holdings. Octava generación Artículo principal: Videoconsolas de octava generación Lo destacable de esta generación es el uso de internet como eje central de la funcionalidad de las consolas, estas convertidas en media centers uniendo en un único aparato las funciones de consola de videojuegos y bazar de venta de películas, series de TV y otros contenidos desde el propio aparato. Aunque las generaciones anteriores de videoconsolas normalmente se han sucedido en ciclos de cinco años, la transición de la séptima a la octava generación ha durado más de seis años. La transición de consolas de sobremesa es similar a la de la anterior generación que tuvo más ventas, la Wii, la primera en tener sucesora. PlayStation 4 de Sony. PlayStation 4 de Sony. PlayStation 4 Pro de Sony PlayStation 4 Pro de Sony PlayStation Vita de Sony PlayStation Vita de Sony Nintendo 3DS/XL de Nintendo Nintendo 3DS/XL de Nintendo Nintendo 2DS de Nintendo Nintendo 2DS de Nintendo New Nintendo 3DS/XL de Nintendo New Nintendo 3DS/XL de Nintendo New Nintendo 2DS XL de Nintendo New Nintendo 2DS XL de Nintendo Xbox One de Microsoft. Xbox One de Microsoft. Xbox One X de Microsoft Xbox One X de Microsoft Xbox one S de Microsoft Xbox one S de Microsoft Wii U de Nintendo. Wii U de Nintendo. Ouya, basada en el sistema Android. Ouya, basada en el sistema Android. Nintendo Switch de Nintendo Nintendo Switch de Nintendo Nintendo Switch OLED de Nintendo Nintendo Switch OLED de Nintendo Novena generación Artículo principal: Videoconsolas de novena generación Durante esta nueva generación Sony y Microsoft lanzaron sus nuevas videoconsolas. Lo destacable de esta generación es el lanzamiento de consolas cuyo formato de videojuegos es completamente digital, como por ejemplo Xbox Series S de Microsoft y la versión digital de PlayStation 5 de Sony, aunque en la anterior generación ya se buscó terreno con la Xbox One S all digital. Por otro lado, Google lanzó su Stadia, un mando que se utiliza para jugar juegos en celulares (teléfonos móviles) y que se conecta mediante bluetooth o cable. Otro elemento destacado de esta generación ha sido el uso de la unidad de estado sólido (SSD) para el almacenamiento. Xbox Series X de Microsoft Xbox Series X de Microsoft Xbox Series S de Microsoft Xbox Series S de Microsoft Atari VCS de Atari SA Atari VCS de Atari SA Steam Deck de Valve Steam Deck de Valve Nintendo Switch 2 de Nintendo Nintendo Switch 2 de Nintendo Visión detallada Consolas de sobremesa Artículo principal: Anexo:Fabricantes de videoconsolas de sobremesa Las generaciones son las siguientes: Generación Periodo Bits Principales consolas Primera generación 1972-1983 1 bit Magnavox Odyssey, Coleco Telstar, TV-Game 6, Atari Pong Segunda generación 1976-1992 4 bits Fairchild Channel F, Atari 2600, Magnavox Odyssey², Bally Astrocade, Intellivision, Atari 5200, Vectrex, Arcadia 2001, ColecoVision Tercera generación 1983-1998 8 bits Atari 7800, Nintendo Entertainment System, Sega Master System, PV-1000, Epoch Cassette Vision, Supergame VG 3000, Sega SG-1000 Cuarta generación 1987-2003 16 bits Sega Mega Drive, Neo-Geo, Neo Geo CD, Super Nintendo Entertainment System, TurboGrafx-16/PC Engine, CD-TV, CD-i, Super ACan Quinta generación 1993-2006 32 bits y 64 bits 3DO, AmigaCD32, Atari Jaguar, Sega Saturn, Virtual Boy, PlayStation, Nintendo 64, Apple Pippin, Casio Loopy, PC-FX, Playdia, FM Towns Marty Sexta generación 1998-2013 128 bits Sega Dreamcast, PlayStation 2, Xbox, Nintendo GameCube Séptima generación 2005-2012 128 bits Xbox 360, PlayStation 3, Wii, Zeebo Octava generación 2012-2020 128 bits Wii U, PlayStation 4, Xbox One, Ouya, Nintendo Switch, Nintendo Switch OLED Novena generación 2020-Actual 128 bits Xbox Series X y Series S, PlayStation 5, Atari VCS 2020, Steam Deck, Nintendo Switch 2 A pesar de las limitaciones técnicas de la segunda generación, la Intellivision ya contaba con un CPU CP1600 de 16 bits. También aunque la sexta generación es considerada la de los 128 bits, dos consolas tenían doble CPU de 64 bits (Sega Dreamcast y PlayStation 2), mientras que la Xbox tenía una CPU principal de 32 bits pero trabajando a 733 MHz, lo que suponía que ésta fuese el doble de potente que sus competidoras. Además las consolas de la séptima tienen un GPU distinto como la Wii tiene un GPU ATI Hollywood 243 MHz, la Xbox 360 tiene un ATIXenos 500 MHz y la PlayStation 3 tiene un NVIDIA/SCEI RSX 550 MHz. Consolas portátiles Artículo principal: Videoconsola portátil Handheld Video Game o Handheld Game es un dispositivo electrónico ligero que permite jugar Videojuegos y que, a diferencia con una Videoconsola clásica, los controles, la pantalla, los altavoces y la alimentación – pilas, baterías,etc - están todos integrados en la misma unidad y todo ello con un pequeño tamaño, para poder llevarla y jugar en cualquier lugar o momento. Podemos establecer tres tipos de videojuegos portátiles: Videojuego electrónico portátil que no tienen cartuchos intercambiables, discos ópticos, discos magnéticos, tarjetas de memoria, etc, o no son reprogramables y de un solo juego normalmente. El ejemplo más claro es Game & Watch Videoconsola portátil un dispositivo electrónico ligero que permite jugar videojuegos y que además de que los controles, la pantalla, los altavoces y la alimentación – pilas, baterías - están todos integrados en la misma unidad, permite diferentes videojuegos a través de cartuchos intercambiables, discos ópticos, discos magnéticos, tarjetas de memoria, etc. El mejor ejemplo es Game Boy Videoconsola portátil de código abierto que permiten ser programadas libremente como GP2X. Parten de la misma idea de una consola portátil, pero permiten ser totalmente reprogramadas, y tienen una práctica ausencia de juegos oficiales. Funcionan a través de programas gratuitos creados en comunidades de internet. El mejor ejemplo sería la GP2X El primer videojuego portátil que aparece en el mercado con su propia pantalla LCD es un minijuego de Mattel llamado Mattel Auto Race en 1976, con una tecnología similar a la de los Game & Watch y poco después en 1978 Coleco saca un juego de similares características llamado Electronic Quaterback. Esta podría considerarse como la primera generación de videojuegos portátiles. La primera consola portátil con juegos intercambiables, que aparece en el mercado es la consola de MB juegos Microvision del año 1979. Era una consola de cartuchos intercambiables con su propia pantalla de LCD de 16x16 píxeles, con una CPU de 4 bits a 0,1 MHz cuyo control era una simple rueda y funcionaba con dos pilas. Se conservan muy pocas porque la pantalla se estropeaba con facilidad. Fue retirada del mercado en 1982. El mercado de las consolas portátiles ha sufrido muchas transformaciones a lo largo de su historia, épocas tempranas a finales de los años 1970 pocos exitosos pero que sentaron las bases de las futuras generaciones portátiles, los años 1990 donde se popularizan, y el nuevo milenio donde las consolas portátiles son verdaderos centros multimedia. El sector de los videojuegos portátiles ha tenido dos épocas álgidas, en torno a la cuarta generación de consolas de sobremesa cuando las portátiles suponían un 17% del mercado de los videojuegos, y vivieron su máximo auge en la séptima generación de consolas suponiendo el 56% del mercado del videojuego de esa generación. Es la empresa japonesa Nintendo, la que populariza el videojuego portátil. Primero con los juegos Game & Watch entre 1990 y 1991 que permite jugar a un único juego de bolsillo en cualquier lugar. Segundo con la popular Game Boy que permite jugar en cualquier parte con juegos intercambiables. Por último, Nintendo DS que consigue habituar a jugar en cualquier parte a un público no necesariamente interesado en los videojuegos. La mayoría de las empresas que han intentado sumarse a este mercado han fracasado estrepitosamente. Tan solo Sega Game Gear y PlayStation Portable han conseguido arrebatar una porción de mercado suficiente para generar beneficios y prolongar su vida comercial durante el ciclo normal de una generación. El resto de sistemas con ventas paupérrimas han supuesto un fracaso, e importantes pérdidas para sus creadores. Solo 10 de las muchas consolas portátiles que se han lanzado al mercado han superado los 10 millones de unidades vendidas.[cita requerida] A partir de 2001, irrumpe un nuevo modelo de videoconsolas portátiles, las Opensource o de Código Abierto. Consolas que buscan un mercado alternativo al de las grandes firmas. No tienen juegos oficiales o tienen muy pocos, y su baza es que su licencia es libre y permiten ser programadas por quien quiera. Su público objetivo son los geeks y los fanes de la emulación, ya que este es su fin. Son consolas para quién quiera programar juegos, emuladores, aplicaciones y un largo etcétera. Y funcionan mediante comunidades en internet que intercambian programas. En el fondo son aparatos multimedia, para quienes les gusta trastear con su consola. La empresa más exitosa en este campo es la surcoreana Game Park, con sus consolas GP32, GP2X, y GP2X wiz bastante exitosas. Otras opciones son la Dingoo o la Open Pandora. Generación Periodo Principales consolas Primera generación 1976-1979 Mattel Auto Race, Electronic Quarterback Segunda generación 1979-1989 MB Microvision, Entex Select A Game, Game & Watch, Epoch Game Pocket Tercera generación 1989-1991 Game Boy, Atari Lynx Cuarta generación 1991-1998 Watara Supervision, Game Boy Pocket, TurboExpress, Sega Game Gear Quinta generación 1998-2001 Sega Nomad, Game Boy Color, Game Boy Light, Neo Geo Pocket, Mega Duck, Gamate, Game.com, WonderSwan Sexta generación 2001-2004 Neo Geo Pocket Color, SwanCrystal, Game Boy Advance, Game Boy Advance SP, Game Boy Micro, Tapwave Zodiac, GP32, (N-Gage y N-Gage QD) Séptima generación 2004-2011 Nintendo DS, Nintendo DS Lite, Nintendo DSi, Nintendo DSiXL, PlayStation Portable, PlayStation Portable Slim & Lite (2000), PSP 3000, PlayStation Portable Go,PSP E1000 (Street), GP2X, GP2X Wiz Octava generación 2011-2020 Nintendo 3DS-Nintendo 3DS XL, PlayStation Vita, Nintendo 2DS y New Nintendo 3DS-New Nintendo 3DS XL, New Nintendo 2DS XL, Nintendo Switch, Nintendo Switch Lite, Nintendo Switch OLED Novena generación 2020-Actual Steam Deck Consolas híbridas Debido a los grandes avances técnicos a lo largo de toda la época de los videojuegos, las diferencias entre las consolas portátiles y de sobremesa se han reducido, sobre todo en la arquitectura. Por ello, muchas empresas han hecho lo posible por desarrollar sistemas con pantallas en los controles y función TV-off (siendo pionero SEGA quien dio un pequeño empuje conSega Nomad en este campo). Por ello, desde que fue anunciado el proyecto NX (17 de marzo del 2015), se dieron muchas expectativas basadas en el concepto del Wii U Gamepad; hasta que se mostró el anuncio de la Nintendo Switch, con el concepto de un dock para adaptarse a la TV, un mando con pantalla, Joy-Cons desacoplables y un Pro Controller. Sega Nomad Sega Nomad SUP GAME BOX SUP GAME BOX PC Engine LT PC Engine LT Nintendo Switch Nintendo Switch Ventas Artículo principal: Anexo:Consolas de videojuegos más vendidas Ventas en Japón Fabricante Consola Lanzamiento Unidades vendidas Nintendo Nintendo DS 2 de diciembre de 2004 32.990.000 Nintendo Game Boy y Game Boy Color 21 de abril de 1989 32.470.000 Nintendo Nintendo 3DS 26 de febrero de 2011 24.700.000 Sony PlayStation 2 4 de marzo de 2000 21.454.325 Nintendo Famicom (NES) 15 de julio de 1983 19.350.000 Nintendo Super Famicom (Super NES) 21 de noviembre de 1990 17.170.000 Nintendo Game Boy Advance 21 de febrero de 2001 16.960.000 Sony PlayStation Portable 12 de diciembre de 2004 16.867.853 Nintendo Wii 2 de diciembre de 2006 12.750.000 Nintendo Nintendo Switch 3 de marzo de 2017 Más de 10.000.000 NEC PC Engine (TurboGrafx-16) 30 de octubre de 1987 8.000.000 Sony PlayStation 4 22 de febrero de 2014 8.506.964 Sony PlayStation Vita 17 de diciembre de 2011 5.643.626 Nintendo Nintendo 64 23 de junio de 1996 5.540.000 Sega Sega Saturn 22 de noviembre de 1994 5.000.000[cita requerida] Nintendo Nintendo Switch 3 de marzo de 2017 4.380.000 Nintendo Nintendo GameCube 14 de septiembre de 2001 4.040.000 Sega Mega Drive 29 de octubre de 1988 3.580.000 Nintendo Wii U 8 de diciembre de 2012 3.340.000 Sega Dreamcast 27 de noviembre de 1998 2.320.000 Microsoft Xbox 360 10 de diciembre de 2005 1.448.665 Sega Sega Mark III (Master System) 20 de octubre de 1985 1.000.000 Microsoft Xbox One 4 de septiembre de 2014 87.592 Ventas en Norteamérica Fabricante Consola Lanzamiento Unidades vendidas Nintendo Nintendo DS 21 de noviembre de 2004 59.930.000 Nintendo Wii 19 de noviembre de 2006 48.640.000 Sony PlayStation 2 26 de octubre de 2000 47.680.000 Nintendo Game Boy y Game Boy Color 31 de julio de 1989 44.060.000 Nintendo Game Boy Advance 11 de junio de 2001 41.640.000 Sony PlayStation 9 de septiembre de 1995 40.780.000 Nintendo Nintendo Entertainment System 18 de octubre de 1985 34.000.000 Nintendo Nintendo 3DS 27 de marzo de 2011 25,170,000 Nintendo Super Nintendo Entertainment System 23 de agosto de 1991 23.350.000 Sega Genesis (Mega Drive) 14 de agosto de 1989 21.4-22.4 millones[cita requerida] Nintendo Nintendo 64 29 de septiembre de 1996 20.630.000 Nintendo Nintendo Switch 3 de marzo de 2017 17.500.000 Nintendo Nintendo Gamecube 18 de noviembre de 2001 12.940.000 Nintendo Wii U 18 de noviembre de 2012 6.490.000 Ventas en Reino Unido Fabricante Consola Lanzamiento Unidades vendidas Nintendo Nintendo DS 11 de marzo de 2005 8.8 millones[cita requerida] Nintendo Wii 8 de diciembre de 2006 4.9 millones[cita requerida] Microsoft Xbox 360 2 de diciembre de 2005 3.2 millones[cita requerida] Sony PlayStation Portable 1 de septiembre de 2005 3.2 millones[cita requerida] Sega Mega Drive 30 de noviembre de 1990 2.1 millones[cita requerida] Sony PlayStation 3 23 de marzo de 2007 1.9 millones[cita requerida] Sega Master System Septiembre de 1987 1.35 millones Nintendo Nintendo 64 1 de marzo de 1997 1.3 millones[cita requerida] Nintendo Nintendo Entertainment System 1987 1.1 millones Nintendo Super NES 1992 1.05 millones Ventas en España color #b6fcb6 El color verde indica que la consola aún se encuentra en producción. Fabricante Consola Lanzamiento Unidades vendidas Nintendo Nintendo DS 11 de marzo de 2005 5.7 millones (hasta 2012) Sony PlayStation 2 24 de noviembre de 2000 5 millones (hasta 2009) Sony PlayStation 4 29 de noviembre de 2013 3.3 millones (hasta 2019) Sony PlayStation 29 de septiembre de 1995 Más de 3 millones (hasta 2005) Nintendo Switch 3 de marzo de 2017 3 millones (hasta 2023) Nintendo Wii 8 de diciembre de 2006 2.7 millones (hasta 2011) Sony PlayStation 3 23 de marzo de 2007 2.7 millones (hasta 2014) Nintendo Game Boy Enero de 1991 2.3 millones (hasta 1999) Sony PlayStation Portable 1 de septiembre de 2005 2.1 millones (hasta 2009) Nintendo Nintendo 3DS 25 de marzo de 2011 2 millones (hasta 2018) Nintendo Game Boy Advance 22 de junio de 2001 1.6 millones (hasta 2004) Sony PlayStation 5 19 de noviembre de 2020 1 126 000 (hasta 2023) Microsoft Xbox 360 2 de diciembre de 2005 1 millón (hasta 2012) Sega Mega Drive Septiembre de 1990  Menos de 700 000 (hasta 1994)[n. 1] Nintendo SNES 1 de junio de 1992 630 000 (hasta 1998)< Sony PlayStation Vita 22 de febrero de 2012 600 000 (hasta 2015) Sega Master System 1987 550 000 (hasta 1993) Nintendo NES 1987/1988 440 000 (hasta 1993) Microsoft Xbox Series X/S 10 de noviembre de 2020 310 000 (hasta 2023) Microsoft Xbox One 22 de noviembre de 2013 305 000 ~500 000 (hasta 2018) Nintendo GameCube 3 de mayo de 2002 Menos de 300 000 (hasta 2006)[n. 2] Sega Dreamcast 14 de octubre de 1999 200 000 (hasta 2001) Sega Game Gear Julio de 1991 175 000 (hasta 1992) Sega Saturn 7 de julio de 1995 70 000 (hasta 1997) Philips CD-i 1992 54 000 (hasta 1994)[n. 3] Ventas durante la pandemia de COVID-19 Durante los meses entre enero y julio se vendió un total de 22 284 462 consolas de videojuegos en todo el mundo, presentando un aumento del 36.54% en comparación a las vendidas el mismo periodo pero del año 2019. Estos datos fueron recopilados por la firma británica Safe Betting Sites, el cual muestra que la cifra es 36.54% superior en comparación a las 16 319 770 consolas vendidas. Para 2020 el número de ventas de consolas tuvo un registro de 4 147.251. El mes en el que se presentó el número mayor de venta con 2 939 647 unidades."
ksampletext_wikipedia_vide_nintendo: str = "Nintendo. Nintendo Company, Ltd. es una empresa japonesa de entretenimiento dedicada a la investigación, desarrollo y distribución de software y hardware de videojuegos, y juegos de cartas, con sede en Kioto, Japón. Su origen se remonta a 1889, cuando comenzó a operar como Nintendo Koppai tras ser fundada por el artesano Fusajirō Yamauchi con el objetivo de producir y comercializar naipes hanafuda. Tras incursionar en varias líneas de negocio durante la década de 1960 y adquirir una personalidad jurídica de empresa de capital abierto bajo la denominación actual, en 1977 distribuyó su primera videoconsola en Japón, la Color TV Game 15.  Sus productos incluyen algunas de las consolas y títulos más influyentes y exitosos en la industria de los videojuegos, como la Nintendo Entertainment System, la Super Nintendo Entertainment System, la Wii  y la Game Boy; así como los juegos Donkey Kong (1981), Super Mario Bros. (1985), The Legend of Zelda (1986), Metroid (1986), Fire Emblem (1990), Star Fox (1993) y Pokémon Red y Blue (1996), que dieron origen a sus correspondientes franquicias. Nintendo cuenta con varias filiales en múltiples ubicaciones tanto en Japón como en el extranjero, en las cuales delega la responsabilidad de producir y distribuir sus productos, además de socios comerciales como The Pokémon Company y Warpstar, Inc.. Tanto la organización como su personal han recibido varios premios por sus logros, entre los cuales se incluyen reconocimientos Emmy de tecnología e ingeniería, Game Developers Choice Awards y galardones BAFTA de videojuegos, entre otros. Asimismo, es una de las empresas japonesas con mayor riqueza y valor en el mercado,  además de poseer una de las políticas de responsabilidad social con «mejor reputación» en el mundo.  Historia 1889-1929: Antecedentes Los antecedentes de Nintendo se remontan al 23 de septiembre de 1889 cuando el artesano Fusajirō Yamauchi fundó la empresa Nintendo Koppai ,en caracteres chinos: 任天堂骨牌, en Kioto, Japón, con el objetivo de producir y comercializar naipes Hanafuda, cuya principal característica es que incorporan ilustraciones en vez de números. Este rasgo le permitió a Yamauchi comercializar sus cartas pese a que las autoridades de Japón tenían prohibidos los juegos de apuestas desde 1633. Para su fabricación usaba la corteza de moreras, que él mismo pintaba a mano. La demanda del producto finalmente se incrementó y Yamauchi debió contratar asistentes para producir en serie. Aunque el término «Koppai» está asociado con «cartas», existe imprecisión en la etimología de «Nintendo» ,「運を天に任せる」,. Por lo general se considera que es una palabra compuesta cuyo significado es «deja la suerte al cielo» o «deja la fortuna en manos del destino» ,en alusión a la prohibición de los juegos de azar en Japón,, aunque existe la hipótesis de que los sinogramas kanjis podrían referirse también al «juego de azar». La oferta inicial de Nintendo Koppai se vio completada con otras barajas como la dedicada al Hyakunin Isshu; las Daitoryo ,«presidente», que eran las más populares en Kioto y otras de estilo occidental, las primeras de ese tipo en Japón. A pesar de un comienzo favorable, la compañía atravesó ciertas dificultades financieras debido a que el proceso de fabricación de las cartas era lento y costoso, mientras que los precios fijados por Yamauchi eran elevados. Otros factores de riesgo eran el limitado nicho de mercado al que pertenecía la empresa, y la amplia durabilidad de las cartas, un aspecto que impactaba en las ventas debido a la escasa tasa de reposición del producto. A manera de solución, el empresario redujo los precios, comenzó a producir cartas de menor calidad a las cuales denominó Tengu y buscó ofrecer sus productos en otras ciudades como Osaka, donde se movían cantidades considerables de dinero en partidas de cartas donde se apostaban sumas que podían llegar a ser muy elevadas, además de que los empresarios locales del giro podían estar interesados en una renovación casi continua de sus barajas, evitando así las suspicacias que generaba reutilizar cartas. Según datos de la propia compañía, su primera baraja occidental fue puesta en el mercado en 1902, aunque otros documentos retrasan la fecha hasta 1907, poco después de la guerra entre Japón y Rusia. Este contexto bélico generó notables dificultades a las empresas de la época, en especial a las del sector del ocio que estuvieron sujetas a nuevos gravámenes como la tasa Karuta Zei. Pese a lo anterior, la compañía subsistió y, en 1907, estableció un acuerdo con Nihon Senbai ,más tarde titulada Japan Tobacco & Salt Corporation, para comercializar sus cartas en varios locales de cigarrillos a lo largo del país.  La cultura japonesa estipulaba que, para que Nintendo Koppai continuara como una empresa familiar tras la jubilación de Yamauchi, este debía adoptar a su yerno como hijo propio para que pudiera hacerse cargo del negocio. Como resultado, Sekiryo Kaneda adoptó el apellido «Yamauchi» en 1907 y, un par de décadas después, en 1929, se convirtió en el segundo presidente de Nintendo Koppai. Para entonces, era «la compañía más grande de juegos de cartas» en Japón. Logotipo y sede de Nintendo en 1889, junto con algunas cartas producidas en ese período. 1929-1968: Expansión y diversificación En 1933, Sekiryo constituyó la empresa como una sociedad colectiva bajo el nombre de Yamauchi Nintendo Co, Ltd. e invirtió en la construcción de una nueva sede corporativa ubicada al lado del edificio original, cerca de la estación de tren Toba-kaidō. Su matrimonio con la hija de Yamauchi tampoco dio lugar a ningún hijo varón, por lo que tenía pensado adoptar a Shikanojo Inaba, un artista que colaboraba en la producción de las cartas y que estaba casado con su hija Kimi, con quien tuvo un hijo llamado Hiroshi, nacido en 1927. Sin embargo, Inaba abandonó a su familia y la empresa, por lo que Hiroshi se habría de convertir en el sucesor de Sekiryo tras su jubilación. El periodo de la Segunda Guerra Mundial resultó especialmente negativo para la compañía ya que las autoridades japonesas volvieron a prohibir la difusión de juegos de cartas extranjeros, mientras que la sociedad nipona redujo su interés en este tipo de actividades lúdicas para darle prioridad a otras necesidades. Aun así, logró salir adelante, en parte apoyada por la inyección económica que supuso la boda del joven Hiroshi con Michiko Inaba, que provenía de una familia adinerada. En 1947 Sekiryo fundó Marufuku Co. Ltd., con la finalidad de producir en cadena las cartas y distribuir juegos de cartas con estilo occidental, tales como el pinochle y el póker. Un par de años después se cambió la razón social por Marufuku Karuta Hanbai Co., Ltd. En 1950, debido al deterioro en la salud de Sekiryo, Hiroshi asumió la presidencia de Nintendo Company. Sus primeras acciones implicaron varios cambios importantes en la operación de la compañía: en 1951 cambió la razón social por Nintendo Playing Card Co. Ltd., ,mientras que Marufuku Company adoptó la denominación Nintendo Karuta Company, Ltd., y, al año siguiente, centralizó la producción de cartas en las fábricas de Kioto, lo que le llevó a expandir las oficinas. Asimismo, su nueva línea de cartas de plástico gozó de un considerable éxito en el país. Acostumbrados a una dirección más cautelosa y conservadora, una parte de los empleados de la empresa vio con preocupación las nuevas medidas y la situación de tensión entre ambas partes derivó en una convocatoria de huelga. No obstante, la medida no tuvo mayor impacto debido a que Hiroshi recurrió al despido de varios trabajadores inconformes. En 1959 Nintendo concretó un acuerdo con Walt Disney para incorporar sus personajes animados en las cartas, lo cual representó un hito en la industria nipona. Asimismo, desarrolló un sistema de distribución que le permitió ofrecer sus productos en jugueterías. En 1961 la empresa vendió más de 1.5 millones de cartas y tenía una elevada cuota de mercado, para lo cual se apoyó en campañas publicitarias para televisión. La necesidad de diversificación llevó a la empresa a cotizar en la segunda sección de las bolsas de valores de Osaka y Kioto a partir de 1962 ,en 1964 obtuvo ingresos por 150 millones JPY,, además de volverse empresa de capital abierto y cambiar su razón social a Nintendo Company, Ltd., en octubre de 1963. Aunque la compañía vivía un momento de prosperidad y bonanza económica, las cartas Disney y productos derivados la hacían muy dependiente del mercado infantil. Esta situación se agravó debido a que las ventas de Hanafuda, más enfocadas hacia un público adulto habían caído notablemente porque la sociedad japonesa prefería otros pasatiempos como el pachinko, los bolos o las salidas nocturnas. Por ello, cuando las ventas de barajas Disney empezaron a mostrar signos de agotamiento, Nintendo se dio cuenta de que no tenía alternativas reales con las cuales palíar esa situación. Ni eventos como los Juegos Olímpicos de Tokio y sus productos asociados lograron recuperar las ventas, y en 1964 el precio de mercado de la compañía decreció a 60 JPY, el nivel más bajo en toda su historia.  Entre 1963 y 1968 Yamauchi invirtió en varias líneas de negocio para Nintendo apartadas de su mercado tradicional y que, mayormente, resultaron fallidas. Entre estos intentos se encuentran paquetes de arroz instantáneo, sucursales de hoteles del amor y un servicio de taxis denominado Daiya. Si bien este último esfuerzo tuvo una mejor recepción que los anteriores, Yamauchi desestimó esta iniciativa tras una serie de desencuentros con los sindicatos locales.  Logotipos de Nintendo utilizados entre 1960 y 1965, así como el antiguo edificio con su respectivo letrero en el que se aprecia la denominación original de la empresa. 1969-1978: Juguetes electrónicos y primeros videojuegos Había algo diferente en Nintendo [con respecto a sus competidores]. Tenía empleados que pensaban en el contenido de un juego. Otras empresas [se dedicaban a] importar ideas de América para adaptarlas al mercado japonés, y nada más las volvían más económicas y pequeñas. Pero Nintendo se interesaba en [desarrollar] ideas originales. ,Masayuki Uemura, empleado del departamento de juegos de Nintendo. Las experiencias obtenidas con las anteriores iniciativas llevaron a Yamauchi a incrementar la inversión de Nintendo en un departamento de investigación y desarrollo que dejó a cargo de Hiroshi Imanishi, un empleado con una extensa trayectoria en otras áreas de la organización. Finalmente Gunpei Yokoi se sumó al equipo en 1969 como responsable de la coordinación de los distintos proyectos de ese departamento. La experiencia de Yokoi en la manufactura de dispositivos electrónicos llevó a Yamauchi a dejarlo a cargo del departamento de juegos de la compañía, mediante el cual habrían de producirse en serie sus invenciones. En ese período se llevó a cabo la construcción de una nueva planta de producción en Uji City, a las afueras de Kioto, y la empresa distribuyó juegos de mesa clásicos como el ajedrez shōgi, el tablero go y el mahjong, así como otros extranjeros bajo la marca Nippon Game. La reestructuración de la compañía preservó un par de áreas dedicadas a la manufactura de cartas japonesas. El comienzo de la década de 1970 representó un momento decisivo en la historia de Nintendo debido a que puso a la venta el primer juguete electrónico en Japón: la pistola optoelectrónica Nintendo Beam Gun, diseñada por Masayuki Uemura. En total se vendieron más de un millón de unidades a principios de los años 1970. Igualmente, en ese período la empresa comenzó a cotizar en la sección principal de la bolsa de Osaka e inauguró sus nuevas oficinas centrales. Algunos otros juguetes característicos de esta época fueron la Ultra Hand, la Ultra Machine, la Ultra Scope y el Love Tester, todos ellos diseñados por Yokoi. Hasta 1970 se habían vendido más de 1.2 millones de unidades del primero de estos. La creciente demanda de los productos de Nintendo llevó a Yamauchi a expandir nuevamente las oficinas, para lo cual adquirió el terreno circundante y destinó la producción de cartas al edificio original, una línea de negocios que mantuvo pese al declive en sus ventas. Mientras tanto, Yokoi, Uemura y nuevos empleados como Genyo Takeda continuaron desarrollando productos innovadores para la compañía, como fue el caso del sistema de disparo de luz Laser Clay Shooting System ,en japonés: レーザークレー射撃システム,, cuya popularidad en 1973 superó en Japón a los bolos, que había sido el principal pasatiempo nipón desde los años 1960; y Wild Gunman, un simulador arcade de tiro al plato integrado por un proyector de imagen de 16 mm con un sensor que detecta el haz de luz de la pistola del jugador. Ambos productos se exportaron exitosamente a Europa y América. Pese a lo anterior, los procesos productivos de Nintendo todavía eran lentos en comparación con los de otras compañías como Bandai, y sus precios resultaban elevados, lo cual le llevó a descontinuar algunos productos como Custom, una pistola de luz con mayor rango de alcance que las originales. Estos proyectos eran desarrollados por la filial Nintendo Leisure System Co., Ltd., hasta su clausura en 1973, una decisión motivada en parte por el impacto económico de la crisis del petróleo de ese año. Motivado por el éxito de compañías como Atari y Magnavox con sus sistemas de videojuegos, Yamauchi adquirió los derechos de distribución del sistema Magnavox Odyssey para su difusión en Japón, y llegó a un acuerdo con Mitsubishi Electric para desarrollar otros productos similares entre 1975 y 1978, como el primer microprocesador para los sistemas de videojuegos; las consolas TV Game 15 y TV Game 6, de la serie Color TV-Game ,en japonés: カラー テレビゲーム,; y un juego arcade inspirado en Otelo. De forma similar, Takeda diseñó el videojuego EVR Race. Cabe señalarse que en este período se integró Shigeru Miyamoto al equipo de Yokoi, con la responsabilidad de diseñar la carcasa de las Color TV-Games. En 1978 el departamento de investigación y desarrollo se separó en dos áreas que habrían de consolidarse como el Nintendo Research & Development 1 y el Nintendo Research & Development 2, con Yokoi y Uemura como respectivos responsables.  Logotipo usado entre 1975 y 2006; la Ultra Machine y la Color TV-Game 15. 1979-1989: Era arcade y de plataformas, NES y Game Boy El año de 1979 supuso un par de acontecimientos clave en la historia de la compañía: por un lado, se inauguró la filial Nintendo of America en Nueva York, y por el otro se creó un nuevo departamento centrado en el desarrollo de productos arcade. Al año siguiente tuvo su estreno el primer sistema portátil de videojuegos, desarrollado por Yokoi a partir de la tecnología empleada en las calculadoras portátiles ,que en ese entonces eran sobreproducidas en Japón,, y que pasó a denominarse Game & Watch ,en japonés: ゲーム&ウオッチ,. Se convirtió en uno de los productos más exitosos de Nintendo al venderse más de 43 millones de unidades en todo el mundo durante su período de producción, que se extendió hasta comienzos de los años 1990 y para el cual se crearon sesenta videojuegos en total. El auge de los juegos arcade se acrecentó en 1981 tras el estreno de Donkey Kong, desarrollado por Miyamoto y uno de los primeros videojuegos de plataformas que permitió que el personaje jugador pudiera saltar, en este caso Jumpman ,que posteriormente habría de convertirse en Mario y mascota oficial de la compañía; fue nombrado así por la similitud de que guardaba con Mario Segale, arrendador de las oficinas de Nintendo ubicadas en Tukwila, Washington en aquel momento,. Tras inaugurarse una nueva planta de producción en Uji y cotizar en la primera sección de la bolsa de Tokio en 1983, Uemura se hizo cargo del diseño de una nueva consola de juegos en formato de cartuchos y al cual incorporó tanto una unidad central de procesamiento como una de procesamiento físico, tomando como inspiración el ColecoVision. Finalmente dicho sistema, la Family Computer ,o Famicom; en japonés: ファミリーコンピュータ, se lanzó al mercado nipón en julio de 1983 junto con tres juegos adaptados de sus versiones originales para arcade: Donkey Kong, Donkey Kong Jr. y Popeye. Su éxito fue tal que en 1984 ya superaba en cuota de mercado al SG-1000 de Sega. Cabe señalar que Nintendo adoptó una serie de lineamientos que supusieron, por ejemplo, la validación de cada juego producido para la NES antes de su distribución en el mercado; el establecimiento de acuerdos con otras desarrolladoras para garantizar que ningún juego de NES fuese adaptado a otras consolas en un período de dos años desde su estreno; o la restricción que impedía que una desarrolladora pudiera producir más de cinco juegos al año para la NES. A comienzos de la década de 1980 proliferaron varias consolas en Estados Unidos, así como videojuegos de baja calidad producidos por desarrolladores terceros, lo cual sobresaturó el mercado y derivó en la crisis del videojuego de 1983. Como consecuencia de lo anterior, hubo una recesión primordialmente en el mercado estadounidense de videojuegos ,cuyos ingresos pasaron de más de tres mil millones USD a 100 millones USD entre 1983 y 1985,,  e impactó también a Nintendo y su iniciativa de lanzar la Famicom en el continente americano. A manera de estrategia que le permitiera diferenciarse de sus competidores en América, la compañía optó por rediseñar la Famicom como un «sistema de entretenimiento» compatible con «Game Paks» ,expresión para referirse a los cartuchos,, y con un diseño reminiscente de una videograbadora.  El producto resultante fue la Nintendo Entertainment System, o NES, que hizo su primera aparición en EE. UU. en 1985. Algunos de los primeros juegos disponibles para esta consola fueron Excitebike, Super Mario Bros., Metroid, The Legend of Zelda y Punch-Out!!. Cabe resaltar que la producción de Super Mario Bros. y The Legend of Zelda se realizó de forma simultánea y estuvo a cargo de Miyamoto y Takashi Tezuka. La labor del compositor musical Kōji Kondō también resultó determinante en ambos títulos, dado que reforzó la idea de que los temas musicales deben actuar como complemento de la mecánica de juego, y no solamente como un elemento adicional. En cuanto a la consola, su producción se prolongó hasta 1995 en el continente americano y 2003 en Japón. En total se vendieron alrededor de 62 millones de consolas NES en todo el mundo. Para evitar la piratería de sus videojuegos, en ese período Nintendo creó Official Nintendo Seal of Quality, un sello que se añadía a los productos de la marca para que el cliente reconociera su autenticidad en el mercado. Cabe señalarse que para entonces la red de proveedores de electrónica de Nintendo se había extendido a un total de treinta empresas, entre las cuales estaban Ricoh ,la empresa nipona era su principal cliente de semiconductores, y Sharp. Mientras tanto, en 1988 el departamento Nintendo R&D1 ,encabezado por Yokoi, concibió la portátil Game Boy a partir de los sistemas Game & Watch, y se trató de la primera de su tipo compatible con cartuchos de juego intercambiables. Su estreno ocurrió al año siguiente y para su distribución en América se hizo acompañar del juego Tetris tras llegar a un acuerdo con Atari Games. El sistema tuvo un éxito notable: en sus primeras dos semanas de venta en Japón, se agotó el inventario inicial de 300 000 unidades, mientras que en EE. UU. se vendieron 40 000 consolas adicionales en su primer día de exhibición. Cabe destacar que, ese año, Nintendo concretó un acuerdo con Sony para desarrollar el SNES-CD, un periférico para la Super Nintendo capaz de reproducir CD-ROM. Sin embargo, la colaboración no prosperó debido a que Yamauchi prefirió continuar desarrollando dicha tecnología con Philips ,que habría de resultar en el CD-i,, y los esfuerzos de Sony resultaron en la producción de la consola PlayStation, «una formidable rival para Nintendo en la industria». En 1988 también se publicó la primera edición de la revista Nintendo Power, que tuvo un tiraje anual de 1.5 millones de ejemplares en Estados Unidos. A mediados del año siguiente, Nintendo realizó por primera vez la exposición Nintendo Space World bajo el nombre Shoshinkai (初心会?), con el propósito de llevar a cabo anuncios sobre sus próximos productos, y, más tarde, se abrieron las primeras tiendas World of Nintendo en Estados Unidos, con mercancía oficial de la empresa. Se estima que los ingresos de Nintendo en 1989 ascendieron a 2700 millones USD, gracias a la comercialización de más de 250 productos ,de acuerdo con información de la compañía, ese año más del 25 % de las viviendas estadounidenses contaba con una NES,. Game & Watch, la Famicom/NES y Game Boy. 1990-1998: Generación de los 16 y 64 bits, y Game Boy Color Ante la aparición de otros sistemas de juego diseñados con una arquitectura de 16 bits, tales como la PC Engine de NEC Home Electronics y la Sega Genesis de Sega, que les permitía contar con gráficos y un sistema de audio mejorado en comparación con la NES, Uemuera diseñó la Super Family Computer ,en japonés: スーパーファミコン; Super Famicom,, cuyo lanzamiento ocurrió a finales de 1990. El primer lote de 300 000 consolas se agotó en cuestión de horas. Al año siguiente, como ocurriera con la NES, Nintendo distribuyó una versión modificada de la Super Famicom para el mercado estadounidense, que llevó por título Super Nintendo Entertainment System ,SNES, o Super Nintendo,. Algunos juegos iniciales disponibles para esta consola fueron Super Mario World, F-Zero, Pilotwings, SimCity y Gradius III. Hasta mediados de 1992 se habían vendido más de 46 millones de consolas Super Famicom/SNES, cuyo ciclo de vida se prolongó hasta 1999 en Estados Unidos, y 2003 en Japón. Mientras tanto, en marzo de 1990 se llevó a cabo la primera edición del Nintendo World Championship que contó con participantes de treinta ciudades estadounidenses con tal de obtener la distinción del «mejor jugador de Nintendo». Unos meses después, en junio, se inauguraron las oficinas de la subsidiaria Nintendo of Europe, en Großostheim, Alemania y, tres años después, ocurrió lo mismo con las filiales de Países Bajos ,donde Bandai estaba a cargo de distribuir los productos de la compañía,, Francia, Reino Unido, España, Bélgica y Australia. Ese mismo año, en 1993, se distribuyó Starwing, que marcó un hito en la industria al ser el primero en hacer uso del nuevo chip Super FX. Cabe señalar que en 1992 la empresa nipona adquirió la mayoría de las acciones del equipo de béisbol Seattle Mariners con la intención de evitar su quiebra y traslado a otra ciudad; su participación en el equipo cesó en 2016, cuando Nintendo vendió sus acciones. La proliferación de juegos con violencia gráfica, como Mortal Kombat, causó controversia y derivó en la creación de la Interactive Digital Software Association y del sistema de clasificación de contenido de videojuegos ESRB, en cuyo desarrollo colaboró Nintendo durante 1994. Estas medidas propiciaron también que Nintendo abandonara sus lineamientos comerciales que venía requiriendo desde el estreno de la NES. En cuanto a estrategias comerciales, la empresa implementó el servicio de entretenimiento en vuelo Nintendo Gateway, que también estuvo disponible para cruceros y hoteles; y la línea Play It Loud! consistente en consolas Game Boy con carcasas de distintos colores. Otras innovaciones tecnológicas provinieron de los gráficos Advanced Computer Modelling usados por primera vez por el juego Donkey Kong Country tanto para la SNES como para Game Boy; y del satmódem Satellaview para la Super Famicom que permitió la transmisión digital de datos por medio de un satélite de comunicaciones. A mediados de 1993 Nintendo y Silicon Graphics anunciaron una alianza estratégica para desarrollar la Nintendo 64 ,en japonés: ニンテンドウ64; o N64,, en cuya tecnología también contribuyeron otras empresas como NEC, Toshiba y Sharp. De manera comercial, se le consideró como una de las primeras consolas diseñadas con arquitectura de 64 bits. Como parte de un acuerdo con Midway Games, se acordó la adaptación de un par de juegos arcade de Nintendo a dicha consola: Killer Instinct y Cruisn USA. Si bien se tenía contemplado estrenar la N64 en 1995, el itinerario de producción de los juegos para la consola influyó en su retraso, y finalmente se estrenó en junio y septiembre de 1996 en Japón y los Estados Unidos, respectivamente, y en marzo de 1997 en Europa. Hasta el cese de su producción en 2002, se vendieron alrededor de 33 millones de consolas en todo el mundo, y suele ser catalogado como uno de los sistemas de videojuegos más reconocidos en la historia. En total se produjeron 388 juegos, de los cuales algunos han sido distinguidos como «los mejores de todos los tiempos», tales como Super Mario 64, The Legend of Zelda: Ocarina of Time y GoldenEye 007. En 1995 empezó a distribuirse la nueva consola diseñada por Yokoi, Virtual Boy con tecnología de realidad virtual y gráficos estereoscópicos. Sin embargo, la recepción fue desfavorable debido a la baja calidad de sus juegos y Nintendo la descontinuó ese mismo año debido a sus bajas ventas. Dados los resultados de su más reciente proyecto, Yokoi se retiró formalmente de la empresa. Por otro lado, a finales de 1996 se puso a la venta Pokémon Red and Blue, un juego diseñado por Satoshi Tajiri y producido por Game Freak para la portátil Game Boy, que supuso el origen de la popular franquicia de Pokémon. Como parte de las innovaciones para la N64, Nintendo distribuyó el Rumble Pak, un complemento que se conecta con el mando de la consola y produce una vibración en ciertos momentos de un juego. En 1998 salió al mercado la portátil Game Boy Color,, que tenía una capacidad similar a la NES lo cual se tradujo en ciertas adaptaciones de dicha consola para esta, como Super Mario Bros., además de permitir la retrocompatibilidad con los juegos de la Game Boy. Desde entonces se han vendido más de 118.6 millones de consolas Game Boy y Game Boy Color en el mundo. SNES, Nintendo 64, Virtual Boy y Game Boy Color. 1999-2005: GameCube e innovaciones portátiles Con el advenimiento de la PlayStation 2, en mayo de 1999 Nintendo dio a conocer un acuerdo con IBM y Matsushita para desarrollar el procesador Gekko de 128 bits así como la unidad lectora de DVD que habrían de emplearse en la siguiente consola de sobremesa de la compañía nipona. Mientras tanto, una serie de cambios administrativos ocurrieron en el año 2000, cuando las oficinas corporativas de Japón fueron trasladadas al barrio de Minami-ku, en Kioto; y surgió Nintendo Benelux, a cargo de los territorios neerlandés y belga. El año siguiente, 2001, se vio marcado por la introducción de dos nuevas consolas al repertorio de Nintendo: por un lado, la portátil Game Boy Advance ,en japonés: ゲームボーイアドバンス; GBA,, cuyo diseño a cargo del francés Gwénaël Nicolas se apartó del estilo característico de sus predecesoras; y el sistema de sobremesa Nintendo GameCube ,en japonés: ニンテンドー ゲームキューブ; NGC,. Durante la primera semana de lanzamiento de la portátil, en junio de ese año, se vendieron 500 000 unidades, lo que la convirtió en la consola con más rápidas ventas en Estados Unidos de ese período. Hasta 2010, cuando se dejó de producir, se habían vendido más de 81.5 millones de GBA en todo el mundo. En cuanto a la GameCube, algunos de sus rasgos distintivos son el formato miniDVD de los juegos ,entre los que se incluyen Luigis Mansion y Super Smash Bros. Melee, y la conexión a Internet para una limitada cantidad de juegos. Pese a lo anterior, sus ventas resultaron inferiores a las de sus predecesoras y, durante sus seis años de vida, se comercializaron 21.7 millones de consolas en todo el mundo. A manera de comparación, hasta diciembre de 2005 se habían vendido cien millones de sistemas PS2. Un producto innovador desarrollado por la empresa en este período fue el Nintendo e-Reader, un periférico para la GBA que permite la transferencia de datos almacenados en una serie de tarjetas a la portátil. En 2002 comenzó a distribuirse Pokémon mini, cuyas dimensiones son inferiores a las de la GBA y tiene un peso de 70 g, lo que la convierte en la videoconsola más pequeña de la historia. Ese año surgió una nueva colaboración con Sega y Namco para desarrollar Triforce, una placa de arcade para facilitar la conversión de títulos de ese género a la consola GameCube. Tras el lanzamiento de NGC en Europa en mayo de ese año, Yamauchi anunció su renuncia como presidente de la compañía y, como sucesor, Nintendo eligió a Satoru Iwata. Yamauchi habría de permanecer como asesor y director de la compañía hasta 2005. Murió casi una década después, en 2013. Cabe resaltar que el nombramiento de Iwata como presidente puso fin a la sucesión Yamauchi al frente de la compañía, una práctica que persistía desde su fundación. Los siguientes tres años, de 2003 a 2005, trajeron una serie de lanzamientos para el sector portátil por parte de Nintendo: el primer año salieron a la venta Game Boy Advance SP ,en japonés: ゲームボーイアドバンス, consistente en una versión mejorada de la GBA al incorporar «un diseño abatible, pantalla con iluminación frontal y batería recargable», del cual hasta 2010 se habían vendido 43.5 millones de unidades; y el periférico Game Boy Player, que permite visualizar los juegos de GB y GBA en el televisor. En 2004 apareció Nintendo DS ,en japonés: ニンテンドーDS, con ciertas innovaciones como el uso de una pantalla doble ,una de las cuales es táctil, y una conexión inalámbrica para el modo multijugador. A lo largo de su período de vida, de acuerdo con un reporte de 2016, se habían vendido más de 154 millones de unidades de dicho sistema, lo que la convierte en la más exitosa de las videoconsolas portátiles y la segunda consola más vendida en la historia, solo superada por la PlayStation 2 ,155 millones de unidades hasta 2012,. Finalmente, en 2005, Nintendo presentó la Game Boy Micro ,en japonés: ゲームボーイミクロ,, la última consola de la serie Game Boy caracterizada por contar con una pantalla retroiluminada. Pese a ello, sus ventas no cumplieron las expectativas de la empresa al comercializarse 2.5 millones de unidades hasta 2007. A mediados de este año se inauguró la Nintendo World Store en Nueva York. Game Boy Advance, Nintendo GameCube, Pokémon mini y Nintendo DS. 2006-2015: El impacto de Wii y 3DS, y el fracaso de Wii U La concepción de la siguiente consola de sobremesa de Nintendo se remonta a 2001, aunque las labores de diseño comenzaron en 2003 tomando como inspiración la portátil DS. Finalmente, la Wii estuvo disponible a partir del 19 de noviembre de 2006 con un total de 33 juegos compatibles desde su lanzamiento. Con este producto, Nintendo buscó alcanzar una demografía más amplia que sus competidores de la séptima generación con la intención de abarcar también al sector de los «no consumidores». Para esto invirtió en una costosa campaña publicitaria en la que colaboró el cineasta Stephen Gaghan. Algunas de sus innovaciones con relación a sus predecesoras incluyen el mando Wiimote, provisto de un sistema de acelerómetros y sensores infrarrojos que permiten detectar su posición en un entorno tridimensional con ayuda de una barra de sensores; el periférico Nunchuk que incluye un mando analógico así como también un acelerómetro; y la expansión Wii MotionPlus cuya función es aumentar la sensibilidad del mando principal con ayuda de giróscopos. Hasta 2016 se habían vendido más de 101 millones de consolas Wii en todo el mundo, con lo que pasó a ser la consola de videojuegos más exitosa de dicha generación al superar las cifras de Xbox 360 y PlayStation 3, un hito que Nintendo no conseguía desde la SNES, a principios de los años 1990. Entre 2007 y 2010 surgieron varios juegos para la Wii, tales como Big Brain Academy: Wii Degree, Super Mario Galaxy, Metroid Prime 3: Corruption, Wii Fit, Mario Kart Wii y New Super Mario Bros. Wii, así como una serie de accesorios para la consola, como fue el caso de Wii Balance Board y la Wii Wheel; y mejoras como el servicio de descargas WiiWare. En 2009 Nintendo Ibérica S.A., extendió sus operaciones comerciales a Portugal a través de una nueva oficina en Lisboa, y en 2010 el corporativo celebró el 25° aniversario de la primera aparición de Mario, para lo cual se pusieron a la venta ciertos productos alusivos al evento entre los cuales se incluyen el juego Super Mario All-Stars 25th Anniversary Edition y ediciones especiales de Nintendo DSi XL y Wii. Cabe añadir que en 2009 la empresa tenía una cuota de mercado del 68.3 % en el segmento de los videojuegos portátiles. Tras un anuncio realizado en marzo de 2010, al año siguiente se lanzó la portátil Nintendo 3DS capaz de producir efectos estereoscópicos sin necesidad de gafas 3D. Hasta 2018 se habían vendido más de 69 millones de 3DS en todo el mundo, cifra que se incrementó a 75 millones de unidades a principios de 2019. A mediados de 2011 se celebró el 25° aniversario de The Legend of Zelda para lo cual se organizó la gira de conciertos de orquesta sinfónica The Legend of Zelda: Symphony of the Goddesses y se lanzó el juego The Legend of Zelda: Skyward Sword. El siguiente par de años, 2012 y 2013, se vieron marcados por la introducción de dos nuevas consolas de videojuegos por parte de Nintendo: por un lado el sistema de sobremesa Wii U ,en japonés: ウィー・ユー, que supuso la incorporación de gráficos de alta definición y del mando Gamepad con tecnología de comunicación de campo cercano en contraste con su predecesor; y por el otro, la portátil Nintendo 2DS ,ニンテンドー2DS,, que posee características técnicas similares a la 3DS aunque carece del diseño tipo clamshell de las anteriores portátiles de la empresa, así como de los efectos estereoscópicos de la 3DS. Con un total de 13.5 millones de consolas Wii U vendidas en el mundo esta consola es la menos exitosa de Nintendo en toda su historia, seguida de la GameCube. En 2014 surgió una nueva línea de productos consistentes en figuras de los personajes de la empresa y que llevan por nombre amiibos. Para celebrar el 30.º aniversario de Super Mario Bros., en 2015 se implementó la campaña Lets Super Mario en la que los jugadores podían enviar videos alusivos al personaje. A finales de 2013 Nintendo adquirió parte de las acciones de Pux Corporation, subsidiaria de Panasonic, para desarrollar programas de reconocimiento facial, de voz y de texto para sus videojuegos, primordialmente de la DS. Debido a un decremento del 30 % en los ingresos de la compañía entre abril y diciembre de ese mismo año, Iwata anunció el recorte temporal tanto de su sueldo como el de algunos ejecutivos a partir de 2014. En enero de 2015, la empresa emitió un comunicado para informar el cese de sus operaciones en el mercado brasileño debido en parte a las elevadas tasas de impuestos de importación. Si bien esto no afectó al resto de Latinoamérica debido a la alianza con la compañía Juegos de Video Latinoamérica, dos años después se llegó a un acuerdo con NC Games para que los productos de la empresa nipona volviesen al territorio brasileño. Como parte de una nueva estrategia publicitaria, a partir de 2014 Nintendo sustituyó sus conferencias en la Electronic Entertainment Expo (E3) por retransmisiones en video en las que da a conocer sus novedades para las plataformas de juego. En julio de 2015 Iwata falleció debido a un cáncer de vía biliar, y Tatsumi Kimishima fue nombrado su sucesor al frente de la organización, tras un par de meses en los que Miyamoto y Takeda estuvieron a cargo de sus operaciones. Como parte de una reestructuración de los mandos directivos, estos últimos recibieron las responsabilidades de consejero creativo y consejero tecnológico, respectivamente. Wii, Nintendo 3DS y Wii U. 2015-2020: Incursión en móviles, Switch y otros productos Las pérdidas financieras ocasionadas por las ventas de la Wii U y la intención de Sony de adaptar a partir de 2014 sus videojuegos a otros sistemas de entretenimiento, tales como las televisiones inteligentes, motivó a Nintendo a replantear su principal estrategia centrada en la producción de videoconsolas y juegos virtuales. Como resultado, a principios de 2015 formalizó un par de acuerdos con DeNA y Universal Parks & Resorts para extender su presencia a dispositivos inteligentes y parques temáticos, respectivamente.  En marzo de 2016 se estrenó Miitomo, la primera aplicación móvil para los sistemas operativos iOS y Android. Desde entonces la empresa ha producido otras aplicaciones similares, tales como Super Mario Run, Fire Emblem Heroes, Animal Crossing: Pocket Camp, Mario Kart Tour y Pokémon Go, este último desarrollado por la estadounidense Niantic, Inc. ,gracias a esta última aplicación, en 2016 Nintendo generó ingresos adicionales de 115 millones USD,. Otros sucesos relevantes acontecidos en 2016 tuvieron que ver con el reemplazo del servicio de fidelización Club Nintendo por My Nintendo, y el lanzamiento de NES Classic Edition ,en japonés: ニンテンドークラシックミニ ファミリーコンピュータ,, una versión rediseñada de la NES con novedades como el soporte de la interfaz HDMI y su compatibilidad con el Wiimote para reproducir juegos de Wii y Wii U de la Consola Virtual. Casi un año después, en septiembre de 2017, se lanzó SNES Classic Edition ,en japonés: ニンテンドークラシックミニ スーパーファミコン,, con un funcionamiento similar a su predecesora, pero esta vez centrada en el catálogo de títulos para la SNES. Hasta junio de 2018 se habían comercializado alrededor de seis millones de unidades de ambas consolas en el mundo. El producto que sustituyó a la Wii U en la octava generación de videoconsolas, Nintendo Switch ,en japonés: ニンテンドースイッチ,, comenzó a distribuirse en marzo de 2017. Algunas de sus características principales son el diseño híbrido a manera de consola de sobremesa y portátil; la funcionalidad independiente de sus mandos Joy-Con, que incorporan a su vez su respectivo acelerómetro y giróscopo; y la conexión inalámbrica simultánea de hasta ocho consolas. Para ampliar su catálogo de juegos, la empresa nipona concretó alianzas con varias desarrolladoras terceras e independientes y, hasta febrero de 2019, se tenía noción de más de 1800 juegos para esta plataforma. Asimismo, la cantidad de consolas vendidas hasta marzo de 2020 superaba las 55 millones de unidades en todo el mundo. En abril de 2018 comenzó a comercializarse una nueva línea de productos denominada Nintendo Labo ,en japonés: ニンテンドーラボ, consistente en accesorios de cartón que interactúan con la Switch y sus Joy-Con. De acuerdo con información de la compañía, en su primer año en el mercado se habían vendido más de un millón de productos de uno de los tres kits de productos de esta marca. Entre 2018 y 2019 hubo un par de cambios organizacionales en Nintendo: Shuntaro Furukawa reemplazó a Kimishima en la presidencia del corporativo, mientras que Doug Bowser hizo lo mismo en Nintendo of America, al sustituir a Reggie Fils-Aime, respectivamente. A partir de 2019 la organización puso en marcha una estrategia de expansión comercial en China, a través de una alianza con Tencent Holdings, para la distribución de la Switch en el país asiático, la cual se concretó en diciembre del mismo año. Si bien la pandemia de COVID-19 ocasionó el retraso en la producción y distribución de algunos productos de la compañía, esta situación «tuvo un impacto limitado en sus resultados comerciales». En el apartado digital, gracias principalmente a la contribución del servicio Nintendo Switch Online, en 2019 se reportó un incremento del 75 % de ingresos con respecto al año fiscal anterior. Con motivo del 35° aniversario de lanzamiento de Super Mario Bros, en 2020 se distribuyeron múltiples productos inspirados en la mascota de Nintendo, entre los cuales se encuentran la consola portátil Game & Watch: Super Mario Bros. y los juegos Super Mario 3D All-Stars, Mario Kart Live: Home Circuit y Super Mario 3D World + Bowsers Fury. Cabe agregar que, en abril del mismo año, se inauguró un hotel llamado Marufukuro en la que había sido la sede original de Nintendo en los años 1930.  Logotipo de Nintendo desde 2016; Nintendo Switch; juego Pokémon GO. 2020-actualidad: Adquisiciones, Super Mario Bros.: la película y Nintendo Switch 2 A principios de 2021 Nintendo adquirió la desarrolladora canadiense Next Level Games, mismo año en el que conmemoró el 35° aniversario del lanzamiento de The Legend of Zelda (1986) y que, de forma similar al evento de Super Mario Bros, contó con la reedición de The Legend of Zelda: Skyward Sword y la comercialización de la portátil Game & Watch: The Legend of Zelda. La inauguración del área temática Super Nintendo World, en las instalaciones del Universal Studios Japan, ocurrió en marzo del mismo año tras varios retrasos en su construcción como consecuencia de la pandemia de COVID-19. Tuvo un costo superior a los 60 000 millones JPY ,aproximadamente 580 millones USD en 2020,, y desde su apertura ha influido en un incremento anual de visitantes al parque japonés de Universal hasta en un 85 %. En febrero de 2023 abrió sus puertas en el parque Universal Studios Hollywood, y se espera que extienda también al Universal Epic Universe, ubicado en el Universal Orlando Resort. Otro proyecto de infraestructura en ese período tuvo que ver con la remodelación de la planta de producción ubicada en Uji a manera de galería. Vista del parque Super Nintendo World en Japón; y logotipo de Super Mario Bros.: La película. Nintendo anunció un par de adquisiciones en 2022: por un lado, la compañía Systems Research & Development con la cual ya había colaborado en juegos como Donkey Kong y The Legend of Zelda; y el estudio Dynamo Pictures ,renombrado como Nintendo Pictures,, con la finalidad de reforzar su estrategia de diversificación mediante el desarrollo de producciones visuales. Asimismo, entre mayo de 2022 y febrero de 2023, el Public Investment Fund ,fondo soberano de inversión de Arabia Saudita, adquirió el 7 % de participaciones de Nintendo, con lo que pasó a ser el mayor inversor externo de la organización.  En abril de 2023 se estrenó Super Mario Bros.: La película, una película animada basada en los personajes de Mario y cuyo reparto de voces en inglés incluye a los actores Chris Pratt, Anya Taylor-Joy, Seth Rogen, Jack Black y Charlie Day como Mario, Peach, Donkey Kong, Bowser y Luigi, respectivamente. Recaudó más de 1360 millones USD, con lo que pasó a ser la segunda película con mayores recaudaciones de 2023, solo superada por Barbie; y la segunda película animada con mayores ingresos en taquilla en la historia del cine, por debajo de Frozen II (2019); entre otros récords similares. Si bien obtuvo críticas mixtas primordialmente hacia su trama, la cinta se hizo acreedora a múltiples nominaciones de premios, incluidos los Globos de Oro. Casi un año después de su estreno, se anunció una continuación cuyo estreno está programado para 2026; adicionalmente, Nintendo anunció una producción cinematográfica live action basada en The Legend of Zelda, coproducida con Avi Arad y bajo la dirección de Wes Ball. En el primer semestre de 2023, y como parte de la mercadotecnia de lanzamiento del juego The Legend of Zelda: Tears of the Kingdom, empezó a distribuirse un nuevo modelo OLED de la Switch cuyo diseño está inspirado en el universo de Zelda. En julio del mismo año se llevó a cabo la fusión de las filiales de Francia y Benelux con Nintendo of Europe, como parte de una reestructura de la compañía. Otros videojuegos distribuidos por la compañía en este período, y que tuvieron un buen desempeño comercial, incluyen Pikmin 4, Super Mario Bros. Wonder y Super Mario RPG. El 16 de enero de 2025, se anunció la sucesora de la Nintendo Switch, Nintendo Switch 2, programada para lanzarse el 5 de junio del mismo año.  Productos El giro principal de Nintendo es la investigación y desarrollo, producción y distribución de productos de entretenimiento, primordialmente software y hardware de videojuegos, y juegos de cartas, y sus principales mercados son Japón, América y Europa, aunque más del 70 % de sus ventas totales provienen de este último par. Videoconsolas Artículo principal: Anexo:Consolas de Nintendo Consolas de sobremesa de Nintendo hasta 2016. Desde el lanzamiento de su primera videoconsola, Color TV-Game 6 en 1977, Nintendo ha producido y distribuido varias plataformas de videojuegos, entre las cuales se incluyen videoconsolas de sobremesa, portátiles, dedicadas e híbridas ,es decir, sistemas con características de sobremesa y portátiles,. Cabe mencionar que cada consola es compatible con una variedad de accesorios y controladores; por ejemplo, la Nintendo Zapper, el Super NES Mouse, el Expansion Pak, las tarjetas de memoria, el Wii MotionPlus, el Wii U Pro Controller y el Switch Pro Controller, son algunos productos complementarios para cada una de las consolas de sobremesa, así como la Game Boy Camera y el Rumble Pak para las portátiles Game Boy y DS, respectivamente. A continuación aparecen listadas las consolas de Nintendo con su fecha de lanzamiento, período de vida y, cuando corresponda, las distintas variaciones o modelos existentes: Consolas producidas por Nintendo Consola Fecha de estreno Período de vida Modelo(s)/Variación(es) Fuente(s) Sobremesa Color TV-Game 1 de junio de 1977 1977-1980 Color TV-Game 6 Color TV-Game 15 Color TV-Game Racing 112 Color TV-Game Block Breaker Computer TV-Game  NES 15 de julio de 1983 1983-2003 Famicom Titler Twin Famicom NES-101 Sharp Nintendo Television NES Classic Edition  SNES 21 de noviembre de 1990 1990-2003 New-Style Super NES Super Famicom Naizou TV SF1 SNES Classic Edition  N64 23 de junio de 1996 1996-2002 Nintendo iQue  NGC 14 de septiembre de 2001 2001-2007 Panasonic Q  Wii 19 de noviembre de 2006 2006-2013 Wii Family Edition Wii Mini   Wii U 18 de noviembre de 2012 2012-2017 ,  Híbridas Switch 3 de marzo de 2017 2017-2025 Nintendo Switch Lite Nintendo Switch OLED   Switch 2 5 de junio de 2025 2025-vigente , Portátiles Game & Watch 28 de abril de 1980 1980-1991 ,  GB 21 de abril de 1989 1989-2003 Play It Loud! Game Boy Pocket Game Boy Light  Virtual Boy 21 de julio de 1995 1995 ,  GBC 21 de octubre de 1998 1998-2003 ,  GBA 21 de marzo de 2001 2001-2010 Game Boy Advance SP Game Boy Micro Visteon Dockable Entertainment  Pokémon Mini 16 de noviembre de 2001 2001-2002 ,  DS 21 de noviembre de 2004 2004-2011 Nintendo DS Lite Nintendo DSi Nintendo DSi XL  3DS 26 de febrero de 2011 2011-2020 Nintendo 3DS XL Nintendo 2DS New Nintendo 3DS New Nintendo 3DS XL New Nintendo 2DS XL  Videojuegos Artículos principales: Anexo:Videojuegos para NES, Anexo:Videojuegos para Super Nintendo, Anexo:Videojuegos para Nintendo 64 y Anexo:Videojuegos para Nintendo GameCube. Artículos principales: Anexo:Videojuegos para Wii, Anexo:Videojuegos para Wii U y Anexo:Videojuegos para Nintendo Switch. Artículos principales: Anexo:Videojuegos de la Consola Virtual, Anexo:Videojuegos para Game Boy, Anexo:Videojuegos para Virtual Boy, Anexo:Videojuegos para Game Boy Color y Anexo:Videojuegos para Game Boy Advance. Artículos principales: Anexo:Videojuegos para Nintendo DS y Anexo:Videojuegos para Nintendo 3DS. Consolas con algunos cartuchos y productos complementarios de Nintendo. Los primeros videojuegos producidos por Nintendo pertenecen al género arcade y, si bien EVR Race (1975) fue uno de los primeros títulos electromecánicos de la compañía mientras que Computer Othello y Block Fever estuvieron en el catálogo de lanzamientos iniciales de la Color TV-Game en 1978, se considera que Donkey Kong (1981) es el primero de plataformas en la historia, tanto de la organización como de los videojuegos. Desde entonces tanto Nintendo como otras empresas desarrolladoras han producido y distribuido un extenso catálogo de videojuegos para las distintas plataformas de la compañía ,véase la tabla inferior para más información, entre los cuales se incluyen Super Mario Bros. (1985), Tetris (1989), Super Mario World (1990), Super Mario 64 (1996), Pokémon oro y Pokémon plata (1999), Super Smash Bros. Melee (2001), Pokémon rubí y Pokémon zafiro (2002), New Super Mario Bros. (2006), Wii Sports (2006), Mario Kart 7 (2011), Mario Kart 8 (2014) y Mario Kart 8 Deluxe (2017), que son algunos de los más vendidos en su historia.    Cabe resaltar que los juegos existen tanto en formato físico como en digital ,para este último formato, existen servicios de distribución tales como Nintendo eShop y Nintendo Network,. En la siguiente tabla se muestra la cantidad de juegos lanzados por Nintendo solamente en formato físico: Total de juegos lanzados por consola, por desarrollador y por región hasta marzo de 2023 Consola Nintendo Terceros Total general JPN AME OTROS JPN AME OTROS JPN AME OTROS TOTAL Sobremesa NES 49 72 88 998 607 263 1047 679 351 2077 SNES 30 52 73 1368 667 471 1398 719 544 2661 N64 43 53 55 153 244 193 196 297 248 741 NGC 55 48 47 220 504 406 275 552 453 1280 Wii 76 55 64 387 1210 1213 463 1265 1277 3005 Wii U 40 41 40 73 131 120 113 172 160 445 Híbrida Switch 77 91 92 1449 1777 1727 1526 1868 1819 5213 Portátiles GB 63 106 135 1183 859 817 1246 965 952 3163 GBA 107 71 75 679 950 846 786 1021 921 2728 DS 132 98 117 1713 1639 2022 1845 1737 2139 5721 3DS 94 111 124 575 393 427 669 504 551 1724 Otros Proponer ideas en el campo más amplio del entretenimiento es parte de lo que hace que Nintendo sea único, y creo que generar un nuevo valor es una directiva de la compañía. Se llegó a plantear la cuestión de si estamos planeando convertirnos en una especie de organización conglomerada gigante y con fines de lucro, pero preferiríamos aumentar las ganancias como resultado de desafiarnos continuamente a generar valor en lugar de invertir capital para expandir nuestro negocio [... No obstante] estamos discutiendo una variedad de otros desarrollos que harían uso activo de nuestra propiedad intelectual. ,Shigeru Miyamoto en 2018. Además de las consolas, videojuegos y sus respectivos complementos ,cuyos ingresos hasta 2008 representaban el 90 % del importe de sus ventas,, Nintendo ha desarrollado otro tipo de productos con base en su giro comercial con el propósito de hacer «un uso más activo» de su propiedad intelectual, «expandirla y construir un nuevo eje de negocios» así como «incrementar las oportunidades para los consumidores de todo el mundo para que interactúen con nuestra propiedad intelectual». Si bien sus videojuegos suelen contar con temas musicales creados por algunos compositores como Kōji Kondō, Hirokazu Tanaka, David Wise, Kenta Nagata, Kazumi Totaka y Mahito Yokota, solamente se han puesto a la venta algunos compilatorios de bandas sonoras, entre los cuales se incluyen Super Mario Galaxy: Original Soundtrack, Donkey Kong Returns Original Sound Track, Super Smash Bros. For Nintendo 3DS / Wii U (A Smashing Soundtrack), y The Legend Of Zelda: 25th Anniversary, que en ciertos casos se distribuyeron como parte de una promoción por la compra del videojuego correspondiente. Desde 2013 la empresa continúa produciendo y distribuyendo ocasionalmente sus líneas de barajas tradicionales y naipes japoneses ,dentro de esta última categoría se encuentran las cartas Hanafuda, Kabufuda y Hyakunin Isshu, con temáticas relacionadas con Pokémon, los videojuegos de Mario y The Legend of Zelda, aunque también existe la opción de que el cliente pueda personalizar sus cartas a partir de sus propios diseños. Otros productos similares son los juegos de tablero Shōgi, go, y Mahjong, este último con la marca comercial Yakoman desde 1964. Entre las publicaciones de Nintendo se encuentran los manuales de instrucciones y la revista Nintendo Power. Desde el estreno de la NES, Nintendo producía manuales impresos con instrucciones sobre las mecánicas de juego de sus títulos, además de otras guías como la Nintendo Buyers Guide. La transición del formato impreso al digital de estas publicaciones se concretó durante el período de vida de la Wii U. A su vez, el contenido de Nintendo Power versaba sobre información de los videojuegos de Nintendo y su tiraje perduró desde 1988 a 2012; cinco años después, la revista se adaptó en forma de pódcast. Un esfuerzo similar sucedió con la revista mexicana Club Nintendo, cuyas ediciones impresas estuvieron disponibles de 1991 a 2014, mientras que su distribución digital cesó en 2019. Si bien la línea de juguetes de Nintendo es extensa y se remonta a su etapa previa como desarrolladora de videojuegos, un par de menciones relevantes son las figuras y tarjetas amiibo, y Nintendo Labo. Las primeras están diseñadas a partir de algunos personajes de sus videojuegos y permiten que el jugador acceda a contenido adicional en los juegos con los que poseen compatibilidad; de acuerdo con cifras de 2018, anualmente se venden más de diez millones de estas figuras en todo el mundo. Por otra parte, los productos de Nintendo Labo consisten en accesorios de cartón que interactúan con la Switch y sus Joy-Con; en abril de 2019 salió a la venta el modelo VR Kit, cuarto modelo de esta línea. Asimismo, la empresa colabora con otras compañías japonesas para difundir información sobre su tecnología a estudiantes, mientras que ciertas filiales poseen alianzas con organismos como la Starlight Childrens Foundation. Aunque la empresa se rehusaba a producir juegos para teléfonos inteligentes, a partir de 2014 incursionó en este sector comercial y, desde entonces, ha producido aplicaciones como Super Mario Run, Fire Emblem Heroes y Pokémon Go. Al año, la empresa tiene considerado producir anualmente entre dos y tres juegos para móviles. La estrategia de diversificación se extendió también a la creación de una área temática en Universal Studios Japan, Super Nintendo World. De forma similar la empresa concretó acuerdos con Illumination Entertainment para producir Super Mario Bros.: La película, estrenada en salas de cine en 2023; y con Avi Arad y Sony Pictures para una producción cinematográfica live action basada en The Legend of Zelda. En 2024, la Reseña Anual del sistema de Madrid de la Organización Mundial de la Propiedad Intelectual (OMPI) clasificó a Nintendo como el décimo cuarto lugar mundial en número de solicitudes de marcas realizadas en el marco del Sistema de Madrid, con 55 solicitudes de marcas realizadas durante 2023, 74 en 2022 y 58 en 2021. Organización Sede de Nintendo of America en Redmond (Washington). Nintendo cuenta con cinco oficinas en Japón: tres ubicadas en Kioto y dos en Tokio. El edificio central original, inaugurado en 1889 y demolido en 2004, se encontraba al este de Kioto, a las orillas del río Kamo, en la calle Shomen Dori. En 1933 se utilizó un nuevo edificio corporativo situado al lado del original, cerca de la estación de tren Toba-kaidō. Un par de décadas después, en 1959, la sede se movió a otra edificación en el distrito de Higashiyama-ku, y dos años después abrieron sus puertas las oficinas del barrio de Chiyoda, en Tokio. A partir del año 2000, la sede se encuentra en 11-1 Hokotate-cho, Kamitoba, en el distrito de Minami-Ku, en Kioto, Japón. En 2014 comenzó a operar un centro de investigación y desarrollo de Nintendo, cerca de la sede de Kioto, y en 2022 se adquirió una superficie adyacente a estas oficinas en la cual se habrá de construir un segundo centro de investigación y desarrollo, cuya apertura está programada para diciembre de 2027. Takashi Tezuka, Shigeru Miyamoto y Kōji Kondō durante un evento en Tokio a finales de 2015. El consejo de administración de Nintendo está integrado por diez directores,[A] de los cuales cuatro pertenecen al comité de auditoría y supervisión ,y de estos, tres son ejecutivos externos a la organización,. El presidente propone el listado de candidatos a ocupar un puesto en el comité directivo, con base en su personalidad, experiencia y conocimientos. Únicamente los integrantes del comité de auditoría son elegidos cada dos años, mientras que las responsabilidades del resto del consejo se renuevan anualmente, decisiones que recaen en los accionistas de la empresa.[B] El consejo es responsable de la elección del presidente y director ejecutivo de la compañía así como de los ejecutivos a cargo de cada división o departamento,[C] y de las correspondientes subsidiarias, aunque estos últimos responden a la autoridad de la presidencia. Existe adicionalmente un comité de dirección ejecutiva que emite recomendaciones al presidente, así como un comité de conformidad cuya función principal es validar las actividades de los ejecutivos divisionales. La auditoría interna es realizada por el departamento homónimo. En 2015 surgió Nintendo Entertainment Planning & Development a partir de la fusión de las divisiones Nintendo Entertainment Analysis and Development ,Nintendo EAD, y Nintendo Software Planning & Development ,Nintendo SPD,, la cual es presidida por Shinya Takahashi. Algunos de los que han trabajado para Nintendo son Gunpei Yokoi, Masayuki Uemura, Satoru Iwata, Reggie Fils-Aimé, Satoru Shibata, Shigeru Miyamoto, Takashi Tezuka, Yoshiaki Koizumi, Katsuya Eguchi, Eiji Aonuma, Yoshio Sakamoto, Genyo Takeda, Hisashi Nogami, Kōji Kondō y Tōru Minegishi. El más reciente presidente y director ejecutivo de Nintendo es Shuntaro Furukawa, que asumió sus funciones en abril de 2018. Nintendo posee un total de veintinueve filiales, de las cuales siete se encuentran en Tokio y el resto son extranjeras. Las filiales existentes fuera de Japón se encargan primordialmente de la distribución de los productos de la matriz ,como Nintendo of America y Nintendo of Europe,, aunque en ciertos casos también poseen la atribución de producir videojuegos. En esta última categoría se encuentran Nintendo Software Technology, iQue y Retro Studios. En contraste, solo una de las filiales japonesas está a cargo de la administración de las ventas, y las demás son desarrolladoras exclusivas ,NDcube, 1-UP Studio, Inc., Monolith Soft y Mario Club Co., y de producciones visuales ,Nintendo Pictures Co.,. Por otra parte, entre sus socios comerciales se encuentran The Pokémon Company ,propietario de la marca Pokémon,, Warpstar, Inc. ,responsable de administrar la propiedad intelectual y de producir animaciones,, PUX Corporation ,desarrollador de tecnología software, y First Avenue Entertainment, los primeros tres radicados en territorio japonés. Entre 1983 y 2022 la empresa cotizó en la primera sección de la bolsa de Tokio, y desde entonces se encuentra listada en la sección principal o de mayor capitalización, de acuerdo con la reestructuración del mercado bursátil japonés.  La plantilla de Nintendo está conformada por 7317 empleados hasta julio de 2023, de los cuales alrededor del 38 % pertenecen a las oficinas de la matriz japonesa, 20 % son empleados de Nintendo of America, 14 % son de Nintendo of Europe, 1.5 % de la sede australiana y el resto forman parte de las otras filiales de la organización. Véanse también: Divisiones de Nintendo y Desarrolladoras de Nintendo. Información financiera A continuación algunos datos financieros de Nintendo: Información financiera de Nintendo (en millones USD) Años 1990 Año fiscal[D] 1997 1998 1999 Ventas netas 4017 4304 3987 Utilidad neta 629 645 774 BPA 0.90 0.90 0.90 Años 2000 Año fiscal 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 Ventas netas 3475 4168 4197 4899 4812 4348 8183 16 724 18 761 15 423 Utilidad neta 726 800 560 316 816 840 1477 2573 2847 2458 BPA 0.90 1.05 1.17 1.33 2.52 3.33 5.85 12.60 14.69 10.00 Años 2010 Año fiscal 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 Ventas netas 12 221 7898 6759 5550 4464 4464 4366 9959 10 914 12 115 Utilidad neta 935 -526 75 -225 348 146 915 1316 2270 3345 BPA 5.42 1.21 1.06 1.78 2.94 1.21 7.62 10.96 14.68 20.01 Años 2020 Año fiscal 2020 2021 2022 Ventas netas 15 990 14 011 12 042 Utilidad neta 6193 5573 4516 BPA 36.66 33.44 2.79 Total de consolas y videojuegos vendidos por región hasta marzo de 2023 (en millones de unidades) Plataforma Consolas Videojuegos Total general JPN AME OTROS JPN AME OTROS Unid. Juegos Sobremesa NES 19.35 34.00 8.56 225.86 230.85 43.29 61.91 500.01 SNES 17.17 23.35 8.58 194.85 135.00 49.22 49.10 379.06 N64 5.54 20.63 6.75 39.75 142.06 43.16 32.93 224.97 NGC 4.04 12.94 4.77 27.54 138.48 42.56 21.74 208.58 Wii 12.75 48.64 40.23 75.94 502.53 343.38 101.63 921.85 Wii U 3.34 6.49 3.74 15.83 56.49 31.28 13.56 103.60 Híbrida Switch 29.59 49.00 47.02 200.15 456.27 379.73 125.62 1063.61 Portátiles GB 32.47 44.06 42.15 157.06 190.61 153.44 118.69 501.11 GBA 16.96 41.64 22.91 73.11 217.24 87.06 81.51 377.42 DS 32.99 59.93 61.10 213.84 399.20 335.72 154.02 948.76 3DS 25.26 26.90 23.78 139.58 134.42 117.37 75.94 391.13 Tecnología empleada Sede de Nintendo en Tokio. Desde su incursión en la industria de videojuegos, Nintendo ha invertido recursos en sus propias áreas de investigación y desarrollo de software y hardware. Originalmente estas atribuciones recaían en cuatro departamentos ,R&D1, R&D2, R&D3 y R&D4,; a partir de la reestructuración de 2015, las divisiones responsables del desarrollo tecnológico de los productos de Nintendo son Nintendo Entertainment Planning & Development y Nintendo Platform Technology Development, en Japón así como Nintendo Software Technology y Nintendo European Research & Development, en América y Europa, respectivamente. Si bien la manufactura de ciertos productos es realizada por empresas externas como Foxconn y Hosiden en el caso de la plataforma Switch, cuya producción es supervisada por representantes de Nintendo y debe estar acorde con sus políticas de responsabilidad social, sus esfuerzos han producido tecnologías como los mandos en forma de cruceta y análogo; la batería de respaldo de los cartuchos de NES que permiten guardar el progreso en el juego en reemplazo de los sistemas de contraseñas; la cámara fotográfica para Game Boy; el dispositivo háptico Rumble Pak; el diseño de cuatro puertos de controlador en las consolas para el modo multijugador local; la interacción con un videojuego a partir de un sistema de detección de movimiento; y Virtual Boy, una de las primeras consolas de realidad virtual. En años recientes, Nintendo ha dirigido sus esfuerzos a la investigación y desarrollo de tecnologías para el almacenamiento de información y para la visualización ,como pantallas de cristal líquido, en adición a componentes electrónicos entre los cuales se incluyen sensores, pantallas táctiles, comunicación inalámbrica, computación en la nube, realidad virtual, realidad aumentada, realidad mixta, aprendizaje profundo y análisis de microdatos. Para tales efectos, sus prioridades se centran en la durabilidad, seguridad, calidad y rendimiento de su catálogo de productos, el diseño y producción de accesorios, la reducción de costos y la conservación de energía. Filosofía de trabajo y temáticas El personal de Nintendo es acreedor a una serie de prestaciones adicionales a su remuneración, entre las cuales se incluyen gastos de transporte, un par de retribuciones anuales, incrementos salariales constantes,. seguro médico y programas educativos, entre otros. En años recientes, una de las prioridades de sus áreas de reclutamiento de personal consisten en la contratación de jóvenes con «capacidad de concentración [...] Aquellos que realmente entienden lo que han logrado a través de sus años universitarios. Si vemos a un artista que se concentraba mucho y estuvo decidido a crear un proyecto a gran escala por su cuenta, y que lo logró durante varios años, o si vemos a una persona que trabajó en una sola película desde el principio hasta el final de su curso. Ese tipo de persona necesita mucha determinación y conocimiento para lograr tal cosa». Otro esfuerzo estratégico de la compañía está relacionado con la igualdad de género al «promover el empleo de mujeres y crear un entorno que les permita establecer trayectorias exitosas»; a nivel global, en 2023 un 23 % de las posiciones de liderazgo de Nintendo era ocupado por alguna mujer. Hasta julio de 2023 la edad media de los empleados era de 39 años, mientras que la antigüedad y el sueldo promedio oscilaban en 14 años y 74 110 USD al año, respectivamente. La gente siempre nos pregunta si asumimos riesgos a propósito. Pero, a nuestro parecer, realmente no asumimos riesgos sino que seguimos intentando cosas nuevas [...] El pensamiento que nos guía es: ¿qué podemos hacer para sorprender gratamente a los jugadores? No es que estemos intentando innovar conscientemente; traramos de encontrar maneras de hacer felices a las personas. Como resultado, hacemos cosas que otros no han hecho. ,Shinya Takahashi, director ejecutivo de Nintendo. La política de responsabilidad social de Nintendo posee una de las «mejores reputaciones» en el mundo y, desde 2015, está centrada en el enunciado de misión «poner sonrisas en la cara de todos». Sus ejes de acción están relacionados con la innovación y seguridad de sus productos, la seguridad y salud laboral, la diversidad y el desarrollo humano del personal, el desarrollo sostenible de sus operaciones acorde con la certificación ISO 14001, las labores de caridad y la estructura organizacional. Los centros de servicio así como de reparación y mantenimiento de sus productos; los formularios en línea y el reporte anual de responsabilidad social son algunos ejemplos de sus servicios de atención al cliente, mientras que la metodología de Deming forma parte de sus procedimientos operativos. La política de responsabilidad social es aplicable también para cualquier empresa que mantenga vínculos comerciales o productivos con Nintendo. Pese a sus esfuerzos para la protección medioambiental ,al prohibir el uso de ftalato, y PVC en el cableado de sus consolas; y establecer un «comité ambiental» y una política correspondiente en 2011,, la empresa es una de las menos sostenibles en comparación con otras del sector electrónico, de acuerdo con un reporte difundido en 2010 por Greenpeace. En cuanto a labor humanitaria, como parte de un convenio con la Starlight Childrens Foundation, en 1992 se crearon las unidades Starlight Fun Center destinadas a los pacientes de menor edad de los hospitales y que incluían distintas formas de entretenimiento multimedia. Hasta 1995 existían un millar de unidades de esta categoría.  Durante su gestión al frente de la compañía, en la etapa de transición a los videojuegos, Hiroshi Yamauchi recalcó al personal la importancia de que sus consolas «fueran fáciles de programar y capaces de hacer toda clase de cosas con las que haya soñado un diseñador». Para esto puso en ejecución ciertas estrategias destinadas a vincular la imagen corporativa de Nintendo con la calidad e innovación de sus productos, sin que esto conlleve necesariamente el desarrollo de tecnologías nuevas y sofisticadas; por ejemplo, se buscó reclutar a «artistas del videojuego» que «puedan desarrollar juegos de excelencia que todo el mundo quiera [jugar]» e «invertir todos los recursos para producir uno o dos juegos principales al año, en vez de varios éxitos [consecutivos] menores». Se considera que parte de la buena recepción de las primeras consolas de Nintendo se debió a los títulos desarrollados por terceros con los que la empresa nipona estableció acuerdos, entre los cuales se incluyen Capcom, Konami, Tecmo, Square, Koei y Enix. A partir de la Switch, esta estrategia se ha extendido a los desarrolladores independientes. En cuanto a los objetivos organizacionales de Nintendo, estos son «crecimiento continuo y a largo plazo, estabilidad operativa y rentabilidad maximizada». Nintendo mantuvo por varios años una estricta política hacia los contenidos y las licencias de videojuegos de sus consolas. Por ejemplo, aunque permitía la violencia gráfica en sus contenidos, prohibía la inclusión de elementos de desnudez y/o sexualidad debido a que se pensaba que este tipo de material podía afectar su imagen corporativa. La filial Nintendo of America estipuló que, además de los anteriores contenidos, sus juegos no habrían de contar con alusiones racistas, sexistas, insultos, sangre, drogas o mensajes políticos ni religiosos, con la excepción de símbolos en desuso como los de la mitología griega. Sin embargo, el impacto que han tenido estos criterios en el rendimiento comercial de Nintendo también ha sido desfavorable; por ejemplo, se vendió una cantidad inferior de copias de la versión de Mortal Kombat (1992) de Super Nintendo que de la edición para Sega Genesis. La diferencia entre ambas versiones radicó esencialmente en la censura de la sangre y otros elementos de violencia gráfica por parte de la desarrolladora Acclaim, a petición de la empresa nipona. De igual forma, para garantizar que sus productos licenciados para la NES cumplieran con sus lineamientos tenía la opción de bloquear sus chips de autentificación. Ante esta situación, desde entonces Nintendo ha adoptado una mayor flexibilidad en el cumplimiento de su política, lo cual se hizo evidente por ejemplo con el lanzamiento de Mortal Kombat II (1993), para la misma consola que su predecesora, sin censura alguna y con una advertencia al jugador, impresa en su embalaje. Tras el establecimiento de la Interactive Digital Software Association y del sistema sistema de clasificación de contenido de videojuegos ESRB, Nintendo abandonó la mayoría de sus políticas de censura que venía requiriendo desde el estreno de la NES con tal de que el público lleve a cabo sus propias elecciones de contenidos. En años más recientes, las principales modificaciones en este aspecto son realizadas directamente por la desarrolladora, y rara vez a petición de Nintendo. Cabe agregar que existen juegos cuya distribución está prohibida en el mercado americano, los cuales se distinguen por su clasificación «Adults Only» ,por sus siglas: AO; en español: «solo para adultos», por parte de la ESRB. Por lo general los procesos productivos comienzan con el análisis y discusión de ideas por parte de uno o más grupos reducidos de desarrolladores e ingenieros tanto de software como de hardware, cuyas conclusiones dan lugar a la definición tanto del diseño como del propósito del producto. Por ejemplo, en el caso del título 1-2-Switch, la intención era «crear un juego de socialización en el que los jugadores no tengan que mirar la pantalla, sino que se miren entre sí». A su vez, para la línea Nintendo Labo, la empresa recurrió a un producto «anticuado» en vez de tecnología sofisticada para cumplir con sus objetivos. Otro ejemplo similar es el rendimiento gráfico inferior de la Wii en contraste con la interfaz de la PlayStation 3 y de la Xbox 360, un aspecto que no impidió que la consola tuviera un éxito sin precedentes para la compañía. El trabajo en equipo es también un aspecto relevante de su filosofía; Pokémon Stadium fue el primer juego en el que colaboraron equipos desde múltiples ubicaciones. Cabe destacar que la protección de su propiedad intelectual es un aspecto importante para la compañía, que desaconseja la emulación de sus títulos sin previa autorización,  así como la producción de cualquier contenido que haga uso de alguna variación de su propiedad intelectual. Para reducir estas prácticas, las filiales estadounidense y europea suelen incorporar el sello «Seal of Quality» junto con el enunciado «Official Nintendo Licensed Product» en cualquier producto de la empresa para distinguirlos de aquellos que no están oficialmente reconocidos por Nintendo. En cuanto a las temáticas que ofrecen los productos de Nintendo, estas suelen provenir de varias fuentes de inspiración. Por ejemplo, en el caso de Miyamoto con Super Mario Bros., la trama y los diseños están basados en el folclore, la literatura y la cultura pop como Star Trek y Las aventuras de Alicia en el país de las maravillas así como de su «manera de experimentar el mundo» y de sus recuerdos. En el caso de The Legend of Zelda: «cuando era niño, durante una excursión me encontré con un lago. Para mí fue una sorpresa topármelo. Mientras viajaba por el país sin ningún mapa, intentando hacer mi camino, y encontrándome con cosas asombrosas a medida que continuaba [mi trayecto], me percaté de lo que se siente tener una aventura». En su opinión, las audiencias adultas disfrutan jugar títulos de Mario debido a que «estos les traen recuerdos de su infancia» y «es un detonante volverme nuevamente primitivo como una forma de pensar y recordar [...] Un adulto es un niño que tiene más ética y moral. Es todo. Cuando era un niño, lo que creaba no era un juego. Yo estaba en el juego. [Así que] el juego no es para niños, sino para mí. Para ese adulto que todavía tiene la esencia de un niño». Por lo general, sus consolas ofrecen la posibilidad de activar un control paterno que restringe el contenido del software de acuerdo con los lineamientos del sistema ESRB, y los resultados de seminarios que realiza continuamente la empresa en algunas prefecturas de Japón. Impacto Casi en todas las generaciones [de videoconsolas], Nintendo ha liderado una carga de innovación que ha cambiado radicalmente el mundo de los juegos. Estas innovaciones no siempre han sido bien recibidas, pero las huellas dactilares de Nintendo están tan firmemente grabadas en nuestra industria, que la compañía es posiblemente la figura más importante de la misma. ,Ben Reeves, de Game Informer, en 2011. Se considera que las decisiones estratégicas de Yamaushi, entre las cuales se incluyen la incursión de Nintendo en los videojuegos, no solo garantizaron el éxito de la compañía sino la subsistencia de tal segmento industrial, puesto que «restauró la confianza pública en los videojuegos tras el sombrío desplome del mercado estadounidense a principios de los años 1980». En 1991 ya era la empresa más exitosa en Japón y, desde entonces, sus productos «han redefinido la manera en la que jugamos» al igual que su modelo de negocios que priorizó las estrategias de ventas de software en vez de las consolas, como habitualmente hacían los distribuidores mayoristas de videojuegos. Su política de responsabilidad social así como la filosofía de trabajo centrada en la calidad e innovación de sus productos han llevado a Nintendo a ser catalogado como «un fabricante centrado en el consumidor», un par de rasgos característicos que le han permitido distinguirse de sus competidores históricos, Sony y Microsoft. Desde 2013 la publicación estadounidense Forbes ha incluido a Nintendo en sus listas de «Mejores empleadoras del mundo» al tomar en consideración su entorno laboral y la diversidad del personal; mientras que, en 2018, Time la clasificó como una de las «empresas genio del año» al considerar que «ha vuelto un hábito la resurrección» en alusión al éxito de Switch en contraste con el resultado obtenido por la Wii U. Su capital excedía los diez millardos JPY y sus ventas netas sobrepasaban los mil millardos JPY ,o nueve millardos USD, mayormente del mercado americano que provee alrededor del 42 % de este importe, en 2020, lo cual la convierte en una de las empresas con mayor riqueza de Japón y una de las «más valiosas» del país. Hasta 2023 se habían vendido más de cinco mil millones de videojuegos y 836 millones de consolas de Nintendo en todo el mundo.  Presentación de Nintendo en el E3 de 2017. Tanto sus personajes como algunos de sus videojuegos han tenido un impacto en la cultura popular contemporánea; por ejemplo, Mario pasó de ser la mascota corporativa a un «ícono cultural» así como uno de los personajes más famosos de los videojuegos. En opinión de John Taylor, de Arcadia Investment Corp.: «[Mario] es, de lejos, la propiedad de videojuegos más grande que existe. [... Es] donde empiezas a escalar el tamaño del mercado global para juegos». Otros personajes notables de la compañía son la Princesa Peach, Pikachu, Link, Donkey Kong, Kirby, Samus Aran y Ness. Por otro lado, la influencia de juegos como Donkey Kong (1981), Super Mario Bros. (1985), The Legend of Zelda (1986), Metroid (1986), Tetris (1989), Fire Emblem (1990) Star Fox (1993) y Pokémon Red y Blue (1996) dio lugar a la consolidación de sus respectivas franquicias. Por ejemplo: Super Mario Bros. «popularizó el concepto de videojuegos de plataformas creando un mundo lleno de secretos y enemigos memorables, al mismo tiempo que proporciona un control mejorado sobre la física del protagonista principal»; Tetris, cuyo «diseño engañosamente simple hizo que el título fuera enormemente accesible y diabólicamente convincente, e inspiró a una nueva generación de títulos abstratos de rompecabezas»; The Legend of Zelda: Ocarina of Time que «con su abundante mundo animado, hilos narrativos de múltiples capas, diversas interacciones y espíritu juguetón, es efectivamente una escuela de diseño de juegos y un taller de ideas sobre la agencia, deleite y descubrimiento del jugador»; y Metroid, «[un juego] oscuro y solemne, con una inminente sensación de aislamiento y una poderosa sensación alienígena». De manera similar, además de los ya mencionados, la crítica ha catalogado algunos otros como «los mejores de la historia», como es el caso de Super Mario World (1990), Super Mario Kart (1992), EarthBound (1994), Yoshis Island (1995), GoldenEye 007 (1997), Animal Crossing (2001), Super Smash Bros. Melee (2001), Metroid Prime (2002), Super Mario Galaxy (2007) y The Legend of Zelda: Breath of the Wild (2017). Adicionalmente, algunas de sus videoconsolas han marcado un hito en la industria como fue el caso de la SNES, cuyo éxito «cimentó a Nintendo como el líder de consolas domésticas» y alentó a otras empresas a incursionar en la industria de videojuegos, al ver cómo Nintendo «disfrutaba en el mercado sin preocuparse por la [ausencia de] competencia». De igual manera, el sistema de detección del movimiento de la Wii permitió la producción de juegos y accesorios que contribuyen al acondicionamiento físico y terapéutico del usuario.  A su vez, la Game Boy y sus distintos productos complementarios ,como la Game Boy Camera, la Game Boy Printer y el Game Link Cable, «combinan la mecánica de juego con aventuras del mundo real [...] agregando funcionalidad extra que permitiría a los juegos portátiles saltar a los dispositivos domésticos». De acuerdo con un reporte publicado por la compañía en 1995, casi la mitad de los usuarios de esta portátil eran mujeres a diferencia de los sistemas predecesores, cuyo mercado femenino era inferior al 30 % de la población. Cosplay de algunos personajes de Nintendo durante un evento realizado en 2011. Además de los diversos reconocimientos obtenidos por sus productos ,que incluyen un par de premios Emmy de Tecnología e Ingeniería para las consolas Wii y DS; otros dos más por Nintendo Labo en el rubro de «Innovación» de los Game Developers Choice Awards y de los BAFTA de videojuegos de 2019; o sus reconocimientos en los Game Critics Awards de 2017 por Super Mario Odyssey y Mario + Rabbids Kingdom Battle,, algunos de sus colaboradores se han hecho acreedores a varias distinciones por su trabajo. Por ejemplo, en 2010 la Academia Británica de las Artes Cinematográficas y de la Televisión reconoció a Miyamoto con un premio «Fellowship» a su trayectoria; de forma similar a Iwata y Aunuma, a quienes se les distinguió en 2015 y 2016 con un distintivo Golden Joystick; o Takeda, que en 2018 consiguió un premio DICE por la Academy of Interactive Arts & Sciences; mismo año en que Charles Martinet recibió la acreditación de récord Guiness por brindar su voz a Mario en más de un centenar de productos."
ksampletext_wikipedia_vide_minecraft: str = "Minecraft. Minecraft es un videojuego de construcción de tipo «mundo abierto» o en inglés sandbox creado originalmente por el sueco Markus Persson (conocido comúnmente como «Notch»), que creó posteriormente Mojang Studios (actualmente parte de Microsoft). Está programado en el lenguaje de programación Java para la versión Java Edition y posteriormente desarrollado en C++ para la versión de Bedrock Edition. Fue lanzado el 17 de mayo de 2009, y después de numerosos cambios, su primera versión estable «1.0» fue publicada el 18 de noviembre de 2011. Markus Persson, el creador de Minecraft. Un mes antes del lanzamiento de su versión completa se estrenó una versión para dispositivos móviles llamada Minecraft: Pocket Edition en Android, y el 17 de noviembre del mismo año fue lanzada la misma versión para iOS, aunque posteriormente esta pasaría a ser Minecraft: Bedrock Edition. El 9 de mayo de 2012 fue lanzada la versión del juego para Xbox 360 y PS3. Todas las versiones de Minecraft reciben actualizaciones constantes desde su lanzamiento. En octubre de 2014, Minecraft lanzó su edición para PlayStation Vita, desarrollada por Mojang y 4J Studios. Esta versión presenta las mismas actualizaciones y similares características que las otras versiones de consola; además, cuenta con el sistema de venta cruzada, es decir que al comprar la versión de PlayStation 3 se obtiene también la de PlayStation Vita. A marzo de 2024 se habían vendido más de 300 millones de copias, siendo actualmente el videojuego más vendido de la historia.  El 15 de septiembre de 2014, fue adquirido por la empresa Microsoft por un valor de 2500 millones de dólares estadounidenses. Este suceso provocó el alejamiento de Markus Persson de la compañía. En noviembre de 2016, Microsoft anunció el lanzamiento de la versión completa de Minecraft: Education Edition. Jugabilidad Un jugador explorando el mundo y mirando a un Creeper, uno de los enemigos de Minecraft Un videojuego similar a Minecraft de dominio público hecho en Luanti. Minecraft introdujo millones de niños al mundo digital. Minecraft es un juego de mundo abierto, y no tiene un fin claramente definido (aunque sí que tiene una dimensión llamada a sí misma The End, o en español El Final en donde después de entrar y matar a la dragona aparecen los créditos del juego y un poema). Esto permite una gran libertad en cuanto a la elección de su forma de jugar. A pesar de ello, el juego posee un sistema que otorga logros por completar ciertas acciones. La cámara es en primera persona, aunque los jugadores tienen la posibilidad de cambiarla a una perspectiva de tercera persona en cualquier momento. El juego se centra en la colocación y destrucción de bloques, siendo que este se compone de objetos tridimensionales cúbicos, colocados sobre un patrón de rejilla fija. Estos cubos o bloques representan principalmente distintos elementos de la naturaleza, como tierra, piedra, minerales, troncos, entre otros. Los jugadores son libres de desplazarse por su entorno y modificarlo mediante la creación, recolección y transporte de los bloques que componen al juego, los cuales solo pueden ser colocados respetando la rejilla fija del juego. Los jugadores son capaces de crear además las denominadas «granjas», que son estructuras y mecanismos para conseguir un fin, aprovechándose de ciertas mecánicas del juego. En ciertos casos, algunas de ellas se terminan implementando al juego, como el observador. En el juego se pueden encontrar estructuras especiales como aldeas, galerías mineras, templos marinos, pirámides y templos selváticos. El jugador tiene la libertad de definir la dificultad del juego, siendo el modo más tranquilo el pacífico, que a diferencia de las demás dificultades, no permite que aparezcan monstruos en el juego que puedan herir al jugador. Generación del mundo Al inicio del juego, el jugador se encuentra en un mundo generado de manera procedural mediante varios algoritmos, como el ruido Perlin, el ruido rosa entre otros, lo que permite que este sea inmenso, teniendo hasta 30 millones de bloques en todas las direcciones en las versiones actuales. Hay una gran cantidad de diferentes mundos (diferentes versiones de los algoritmos mencionados) generados por un código que identifica cada mundo llamado semilla (seed en inglés). El jugador es libre de desplazarse por el terreno, conformado por distintos biomas, entre los que se encuentran desiertos, sabanas, selvas, océanos, llanuras, tundras, etcétera.  El juego posee su propio ciclo de tiempo de día y noche, siendo que un día en el juego equivale a 20 minutos en la realidad. El mundo no se genera por completo al principio, sino que está dividido en chunks («trozos», «pedazos» traducido literalmente al español, pero sin adaptación oficial) de 16 × 16 bloques en horizontal y en las versiones actuales de 384 bloques en vertical. Los chunks cercanos al jugador se cargan en la memoria, pudiendo elegir la distancia mínima a la que se cargan. A medida que este se desplaza, se generan y añaden nuevos chunks al mundo. Modos de juego Supervivencia El modo supervivencia (survival en inglés) se basa en la vida real combinada con un poco de fantasía y se trata de la supervivencia al ataque de las múltiples criaturas que surgen en la oscuridad o de noche. El máximo aguante que tienen los personajes consta de 10 corazones (20 puntos de salud). En este modo las herramientas, armas y armadura se gastan con el uso. Creativo En el modo creativo (creative en inglés) se centra enteramente en el aspecto de la construcción libre. Los jugadores poseen un suministro ilimitado de todos los bloques y objetos del juego, que pueden colocar y destruir de forma instantánea. Además, no son atacados por los monstruos, son inmunes a todo daño (aunque sí pueden morir cayendo al vacío si están en Java Edition, o mediante el comando /kill) y pueden volar libremente por el mapa. En este modo no se pueden romper bloques sosteniendo espada, para prevenir la destrucción del entorno cuando el jugador golpea o ataca. Extremo El modo extremo (hardcore en inglés) es idéntico al modo supervivencia, con la diferencia de que tras la muerte del jugador este ya no puede volver a revivir y la dificultad está fijada en difícil.  Espectador El modo espectador permite a los jugadores volar a través de bloques y ver el mundo sin interactuar con él. En este modo, la barra de acceso rápido se convierte en un menú que permite al jugador teletransportarse a los jugadores en la partida. También es posible ver desde el punto de vista de otro jugador o criatura. Aventura El modo aventura (adventure en inglés) está destinado a los jugadores que se dedican a crear mapas para usuarios que deseen jugar en línea o solos. Este modo de juego se basa en los siguientes criterios que afectan al jugador en distintos sentidos: El jugador solo puede romper un bloque si tiene la herramienta adecuada, y esta programada con comandos para que pueda ser rota. Si el creador del mapa así lo configura, la dificultad no puede ser modificada por los jugadores. Mods y modos de juego no oficiales Algunos usuarios o desarrolladores avanzados optan por crear sus propios modos de juego y aplicarlos en servidores o mapas de aventura para jugar en modo de un jugador. Sin embargo, estos modos de juego suelen verse poco y la mayoría de los servidores modifican algunas extensiones para hacer parecer que tienen un modo de juego personalizado.[cita requerida] En las primeras versiones preliminares de Minecraft, el único modo de juego disponible era similar al creativo actual. En particular, la versión clásica (que ya no recibe actualizaciones) aún puede jugarse gratuitamente desde navegadores web. Multijugador Véase también: Servidor de Minecraft El modo multijugador en Minecraft permite que varios jugadores interactúen y se comuniquen entre sí en un solo mundo. Está disponible a través de multijugador directo de juego a juego, juego LAN, pantalla dividida local (solo consola) y servidores (alojados por jugadores y negocios). Los jugadores pueden ejecutar sus propios servidores, usar un proveedor de alojamiento o conectarse directamente al juego de otro jugador a través de Xbox Live. Los mundos de un solo jugador tienen soporte de red de área local, lo que permite a los jugadores unirse a un mundo en computadoras interconectadas localmente sin una configuración de servidor. Los servidores multijugador de Minecraft están guiados por operadores de servidores, que tienen acceso a los comandos del servidor, como configurar la hora del día y teletransportar a los jugadores. Los operadores también pueden establecer restricciones con respecto a qué nombres de usuario o direcciones IP pueden o no ingresar al servidor. Los servidores multijugador tienen una amplia gama de actividades, y algunos servidores tienen sus propias reglas y costumbres únicas. El servidor más grande y popular es Hypixel, que ha sido visitado por más de 14 millones de jugadores únicos. El combate jugador contra jugador (PvP) se puede habilitar para permitir la lucha entre jugadores. Muchos servidores tienen complementos personalizados que permiten acciones que normalmente no son posibles. Minecraft Realms En 2013, Mojang anunció Minecraft Realms, un servicio de alojamiento de servidor destinado a permitir a los jugadores ejecutar juegos multijugador en servidor de manera fácil y segura sin tener que configurar uno propio. A diferencia de un servidor estándar, solo los jugadores invitados pueden unirse a los servidores de Realms y estos servidores no usan direcciones IP. Los propietarios de servidores de Minecraft: Java Edition Realms pueden invitar hasta veinte personas a jugar en su servidor, con hasta diez jugadores en línea a la vez. Los propietarios de servidores de Minecraft Realms pueden invitar hasta 3000 personas a jugar en su servidor, con hasta diez jugadores en línea a la vez. Los servidores de Minecraft: Java Edition Realms no admiten complementos creados por el usuario, pero los jugadores pueden jugar mapas personalizados de Minecraft . Los servidores de Minecraft Realms admiten complementos creados por el usuario, paquetes de recursos, paquetes de comportamiento y mapas personalizados de Minecraft . En la Electronic Entertainment Expo 2016 (conocida normalmente como E3), se agregó soporte para juegos multiplataforma entre las plataformas Windows 10, iOS y Android a través de Realms a partir de junio de 2016, con soporte para Xbox One y Nintendo Switch más adelante en 2017, y soporte para dispositivos de realidad virtual. El 31 de julio de 2017, Mojang lanzó la versión beta de la actualización que permite el juego multiplataforma. El soporte de Nintendo Switch para Realms se lanzó en julio de 2018. Personalización Artículo principal: Modding en Minecraft Fundidora del mod Tinkers Construct La comunidad de modding está formada por fanes, usuarios y programadores externos. Usando una variedad de interfaces de programas de aplicaciones que han surgido con el tiempo, han producido una amplia variedad de contenido descargable para Minecraft, como modificaciones, paquetes de texturas y mapas personalizados. Las modificaciones del código de Minecraft, llamadas mods, agregan una variedad de cambios en el juego, que van desde nuevos bloques, nuevos elementos, nuevas turbas hasta conjuntos completos de mecanismos para crear.  La comunidad de modding es responsable de un suministro sustancial de mods, desde los que mejoran el juego, como minimapas, puntos de ruta y contadores de durabilidad, hasta los que agregan al juego elementos de otros videojuegos y medios. Si bien una variedad de marcos de mod se desarrollaron de forma independiente mediante la ingeniería inversa del código, Mojang también mejoró Minecraft estándar con marcos oficiales para la modificación, lo que permite la producción de paquetes de recursos creados por la comunidad, que alteran ciertos elementos del juego, incluidas texturas y sonidos. Los jugadores también pueden crear sus propios mapas (archivos guardados personalizados del mundo) que a menudo contienen reglas específicas, desafíos, acertijos y misiones, y compartirlos para que otros jueguen. A estos mismos, se les suele llamar mapmakers, o en español creadores de mapas. Mojang agregó un modo aventura en agosto de 2012 y bloques de comando en octubre de 2012, que se crearon especialmente para mapas personalizados en Java Edition . Los paquetes de datos, introducidos en la versión 1.13 de Java Edition, permiten una mayor personalización, incluida la capacidad de agregar nuevos logros, dimensiones, funciones, tablas de botín, predicados, recetas, estructuras, etiquetas, configuraciones de generación mundial y biomas‌.  La edición Xbox 360 admite contenido descargable, que está disponible para comprar a través de Xbox Games Store ; estos paquetes de contenido suelen contener máscaras de personajes adicionales. Más tarde recibió soporte para paquetes de texturas en su duodécima actualización de título mientras presentaba paquetes de mezcla, que combina paquetes de texturas con paquetes de máscaras y cambios en los sonidos, la música y la interfaz de usuario del juego. El primer paquete de mash-up (y, por extensión, el primer paquete de texturas) para Xbox 360 Edition se lanzó el 4 de septiembre de 2013 y tenía como tema la franquicia Mass Effect. Sin embargo, a diferencia de la Edición Java, la Edición Xbox 360 no admite modificaciones creadas por jugadores ni mapas personalizados. El 17 de mayo de 2016 se lanzó en todo el mundo un paquete de recursos de promoción cruzada basado en la franquicia Super Mario de Nintendo para la edición de Wii U. Se anunció el lanzamiento de un paquete combinado basado en Fallout en Wii U Edition . En abril de 2018, se descubrió malware en varias máscaras descargables de Minecraft creadas por usuarios para usar con la edición Java del juego. Avast declaró que casi 50.000 cuentas estaban infectadas y, cuando se activaba, el malware intentaba reformatear el disco duro del usuario. Mojang solucionó rápidamente el problema y emitió una declaración que decía que el juego en sí no ejecutaría ni leería el código, y solo se ejecutaría cuando se abriera la imagen que contenía la máscara. En junio de 2017, Mojang lanzó una actualización conocida como Discovery Update para la versión Bedrock del juego. La actualización incluye un nuevo mapa, un nuevo modo de juego, el Mercado, un catálogo de contenido generado por los usuarios que brinda a los creadores de Minecraft otra forma de ganarse la vida con el juego, y más.  Mods populares En la comunidad de Minecraft y de modding en general, existen una serie de mods que se utilizan mucho o son importantes para la estabilidad y compatibilidad de otros mods o del juego en general. Forge, Fabric, Bukkit, Paper entre otros proyectos que modifican una instalación de un servidor o del cliente (jugador), las cuales implementan una API para desarrollar mods. Optifine, mod para el cliente o el servidor que agrega características para mejorar el rendimiento y estética (capas). Red Power 2, mod que agrega características avanzadas a Minecraft en forma de tecnología industrial, como bombas hidráulicas, paneles solares y tubos neumáticos, permitiendo incluso emular un ordenador completo basado en el MOS 6502 a través de las mecánicas del propio juego.  Ediciones de Minecraft La edición principal es la versión Java de PC, pero aun así existen otras tres ediciones: Bedrock Edition, Console o Legacy Edition y Education Edition. Bedrock Edition Artículo principal: Minecraft Bedrock En un principio, esta edición de Minecraft se desarrolló para ser una edición de bolsillo, siendo la edición disponible para teléfonos y tabletas. También hay una edición llamada Windows 10 Edition que se basa en la misma línea de desarrollo y el mismo código de Pocket Edition, aunque sólo está disponible para Windows 10 y está preparada para funcionar también en Holo Lens, dispositivos de realidad virtual de Microsoft y Oculus Rift. También la Pocket Edition normal tenía su soporte para estos dispositivos como con el Samsung Gear VR. Otras variantes de esta edición son Apple TV Edition y Fire TV Edition. Sin embargo, a partir de la versión 1.2, se unificó la jugabilidad de Pocket Edition con las de Windows 10, Xbox One S y los dispositivos de realidad virtual, bajo la llamada Bedrock Edition, permitiendo el juego multijugador entre las plataformas anteriores, tanto en mundos con interconexión como en servidores multijugador. Luego de un tiempo se sumó la Nintendo Switch a esta edición, y finalmente la Playstation 4 y 5. Legacy Edition Esta edición del juego, antes conocida como Console Edition, estaba preparada para funcionar en consolas como Xbox 360, Xbox One, Wii U, Nintendo Switch, PS3 y PS4. La edición era muy similar a la edición Java y se podía jugar multijugador en distintas consolas con suscripciones, como es el caso de Xbox, con su servicio Xbox LIVE Gold. Con la llegada de la edición Bedrock a PS4 a fines de 2019 todas las ediciones Legacy fueron retiradas del mercado. Aunque siguen siendo accesibles en algunas plataformas, no recibirán más actualizaciones. Education Edition Artículo principal: Minecraft Education Edition Esta es una edición especial de Minecraft Bedrock orientada al entorno estudiantil. Incorpora características exclusivas como los mundos tutorial, los PNJ (personajes no jugadores) y el modo de clase, junto con ítemes exclusivos como un pizarrón, además de una mesa de crafteo especial, recipiente y reductor para su uso en clases de química. Edición para la República Popular China Existe una edición localizada de Minecraft en chino simplificado desarrollada por Mojang Studios y publicada por NetEase, destinada a los jugadores de la República Popular China. Esta edición está disponible tanto en Java Edition como en Bedrock y es idéntica a la edición comercializada por Mojang/Microsoft. Sin embargo, cuenta con una serie de diferencias, entre las cuales está su descarga gratuita (pero requiere registro y cumplir con las obligaciones de identidad de la República Popular China), uso de cuenta de NetEase en lugar de una cuenta de Mojang o Microsoft, el uso de un lanzador exclusivo, y su propia tienda, además de incluir unos mods preinstalados. Se han modificado y/o censurado los nombres de algunos elementos del juego, con tal de cumplir con las regulaciones del gobierno chino.[cita requerida] Otros títulos Artículo principal: Minecraft (franquicia) Minecraft: Story Mode Artículo principal: Minecraft: Story Mode Minecraft: Story Mode es un juego point-and-click desarrollado y publicado por Telltale Games, basado en el videojuego sandbox Minecraft. El juego fue lanzado para Microsoft Windows, OS X, PlayStation 3, PlayStation 4, PlayStation Vita, Wii U, Xbox 360, Xbox One, Android y iOS. El juego fue desarrollado en asociación con Mojang, el desarrollador original de Minecraft. El juego sigue el formato episódico que Telltale Games ha utilizado en sus títulos como The Walking Dead, The Wolf Among Us, Tales from the Borderlands y Game of Thrones. El juego se centra en torno a un nuevo personaje, llamado Jesse, que puede ser tanto hombre o mujer, esto a elección del jugador. El protagonista y sus aliados intentan salvar su mundo al derrotar al Wither Storm. Minecraft Dungeons Esta sección es un extracto de Minecraft Dungeons.[editar] Minecraft Dungeons es un videojuego de rol de acción que se lanzó el 26 de mayo de 2020 para Windows, Xbox One, Nintendo Switch y PlayStation 4; más tarde se lanzó para macOS, Xbox Series X|S y PlayStation 5. Fue desarrollado por Mojang y, cuya conversión a las consolas, fue realizada por Double Eleven. Se puede jugar solo o en línea con hasta cuatro jugadores. Minecraft Earth Esta sección es un extracto de Minecraft Earth.[editar] Minecraft Earth es un videojuego descontinuado perteneciente al género de sandbox de realidad aumentada, desarrollado por Mojang y publicado por Xbox Game Studios. Es una derivación del videojuego Minecraft, se anunció por primera vez en mayo del año 2019 y estuvo disponible para Android, iOS y iPadOS. El juego fue gratuito y se lanzó como acceso temprano (Early access) por primera vez a principios de octubre de 2019. El juego recibió su última actualización el 5 de enero de 2021, dejando de estar disponible el 30 de junio del mismo año. Minecraft Legends Esta sección es un extracto de Minecraft Legends.[editar] Minecraft Legends es un juego derivado de Minecraft con la temática de estrategia desarrollado por Mojang Studios y Blackbird Interactive y publicado por Xbox Game Studios. Lanzado en Microsoft Windows, Xbox One, Xbox Series X/S, Nintendo Switch, PlayStation 4 y PlayStation 5, el 18 de abril de 2023."

ksampletext_wikipedia_arts_musica: str = "Música. La música es, según la definición tradicional del término, el arte de crear y organizar sonidos y silencios respetando los principios fundamentales de la melodía, la armonía y el ritmo, mediante la intervención de complejos procesos psicoanímicos. El concepto de música ha ido evolucionando desde su origen en la Antigua Grecia, en que se reunía sin distinción a la poesía, la música y la danza como arte unitario. Desde hace varias décadas se ha vuelto más compleja la definición de qué es y qué no es la música, ya que destacados compositores en el marco de diversas experiencias artísticas fronterizas han realizado obras que, si bien podrían considerarse musicales, expanden los límites de la definición de este arte. La música, como toda manifestación artística, es un producto cultural con múltiples finalidades, entre otras, la de suscitar una experiencia estética en el oyente, la de expresar sentimientos, emociones, circunstancias, pensamientos o ideas, y cada vez más, cumplir una importante función terapéutica a través de la musicoterapia. La música cumple una función importante en el desarrollo cognitivo del ser humano. Está relacionada con el pensamiento lógico matemático, la adquisición del lenguaje, el desarrollo psicomotriz, las relaciones interpersonales, el aprendizaje de lenguas no nativas y ayuda a potenciar la inteligencia emocional, entre otros.  La música es un estímulo sonoro que afecta al campo perceptivo de la persona; así, el flujo sonoro puede cumplir variadas funciones (entretenimiento, comunicación, ambientación, diversión, etc.). En muchas culturas, la música es una parte importante del modo de vida de la gente, ya que desempeña un papel fundamental en rituales religiosos, en las ceremonias de rito de paso (por ejemplo, la graduación y el matrimonio), en las actividades sociales (por ejemplo, en el baile) y en las actividades culturales que van desde el canto aficionado en el karaoke hasta tocar en una banda amateur de funk o cantar en un coro comunitario. La gente puede hacer música por afición, como, por ejemplo, un adolescente que toca el violonchelo en una orquesta juvenil, o trabajar como músico o cantante profesional. La industria musical incluye a las personas que crean nuevas canciones y piezas musicales (como los cantautores y los compositores), a las personas que interpretan música (que incluyen a los músicos de orquesta, de bandas de jazz y de bandas de rock, a los cantantes y a los directores de orquesta), a las personas que graban música (productores musicales e ingenieros de sonido), a las personas que organizan giras de conciertos y a las personas que venden grabaciones y partituras a los clientes. Incluso una vez que se ha interpretado una canción o pieza, la crítica musical, el periodismo musical y la musicología pueden valorar y evaluar la pieza y su interpretación. Etimología En la mitología griega, las nueve musas fueron la inspiración de muchos esfuerzos creativos, incluidas las artes. La palabra «música» deriva del griego μουσική (mousike; «(arte) de las musas») En la mitología griega, las nueve musas eran las diosas que inspiraban la literatura, la ciencia y las artes y eran la fuente del conocimiento plasmado en la poesía, los cantos y los mitos de la cultura griega. Según el Diccionario etimológico en línea: El término «música» deriva de «musike, de mediados del siglo XIII, del francés antiguo musique (siglo XII) y directamente del latín musica el arte de la música, que también incluye la poesía (también la fuente del español música, italiano musica, alto alemán antiguo mosica, alemán Musik, holandés muziek, danés musik)». Se deriva del «...griego mousike (techne) (arte) de las musas, del femenino de mousikos «perteneciente a las musas», de Mousa «Musa». La ortografía moderna data de la década de 1630. En la Grecia clásica, el término «música» se refiere a cualquier arte en el que las musas presidían, pero especialmente la música y la poesía lírica. Definición La música académica occidental ha desarrollado un método de escritura basado en dos ejes: el horizontal representa el transcurso del tiempo, y el vertical la altura del sonido; la duración de cada sonido está dada por la forma de las figuras musicales. Las definiciones parten desde el seno de las culturas, y así, el sentido de las expresiones musicales se ve afectado por cuestiones psicológicas, sociales, culturales e históricas. De esta forma, surgen múltiples y diversas definiciones que pueden ser válidas en el momento de expresar qué se entiende por música. Ninguna, sin embargo, puede ser considerada como perfecta o absoluta. Una definición bastante amplia determina que música es sonoridad organizada (según una formulación perceptible, coherente y significativa). Esta definición parte de que ,en aquello a lo que consensualmente se puede denominar «música», se pueden percibir ciertos patrones del «flujo sonoro» en función de cómo las propiedades del sonido son aprendidas y procesadas por los humanos (hay incluso quienes consideran que también por los animales). La definición que se atribuye a Edgard Varese de música como sonido organizado, y también la de la Enciclopedia Británica que en su decimoquinta edición describe que «aunque no haya sonidos que no puedan ser descritos como inherentemente no-musicales, en cada cultura los músicos han tendido a restringir la gama de sonidos que estaban dispuestos a admitir». John Blacking añadió este detalle importante a la definición de Varèse: «la música es sonido organizado humanamente». Hoy en día es frecuente trabajar con un concepto de música basado en tres atributos esenciales: que utiliza sonidos, que es un producto humano (y en este sentido, artificial) y que predomina la función estética. Si tomáramos en cuenta solo los dos primeros elementos de la definición, nada diferenciaría a la música del lenguaje. En cuanto a la función «estética», se trata de un punto bastante discutible; así, por ejemplo, un «jingle» publicitario no deja de ser música por cumplir una función no estética (tratar de vender una mercancía). Por otra parte, hablar de una función «estética» presupone una idea de la música (y del arte en general) que funciona en forma autónoma, ajena al funcionamiento de la sociedad, tal como la vemos en la teoría del arte del filósofo Immanuel Kant. Jean-Jacques Rousseau, autor de las voces musicales en LEncyclopédie de Diderot, después recogidas en su Dictionnaire de la Musique, la definió como el «arte de combinar los sonidos de una manera agradable al oído». Según el compositor Claude Debussy, la música es «un total de fuerzas dispersas expresadas en un proceso sonoro que incluye: el instrumento, el instrumentista, el creador y su obra, un medio propagador y un sistema receptor». La definición más habitual en los manuales de música se parece bastante a esta: «la música es el arte del bien combinar los sonidos en el tiempo». Esta definición no se detiene a explicar lo que es el arte, y presupone que hay combinaciones «bien hechas» y otras que no lo son, lo que es por lo menos discutible. Algunos eruditos han definido y estudiado a la música como un conjunto de tonos ordenados de manera horizontal (melodía) y vertical (armonía). Este orden o estructura que debe tener un grupo de sonidos para ser llamados música está, por ejemplo, presente en las aseveraciones del filósofo alemán Johann Wolfgang von Goethe cuando la comparaba con la arquitectura, definiendo metafóricamente a la arquitectura como «música congelada». La mayoría de los estudiosos coincide en el aspecto de la estructura, es decir, en el hecho de que la música implica una organización; pero algunos teóricos modernos difieren en que el resultado deba ser placentero o agradable. Parámetros del sonido Véase también: Nota musical Distribución de las notas musicales en el teclado de un piano. Cada nota representa una frecuencia de sonido distinta. El sonido es la sensación percibida por el oído al recibir las variaciones de presión generadas por el movimiento vibratorio de los cuerpos sonoros. Se transmite por el medio que los envuelve, que generalmente es el aire de la atmósfera. La ausencia perceptible de sonido es el silencio, aunque es una sensación relativa, ya que el silencio absoluto no se da en la naturaleza. Los cuatro parámetros principales del sonido son: intensidad, altura, duración y timbre. La intensidad describe si un sonido es fuerte o suave, la altura determina si es agudo o grave, la duración se refiere a si el sonido es largo o corto, y el timbre permite distinguir entre fuentes sonoras diferentes. El sonido tiene cuatro parámetros fundamentales: La altura es el resultado de la frecuencia que produce un cuerpo sonoro; es decir, de la cantidad de ciclos de las vibraciones por segundo o de hercios (Hz) que se emiten. De acuerdo con esto se pueden definir los sonidos como graves y agudos. Cuanto mayor sea la frecuencia, más agudo (o alto) será el sonido. La longitud de onda es la distancia medida en la dirección de propagación de la onda, entre dos puntos cuyo estado de movimiento es idéntico; es decir, que alcanzan sus máximos y mínimos en el mismo instante. La duración corresponde al tiempo que duran las vibraciones que producen un sonido. La duración del sonido está relacionada con el ritmo. La duración viene representada en la onda por los segundos que esta contenga. La intensidad es la fuerza con la que se produce un sonido; depende de la energía. La intensidad viene representada en una onda por la amplitud. El timbre es la cualidad que permite distinguir los diferentes instrumentos o voces a pesar de que estén produciendo sonidos con la misma altura, duración e intensidad. Los sonidos que escuchamos son complejos; es decir, son el resultado de un conjunto de sonidos simultáneos (tonos, sobretonos y armónicos), pero que nosotros percibimos como uno (sonido fundamental). El timbre depende de la cantidad de armónicos o la forma de la onda que tenga un sonido y de la intensidad de cada uno de ellos, a lo cual se lo denomina espectro. El timbre se representa en una onda por el dibujo. Un sonido puro, como la frecuencia fundamental o cada sobretono, se representa con una onda sinusoidal, mientras que un sonido complejo es la suma de ondas senoidales puras. El espectro es una sucesión de barras verticales repartidas a lo largo de un eje de frecuencia y que representan a cada una de las senoides correspondientes a cada sobretono, y su altura indica la cantidad que aporta cada una al sonido resultante. Elementos de la música Un ejemplo de las escalas de la antigua Grecia es la octava hipofrigia de género enarmónico (en mi). Ya existía el concepto fundamental de octava. La nota varía según la armonía (αρμονία), transcritas como modos griegos. La polifonía y variedad de instrumentos aerófonos, cordófonos y membranófonos, han sido documentadas en infinidad de grabados y dibujos. La música contiene dos elementos: el material acústico y la idea intelectual. Ambos no se hallan yuxtapuestos como forma y contenido, sino que se combinan, en la música, para formar una imagen unitaria. Para convertirse en vehículo de la idea intelectual, el material acústico experimenta una preparación pre-musical, mediante un proceso de selección y ordenamiento. La estructura del sonido, la escala de sonidos armónicos, exhibe ya un ordenamiento que la predestina para ser el vehículo de la intención intelectual. Con el fin de un entendimiento general previo, dentro del material acústico para la organización de la música, encontramos diversas clasificaciones, dentro de las cuales la más habitual en ambientes académicos es la que divide la música en melodía, armonía y ritmo. La manera en la que se definen y aplican estos principios, varían de una cultura a otra (también hay variaciones temporales). La melodía es un conjunto de sonidos ,concebidos dentro de un ámbito sonoro particular, que suenan sucesivamente uno después de otro (concepción horizontal), y que se percibe con identidad y sentido propio. Los silencios también forman parte de la estructura de la melodía, poniendo pausas al «discurso melódico». El resultado es como una frase bien construida semántica y gramaticalmente. Es discutible ,en este sentido, si una secuencia dodecafónica podría ser considerada una melodía o no. Cuando hay dos o más melodías simultáneas se denomina contrapunto. La armonía bajo una concepción vertical de la sonoridad, y cuya unidad básica es el acorde o tríada, regula la concordancia entre sonidos que suenan simultáneamente y su enlace con sonidos vecinos. El ritmo es el resultado final de los elementos anteriores, a veces con variaciones muy notorias, pero en una muy general apreciación se trata de la capacidad de generar contraste en la música, esto es provocado por las diferentes dinámicas, timbres, texturas y sonidos. En la práctica se refiere a la acentuación del sonido y la distancia temporal que hay entre el comienzo y el fin del mismo o, dicho de otra manera, su duración. La articulación refiere a cómo un sonido es ejecutado, así como la transición entre dos (o más) notas. Entre las variadas formas de articulación elaboradas a lo largo de la historia, destacan principalmente el legato, el staccato, el portato, el tenuto, el acento, el marcato y el calderón. La textura es la estructura y se divide en partes, movimientos o secciones. También se puede fraccionar aún más en frases y motivos. La mayoría de géneros tienen una forma definida y secciones que las caracterizan. Es importante tener este elemento de la música presente a la hora de empezar a crear una pieza sonora porque ayuda a entender y visualizar de manera global una composición. La incorporación de un material acústico ampliado en el siglo XX, produjo a veces dificultades de información, por falta de un sistema válido de entendimiento previo, y es por eso que otros elementos se toman en cuenta a la hora de analizar y estudiar el fenómeno de la música, como son la forma, la instrumentación, la textura, etc. A partir de todos estos elementos, se originan nuevos principios de ordenamiento y posibilidades de composición. Historia La historia de la música abarca desde la prehistoria, donde se cree que surgió a través de la imitación de sonidos y ritos, hasta la actualidad, con una gran diversidad de géneros. Se divide en diferentes períodos, como la Edad Media (con el canto gregoriano), el Renacimiento, el Barroco, el Clasicismo (Mozart, Haydn), el Romanticismo (Chopin) y el siglo XX y XXI, donde se diversificaron géneros como el jazz, el rock, el pop y la música electrónica. Artículo principal: Historia de la música Prehistoria Artículo principal: Música en la Prehistoria La flauta de hueso encontrada en Divje Babe (Eslovenia), con más de 40 000 años de antigüedad. La música prehistórica solo se puede teorizar sobre la base de los hallazgos de los sitios arqueológicos paleolíticos. A menudo se descubren flautas talladas en huesos en los que se han perforado agujeros laterales; se cree que se soplaban en un extremo como el shakuhachi japonés. Se piensa que la flauta de Divje Babe, tallada en el fémur de un oso de las cavernas, tiene al menos 40 000 años. Se han recuperado instrumentos como la flauta de siete agujeros y varios tipos de instrumentos de cuerda, como el ravanahatha, de los yacimientos arqueológicos de la civilización del valle del Indo. La India tiene una de las tradiciones musicales más antiguas del mundo; las referencias a la música clásica india (marga) se encuentran en los Vedas, antiguas escrituras de la tradición hindú. La colección más antigua y más grande de instrumentos musicales prehistóricos se encontró en China y data de entre el 7000 y el 6600 a. C. El «Himno hurrita a Nikkal», que se encuentra en tablillas de arcilla que se remontan aproximadamente al 1400 a. C., es la obra musical escrita más antigua que se conserva.  Antiguo Egipto Artículo principal: Música de Egipto Los antiguos egipcios atribuyeron a uno de sus dioses, Thoth, la invención de la música, y Osiris, a su vez, lo utilizó como parte de su esfuerzo por civilizar el mundo. La evidencia material y representativa más antigua de los instrumentos musicales egipcios data del período predinástico, pero la evidencia está atestiguada con mayor seguridad en el Imperio Antiguo cuando se tocaron arpas, flautas y clarinetes dobles. Durante el Imperio Medio se agregaron instrumentos de percusión, liras y laúdes a las orquestas. Los platillos frecuentemente acompañaban a la música y danza, como lo hacen todavía hoy en Egipto. La música folclórica egipcia, incluidos los rituales tradicionales sufíes dhikr, es el género musical contemporáneo más cercano a la música del antiguo Egipto y han conservado muchas de sus características, ritmos e instrumentos.  Culturas asiáticas Mujeres indias vestidas con atuendos regionales tocando una variedad de instrumentos musicales populares en diferentes partes de la India. Categoría principal: Música de Asia La música clásica india (marga) es una de las tradiciones musicales más antiguas del mundo. La civilización del valle del Indo tiene esculturas que muestran danzas e instrumentos musicales antiguos, como la flauta de siete agujeros. Se han recuperado varios tipos de instrumentos de cuerda y tambores de Harappa y Mohenjo-Daro mediante excavaciones realizadas por Mortimer Wheeler. El Rigveda tiene elementos de la música india actual, con una notación musical para denotar la métrica y el modo de cantar. La música clásica india es monofónica y se basa en una sola línea melódica o raga, organizada rítmicamente a través de talas. Silappatikaram de Ilango Adigal proporciona información sobre cómo se pueden formar nuevas escalas mediante el cambio modal de la tónica de una escala existente. La música hindi actual fue influenciada por la música tradicional persa y por los mogoles afganos. La música carnática, popular en los estados del sur, es en gran parte devocional; la mayoría de las canciones están dirigidas a las deidades hindúes. También hay muchas canciones que enfatizan el amor y otros temas sociales. La música clásica china, el arte tradicional o la música de la corte de China, tiene una historia que se remonta a unos tres mil años. Tiene sus propios sistemas únicos de notación musical, así como afinación y tono musical, instrumentos y estilos musicales o géneros musicales. La música china es pentatónica-diatónica, con una escala de doce notas a una octava (5 + 7 = 12) al igual que la música de influencia europea. Referencias en la Biblia Artículo principal: Anexo:Música en la Biblia David componiendo salmos, Salterio de París, c. 960 (Constantinopla). El conocimiento del período bíblico proviene principalmente de referencias literarias en la Biblia y fuentes posbíblicas. El historiador de religión y música Herbert Lockyer escribió que «la música, tanto vocal como instrumental, fue bien cultivada entre los hebreos, los cristianos del Nuevo Testamento y la iglesia cristiana a través de los siglos». Agrega que «una mirada al Antiguo Testamento revela cómo el pueblo antiguo de Dios se dedicó al estudio y la práctica de la música, que ocupa un lugar único en los libros históricos y proféticos, así como en el Salterio». Los estudiosos de la música y el teatro que estudian la historia y la antropología de la cultura semítica y judeocristiana temprana han descubierto vínculos comunes en la actividad teatral y musical entre las culturas clásicas de los hebreos y las de los griegos y romanos posteriores. El área común de actuación se encuentra en un «fenómeno social llamado letanía», una forma de oración que consiste en una serie de invocaciones o súplicas. Journal of Religion and Theatre señala que entre las primeras formas de letanía, «la letanía hebrea estuvo acompañada de una rica tradición musical»: Genesis 4.21 indicated that Jubal is the father of all such as handle the harp and pipe, the Pentateuch is nearly silent about the practice and instruction of music in the early life of Israel. In I Samuel 10, there are more depictions of large choirs and orchestras. These large ensembles could only be run with extensive rehearsals. This had led some scholars to theorize that the prophet Samuel led a public music school to a wide range of students. El Génesis 4.21 indicaba que Jubal es el «padre de todos los que manejan el arpa y la flauta», el Pentateuco casi guarda silencio sobre la práctica y la instrucción de la música en la vida temprana de Israel. En I Samuel 10, hay más representaciones de «grandes coros y orquestas». Estos grandes conjuntos sólo podían ejecutarse con ensayos extensos. Esto había llevado a algunos estudiosos a teorizar que el profeta Samuel dirigió una escuela pública de música para una amplia gama de estudiantes. Antigua Grecia Epitafio de Sícilo Duración: 50 segundos.0:50 La inscripción cantada, según la pronunciación del griego koiné. ¿Problemas al reproducir este archivo? Artículo principal: Música de la Antigua Grecia La música era una parte importante de la vida social y cultural en la Antigua Grecia. Los músicos y cantantes desempeñaron un papel destacado en el teatro griego. Se realizaban interpretaciones de coros mixtos para el entretenimiento, la celebración y ceremonias espirituales. Los instrumentos incluían el aulós de doble lengüeta y un instrumento de cuerda pulsada, la lira, principalmente del tipo especial llamado kithara. La música era una parte importante de la educación y a los niños se les enseñaba música a partir de los seis años. La alfabetización musical griega creó un florecimiento del desarrollo musical. La teoría de la música griega incluía los modos musicales griegos, que finalmente se convirtieron en la base de la música clásica y religiosa occidental. Más tarde, las influencias del Imperio Romano, Europa del Este y el Imperio Bizantino cambiaron la música griega. El Epitafio de Sícilo es el ejemplo más antiguo que se conserva de una composición musical completa, incluida la notación musical, de cualquier parte del mundo. La obra escrita más antigua que sobrevive sobre el tema de la teoría musical es Harmonika Stoicheia de Aristóxeno. Antigua Roma Trío de músicos tocando aulós, cymbala y tympanum (mosaico de Pompeya). Artículo principal: Música de la Antigua Roma La música de la Antigua Roma fue parte de la cultura romana desde los tiempos más remotos. Las canciones (carmen) eran una parte integral de casi todos los eventos sociales. El Carmen saeculare de Horacio, por ejemplo, fue encargado por Augusto e interpretada por un coro mixto de niños en los Juegos Seculares del 17 a. C. La música era habitual en los funerales y la tibia (el aulós griego), un instrumento de viento-madera, se tocaba en los sacrificios para protegerse de las malas influencias. Bajo la influencia de la teoría griega antigua, se pensaba que la música reflejaba el orden del cosmos y se asociaba particularmente con las matemáticas y el conocimiento. La música etrusca tuvo una influencia temprana en la cultura de los romanos. Durante el período imperial, los romanos llevaron su música a las provincias, mientras que las tradiciones de Asia Menor, África del Norte y la Galia se convirtieron en parte de la cultura romana. La música acompañaba espectáculos y eventos en la arena, y era parte de la forma de artes escénicas llamada pantomima (pantomimus), una forma temprana de ballet narrativo que combinaba danza expresiva, música instrumental y un libreto cantado. Edad Media El introito de Gaudeamus omnes, escrito en notación neumática en el Graduale Aboense de los siglos XIV y XV, rinde homenaje a Enrique, patrón de Finlandia. Gaudeamus omnes Duración: 3 minutos y 0 segundos.3:00 Artículo principal: Música de la Edad Media La Edad Media (476 a 1400) comenzó con la introducción del canto monofónico (línea melódica única) en los servicios de la Iglesia católica. La notación musical se usó desde la antigüedad en la cultura griega, pero en la Edad Media, la notación fue introducida por primera vez por la Iglesia católica para que las melodías de los cánticos pudieran escribirse, para facilitar el uso de las mismas melodías para la música religiosa en todo el mundo católico.[a] El único repertorio medieval europeo que se ha encontrado en forma escrita desde antes del 800 es el canto llano litúrgico monofónico de la Iglesia católica, cuya tradición central se llamaba canto gregoriano. Junto a estas tradiciones de música sacra, existía una vibrante tradición de canto secular. Ejemplos de compositores de este período son Léonin, Perotín, Guillaume de Machaut y Walther von der Vogelweide. Renacimiento Artículo principal: Música del Renacimiento Amicus meus Duración: 2 minutos y 47 segundos.2:47 De Tomás Luis de Victoria. ¿Problemas al reproducir este archivo? La música del Renacimiento (c. 1400 a 1600) se centró más en temas seculares, como el amor cortés. Alrededor de 1450, se inventó la imprenta, que hacía partituras impresas mucho menos costosas y más fáciles de producir en masa.[b] La mayor disponibilidad de partituras ayudó a difundir los estilos musicales más rápidamente y en un área más amplia. Los músicos y cantantes a menudo trabajaban para la iglesia, las cortes y las ciudades. Los coros de la iglesia crecieron en tamaño y la Iglesia siguió siendo un mecenas importante de la música. A mediados del siglo XV, los compositores escribieron música sacra ricamente polifónica, en la que se entrelazaban simultáneamente diferentes líneas melódicas. Entre los compositores destacados de esta época se incluyen Josquin des Prés, Guillaume Dufay, Giovanni Pierluigi da Palestrina, Tomás Luis de Victoria, Thomas Morley y Orlando di Lasso. A medida que la actividad musical pasó de la Iglesia a las cortes aristocráticas, reyes, reinas y príncipes compitieron por los mejores compositores. Muchos compositores importantes provenían de los Países Bajos, Bélgica y el norte de Francia. Se les llama compositores franco-flamencos y ocuparon cargos importantes en toda Europa, especialmente en Italia. Otros países con una gran actividad musical fueron Alemania, Inglaterra y España. Barroco Johann Sebastian Bach fue un destacado compositor de la era barroca de la música. Tocata y fuga en re menor, BWV 565 Duración: 8 minutos y 34 segundos.8:34 Artículo principal: Música del Barroco La era barroca de la música tuvo lugar entre 1600 y 1750, cuando el estilo artístico barroco floreció en toda Europa y durante este tiempo la música se expandió en su rango y complejidad. La música barroca comenzó cuando se escribieron las primeras óperas (música vocal dramática solista acompañada de orquesta). Durante la época barroca, la música polifónica contrapuntística, en la que se utilizaban múltiples líneas melódicas independientes simultáneas, siguió siendo importante.[c] Los compositores barrocos alemanes escribieron para pequeños conjuntos que incluían cuerdas, metales y viento-madera, así como para coros e instrumentos de teclado, como órgano, clavecín y clavicordio. Durante este período se definieron varias formas musicales importantes que perduraron en períodos posteriores cuando se expandieron y evolucionaron aún más, incluida la fuga, la invención, la sonata y el concierto. El estilo barroco tardío era polifónicamente complejo y ricamente ornamentado. Entre los compositores importantes de la época barroca se encuentran Johann Sebastian Bach, Georg Friedrich Händel, Jean Philippe Rameau, Georg Philipp Telemann, Domenico Scarlatti y Antonio Vivaldi. Clasicismo 1. Luigi Cherubini 2. Carl Philipp Emanuel Bach 3. Joseph Haydn 4. Johann Christoph Friedrich Bach 5. Ludwig van Beethoven 6. Franz Xaver Richter 7. Wolfgang Amadeus Mozart 8. Luigi Boccherini 9. Carl Friedrich Abel 10. Christoph Willibald Gluck 11. Maria Anna Mozart 12. Johann Christian Bach Artículo principal: Música del Clasicismo La música del período clásico (1730 a 1820) tenía como objetivo imitar lo que se consideraba los elementos clave del arte y la filosofía de la Antigua Grecia y Antigua Roma: los ideales de equilibrio, proporción y expresión disciplinada.[d] La música del período clásico tiene una textura más ligera, clara y considerablemente más simple que la música barroca que la precedió. El estilo principal fue la homofonía, donde una melodía prominente y una parte de acompañamiento de acordes subordinados son claramente distintas. Las melodías instrumentales clásicas tendían a ser casi vocales y cantables. Se desarrollaron nuevos géneros y el fortepiano, el precursor del piano moderno, reemplazó al clavicémbalo y al órgano de la época barroca como principal instrumento de teclado, aunque este último siguió utilizándose en la música sacra, como las misas. Se le dio importancia a la música instrumental. Estuvo dominado por un mayor desarrollo de formas musicales inicialmente definidas en el período barroco: la sonata, el concierto y la sinfonía. Otros tipos principales fueron el trío, el cuarteto de cuerda, la serenata y el divertimento. La sonata fue la forma más importante y desarrollada. Aunque los compositores barrocos también escribieron sonatas, el estilo clásico de la sonata es completamente distinto. Todas las formas instrumentales principales de la era clásica, desde los cuartetos de cuerda hasta las sinfonías y los conciertos, se basaron en la estructura de la sonata. Los instrumentos tocaban música de cámara y la orquesta se volvió más estandarizada. En lugar del grupo de bajo continuo de la época barroca, que consistía en clavecín, órgano o laúd junto con varios instrumentos bajos seleccionados a discreción del líder del grupo (por ejemplo, viola, violonchelo, tiorba o serpentón), los grupos de cámara clásicos usaron instrumentos estandarizados especificados (por ejemplo, un cuarteto de cuerdas sería interpretado por dos violines, una viola y un violonchelo). La interpretación de acordes improvisados de la época barroca del tecladista de continuo o el laudista se eliminó gradualmente entre 1750 y 1800. Uno de los cambios más importantes realizados en el período clásico fue el desarrollo de conciertos públicos. La aristocracia seguía desempeñando un papel importante en el patrocinio de conciertos y composiciones, pero ahora los compositores podían sobrevivir sin ser empleados permanentes de reyes o príncipes. La creciente popularidad de la música clásica llevó a un crecimiento en el número y tipos de orquestas. La expansión de los conciertos orquestales requirió la construcción de grandes espacios para espectáculos públicos. La música sinfónica, incluidas las sinfonías, el acompañamiento musical al ballet y los géneros vocales/instrumentales mixtos, como la ópera y el oratorio, se hicieron más populares. Los compositores más destacados del clasicismo son Carl Philipp Emanuel Bach, Christoph Willibald Gluck, Johann Christian Bach, Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven y Franz Schubert. Beethoven y Schubert también se consideran compositores de la última parte de la era clásica, ya que comenzó a moverse hacia el Romanticismo. Romanticismo El piano fue la pieza central de la actividad social de los urbanitas de clase media en el siglo XIX (Moritz von Schwind, 1868). El hombre al piano es el compositor Franz Schubert. Artículo principal: Música del Romanticismo La música romántica (c. 1810 a 1900) del siglo XIX tenía muchos elementos en común con los estilos románticos en la literatura y la pintura de la época. El Romanticismo fue un movimiento artístico, literario e intelectual que se caracterizó por su énfasis en la emoción y el individualismo, así como por la glorificación de todo el pasado y la naturaleza. La música romántica se expandió más allá de los estilos y formas rígidos de la era clásica hacia piezas y canciones expresivas más apasionantes y dramáticas. Compositores románticos como Richard Wagner y Johannes Brahms intentaron aumentar la expresión emocional y el poder en su música para describir verdades más profundas o sentimientos humanos. Con poemas sinfónicos, los compositores intentaron contar historias y evocar imágenes o paisajes utilizando música instrumental. Algunos compositores promovieron el orgullo nacionalista con música orquestal patriótica inspirada en la música folclórica. Las cualidades emocionales y expresivas de la música llegaron a prevalecer sobre la tradición. Los compositores románticos crecieron en idiosincrasia y fueron más allá en el sincretismo de explorar diferentes formas de arte en un contexto musical (como la literatura), la historia (personajes históricos y leyendas) o la naturaleza misma. El amor romántico o el anhelo fue un tema predominante en muchas obras compuestas durante este período. En algunos casos, se siguieron utilizando las estructuras formales del período clásico (por ejemplo, la forma sonata utilizada en cuartetos de cuerda y sinfonías), pero estas formas se ampliaron y modificaron. En muchos casos, se exploraron nuevos enfoques para géneros, formas y funciones existentes. Además, se crearon nuevos formularios que se consideraron más adecuados para el nuevo tema. Los compositores continuaron desarrollando música de ópera y ballet, explorando nuevos estilos y temas. En los años posteriores a 1800, la música desarrollada por Ludwig van Beethoven y Franz Schubert introdujo un estilo más dramático y expresivo. En el caso de Beethoven, los motivos cortos, desarrollados orgánicamente, llegaron a reemplazar a la melodía como la unidad compositiva más significativa.[e] Compositores románticos posteriores como Piotr Ilich Chaikovski, Antonín Dvořák y Gustav Mahler utilizaron acordes más inusuales y más disonancia para crear una tensión dramática. Generaron obras musicales complejas y, a menudo, mucho más largas. Durante el período romántico tardío, los compositores exploraron las alteraciones cromáticas dramáticas de la tonalidad, como los acordes extendidos y los acordes alterados, que crearon nuevos colores de sonido. A finales del siglo XIX se produjo una expansión espectacular en el tamaño de la orquesta y la Revolución industrial ayudó a crear mejores instrumentos, creando un sonido más potente. Los conciertos públicos se convirtieron en una parte importante de la sociedad urbana acomodada. También vio una nueva diversidad en la música para teatro, incluida la opereta, la comedia musical y otras formas de teatro musical. El piano fue un instrumento representativo de esta época, no solo en el ámbito musical sino también en el técnico ya que gracias a los avances logrados debido a la Revolución industrial su mecanismo fue mejorado sustancialmente. De la misma forma que el capitalismo influyó en el modo de organización jerárquico de las cadenas de producción, el piano se utilizó como herramienta para conseguir la expresión musical y como ayuda a la composición. En dicho período hubo importantes compositores como Félix Mendelssohn, Frédéric Chopin, Franz Liszt, Robert Schumann, Franz Schubert, Johannes Brahms, Edvard Grieg e Isaac Albéniz que realizaron obras para piano. Del romanticismo tardío destacan compositores como Aleksandr Skriabin, Serguéi Rajmáninov y Gabriel Fauré. Enrico Caruso con un gramófono. Un gramófono clásico de principios del siglo XX. Siglo XX y XXI Artículo principal: Música clásica del siglo XX En el siglo XIX, una de las formas clave en que las nuevas composiciones se dieron a conocer al público fue mediante la venta de partituras, que los amantes de la música aficionados de clase media tocaban en casa en su piano u otros instrumentos comunes, como el violín. Con la música del siglo XX, la invención de nuevas tecnologías eléctricas como la radiodifusión y la disponibilidad de discos de gramófono en el mercado masivo significó que las grabaciones sonoras de canciones y piezas escuchadas por los oyentes (ya sea en la radio o en su tocadiscos) se convirtieron en la principal vía para aprender sobre nuevas canciones y piezas. Hubo un gran aumento en la escucha de música a medida que la radio ganó popularidad y los fonógrafos se utilizaron para reproducir y distribuir música, porque mientras que en el siglo XIX, el enfoque en las partituras restringió el acceso de la nueva música a las personas de clase media y alta que podían leer música y poseían pianos e instrumentos, en el siglo XX, cualquier persona con radio o tocadiscos podía escuchar óperas, sinfonías y grandes bandas en su propia sala de estar. Esto permitió a las personas de bajos ingresos, que no podían pagar una entrada para un concierto de ópera o sinfónico, escuchar esta música. También significaba que las personas podían escuchar música de diferentes partes del país, además de diferentes partes del mundo, incluso si no podían permitirse viajar a estos lugares. Esto ayudó a difundir los estilos musicales. El enfoque de la música artística en el siglo XX se caracterizó por la exploración de nuevos ritmos, estilos y sonidos. Las consecuencias de la Primera Guerra Mundial influyeron en muchas de las artes, incluida la música, y algunos compositores comenzaron a explorar sonidos más oscuros y ásperos. Los compositores utilizaron estilos musicales tradicionales como el jazz y la música folclórica como fuente de ideas para la música clásica. Ígor Stravinski, Arnold Schönberg y John Cage fueron compositores influyentes en la música artística del siglo XX. La invención de la grabación de sonido y la capacidad de editar música dio lugar a un nuevo subgénero de la música clásica, incluidas las escuelas de composición electrónica acusmática y música concreta. La grabación de sonido también fue de gran influencia en el desarrollo de los géneros musicales populares, porque permitió que las grabaciones de canciones y bandas se distribuyeran ampliamente. La introducción del sistema de grabación multipista tuvo una gran influencia en la música rock, porque podía hacer mucho más que grabar la interpretación de una banda. Usando un sistema multipista, una banda y su productor musical podrían sobregrabar muchas capas de pistas de instrumentos y voces, creando nuevos sonidos que no serían posibles en una actuación en vivo. Algunos músicos expositores del jazz. En sentido horario, desde arriba a la izquierda: Louis Armstrong, Charlie Parker, Ella Fitzgerald y Aretha Franklin. El jazz evolucionó y se convirtió en un género musical importante a lo largo del siglo XX y durante la segunda mitad de ese siglo, la música rock hizo lo mismo. El jazz es una forma de arte musical estadounidense que se originó a principios de siglo en las comunidades afroamericanas del sur de Estados Unidos a partir de una confluencia de tradiciones musicales africanas y europeas. El pedigrí de África Occidental del estilo es evidente en el uso de notas de blues, improvisación, polirritmos, síncopa y la nota swing. Representación abstracta de una canción generada por inteligencia artificial. La música rock es un género de música popular que se desarrolló en la década de 1960 a partir del rock and roll de la década de 1950, el rockabilly, el blues y la música country. El sonido del rock a menudo gira en torno a la guitarra eléctrica o acústica y utiliza un fuerte ritmo de fondo establecido por una sección rítmica. Junto con la guitarra o los teclados, el saxofón y la armónica de estilo blues se utilizan como instrumentos solistas. La sección rítmica tradicional de la música popular es la guitarra rítmica, el bajo eléctrico y la batería. Algunas bandas también tienen instrumentos de teclado como órgano, piano o, desde la década de 1970, sintetizadores analógicos. En la década de 1980, los músicos pop comenzaron a usar sintetizadores digitales, como el Yamaha DX7, cajas de ritmos electrónicas como el Roland TR-808 y dispositivos de bajo sintetizado (como el Roland TB-303) o teclados de bajo sintetizado. En la década de 1990, se utilizó una gama cada vez más amplia de dispositivos e instrumentos musicales de hardware computarizado y software (por ejemplo, estaciones de trabajo de audio digital). En la década de 2020, los sintetizadores de software y las aplicaciones de música de computadora permiten crear y grabar variados tipos de música, como música electrónica de baile en su propia casa, agregando instrumentos muestreados y digitales y editando la grabación digitalmente. En la década de 1990, algunas bandas de géneros como el nu metal comenzaron a incluir DJ en sus bandas. Los DJs crean música manipulando música grabada en tocadiscos o reproductores de CD, usando un mezclador de DJ. Cultura y música Existen desacuerdos sobre la neutralidad en el punto de vista de la versión actual de este artículo o sección. En la página de discusión puedes consultar el debate al respecto. Violinista en la catedral de Dublín. Todas las culturas humanas tienen manifestaciones musicales. La mayoría de las especies animales también son capaces de producir sonidos de una forma organizada para comunicar una gran variedad de mensajes. Lo que define a la música de los hombres no es tanto el ser una combinación «correcta» (o «armoniosa» o «bella») de sonidos en el tiempo como el ser una práctica de los seres humanos dentro de un grupo social determinado. Independientemente de lo que las diversas prácticas musicales de diversos pueblos y culturas tengan en común, es importante no perder de vista la diversidad en cuanto a los instrumentos utilizados para producir música, en cuanto a las formas de emitir la voz, en cuanto a las formas de tratar el ritmo y la melodía, y, sobre todo, en cuanto a la función que desempeña la música en las diferentes sociedades: no es lo mismo la música que se escucha en una celebración religiosa, que la música que se escucha en un anuncio publicitario, ni la que se baila en una discoteca. Tomando en consideración las funciones que una música determinada desempeña en un contexto social determinado, podemos ser más precisos a la hora de definir las características comunes de la música, y más respetuosos a la hora de acercarnos a las músicas que no son las de nuestra sociedad. La Música, representación alegórica de la música, diseñada por Józef Gosławski en un edificio en Varsovia. La mayoría de las definiciones de música solo toman en cuenta algunas músicas producidas durante determinado lapso en Occidente, creyendo que sus características son «universales», es decir, comunes a todos los seres humanos de todas las culturas y de todos los tiempos. Dice Schopenhauer, «(la música) repercute en el hombre de manera tan potente y magnífica, que puede ser comparada a una lengua universal, cuya claridad y elocuencia supera a todos los idiomas de la tierra». Muchos piensan que la música es un lenguaje «universal», puesto que varios de sus elementos, como la melodía, el ritmo, y especialmente la armonía (relación entre las frecuencias de las diversas notas de un acorde) son plausibles de explicaciones más o menos matemáticas, y que los humanos en mayor o menor medida, estamos naturalmente capacitados para percibir como bello. Quienes creen esto ignoran o soslayan la complejidad de los fenómenos culturales humanos. Así, por ejemplo, se ha creído que la armonía es un hecho musical universal cuando en realidad es exclusivo de la música de Occidente de los últimos siglos; o, peor aún, se ha creído que la armonía es privativa de la cultura occidental[cita requerida] porque representa un estadio más «avanzado» o «superior» de la «evolución» de la música. La división del trabajo en la práctica musical occidental contemporánea distingue a esta cultura de otras. Históricamente, los compositores e intérpretes solían ser una misma entidad. Sin embargo, investigaciones recientes (Scientific Reports, 2024) indican un cambio hacia patrones musicales más repetitivos y accesibles en respuesta a su consumo a través de plataformas de streaming preferidas por la sociedad actual. En paralelo, los intérpretes contemporáneos tienden a estar más orientados hacia la autoexpresión. La notación musical occidental Artículo principal: Historia de la notación en la música occidental Fragmento de uno de los himnos dedicado al dios de la música Apolo, en Delfos, Grecia. El templo oracular era una acrópolis y fue construido en su honor. Este fragmento contiene la notación musical introducida por los helenos, situando las notas por sobre las estrofas (a modo de «cancionero popular»). Desde la antigua Grecia (en lo que respecta a música occidental) existen formas de notación musical. Sin embargo, es a partir de la música de la Edad Media (principalmente canto gregoriano) que se comienza a emplear el sistema de notación musical que evolucionaría al actual. En el Renacimiento cristalizó con los rasgos más o menos definitivos con que lo conocemos hoy, aunque -como todo lenguaje- ha ido variando según las necesidades expresivas de los usuarios. El sistema se basa en dos ejes: uno horizontal, que representa gráficamente el transcurrir del tiempo, y otro vertical que representa gráficamente la altura del sonido. Las alturas se leen en relación con un pentagrama (del griego «πεντα», «penta»: cinco; y «γραμμa», «grama»: líneas), que al comienzo tiene una clave que tiene la función de atribuir a una de las líneas del pentagrama una determinada nota musical. En un pentagrama encabezado por la «clave de sol en segunda línea» nosotros leeremos como sol el sonido que se escribe en la segunda línea (contando desde abajo), como la el sonido que se escribe en el espacio entre la segunda y la tercera líneas, como si el sonido en la tercera línea, etc. Para los sonidos que quedan fuera de la clave se escriben líneas adicionales. Las claves más usadas son las de: do en tercera línea (clave que toma como referencia al do4 de 261,63 Hz, el do central del piano), sol en segunda línea (que se refiere al sol que se encuentra una quinta por encima del do central), y fa en cuarta línea (referida al fa que está una quinta por debajo del do central). El discurso musical está dividido en unidades iguales de tiempo llamadas compases: cada línea vertical que atraviesa el pentagrama marca el final de un compás y el comienzo del siguiente. Al comienzo del pentagrama habrá una fracción con dos números; el número de arriba indica la cantidad de tiempos que tiene cada compás; el número de abajo nos indica cuál será la unidad de tiempo. Para escribir las duraciones se utiliza un sistema de figuras: la redonda (representada como un círculo blanco), la blanca (un círculo blanco con un palito vertical llamado plica), la negra (igual que la blanca pero con un círculo negro), la corchea (igual que la negra pero con un palito horizontal que comienza en la punta de la plica), la semicorchea (igual que la corchea pero con dos palitos horizontales), etc. Cada una vale la mitad de su antecesora: la blanca vale la mitad que una redonda y el doble que una negra, etc. Las figuras son duraciones relativas; para saber qué figura es la unidad de tiempo en determinada partitura, debemos fijarnos en el número inferior de la indicación del compás: si es 1, cada redonda corresponderá a un tiempo; si es 2, cada blanca corresponderá a un tiempo; si es 4, cada tiempo será representado por una negra, etc. Así, una partitura encabezada por un 3 4 estará dividida en compases en los que entren tres negras (o seis corcheas, o una negra y cuatro corcheas, etc.); un compás de 4 8 tendrá cuatro tiempos, cada uno de ellos representados por una corchea, etc. Para representar los silencios, el sistema posee otros signos que representan un silencio de redonda, de blanca, etc. Como se ve, las duraciones están establecidas según una relación binaria (doble o mitad), lo que no prevé la subdivisión por tres, que será indicada con tresillos. Cuando se desea que a una nota o silencio se le agregue la mitad de su duración, se le coloca un punto a la derecha (puntillo). Cuando se desea que la nota dure, además de su valor, otro determinado valor, se escriben dos notas y se las une por medio de una línea arqueada llamada ligadura de prolongación. En general, las incapacidades del sistema son subsanadas apelando a palabras escritas más o menos convencionales, generalmente en italiano. Así, por ejemplo, las intensidades se indican mediante el uso de una «f» (forte, fuerte) o una «p» (piano, suave), o varias efes y pes juntas. La velocidad de los pulsos se indica con palabras al comienzo de la partitura que son, en orden de velocidad: largo, lento, adagio, moderato, andante, allegro, presto. Beneficios de la música a nivel psicológico y neurológico El canto de los ángeles (obra de William Bouguereau), siglo XIX. La práctica de la ejecución musical sobre la base de un instrumento, promueve un mejor rendimiento a nivel cerebral. Las lecciones musicales activan ambos hemisferios cerebrales. Por esta actividad, la concentración, memoria y disciplina de un estudiante se ven a duelo al ejercitarse, y este ejercicio suele mejorar la capacidad de las aptitudes mencionadas. En el momento en el que el cerebro se ve retado a dividirse en varias funciones que requieren concentración y precisión, como al tocar instrumentos ya sea piano, guitarra, violín, contrabajo, entre otros, mejora sus funciones. Estudios realizados por la Universidad de Harvard y la Universidad de California han comprobado que la práctica de instrumentos musicales hace que los dos hemisferios cerebrales formen nuevas conexiones, cuya realización produce que el cerebro tenga un mejor rendimiento en los campos de la concentración, memoria y aprendizaje. El legendario científico español de la neurociencia moderna, Santiago Ramón y Cajal, descubrió que la única actividad que hacía más conexiones en las células cerebrales era tocar el piano, ya que en este instrumento se emplea cada dedo en una tecla distinta, enfocándose cada mano en distintos ritmos y velocidades, y en adición, los pies, que también tienen una importante función al utilizarse los pedales. A nivel mental, también se considera muy útil la teoría musical para facilitar el aprendizaje de otros idiomas. Características importantes de la música, como el tono, el timbre, la intensidad y el ritmo, tienen mucho que ver con las variaciones del habla en los distintos idiomas. Cada uno de estos tiene un acento distinto, y en la música descubrimos los diversos tonos, timbres, y ritmos que se podrían acoplar a los diferentes idiomas."
ksampletext_wikipedia_arts_pintura: str = "Pintura. La pintura es el arte de la representación gráfica utilizando pigmentos mezclados con otras sustancias aglutinantes, orgánicas o sintéticas. En este arte se emplean técnicas de pintura, conocimientos de teoría del color y de composición pictórica, y el dibujo. La práctica del arte de pintar, consiste en aplicar, en una superficie determinada ,una hoja de papel, un lienzo, un muro, una madera, fragmento de tejido, etc., una técnica determinada, para obtener una composición de formas, colores, texturas, dibujos, etc. dando lugar a una obra de arte según algunos principios estéticos. Lo relativo a la pintura se conoce como «pictórico». El arquitecto y teórico del clasicismo André Félibien, en el siglo XVII, en un prólogo de las Conferencias de la Academia francesa hizo una jerarquía de géneros de la pintura clásica: «la historia, el retrato, el paisaje, los mares, las flores y los frutos». La pintura es una de las expresiones artísticas más antiguas y una de las siete Bellas Artes. En estética o teoría del arte, la pintura está considerada como una categoría universal que comprende todas las creaciones artísticas hechas sobre superficies. Una categoría aplicable a cualquier técnica o tipo de soporte físico o material, incluidos los soportes o las técnicas efímeras así como los soportes o las técnicas digitales. Una parte de la historia de la pintura en el arte oriental y occidental está dominada por el arte religioso. Los ejemplos de este tipo de pintura van desde obras de arte que representan figuras mitológicas en cerámica, a escenas bíblicas del techo de la Capilla Sixtina, a escenas de la vida de Buda u otras imágenes de origen religioso oriental. Definición El arte de la pintura por Johannes Vermeer (1665) Kunsthistorisches Museum de Viena. Una pintura es el soporte pintado sobre un muro, un lienzo, o una lámina. La palabra pintura se aplica también al color preparado para pintar, asociado o no a una técnica de pintura; en este sentido es empleado en la clasificación de la pintura atendiendo a las técnicas de pintar, por ejemplo: «pintura al fresco» o «pintura al óleo». La clasificación de la pintura puede atender a criterios temáticos (como la «pintura histórica» o la «pintura de género») o a criterios históricos basados en los periodos de la Historia del Arte (como la «pintura prehistórica», la «pintura gótica») y en general de cualquier período de la historia de la pintura. El origen de la Pintura (1786), de Jean-Baptiste Regnault. Según una leyenda de Plinio el Viejo, la pintura la inventó una muchacha griega que trazó la silueta de su novio antes de partir a la guerra para perpetuar su recuerdo (Naturalis Historia, XXXV, 15). Las pinturas son obras de arte, atendiendo a su sentido estético Ernst Gombrich dice que: No hay nada de malo en que nos deleitemos con la pintura de un paisaje porque nos recuerda nuestra casa o en un retrato porque nos recuerda un amigo, ya que como hombres que somos, cuando miramos una obra de arte estamos sometidos al recuerdo de una multitud de cosas que para bien o para mal influyen sobre nuestros gustos. Gombrich, Historia del arte (2002) Y parafraseando a Arnold Hauser: Las interpretamos (las pinturas) de acuerdo con nuestras propias finalidades y aspiraciones, les trasladamos un sentido, cuyo origen está en nuestras formas de vida y hábitos mentales. Ernst Bloch en El espíritu de la utopía (1918), defiende el arte no figurativo, relacionándolo con una concepción utópica del hombre, como un destino no revelado pero presente de forma inconsciente en lo más profundo del ser humano. Si la tarea de la pintura fuera ponernos ante los ojos del aire y la preciosa vastedad del espacio y de todo lo demás, más valdría ir a disfrutar directa y gratuitamente de todo aquello. Ernst Bloch, El espíritu de la utopía (1918) Erwin Panofsky y otros historiadores del arte, analizan el contenido de las pinturas mediante la iconografía (forma) y la iconología (su contenido), primero se trata de comprender lo que representa, luego su significado para el espectador y, a continuación, analizan su significado cultural, religioso y social más ampliamente. Historia de la pintura Artículo principal: Historia de la pintura Réplica de unas pinturas de la cueva de Chauvet del período Auriñaciense. La historia de la pintura comprende desde la prehistoria hasta la Edad Contemporánea, e incluye todas las representaciones realizadas con las diferentes técnicas y cambios, que coincide con la historia del arte en su contexto histórico y cultural. El llamado arte parietal de pintura mural en cuevas, se concentra fundamentalmente en algunas regiones pirenaicas pertenecientes a Francia y España y en la costa mediterránea en el arte levantino, y en otras muestras inferiores que se encuentran en Portugal, Norte de África, Italia y Europa oriental. Las pinturas rupestres más antiguas conocidas se encuentran en la Cueva de Chauvet, en Francia, fechadas por algunos historiadores en unos 32 000 años, de los períodos entre el Auriñaciense y el Gravetiense. Fueron realizadas con ocre de arcilla, rojo de óxido de hierro y negro de dióxido de manganeso. También destacan las cuevas de Lascaux y Altamira. Se encuentran dibujados rinocerontes, leones, búfalos, mamuts, caballos o seres humanos a menudo en actitud de caza. Las imágenes que se observan en papiros o las paredes de las tumbas egipcias, desde hace unos 5.000 años, son escenas de la vida cotidiana y mitológicas, simbolizadas con los rasgos característicos de perfil y utilizando el tamaño de las figuras como rango social. En la Antigua Roma era normal decorar los muros de las casas y palacios principales y entre las mejor conservadas se encuentran las de Pompeya y Herculano. En la época paleocristiana se decoraron las catacumbas con escenas del Nuevo Testamento y con la representación de Jesús como el «Buen Pastor». Eran figuras estáticas con grandes ojos que parecían mirar al espectador. Este estilo continuó en la escuela bizantina de Constantinopla. La pintura románica se desarrolla entre los siglos XII y XIII, siendo las zonas más interesantes las del Sur de Francia y las de Cataluña, la mayoría de las veces eran temas religiosos realizados para los ábsides y muros de las iglesias con representaciones del Pantocrátor, la Virgen María y la vida de santos. En pintura gótica además de los temas religiosos se representan temas laicos principalmente en Francia e Italia, donde destacó la figura del pintor Giotto. La Gioconda pintura del renacimiento por Leonardo da Vinci. En el renacimiento tuvo la pintura una gran influencia clásica, se desarrolló la perspectiva lineal y el conocimiento de la anatomía humana para su aplicación en la pintura, también en esta época apareció la técnica del óleo. Fue una época de grandes pintores entre los que destacaron Leonardo da Vinci, Miguel Ángel, Rafael Sanzio y Tiziano. En la obra de La Gioconda descuellan las nuevas técnicas empleadas por Leonardo, el sfumato y el claroscuro. Miguel Ángel realizó una de las más grandes obras pictóricas: los frescos de la Capilla Sixtina. Los artistas que más emplearon temas simbólicos fueron los del Norte de Europa encabezados por los hermanos Jan van Eyck y Hubert van Eyck. En Alemania sobresalió el pintor y humanista Durero. Hipnotizador (1912), de Bohumil Kubišta (Ostrava). Pintura del expresionismo con mezcla del cubismo. La Iglesia de la Contrarreforma busca el arte religioso auténtico con el que quiere contrarrestar la amenaza del protestantismo, y para esta empresa las convenciones artificiales de los manieristas, que habían dominado el arte durante casi un siglo, ya no parecían adecuadas. Las dos características más importantes del manierismo eran el rechazo de las normas y la libertad en la composición, en los colores y en las formas, la novedad de los caravaggistas era un naturalismo radical que combinaba la observación física detallada con una aproximación, incluso teatral y dramática mediante el claroscuro, el uso de luz y sombra. Caravaggio y Annibale Carracci son dos pintores coetáneos, considerados decisivos en la conformación pictórica del barroco. La pintura barroca se caracteriza por el dinamismo de sus composiciones; se distinguieron entre otros Velázquez, Rubens y Rembrandt. En la primera mitad del siglo XVIII se impuso el rococó, más alegre y festivo que el barroco. Tuvo especial importancia en Francia y Alemania. El romanticismo de principios del siglo XIX expresaba estados de ánimos y sentimientos intensos. En Francia el pintor más importante fue Delacroix; en el Reino Unido, Constable y Turner; en los Estados Unidos, Thomas Cole; y en España, Francisco de Goya. Con la invención de la fotografía a mediados del siglo XIX, la pintura empezó a perder su objetivo histórico de proporcionar una imagen realista; el impresionismo, con Manet como precursor, es un estilo de pinceladas sueltas y yuxtaposición de colores que busca reconstruir un instante percibido, una impresión, sin interesarse por los detalles concretos. El inicio del siglo XX se caracteriza por la diversidad de corrientes pictóricas: el Fovismo, que rechaza los colores tradicionales y se acerca a colores violentos; el Expresionismo, que mostraba más los sentimientos que la reproducción fiel de la realidad; el Cubismo con Georges Braque y Picasso, con la descomposición de las imágenes tridimensionales a puntos de vista bidimensionales; y la pintura abstracta, heredera del cubismo. El expresionismo abstracto se desarrolló en Nueva York entre los años 1940-1950, el Pop art llegó un poco después, con un conocido exponente en Andy Warhol. El minimalismo se caracteriza por la búsqueda de la máxima expresión con los mínimos recursos estéticos. El siglo XXI demuestra una idea de pluralismo y las obras se siguen realizando en una amplia variedad de estilos y gran estética. Géneros pictóricos Artículo principal: Jerarquía de géneros Entierro del Conde de Orgaz (1586-1588), de El Greco. Se puede considerar dentro del género de pintura histórica: describe una leyenda local según la que el conde fue enterrado por san Esteban y san Agustín. En la parte inferior, se describe un enterramiento con la pompa del siglo XVI; en la parte superior, está representada la Gloria y la llegada del alma del conde. Retrato de El doctor Paul Gachet (1890) por Van Gogh. Pintura de género: Boda campesina (1568) por Pieter Brueghel el Viejo. Paisaje tipo vedutismo de la Iglesia de Santa Lucía desde el Gran Canal (siglo XVIII) por Francesco Guardi. Canasta de fruta (c. 1599) naturaleza muerta por Caravaggio. Los géneros artísticos, además de clasificar las obras por temas, han sido la presentación artística a través de la historia de la pintura, que ha afectado también la técnica, las dimensiones, al estilo y a la expresión de las obras de arte. Los autores como Platón (427-347 a. C.), Aristóteles (384-322 a. C.) y Horacio (65-8 a. C.) afirmaron que el arte es siempre una mímesis y que su mérito está en el valor didáctico de lo que representa y su buena representación, sin establecer diferencias entre el retrato imaginado o real. Vitruvio en la segunda parte del siglo I, describió la decoración de comedores donde se veían imágenes con comida y de otras salas con paisajes o escenas mitológicas. En el renacimiento, Leon Battista Alberti quiso elevar el grado de «artesano de la pintura» al de «artista liberal» afirmando: «El trabajo más importante del pintor es la historia», con la palabra historia se refería a la pintura narrativa, con escenas religiosas o épicas «... la que retrata los grandes hechos de los grandes hombres dignos de recordarse difiere de la que describe las costumbres de los ciudadanos particulares, de la que pinta la vida de los campesinos. La primera tiene carácter majestuoso, debe reservarse para edificios públicos y residencias de los grandes, mientras que la otra será adecuada para jardines...» La aparición de la pintura al óleo en el siglo XVI y el coleccionismo, hizo que, aunque no se perdiera la monumentalidad para murales narrativos, surgieran las pinturas más comerciales y en otros formatos más manejables, así comenzaron a clasificarse los géneros pictóricos y su especialización por parte de los artistas. En la Italia central se continuó haciendo pintura histórica, los pintores de la parte norte de la península itálica realizaban retratos y los de los Países Bajos realizaron la pintura de género a pequeña escala presentando la vida campesina, el paisaje y la naturaleza muerta. En 1667, André Félibien historiógrafo, arquitecto y teórico del clasicismo francés, en un prólogo de las Conferencias de la Academia hace una jerarquía de géneros de la pintura clásica: «la historia, el retrato, el paisaje, los mares, las flores y los frutos». Pintura histórica La pintura histórica era considerada grande genre e incluía las pinturas con temas religiosos, mitológicos, históricos, literarios o alegóricos, era prácticamente una interpretación de la vida y mostraba un mensaje intelectual o moral. Sir Joshua Reynolds, en sus Discursos sobre arte expuestos en la Royal Academy of Arts entre 1769 y 1790 comentaba: «El gran fin del arte es despertar la imaginación ... De acuerdo en correspondencia con la costumbre, yo llamo esta parte del arte Pintura Histórica, pero debería decirse Poética.(...) Debe algunas veces desviarse de lo vulgar y de la estricta verdad histórica a la búsqueda de grandeza para su obra». Aunque Nicolás Poussin fue el primer pintor que realizó este género en formato más reducido, esta innovación tuvo poco éxito, Diego Velázquez en 1656 realizó Las Meninas con un tamaño que demuestra simbólicamente que este retrato de la familia real entra dentro del género de la pintura histórica, mucho más tarde Pablo Picasso en su obra Guernica de 1937, también emplea una gran dimensión para esta pintura histórica. Retrato Dentro de la jerarquía de géneros, el retrato tiene una ubicación ambigua e intermedia, por un lado, representa a una persona hecha a semejanza de Dios, pero por otro lado, al fin y al cabo, se trata de glorificar la vanidad de una persona. Históricamente, se ha representado los ricos y poderosos. Pero con el tiempo, se difundió, entre la clase media, el encargo de retratos de sus familias. Aún hoy, persiste la pintura de retrato como encargo de gobiernos, corporaciones, asociaciones o particulares. Cuando el artista se retrata a sí mismo se trata de un autorretrato. Rembrandt exploró en este sentido con sus más de sesenta autorretratos. El artista en general intenta un retrato representativo, como afirmó Edward Burne-Jones: «La única expresión que se puede permitir en la gran retratística es la expresión del carácter y la calidad moral, nada temporal, efímero o accidental». En la técnica del óleo fue Jan van Eyck uno de los primeros que lo impuso en los retratos, su Matrimonio Arnolfini fue un ejemplo de retrato de pareja en cuerpo completo. Durante el renacimiento, representaron el estatus y éxito personal del retratado, sobresalieron Leonardo da Vinci, Rafael Sanzio y Durero. En España descollaron Zurbarán, Velázquez y Francisco de Goya. Los impresionistas franceses también practicaron este género, Degas, Monet, Renoir, Vincent van Gogh, Cézanne etc. En el siglo XX, Matisse, Gustav Klimt, Picasso, Modigliani, Max Beckmann, Umberto Boccioni, Lucian Freud, Francis Bacon o Andy Warhol. Pintura de género La pintura de género o «escena de género» es el retrato de los hábitos privados de las personas en escenas cotidianas y contemporáneas del pintor, también se suele llamar «pintura costumbrista». Los primeros cuadros más populares se dieron en los Países Bajos durante el siglo XVI y entre los artistas más destacados se encuentran Pieter Brueghel el Viejo y Vermeer. No se sabe con seguridad si se trata de simple representación de la realidad con un propósito de mera distracción, a veces cómico, o bien se buscaba una finalidad moralizante a través de los ejemplos cercanos al espectador. No hay duda de que, en el cuadro de género del siglo XVIII, sí estaba presente la intención satírica o moralizante en obras como las de William Hogarth o Jean-Baptiste Greuze. En España, Diego Velázquez lo cultivó con su Vieja friendo huevos o El aguador de Sevilla, Francisco de Goya reflejó, en varias obras de cartones para tapices, las fiestas populares, Bartolomé Esteban Murillo hizo escenas de género de mendigos y jóvenes picarescos. En Francia, Jean-Honoré Fragonard y Antoine Watteau hicieron un tipo de pinturas idealizadas de la vida diaria. Paisaje En China y Japón son los países donde, desde el siglo V, se encuentran pinturas con el tema del paisaje. En Europa, aunque aparecen elementos de paisaje como fondo de escenas narrativas, o tratados de botánica y farmacia, se inicia verdaderamente en el siglo XVI, cuando con la aparición del coleccionismo se empezó a pedir temas de cuadros campestres y a designar como especialistas a los pintores del norte de Europa. Así de una manera específica se impuso el tema del «paisaje holandés», que se caracteriza por su horizonte bajo y los cielos cargados de nubes y con motivos típicos holandeses como los molinos de viento, ganados y barcas de pesca. Los paisajes venecianos de Giorgione y sus discípulos son con una apariencia lírica y un bello tratamiento cromático, este tipo de pintura se desarrolló sobre todo a lo largo de todo el siglo XVIII, en un estilo llamado vedutismo, que son vistas generalmente urbanas, en perspectiva, llegando a veces a un estilo cartográfico, donde se reproducen imágenes panorámicas de la ciudad, describiendo con minuciosidad los canales, monumentos y lugares más típicos de Venecia, solos o con la presencia de la figura humana, generalmente de pequeño tamaño y en grandes grupos de gente. Sus mayores exponentes fueron Canaletto, Bernardo Bellotto, Luca Carlevarijs y Francesco Guardi. En la escuela de Barbizon fueron los primeros en pintar al aire libre y hacer un estudio sobre el paisaje a base de la luz y sus variantes que influyeron especialmente en la pintura impresionista. Naturaleza muerta Es el género más representativo de la imitación de la naturaleza de objetos inanimados, en general de la vida cotidiana, como frutas, flores, comida, utensilios de cocina, de mesa, libros, joyas etc. y se puede decir, que es el menos literario de todos los temas. Su origen está en la antigüedad donde se utilizaba para la decoración de grandes salones, como los frescos romanos en Pompeya. Plinio el Viejo relata que los artistas griegos de siglos antes, eran muy diestros en el retrato y la naturaleza muerta. Fue muy popular en el arte occidental desde el siglo XVI, un ejemplo es La carnicería de Joachim Beuckelaer. En el mismo siglo Annibale Carracci y Caravaggio representaron magníficas naturalezas muertas. Durante el siglo XVII evolucionó en los Países Bajos un tipo de bodegón, llamado «vanitas», donde se exponían instrumentos musicales, vidrio, plata y vajilla, así como joyas y símbolos como libros, cráneos o relojes de arena, que servían de mensaje moralizante de lo efímero de los placeres de los sentidos. La Academia francesa lo catalogó en el último lugar de la jerarquía pictórica. Con la llegada del impresionismo y junto con la técnica del color, la naturaleza muerta volvió a ser un tema normal entre los pintores, las pinturas de los Girasoles de Van Gogh son de los más conocidos. Los artistas durante el cubismo pintaron también composiciones de bodegones, entre ellos Pablo Picasso, Georges Braque, Maria Blanchard y Juan Gris. Desnudo El Juicio de Paris (1904) de Enrique Simonet. El desnudo es un género artístico que consiste en la representación del cuerpo humano desnudo. Es considerado una de las clasificaciones académicas de las obras de arte. Aunque se suele asociar al erotismo, el desnudo puede tener diversas interpretaciones y significados, desde la mitología hasta la religión, pasando por el estudio anatómico, o bien como representación de la belleza e ideal estético de perfección, como en la Antigua Grecia. El estudio y representación artística del cuerpo humano ha sido una constante en toda la historia del arte, desde la prehistoria (Venus de Willendorf) hasta nuestros días. Una de las culturas donde más proliferó la representación artística del desnudo fue la Antigua Grecia, donde era concebido como un ideal de perfección y belleza absoluta, concepto que ha perdurado en el arte clasicista llegando hasta nuestros días, y condicionando en buena medida la percepción de la sociedad occidental hacia el desnudo y el arte en general. En la Edad Media su representación se circunscribió a temas religiosos, siempre basados en pasajes bíblicos que así lo justificasen. En el Renacimiento, la nueva cultura humanista, de signo más antropocéntrico, propició el retorno del desnudo al arte, generalmente basado en temas mitológicos o históricos, perdurando igualmente los religiosos. Fue en el siglo XIX, especialmente con el impresionismo, cuando el desnudo empezó a perder su carácter iconográfico y a ser representado simplemente por sus cualidades estéticas, el desnudo como imagen sensual y plenamente autorreferencial. Técnicas Artículo principal: Técnicas de pintura Las técnicas de pintura se dividen de acuerdo a cómo se diluyen y fijan los pigmentos en el soporte a pintar. En general, y en las técnicas a continuación expuestas, si los pigmentos no son solubles al aglutinante permanecen dispersos en él. Óleo Paleta de pintor, pinceles y tubos de pintura al óleo. El dos de mayo de Francisco de Goya. El vehículo empleado para fijar el pigmento son tipos de aceites y el disolvente es la trementina. La pintura al óleo se hace básicamente con pigmento pulverizado seco, mezclado en la viscosidad adecuada con algún aceite vegetal. Estos aceites se secan más lentamente que otros, no por evaporación, sino por oxidación. Se forman capas de pigmento que se incrustan en la base y que, si se controlan cuidadosamente los tiempos de secado, se fijarán correctamente en las siguientes capas de pigmento. Este proceso de oxidación confiere riqueza y profundidad a los colores del pigmento seco, y el artista puede variar las proporciones de aceite y disolventes, como la trementina, para que la superficie pintada muestre toda una gama de calidades, opaca o transparente, mate o brillante. Por esta y por otras razones, el aceite puede considerarse como el medio más flexible. Usado de una manera conveniente, la pintura al óleo cambia muy poco de color durante el secado aunque, a largo plazo, tiende a amarillear ligeramente. Su capacidad de soportar capas sucesivas, permite al artista desarrollar un concepto pictórico por etapas ,Degas llamaba este proceso bien amenée (bien llevado), y la lentitud de secado le permite retirar pintura y repasar zonas enteras. Las fotografías con rayos X demuestran que incluso los grandes maestros introducían a menudo cambios durante el proceso de realización de un cuadro. Cera El vehículo son ceras que normalmente se usan calientes. La encáustica, que deriva del griego enkaustikos (grabar a fuego), es una técnica de pintura que se caracteriza por el uso de la cera como aglutinante de los pigmentos. La mezcla tiene efectos muy cubrientes y es densa y cremosa. La pintura se aplica con un pincel o con una espátula caliente. La terminación es un pulido que se hace con trapos de lino sobre una capa de cera caliente previamente extendida (que en este caso ya no actúa como aglutinante sino como protección). Esta operación se llama «encaustización» y está perfectamente descrita por Vitruvio (c. 70-25 a. C.), que dice así: «Hay que extender una capa de cera caliente sobre la pintura y a continuación hay que pulir con unos trapos de lino bien secos». Caja de acuarelas. Thomas Girtin, Abadía Jedburgh desde el río, acuarela sobre papel, 1798-99. Acuarela La acuarela es una pintura sobre papel o cartulina con colores diluidos en agua. Los colores utilizados son transparentes (según la cantidad de agua en la mezcla) y a veces dejan ver el fondo del papel (blanco), que actúa como otro verdadero tono. Se compone de pigmentos aglutinados con goma arábiga o miel. En sus procedimientos se emplea la pintura por capas transparentes, a fin de lograr mayor brillantez y soltura en la composición que se está realizando. Requiere del artista la seguridad en los trazos y espontaneidad en la ejecución, ya que su mayor mérito consiste en el frescor y la transparencia de los colores. Sin embargo, existe la acuarela hiper realista que va en contra de este postulado y que utiliza barnices para no remover las primeras capas y dar sucesivas veladuras con lo que se consigue un claroscuro muy detallado pero carente de la translucidez de la acuarela clásica. Témpera La témpera o gouache es un medio similar a la acuarela, pero tiene una «carga» de talco industrial. Este añadido adicional al pigmento le aporta a la témpera el carácter opaco y no translúcido que lo diferencia de la acuarela, permitiéndole aplicar tonalidades claras sobre una oscura, procedimiento que en la acuarela «clásica» se considera incorrecto. Es a su vez un medio muy eficaz para complementar dibujos y hacer efectos de trazo seco o de empaste. Igual que la acuarela su aglutinante es la goma arábiga, aunque muchas témperas modernas contienen plástico. Con esta técnica François Boucher logró grandes obras maestras, los artistas del siglo XVIII emplearon la acuarela y el gouache juntos para dar distinción a una zona concreta de la pintura hecha con acuarela. Según el pintor Paul Signac: «... determinados rosas violáceos de los cielos de Turner, ciertos verdes de las acuarelas de Johan Jongkind no se habrían podido conseguir sin un poco de gouache». Acrílico Acrílico sobre lienzo 70x90 cm del pintor Henry A. Quesada La pintura acrílica es una clase de pintura de secado rápido, en la que los pigmentos están contenidos en una emulsión de un polímero acrílico (cola vinílica, generalmente). Aunque son solubles en agua, una vez secas son resistentes a la misma. Se destaca especialmente por la rapidez del secado. Asimismo, al secar se modifica ligeramente el tono, más que en el óleo. La pintura acrílica data de la primera mitad del siglo XX, y fue desarrollada paralelamente en Alemania y Estados Unidos. El pintor Jackson Pollock utilizó las pinturas acrílicas tal como salen de los tubos para conseguir texturas nuevas y espesas mientras que Morris Louis las diluía con gran cantidad de agua para pintar grandes telas que quedaban con un efecto de teñido más que de pintura. De la misma forma con los acrílicos se pueden dar varias capas de colores traslucidos para generar efectos. Si se trabaja bien, estos efectos pueden ser muy semejantes a los que se logran con la técnica al óleo. Pastel Caja de barritas de pastel. El barreño, Degas, pintura al pastel. La técnica de la pintura al pastel consiste en la utilización de unas barras de colores cuyos pigmentos en polvo están mezclados con la suficiente goma o resina para que queden aglutinados y formen una pasta seca y compacta. El vocablo «pastel» deriva de la pasta que así se forma; es pasta modela en la forma de una barrita del grueso aproximado de un dedo que se usa directamente (sin necesidad de pinceles ni espátulas, ni de disolvente alguno) sobre la superficie a trabajar, como soporte es común utilizar papel de buena calidad de buen gramaje de color neutro no blanco y de ligera rugosidad, aunque la técnica es lo suficientemente versátil para que se pueda usar sobre otras superficies como madera. Son colores fuertes y opacos, la mayor dificultad es la adhesión del pigmento a la superficie a pintar, por lo que se suele usar al finalizar el dibujo fijadores atomizados (spray) especiales. El pastel generalmente se usa como el «crayón» o el lápiz, su recurso expresivo más afín es la línea con la cual se puede formar tramas, también suele usarse el polvo, que tiende a soltar la barra del pastel, para aplicar el color. Muchos artistas han empleado esta técnica desde el siglo XVI, Leonardo da Vinci, fue uno de los primeros en utilizarlo en Italia en el dibujo de Isabel de Este. Otros artistas son Hans Holbein el Joven, Correggio, Fragonard o Degas. Temple La pintura al temple tiene como aglutinante una emulsión de agua, clara y yema de huevo y aceite. Conviene primero hacer la mezcla del huevo con el aceite hasta lograr una mezcla homogénea, después gradualmente agregar el agua hasta crear la emulsión o médium de la técnica al temple. La proporción es de un huevo entero, más una parte igual de aceite, más una, dos o tres partes de agua, dependiendo de la fluidez que se quiera alcanzar. También se puede agregar un poco de barniz «dammar» que reemplaza la parte de aceite de linaza, con este procedimiento se logra mayor firmeza o agarre y un secado más rápido, sin embargo el acabado es más impermeable a las nuevas veladuras. En lugar del agua se puede emplear leche desnatada, látex de higuera o cera siempre con agua. Giorgio Vasari también empleó en su descripción la palabra «temple» para la composición de aceite con barniz. Grandes obras maestras como por ejemplo El nacimiento de Venus de Sandro Botticelli están realizadas con esta técnica. Según explica D.V. Thompson: Una pintura al huevo bien hecha está entre las formas de pintura más duraderas que ha inventado el hombre. Bajo la suciedad y los barnices, muchas obras medievales al temple de huevo están tan frescas y brillantes como cuando se pintaron. Normalmente las pinturas al temple han cambiado menos en quinientos años que cuadros al óleo en treinta. D.V. Thompson, The Materials and Tecniques of Medieval Painting (1956) Nova York. Tinta Pincel, barra de tinta y tintero. Tinta sobre papel, siglo XIV, Japón. La presentación de la tinta, también llamada tinta china, es generalmente líquida aunque también puede ser una barra muy sólida que se debe moler y diluir para su uso. Se usa sobre papel y los colores de tinta más utilizados son el negro y el sepia, aunque actualmente se usan muchos otros. La tinta se aplica de varias maneras, por ejemplo con pluma o plumín, que son más adecuados para dibujo o caligrafía y no para pinturas, las diferentes puntas de plumilla se utilizan cargadas de tinta para hacer líneas y con ellas dibujar o escribir. Otro recurso para aplicar la tinta es el pincel, que se utiliza básicamente como la acuarela y que se llama aguada, pero la técnica milenaria llamada caligrafía o escritura japonesa también está hecha con tinta y pincel sobre papel. Otras formas más utilitarias de usar la tinta es el tiralíneas (cargador de tinta) o rapidograph. La tinta junto con el grafito son más bien técnicas de dibujo. Fresco A menudo el término fresco se usa incorrectamente para describir muchas formas de pintura mural. El verdadero fresco es a las técnicas pictóricas modernas lo que el latín es a los idiomas modernos. La técnica del fresco se basa en un cambio químico. Los pigmentos de tierra molidos y mezclados con agua pura, se aplican sobre una argamasa reciente de cal y arena, mientras la cal está aún en forma de hidróxido de calcio. Debido al dióxido de carbono de la atmósfera, la cal se transforma en carbonato de calcio, de manera que el pigmento cristaliza en el seno de la pared. Los procedentes para pintar al fresco son sencillos pero laboriosos y consumen mucho tiempo. Esta técnica de pintura suele ser estable y de larga duración, aunque se puede dañar por causas físicas, químicas o bacteriológicas, la más frecuente es la humedad que consigue la alteración de los colores ante la disolución del carbonato de calcio y el desarrollo del moho. Véase también: Fresco Grisalla Pigmento de óxido de hierro. Mural en la catedral de Vich de Josep Maria Sert. Es una técnica pictórica basada en una pintura monocroma en claroscuro: «luz y sombra» como la llamó Giorgio Vasari, el color está hecho de una mezcla de óxidos de hierro y de cobre y de un fundente, que produce la sensación de ser un relieve escultórico. En el siglo XIV se utilizó para esbozos preparatorios de los escultores para conseguir el efecto de relieve mediante diversas gradaciones de un solo color. Bajo el reinado de Carlos V de Francia, el uso de la grisalla fue sobre todo en la miniatura, en los vitrales y en la pintura. Su utilización será una de las características de la pintura flamenca: en el dorso de los retablos se solía representar una Anunciación en grisalla (Políptico de Gante, Jan Van Eyck, para la catedral de San Bavón en Gante). Josep Maria Sert enfatiza aún más por su evolución cromática, que termina apoyándose en un predominio de la monocromía dorada. Empleaba una gama cromática limitada: oros, ocres, tierras tostadas, con toques de carmín, utilizando como fondo una rica preparación en metal, plata y pan de oro. Puntillismo Contraloría General de la República de Costa Rica, representada en puntillismo. Por: Henry Quesada. El puntillismo es la técnica que surgió en el neoimpresionismo por el estudio practicado principalmente por el pintor Georges Seurat, que consiste en colocar puntos pequeños esféricos de colores puros, en lugar de la técnica de pinceladas sobre el soporte para pintar. Al haber relaciones físicas entre los colores, la interacción entre los primarios y complementarios, consiguen con la posición de unos junto a otros la mezcla óptica, a partir de una cierta distancia del cuadro, que es capaz de producir el efecto de la unión entre ellos. Dripping El dripping es una pintura automática, que según los surrealistas se consigue con ella una pintura casual, hecha con gotas y salpicaduras de pintura, es la técnica pictórica característica de la «action painting» estadounidense (pintura de acción). La pintura se realiza por el artista caminando sobre la superficie a pintar con grandes brochas o con el mismo bote de pintura, dejando caer el goteo del color, normalmente esmalte, que es el que forma las manchas sobre el soporte. Grafiti Pintura envasada en spray. Grafiti en una calle de Barcelona. Se realiza con una pintura envasada en aerosoles que se utiliza pulsando el botón superior por lo que sale en una aspersión muy fina y permite pintar grandes superficies, normalmente los muros de las calles, a la pintura conseguida de esta manera se le denomina grafiti. A finales del año 1970 se empezaron a ver muchas de estas obras urbanas firmadas y cada vez más elaboradas, incluso se fabrican pintura en aerosol exclusivamente para estos artistas, a veces se utilizan plantillas para recortar la superficie que se quiere pintar, así como también hay otras plantillas para letras en el mercado, aunque lo más corriente es que los propios artistas se hagan las suyas. Técnicas mixtas A veces se emplean diversas técnicas en un mismo soporte. El collage por ejemplo, que es una técnica artística (no pictórica por no ser pintada) se convierte en una técnica mixta cuando tiene alguna intervención con guache, óleo, tinta o cualquier otra pintura. Finalmente, sería conveniente distinguir entre «procedimiento pictórico» y «técnica pictórica». Se entiende por procedimiento pictórico la unión de los elementos que constituyen el aglutinante o adhesivo, y los pigmentos. La forma de aplicar este procedimiento pictórico se denomina técnica pictórica. Materiales Pintura con cera o encáustica sobre tabla en Fayum. 55-70 d.c Existe información sobre los materiales empleados por los artistas en documentos escritos, notas dirigidas a otros artistas y otra fuente es el examen técnico y científico de las obras de arte. Estos exámenes sirven también para reforzar las pruebas documentales. Como es natural los materiales empleados a partir del siglo XX son mucho más numerosos y exhaustivos. Soportes El soporte cumple la función de ser el portador del fondo y de las capas de pintura. Los soportes son muy variados, los más tradicionales son el papel, el cartón, la madera, el lienzo y los muros, a los que se puede añadir el metal, el vidrio, el plástico o el cuero entre otros. Todos necesitan de una imprimación especial según el procedimiento pictórico que se quiera seguir. Tabla de madera Artículo principal: Tabla (pintura) La tabla de madera ha sido de los soportes más utilizados desde siempre, los artistas egipcios ya pintaban sobre la madera de los sarcófagos y especialmente en la Edad Media, los retablos o los frontales de altar. Su imprimación es suficiente con una capa de cola o en caso de tener que dorar con pan de oro, hay que hacer otra preparación de colas, yeso y arcilla previas, y también fue el principal soporte para la pintura de caballete europea hasta el siglo XV. La madera maciza empleada antiguamente se había de cubrir con tiras de tela de lino encoladas para disimular las juntas, también a veces se cubría completamente con la tela, así se evitaban posibles grietas posteriores. Así lo explica Cennino en su obra Il Libro dellArte del año 1390. Se utilizan también el contrachapado y el conglomerado, tableros prefabricados que ofrecen la característica de tener las superficies lisas y sin uniones, se encuentra el llamado táblex que además de ligero, tiene dos caras una lisa y otra rugosa, se suele utilizar por la parte rugosa ya que la lisa necesita una preparación para que la pintura se adhiera a ella correctamente. Lienzo Artículo principal: Lienzo Vista posterior de un bastidor en construcción, soporte de madera sobre el cual tensar el lienzo. Plinio el Viejo narró que el emperador Nerón encargó un retrato suyo sobre una tela de 36,5 metros de largo. Heraclio en su manuscrito De Coloribus te Artibus Romanorum del siglo X, describía cómo se preparaba un lienzo de lino para poder pintarlo y dorarlo, tensando la tela y preparándola con cola de pergamino. La pintura sobre tela fue utilizada sobre todo en el norte de Europa y después en Italia por su gran ligereza, a partir del siglo XVIII se hizo corriente su utilización en bastidor fijo y desde el siglo XIX se comercializó en serie. Los lienzos más usados son los provenientes de fibras vegetales como: el cáñamo, el lino, el yute con tramado fino o el algodón, todos se presentan con grano fino o grueso según el resultado que quiera el artista de su trabajo, también hay soportes realizados con tejido de poliéster. Estos lienzos se pueden adquirir a metros y montarlos sobre marco el propio pintor o utilizar los que hay en el mercado de diferentes tipos y formatos. Existe una numeración internacional para las medidas de largo y ancho de cada bastidor, además tres formatos diferentes para cada número que corresponde a: «figura», «paisaje» y «marina», el tamaño de un lado es siempre el mismo y el otro va disminuyendo, por ejemplo el «40 figura» mide 100 × 81 cm, el «40 paisaje» mide 100 × 73 cm y el «40 marina» mide 100 × 64 cm. Naturalmente no hay que seguir esta regla, cada autor puede realizar su obra libremente en la medida que más desee. La mayoría de lienzos del mercado están preparados con aceite de linaza y tapaporos y también existen preparaciones a base de emulsiones aptos para el óleo o el acrílico, así se simplifica la preparación de imprimaciones para diferentes tipos de pintura y se obtiene siempre el mismo resultado. Vidrio pintado greco-romano del siglo II. Cobre No es un soporte muy común, pero fue usado principalmente durante el siglo XVI en láminas muy delgadas y por pintores del norte de Europa, como el artista alemán Adam Elsheimer. El tamaño normalmente pequeño de estas planchas hace pensar que los artistas que las emplearon, las habían reciclado de antiguos grabados. Vidrio Artículo principal: Vidrio Otro soporte para pintar es el vidrio, realizado en objetos (jarras, vasos) con esmalte que una vez decorados en frío, debe ser sometido, para su fijación al soporte, al calor del horno con una temperatura inferior a la fusión del vidrio. Fue el soporte para vidrieras de catedrales desde el siglo XII, donde se colocaban cristales de colores y la pintura sobre el mismo vidrio por medio de la grisalla, así se conseguía por un lado, cambiar el color del cristal de fondo y por otro, hacer los trazos de las figuras representadas, especialmente los rostros. Papel Artículo principal: Papel Dibujo sobre papel de arroz chino (1729). Se han datado hallazgos de papel procedentes de China cerca del 200 a. C. Se da como inventor del papel al chino Cai Lun (50 a. C.-121), eunuco imperial, que mejoró la fórmula del papel, convirtiéndolo en una alternativa al papiro y al pergamino, los soportes tradicionales para la escritura, gracias al añadido de almidón que protegía las fibras vegetales. El soporte del papel es utilizado en diversas técnicas pictóricas, las más corrientes son la acuarela, el gouache, el pastel y la tinta china negra o en colores. Hay gran variedad de texturas, pesos y colores, y su elección depende del estilo del artista. Existen tres tipos estándares: Papel prensado en caliente: tiene una superficie dura y lisa, muchos artistas consideran una superficie demasiado resbaladiza para la acuarela. Papel prensado en frío: es texturado, semiáspero, adecuado para lavados amplios y lisos. Papel áspero: con una superficie granulada, cuando se aplica un lavado se obtiene un efecto moteado por las cavidades del papel. El peso del papel es la segunda consideración para su elección, ya que un papel más pesado tiene menos tendencia a ondularse. Para evitar que el papel se ondula hay tensarlo. El gramaje apropiado para la acuarela es entre 120 g/m² subasta 850 g/m². Pinceles Los pinceles, son un instrumento clásico y efectivo que el pintor emplea en su trabajo. Los pinceles pueden variar en tamaño, anchura, y calidad. Los materiales de los componentes de los pinceles y brochas pueden ser orgánicos o sintéticos. Pinceles diversos. El pincel consta de tres partes: el pelo, la férula o virola y el mango. Se distinguen por el pelo y su forma, los planos y los de «lengua de gato» suelen ser de pelo duro y los redondos de pelo fino. Los pinceles los escogen los artistas según el trabajo a realizar y su forma de tratar la pintura. Para preparar grandes superficies utilizan las brochas grandes, el interior de las cuales está vacío para recoger una mayor cantidad de pintura, otras brochas más pequeñas ya no tienen el vacío central. Las cerdas de los pinceles suelen ser naturales provenientes de diferentes animales (caballo, marta, cerdo, etc.) o de crines artificiales. Los pinceles, requieren ser tratadas con cuidado para así prolongar su vida útil; esto incluye su limpieza continua. Una forma eficaz de mantener las cerdas de los pinceles en buen estado, es quitar el excedente de pintura, limpiarlos con disolvente y lavarlos con jabón, secar la humedad con una franela y guardarlos horizontalmente o con las cerdas hacia arriba. Se utiliza también como medio para imprimir la pintura rodillos de diferentes tamaños y materiales, como los de lana, goma-espuma o fibras, esponjas naturales o artificiales y los cuchillos paleta y las espátulas metálicas de hoja flexible en formas diversas sirven para unir diferentes colores y también para pintar con ellas. Fondos Interior con una mujer bebiendo en compañía de dos hombres por Pieter de Hooch. A través de la transparencia de la falda de la mujer de la derecha y de la capa del hombre, a medida que pason los años se aprecia las baldosas pintadas anteriormente como fondo de la pintura. En la diversidad de soportes, se acostumbra a modificar antes de comenzar la pintura en ellos, con un tratamiento de imprimación el fondo, que alcanza una superficie pintada con el color y la textura deseada por el artista. Solían hacerse por medio orgánico como el aceite o la cola, mezclado con el color blanco o coloreado, estos medios adhesivos han sido las colas de animales y de pescado, los secantes y las emulsiones de huevo, aceite o resina , el color sólido solía ser la cal, la piedra pómez y la tierra ocre. Esto definía también el efecto visual de la obra finalizada. Este color en el fondo del soporte, consigue en un blanco reflejar la luz a través de las capas de pintura y si es un color oscuro tiende a rebajar el tono de la pintura. Según Giorgio Vasari explica en su tratado Sobre la técnica en el prólogo técnico de Las Vidas (1550), los fondos oleosos tienen la ventaja de conservar su flexibilidad en los lienzos de grandes dimensiones y que se puedan enrollar para trasladarlos, aunque necesitan de un tiempo mayor para su secado. Durante los siglos XVII y XVIII se utilizaron mucho los fondos pintados con tonos de tierra rojizas, lo que permitía dejar algunos espacios sin poner pintura y el cuadro ganaba en uniformidad tonal. Desde el siglo XIX los fondos comerciales han sido preparados industrialmente con blanco de plomo y secante. Pigmentos Los pigmentos se dividen en inorgánicos como los derivados de minerales, las tierras, sales u óxidos con los que se consiguen los colores de tierras ocres y sienas, y los orgánicos derivados de vegetales o animales como los conseguidos por cocción de semillas o calcinación y los obtenidos por vía sintética como anilinas también de compuesto orgánico. Los orgánicos suelen ser menos estables que los inorgánicos. El pigmento junto con el aglutinante forma la pintura. El aglutinante es el que permite alcanzar la fluidez en el pigmento y conseguir la adhesión de la pintura en la superficie, puede ser acuoso o graso. El disolvente tiene la misión de diluir o disolver y su tipo depende de la clase del aglutinante empleado. Así el aguarrás diluye el aceite y disuelve la resina, y el agua disuelve la goma y una vez disuelta, también puede diluirla más. Según el índice de opacidad de la pintura utilizada, a medida que pasan los años, puede captar mejor el fondo de una pintura y los «arrepentimientos» del artista durante su ejecución. En pinturas realizadas anteriormente al siglo XVIII se pueden observar las partículas del pigmento mediante un microscopio, cuanto más grueso era el grano del pigmento más baja calidad tenía la pintura. Durante el siglo XIX se sintetizaron materias colorantes que se usaban como pigmentos, el azul cobalto, el amarillo zinc y el óxido de cromo entre otros. El número de pigmentos ha ido creciendo hasta la actualidad en que existe una gran variedad y todos de excelente calidad."
ksampletext_wikipedia_arts_teoriadelarte: str = "Teoría del arte. La teoría del arte (también teoría de las artes) es una disciplina académica que engloba toda descripción de las manifestaciones artísticas (fenómenos artísticos u obras de arte), empezando por su consideración o aceptación como tales, en todos los géneros del arte, pero especialmente de las llamadas bellas artes (que incluyen tanto las artes visuales -pintura, escultura y arquitectura- como la literatura, la música u otras artes escénicas). En cambio, las llamadas artes aplicadas (también denominadas artes menores, artes decorativas o artes y oficios) han merecido históricamente un aprecio menor (junto al de otras artesanías y por oposición a las mejor valoradas artes liberales), aunque desde finales del siglo XIX se han reivindicado (movimiento de Arts and Crafts) y desde el siglo XX han alcanzado la etiqueta de diseño, cuya generalización a cualquier ámbito de la creación y la producción o incluso de los servicios corre el peligro de aplicarse sin criterio de forma abusiva e incluso ridícula, desvirtuando su contenido. Las teorías del arte analizan este desde un punto de vista teórico y normativo, proporcionando una metodología para desvelar el significado de sus obras. El marco filosófico en el que puede situarse cada versión de la teoría del arte está estrechamente vinculado a diferentes interpretaciones de la estética, dado que la reflexión en torno a la esencia y función del arte mismo se encontraría en la frontera entre ambas disciplinas, de difícil deslinde. Desde un punto de vista valorativo, la aplicación individual o social de una teoría del arte se denomina gusto artístico. Teoría del arte en la historia Con algunas excepciones, como la civilización china, en la que existe una tradición de teoría del arte desde el siglo VI (los Seis Principios de la Pintura de Xie He), la inmensa mayoría de la producción escrita sobre teoría del arte ha correspondido históricamente a la civilización occidental. Concepto de arte en la civilización occidental Atributos de la pintura, la escultura y la arquitectura (1769), de Anne Vallayer-Coster. La definición de arte es abierta, subjetiva, discutible. No existe un acuerdo unánime entre historiadores, filósofos o artistas. A lo largo del tiempo se han dado numerosas definiciones de arte, entre ellas: «el arte es el recto ordenamiento de la razón» (Tomás de Aquino); «el arte es aquello que establece su propia regla» (Schiller); «el arte es el estilo» (Max Dvořák); «el arte es expresión de la sociedad» (John Ruskin); «el arte es la libertad del genio» (Adolf Loos); «el arte es la idea» (Marcel Duchamp); «el arte es la novedad» (Jean Dubuffet); «el arte es la acción, la vida» (Joseph Beuys); «arte es todo aquello que los hombres llaman arte» (Dino Formaggio),«arte es vida,vida es arte» (Wolf Vostell). El concepto ha ido variando con el paso del tiempo: hasta el Renacimiento, arte solo se consideraban las artes liberales; la arquitectura, la escultura y la pintura eran “manualidades”. El arte ha sido desde siempre uno de los principales medios de expresión del ser humano, a través del cual manifiesta sus ideas y sentimientos, la forma como se relaciona con el mundo. Su función puede variar desde la más práctica hasta la ornamental, puede tener un contenido religioso o simplemente estético, puede ser duradero o efímero. En el siglo XX se pierde incluso el sustrato material: decía Beuys que la vida es un medio de expresión artística, destacando el aspecto vital, la acción. Así, todo el mundo es capaz de ser artista. El término arte procede del latín ars, y es el equivalente al término griego τέχυη (téchne, de donde proviene técnica). Originalmente se aplicaba a toda la producción realizada por el hombre y a las disciplinas del saber hacer. Así, artistas eran tanto el cocinero, el jardinero o el constructor, como el pintor o el poeta. Con el tiempo la derivación latina (ars -> arte) se utilizó para designar a las disciplinas relacionadas con las artes de lo estético y lo emotivo; y la derivación griega (téchne -> técnica), para aquellas disciplinas que tienen que ver con las producciones intelectuales y de artículos de uso. En la actualidad, es difícil encontrar que ambos términos (arte y técnica) se confundan o utilicen como sinónimos. Edades Antigua y Medieval Desde Platón hasta el siglo XIX existía un consenso genérico en la literatura culta sobre qué era arte y qué perseguía, cuyo referente fue desde el inicio la Poética de Aristóteles, y posteriormente los textos latinos del arquitecto Vitrubio o los dramaturgos Terencio y Plauto. La imitación (mímesis) de la naturaleza y la identificación de la belleza con los principios de verdad y bondad (paralelismo de estética y ética, función moral y utilitaria del arte) solían ser las ideas más consideradas. En la Antigüedad Tardía la cristianización (que filosóficamente consistió en la conciliación del neoplatonismo con los textos bíblicos a través de la patrística) colocó el arte bajo sospecha, como a todo lo material. El periodo medieval vio distintas formas de aprecio y desprecio de las artes (estética cisterciense de Bernardo de Claraval, diversas consideraciones de los pensadores escolásticos en la universidad medieval) que, de forma extrema, llegaron a la iconoclastia de un determinado periodo del arte bizantino, y a la ausencia (o muy limitada presencia) de arte figurativo en el arte islámico. En la antigüedad clásica grecorromana, una de las principales cunas de la civilización occidental y primera cultura que reflexionó sobre el arte, se consideraba el arte como una habilidad del ser humano en cualquier terreno productivo, siendo prácticamente un sinónimo de destreza: destreza para construir un objeto, para comandar un ejército, para convencer al público en un debate, o para efectuar mediciones agrónomas. En definitiva, cualquier habilidad sujeta a reglas, a preceptos específicos que la hacen objeto de aprendizaje y de evolución y perfeccionamiento técnico. En cambio, la poesía, que venía de la inspiración, no estaba catalogada como arte. Así, Aristóteles, por ejemplo, definió el arte como aquella «permanente disposición a producir cosas de un modo racional», y Quintiliano estableció que era aquello «que está basado en un método y un orden» (via et ordine). Platón, en el Protágoras, habló del arte, opinando que es la capacidad de hacer cosas por medio de la inteligencia, a través de un aprendizaje. Para Platón, el arte tiene un sentido general, es la capacidad creadora del ser humano. Casiodoro destacó en el arte su aspecto productivo, conforme a reglas, señalando tres objetivos principales del arte: enseñar (doceat), conmover (moveat) y complacer (delectet). Alegoría de la pintura (1666), de Johannes Vermeer. Edad Moderna El Renacimiento de los siglos XV y XVI, además de permitir la valoración social del artista como humanista que reflexionaba teóricamente sobre su propio arte, significó una minusvaloración del arte medieval (despreciado como gótico) y una revalorización la Antigüedad clásica grecorromana, o al menos de lo que se reconstruyó como su canon. Todo ello se fijó e institucionalizó con el clasicismo de los siglos XVII y XVIII en las instituciones académicas (academicismo). La Revolución francesa significó la alteración de ese marco, de forma paralela al desmantelamiento del Antiguo Régimen y a la irrupción del concepto de libertad en el arte de manos del Romanticismo, cuyo concepto de belleza incluía también la vertiente atormentada que representa la novela gótica o las Pinturas negras de Goya. Durante el Renacimiento se empezó a gestar un cambio de mentalidad, separando los oficios y las ciencias de las artes, donde se incluyó por primera vez a la poesía, considerada hasta entonces un tipo de filosofía o incluso de profecía –para lo que fue determinante la publicación en 1549 de la traducción italiana de la Poética de Aristóteles–. En este cambio influyó la situación social del artista del Renacimiento, más valorado que sus antecesores por cuanto los productos que elabora adquieren un nuevo estatus de objetos destinados a un consumo estético. Ello es debido al interés que los nobles y ricos prohombres italianos tenían por la belleza, que se convierte a la vez en un medio de destacar socialmente, incrementando el mecenazgo artístico y fomentando el coleccionismo. Surgieron en ese contexto varios tratados teóricos acerca del arte, como los de Leon Battista Alberti (De Pictura, 1436-1439; De re aedificatoria, 1450; y De Statua, 1460), o Los Comentarios (1447) de Lorenzo Ghiberti. Alberti recibió la influencia aristotélica, pretendiendo aportar una base científica al arte. Habló de decorum, el tratamiento del artista para adecuar los objetos y temas artísticos a un sentido mesurado, perfeccionista. Ghiberti fue el primero en periodificar la historia del arte, distinguiendo antigüedad clásica, periodo medieval y lo que llamó “renacer de las artes”. Con el manierismo comenzó el arte moderno: las cosas ya no se representan tal como son, sino tal como las ve el artista. La belleza se relativiza, se pasa de la belleza única renacentista, basada en la ciencia, a las múltiples bellezas del manierismo, derivadas de la naturaleza. Apareció en el arte un nuevo componente de imaginación, reflejando tanto lo fantástico como lo grotesco, como se puede percibir en la obra de Brueghel o Arcimboldo. Giordano Bruno fue uno de los primeros pensadores que prefiguró las ideas modernas: decía que la creación es infinita, no hay centro ni límites –ni Dios ni el hombre–, todo es movimiento, dinamismo. Para Bruno, hay tantos artes como artistas, introduciendo la idea de originalidad del artista. El arte no tiene normas, no se aprende, sino que viene de la inspiración. Los siguientes avances se hicieron en el siglo XVIII con la Ilustración, donde comenzó a producirse cierta autonomía del hecho artístico: el arte se alejó de la religión y de la representación del poder para ser fiel reflejo de la voluntad del artista, centrándose más en las cualidades sensibles de la obra que no en su significado. Jean-Baptiste Dubos, en Reflexiones críticas sobre la poesía y la pintura (1719), abrió el camino hacia la relatividad del gusto, razonando que la estética no viene dada por la razón, sino por los sentimientos. Así, para Dubos el arte conmueve, llega al espíritu de una forma más directa e inmediata que el conocimiento racional. Dubos hizo posible la democratización del gusto, oponiéndose a la reglamentación académica, e introdujo la figura del genio, como atributo dado por la naturaleza, que está más allá de las reglas. El tribunal de los Uffizi (1772-1778), de Johann Zoffany. Edad Contemporánea A la diversidad de teorías de arte, frecuentemente antagónicas, se sumaron las construcciones de la historiografía del arte convertido en una disciplina universitaria, con un fuerte predominio de autores de habla alemana, que convirtió a la historia del arte en una ciencia social. En un campo muy próximo también se situó la crítica de arte como función estrechamente vinculada al mercado artístico, de modo similar a como la crítica literaria lo está al mundo editorial y la crítica musical al negocio del espectáculo (show business). La fijación del gusto academicista en los premios y exposiciones (como el Salón de París) tuvo su reacción en convocatorias alternativas como el Salon des Refusés (Salón de los Rechazados, 1863) que fueron creando un nuevo concepto de arte independiente que a su vez se institucionalizó y quedó sometido a nuevas críticas y rupturas generacionales. Después de la decisiva ruptura de los esquemas artísticos que supuso el impresionismo de finales del siglo XIX y las sucesivas vanguardias artísticas del siglo XX (cada una de ellas teorizando conscientemente sobre sí misma y su lugar en el arte mediante manifiestos), se fijaron nuevos conceptos de arte moderno y arte contemporáneo, cuyo significado y teoría del arte son cualquier cosa menos algo unívoco. Arte moderno no es el arte de la Edad Moderna, sino nuestro arte contemporáneo, o incluso no todo el arte contemporáneo, sino solo el que estéticamente responde a los imprecisos requisitos que definen la ruptura estética contra el academicismo. Esta se da no con la modernidad (cuyo comienzo se podría establecer cronológicamente desde el siglo XV), sino con la aplicación del término libertad al arte (lo que podría localizarse desde el siglo XIX, y no en todos los artistas o estilos). La reflexión teórica en torno al arte seguía siendo uno de los objetos centrales del pensamiento y la filosofía, sobre todo en la primera mitad del siglo XX, como reacción a la radical transformación tanto del arte contemporáneo como de la sociedad contemporánea (sociedad industrial, de masas o de consumo) en el contexto histórico de entreguerras (1918-1939: revolución soviética, fascismos, crisis de 1929); y se manifestó en conceptos nuevos o de renovada definición, muchas veces en tensión dialéctica, como los de arte puro y arte comprometido (paralelos a los de poesía pura y compromiso del intelectual), arte desinteresado, arte deshumanizado (original de Ortega y Gasset), etc. Siglo XIX En el romanticismo, surgido en Alemania a finales del siglo XVIII con el movimiento denominado Sturm und Drang, triunfó la idea de un arte que surge espontáneamente del individuo, desarrollando la noción de genio –el arte es la expresión de las emociones del artista–, que comienza a ser mitificado. Autores como Novalis y Friedrich von Schlegel reflexionaron sobre el arte: en la revista Athenäum, editada por ellos, surgieron las primeras manifestaciones de la autonomía del arte, ligado a la naturaleza. Para ellos, en la obra de arte se encuentran el interior del artista y su propio lenguaje natural. Arthur Schopenhauer dedicó el tercer libro de El mundo como voluntad y representación a la teoría del arte: el arte es una vía para escapar del estado de infelicidad propio del hombre. Identificó conocimiento con creación artística, que es la forma más profunda de conocimiento. El arte es la reconciliación entre voluntad y conciencia, entre objeto y sujeto, alcanzando un estado de contemplación, de felicidad. La conciencia estética es un estado de contemplación desinteresada, donde las cosas se muestran en su pureza más profunda. El arte habla en el idioma de la intuición, no de la reflexión; es complementario de la filosofía, la ética y la religión. Influido por la filosofía oriental, manifestó que el hombre debe liberarse de la voluntad de vivir, del querer, que es origen de insatisfacción. El arte es una forma de librarse de la voluntad, de ir más allá del yo. Richard Wagner recogió la ambivalencia entre lo sensible y lo espiritual de Schopenhauer: en Ópera y drama (1851), Wagner planteó la idea de la “obra de arte total” (Gesamtkunstwerk), donde se haría una síntesis de la poesía, la palabra –elemento masculino– y la música –elemento femenino–. Opinaba que el lenguaje primitivo sería vocálico, mientras que la consonante fue un elemento racionalizador; así pues, la introducción de la música en la palabra sería un retorno a la inocencia primitiva del lenguaje. A finales del siglo XIX surgió el esteticismo, que fue una reacción al utilitarismo imperante en la época y a la fealdad y materialismo de la era industrial. Frente a ello, surgió una tendencia que otorgaba al arte y a la belleza una autonomía propia, sintetizada en la fórmula de Théophile Gautier “el arte por el arte” (lart pour lart), llegando incluso a hablarse de “religión estética”. Esta postura pretendía aislar al artista de la sociedad, buscando de forma autónoma su propia inspiración y dejándose llevar únicamente por una búsqueda individual de la belleza. Así, la belleza se aleja de cualquier componente moral, convirtiéndose en el fin último del artista, que llega a vivir su propia vida como una obra de arte –como se puede apreciar en la figura del dandy–. Uno de los teóricos del movimiento fue Walter Pater, que influyó sobre el denominado decadentismo inglés, estableciendo en sus obras que el artista debe vivir la vida intensamente, siguiendo como ideal a la belleza. Para Pater, el arte es “el círculo mágico de la existencia”, un mundo aislado y autónomo puesto al servicio del placer, elaborando una auténtica metafísica de la belleza. El taller del pintor (1855), de Gustave Courbet. Por otro lado, Charles Baudelaire fue uno de los primeros autores que analizaron la relación del arte con la recién surgida era industrial, prefigurando la noción de “belleza moderna”: no existe la belleza eterna y absoluta, sino que cada concepto de lo bello tiene algo de eterno y algo de transitorio, algo de absoluto y algo de particular. La belleza viene de la pasión y, al tener cada individuo su pasión particular, también tiene su propio concepto de belleza. En su relación con el arte, la belleza expresa por un lado una idea “eternamente subsistente”, que sería el “alma del arte”, y por otro un componente relativo y circunstancial, que es el “cuerpo del arte”. Así, la dualidad del arte es expresión de la dualidad del hombre, de su aspiración a una felicidad ideal enfrentada a las pasiones que le mueven hacia ella. Frente a la mitad eterna, anclada en el arte clásico antiguo, Baudelaire vio en la mitad relativa el arte moderno, cuyos signos distintivos son lo transitorio, lo fugaz, lo efímero y cambiante –sintetizados en la moda–. Baudelaire tenía un concepto neoplatónico de belleza, que es la aspiración humana hacia un ideal superior, accesible a través del arte. El artista es el “héroe de la modernidad”, cuya principal cualidad es la melancolía, que es el anhelo de la belleza ideal. En contraposición al esteticismo , Hippolyte-Adolphe Taine elaboró una teoría sociológica del arte: en su Filosofía del arte (1865-1869) aplicó al arte un determinismo basado en la raza, el contexto y la época (race, milieu, moment). Para Taine, la estética, la “ciencia del arte”, opera como cualquier otra disciplina científica, sobre la base de parámetros racionales y empíricos. Igualmente, Jean Marie Guyau, en Los problemas de la estética contemporánea (1884) y El arte desde el punto de vista sociológico (1888), planteó una visión evolucionista del arte, afirmando que el arte está en la vida, y que evoluciona como esta; y, al igual que la vida del ser humano está organizada socialmente, el arte debe ser reflejo de la sociedad. La estética sociológica tuvo una gran vinculación con el realismo pictórico y con movimientos políticos de izquierdas, especialmente el socialismo utópico: autores como Henri de Saint-Simon, Charles Fourier y Pierre Joseph Proudhon defendieron la función social del arte, que contribuye al desarrollo de la sociedad, aunando belleza y utilidad en un conjunto armónico. Por otro lado, en el Reino Unido, la obra de teóricos como John Ruskin y William Morris aportó una visión funcionalista del arte: en Las piedras de Venecia (1851-1856) Ruskin denunció la destrucción de la belleza y la vulgarización del arte llevada a cabo por la sociedad industrial, así como la degradación de la clase obrera, defendiendo la función social del arte. En El arte del pueblo (1879) pidió cambios radicales en la economía y la sociedad, reclamando un arte “hecho por el pueblo y para el pueblo”. Por su parte, Morris –fundador del movimiento Arts & Crafts– defendió un arte funcional, práctico, que satisfaga necesidades materiales y no solo espirituales. En Escritos estéticos (1882-1884) y Los fines del arte (1887) planteó un concepto de arte utilitario pero alejado de sistemas de producción excesivamente tecnificados, próximo a un concepto del socialismo cercano al corporativismo medieval. Representación de El cascanueces, de Piotr Chaikovski. Por otro lado, la función del arte fue cuestionada por el escritor ruso Lev Tolstoi: en ¿Qué es el arte? (1898) se planteó la justificación social del arte, argumentando que siendo el arte una forma de comunicación solo puede ser válido si las emociones que transmite pueden ser compartidas por todos los hombres. Para Tolstói, la única justificación válida es la contribución del arte a la fraternidad humana: una obra de arte solo puede tener valor social cuando transmite valores de fraternidad, es decir, emociones que impulsen a la unificación de los pueblos. En esa época se empezó a abordar el estudio del arte desde el terreno de la psicología: Sigmund Freud aplicó el psicoanálisis al arte en Un recuerdo infantil de Leonardo da Vinci (1910), defendiendo que el arte sería una de las maneras de representar un deseo, una pulsión reprimida, de forma sublimada. Opinaba que el artista es una figura narcisista, cercana al niño, que refleja en el arte sus deseos, y afirmó que las obras artísticas pueden ser estudiadas como los sueños y las enfermedades mentales, con el psicoanálisis. Su método era semiótico, estudiando los símbolos, y opinaba que una obra de arte es un símbolo. Pero como es el símbolo el que hace aquello simbolizado, hay que estudiar la obra de arte para llegar al origen creativo de la obra. Igualmente, Carl Gustav Jung relacionó la psicología con diversas disciplinas como la filosofía, la sociología, la religión, la mitología, la literatura y el arte. En Contribuciones a la psicología analítica (1928), sugirió que los elementos simbólicos presentes en el arte son “imágenes primordiales” o “arquetipos”, que están presentes de forma innata en el “subconsciente colectivo” del ser humano. Wilhelm Dilthey, desde la estética cultural, formuló una teoría acerca de la unidad entre arte y vida. Prefigurando el arte de vanguardia, Dilthey ya vislumbraba a finales del siglo XIX cómo el arte se alejaba de las reglas académicas, y cómo cobraba cada vez mayor importancia la función del público, que tiene el poder de ignorar o ensalzar la obra de un artista determinado. Encontró en todo ello una “anarquía del gusto”, que achacó a un cambio social de interpretación de la realidad, pero que percibió como transitorio, siendo necesario hallar «una relación sana entre el pensamiento estético y el arte». Así, ofreció como salvación del arte las “ciencias del espíritu”, especialmente la psicología: la creación artística debe poder analizarse bajo el prisma de la interpretación psicológica de la fantasía. En Vida y poesía (1905) presentó la poesía como expresión de la vida, como vivencia (Erlebnis) que refleja la realidad externa de la vida. La creación artística tiene pues como función intensificar nuestra visión del mundo exterior, presentándolo como un conjunto coherente y pleno de sentido. Siglo XX Fuente, de Marcel Duchamp. El siglo XX supone una pérdida del concepto de belleza clásica para conseguir un mayor efecto en el diálogo artista-espectador. El siglo XX ha supuesto una radical transformación del concepto de arte: la superación de las ideas racionalistas de la Ilustración y el paso a conceptos más subjetivos e individuales, partiendo del movimiento romántico y cristalizando en la obra de autores como Kierkegaard y Nietzsche, suponen una ruptura con la tradición y un rechazo de la belleza clásica. El concepto de realidad fue cuestionado por las nuevas teorías científicas: la subjetividad del tiempo de Bergson, la relatividad de Einstein, la mecánica cuántica, etc. Por otro lado, las nuevas tecnologías hacen que el arte cambie de función, ya que la fotografía y el cine ya se encargan de plasmar la realidad. Todos estos factores producen la génesis del arte abstracto, el artista ya no intenta reflejar la realidad, sino su mundo interior, expresar sus sentimientos. El arte actual tiene oscilaciones continuas del gusto, cambia simultáneamente junto a este: así como el arte clásico se sustentaba sobre una metafísica de ideas inmutables, el actual, de raíz kantiana, encuentra gusto en la conciencia social de placer (cultura de masas). También hay que valorar la progresiva disminución del analfabetismo, puesto que antiguamente, al no saber leer gran parte de la población, el arte gráfico era el mejor medio para la transmisión del conocimiento –sobre todo religioso–, función que ya no es necesaria en el siglo XX. Una de las primeras formulaciones fue la del marxismo: de la obra de Marx se desprendía que el arte es una “superestructura” cultural determinada por las condiciones sociales y económicas del ser humano. Para los marxistas, el arte es reflejo de la realidad social, si bien el propio Marx no veía una correspondencia directa entre una sociedad determinada y el arte que produce. Georgi Plejánov, en Arte y vida social (1912), formuló una estética materialista que rechazaba el “arte por el arte”, así como la individualidad del artista ajeno a la sociedad que lo envuelve. Walter Benjamin incidió de nuevo en el arte de vanguardia, que para él es «la culminación de la dialéctica de la modernidad», el final del intento totalizador del arte como expresión del mundo circundante. Intentó dilucidar el papel del arte en la sociedad moderna, realizando un análisis semiótico en el que el arte se explica a través de signos que el hombre intenta descifrar sin un resultado aparentemente satisfactorio. En La obra de arte en la época de la reproductibilidad técnica (1936) analizó la forma cómo las nuevas técnicas de reproducción industrial del arte pueden hacer variar el concepto de este, al perder su carácter de objeto único y, por tanto, su halo de reverencia mítica; esto abre nuevas vías de concebir el arte –inexploradas aún para Benjamin– pero que supondrán una relación más libre y abierta con la obra de arte. Theodor W. Adorno, como Benjamin perteneciente a la Escuela de Frankfurt, defendió el arte de vanguardia como reacción a la excesiva tecnificación de la sociedad moderna. En su Teoría estética (1970) afirmó que el arte es reflejo de las tendencias culturales de la sociedad, pero sin llegar a ser fiel reflejo de esta, ya que el arte representa lo inexistente, lo irreal; o, en todo caso, representa lo que existe pero como posibilidad de ser otra cosa, de trascender. El arte es la “negación de la cosa”, que a través de esta negación la trasciende, muestra lo que no hay en ella de forma primigenia. Es apariencia, mentira, presentando lo inexistente como existente, prometiendo que lo imposible es posible. Isla Pagoda en la desembocadura del río Min (1870), de John Thomson. La fotografía supuso una gran revolución a la hora de concebir el arte en el siglo XIX y el XX. Representante del pragmatismo, John Dewey, en Arte como experiencia (1934), definió el arte como “culminación de la naturaleza”, defendiendo que la base de la estética es la experiencia sensorial. La actividad artística es una consecuencia más de la actividad natural del ser humano, cuya forma organizativa depende de los condicionamientos ambientales en que se desenvuelve. Así, el arte es “expresión”, donde fines y medios se fusionan en una experiencia agradable. Para Dewey, el arte, como cualquier actividad humana, implica iniciativa y creatividad, así como una interacción entre sujeto y objeto, entre el hombre y las condiciones materiales en las que desarrolla su labor. José Ortega y Gasset analizó en La deshumanización del arte (1925) el arte de vanguardia desde el concepto de “sociedad de masas”, donde el carácter minoritario del arte vanguardista produce una elitización del público consumidor de arte. Ortega aprecia en el arte una “deshumanización” debida a la pérdida de perspectiva histórica, es decir, de no poder analizar con suficiente distancia crítica el sustrato sociocultural que conlleva el arte de vanguardia. La pérdida del elemento realista, imitativo, que Ortega aprecia en el arte de vanguardia, supone una eliminación del elemento humano que estaba presente en el arte naturalista. Asimismo, esta pérdida de lo humano hace desaparecer los referentes en que estaba basado el arte clásico, suponiendo una ruptura entre el arte y el público, y generando una nueva forma de comprender el arte que solo podrán entender los iniciados. La percepción estética del arte deshumanizado es la de una nueva sensibilidad basada no en la afinidad sentimental –como se producía con el arte romántico–, sino en un cierto distanciamiento, una apreciación de matices. Esa separación entre arte y humanidad supone un intento de volver al hombre a la vida, de rebajar el concepto de arte como una actividad secundaria de la experiencia humana. En la escuela semiótica, Luigi Pareyson elaboró en Estética. Teoría de la formatividad (1954) una estética hermenéutica, donde el arte es interpretación de la verdad. Para Pareyson, el arte es “formativo”, es decir, expresa una forma de hacer que, «a la vez que hace, inventa el modo de hacer». En otras palabras, no se basa en reglas fijas, sino que las define conforme se elabora la obra y las proyecta en el momento de realizarla. Así, en la formatividad la obra de arte no es un “resultado”, sino un “logro”, donde la obra ha encontrado la regla que la define específicamente. El arte es toda aquella actividad que busca un fin sin medios específicos, debiendo hallar para su realización un proceso creativo e innovador que dé resultados originales de carácter inventivo. Pareyson influyó en la denominada Escuela de Turín, que desarrollará su concepto ontológico del arte: Umberto Eco, en Obra abierta (1962), afirmó que la obra de arte solo existe en su interpretación, en la apertura de múltiples significados que puede tener para el espectador; Gianni Vattimo, en Poesía y ontología (1968), relacionó el arte con el ser, y por tanto con la verdad, ya que es en el arte donde la verdad se muestra de forma más pura y reveladora. El cómic ha sido una de las últimas incorporaciones a la categoría de bellas artes. En la imagen Little Nemo in Slumberland, el primer gran clásico del cómic publicado en 1905. Una de las últimas derivaciones de la filosofía y el arte es la postmodernidad, teoría sociocultural que postula la actual vigencia de un periodo histórico que habría superado el proyecto moderno, es decir, la raíz cultural, política y económica propia de la Edad Contemporánea, marcada en lo cultural por la Ilustración, en lo político por la Revolución francesa y en lo económico por la Revolución industrial. Frente a las propuestas del arte de vanguardia, los postmodernos no plantean nuevas ideas, ni éticas ni estéticas; tan solo reinterpretan la realidad que les envuelve, mediante la repetición de imágenes anteriores, que pierden así su sentido. La repetición encierra el marco del arte en el arte mismo, se asume el fracaso del compromiso artístico, la incapacidad del arte para transformar la vida cotidiana. El arte postmoderno vuelve sin pudor al sustrato material tradicional, a la obra de arte-objeto, al “arte por el arte”, sin pretender hacer ninguna revolución, ninguna ruptura. Algunos de sus más importantes teóricos han sido Jacques Derrida y Michel Foucault. Como conclusión, cabría decir que las viejas fórmulas que basaban el arte en la creación de belleza o en la imitación de la naturaleza han quedado obsoletas, y hoy día el arte es una cualidad dinámica, en constante transformación, inmersa además en los medios de comunicación de masas, en los canales de consumo, con un aspecto muchas veces efímero, de percepción instantánea, presente con igual validez en la idea y en el objeto, en su génesis conceptual y en su realización material. Morris Weitz, representante de la estética analítica, opinaba en El papel de la teoría en la estética (1957) que «es imposible establecer cualquier tipo de criterios del arte que sean necesarios y suficientes; por lo tanto, cualquier teoría del arte es una imposibilidad lógica, y no simplemente algo que sea difícil de obtener en la práctica». Según Weitz, una cualidad intrínseca de la creatividad artística es que siempre produce nuevas formas y objetos, por lo que «las condiciones del arte no pueden establecerse nunca de antemano». Así, «el supuesto básico de que el arte pueda ser tema de cualquier definición realista o verdadera es falso». En el fondo, la indefinición del arte estriba en su reducción a determinadas categorías –como imitación, como recreación, como expresión–; el arte es un concepto global, que incluye todas estas formulaciones y muchas más, un concepto en evolución y abierto a nuevas interpretaciones, que no se puede fijar de forma convencional, sino que debe aglutinar todos los intentos de expresarlo y formularlo, siendo una síntesis amplia y subjetiva de todos ellos. El arte es una actividad humana consciente capaz de reproducir cosas, construir formas, o expresar una experiencia, si el producto de esta reproducción, construcción, o expresión puede deleitar, emocionar o producir un choque. Władysław Tatarkiewicz, Historia de seis ideas (1976). Teorías del arte y autores más significativos Métodos de investigación de la Historia del Arte Einfühlung (empatía -Einfühlung-) Wilhelm Worringer (de:Wilhelm Worringer) Formalismo Heinrich Wölfflin Escuela de Viena de Historia del Arte Jacob Burckhardt (propiamente pertenece a un periodo anterior) Max Dvorak (Max Dvořák) Iconología Aby Warburg Erwin Panofsky Ernst Hans Josef Gombrich Psicología del arte Teoría de la Gestalt Rudolf Arnheim Sociología del arte Pierre Francastel (fr:Pierre Francastel) Estructuralismo Michel Foucault Umberto Eco."
ksampletext_wikipedia_arts_obradearte: str = "Obra de arte. Los términos obra de arte, obra artística y obra creativa son las denominaciones que se dan al producto de una creación en el campo del arte, creación a la que se atribuye una función estética o social. Dada la clásica identificación del concepto de «arte» con las bellas artes, suele restringirse el concepto de obra de arte a los productos de estas: los de las artes plásticas, denominadas artes mayores (pintura, escultura y arquitectura), las obras literarias y las obras musicales. Contexto Un objeto artístico específico a menudo se considera dentro del contexto de un movimiento artístico o época artística más amplio, como un género, una convención estética, una cultura o en función de una distinción regional-nacional. También se lo puede considerar como parte del conjunto de la obra de un artista. Obras maestras e historia Con la exigencia de una obra maestra en la que se demostrara la excelencia en el dominio del oficio, los gremios regulaban desde la Edad Media el acceso de los artistas a la condición de maestro en las distintas artes, como en otras artesanías. El Renacimiento, con la separación de artes y artesanías, manteniendo todavía las instituciones gremiales, incentivó el surgimiento de artistas completos que conseguían triunfar en más de una de las bellas artes o en todas ellas, cumpliendo el ideal del humanismo (Leonardo da Vinci, Miguel Ángel, Rafael). En España, algunos artistas lograron ser maestros en las tres artes mayores. Arte contemporáneo En el arte contemporáneo el campo de las bellas artes se ha expandido, y vuelve a incorporar a las artes aplicadas prestigiosas dentro del término diseño y se incluyen también nuevas artes: la fotografía (con sus sucesivas variantes tecnológicas y de nuevos soportes, como el anime, las caricaturas, la cinematografía, la televisión, el manga, el videoarte, los videojuegos, etcétera), el cómic y manifestaciones más difícilmente catalogables, como la/el performance, el arte conceptual y las denominadas instalaciones artísticas. Aspectos teóricos La estética y la teoría del arte tienen como objeto determinar la naturaleza, los fines y la función del arte, y con ello si la finalidad de una obra de arte es la de imitar a la naturaleza (mímesis), limitarse a ser un objeto de belleza en sí mismo (abstracción de cualquier otro referente), ser vehículo de la expresión del artista o de la comunicación con el espectador, aportar algún significado o simbolismo (en lo que se centran la semiótica y la iconografía). La naturaleza gratuita del arte (arte desinteresado o arte por el arte) entiende las obras de arte como opuestas a los objetos útiles o prácticos, a pesar de que muchas de ellas tengan funciones utilitarias (como servir de vivienda o divulgar mensajes políticos o religiosos) [véase art nouveau]. Características Algunas de las características más importantes de una obra de arte son: Es producto de la percepción de la realidad y de la imaginación del artista. Es fuente de conocimiento y de placer estético. Está abierto a nuevas interpretaciones. Ayuda a formar nociones más exactas de la vida. Mejora la sensibilidad. Favorece las relaciones sociales. Fomenta la reflexión y predispone a la tranquilidad. El David de Miguel Ángel, una escultura de mármol blanco realizada entre 1501 y 1504. La obra de un artista es tanto cada uno de sus trabajos como el conjunto de todos ellos. A veces se utiliza con cualquiera de estos significados directamente en francés (œuvre) o en latín (opus). Obra maestra Artículo principal: Magnum opus El término «obra maestra», en el contexto del arte y de la estética, se reserva en general para aquellas obras, ya sea artísticas o simplemente técnicas, consideradas, por los motivos que sean, como obras particularmente dignas de admiración. El origen del término «obra maestra» se remonta a los gremios de la Edad Media en Europa, en referencia a una pieza artesanal realizada por todo aspirante que en el seno del gremio desease adquirir el título de maestro [véase Obra maestra (gremio)]. Con el tiempo este término pasó a ser un sinónimo de magnum opus, es decir la obra considerada como de mayor valor de entre todas las producidas por un artesano, un artista o un escritor. En el sentido actual del término, «obra maestra» se usa cada vez más como un término laudatorio, se refiera o no a la mejor obra de un autor (en este sentido suele admitirse que un artista, un pintor por ejemplo, haya pintado más de una obra maestra)."

ksampletext_wikipedia_fict_cienciaficcion: str = "Ciencia ficción. Ciencia ficción es la denominación de uno de los géneros derivados de la literatura de ficción, junto con la literatura fantástica y la narrativa de terror. Algunos autores estiman que el término es una mala traducción del inglés science fiction y que la correcta es ficción científica. Nacida como género en la década de 1920 (aunque hay obras reconocibles muy anteriores) y exportada posteriormente a otros medios, como el cinematográfico, historietístico y televisivo, tiene un gran auge desde la segunda mitad del siglo XX debido al interés popular acerca del futuro que despertó el espectacular avance tanto científico como tecnológico alcanzado durante todos estos años. Es un género especulativo que relata acontecimientos posibles desarrollados en un marco imaginario, cuya verosimilitud se fundamenta narrativamente en los campos de las ciencias físicas, naturales y sociales. La acción puede girar en torno a un abanico grande de posibilidades (viajes interestelares, conquista del espacio, consecuencias de una hecatombe terrestre o cósmica, evolución humana a causa de mutaciones, evolución de los robots, realidad virtual, civilizaciones alienígenas, etc.). Esta acción puede tener lugar en un tiempo pasado, presente o futuro, o, incluso, en tiempos alternativos ajenos a la realidad conocida, y tener por escenario espacios físicos (reales o imaginarios, terrestres o extraterrestres) o el espacio interno de la mente. Los personajes son igualmente diversos: a partir del patrón natural humano, recorre y explota modelos antropomórficos hasta desembocar en la creación de entidades artificiales de forma humana (robot, androide, cíborg) o en criaturas no antropomórficas. Introducción Entre los estudiosos del género no se ha podido llegar a un consenso amplio sobre una definición formal, siendo este un tema de gran controversia. En general se considera ciencia ficción a los cuentos o historias que versan sobre el impacto que producen los avances científicos, tecnológicos, sociales o culturales, presentes o futuros, sobre la sociedad o los individuos. La ciencia ficción es un género de narraciones imaginarias que no pueden darse en el mundo que conocemos, debido a una transformación del escenario narrativo, basado en una alteración de coordenadas científicas, espaciales, temporales, sociales o descriptivas, pero de tal modo que lo relatado es aceptable como especulación racional. La ciencia ficción es un tipo de ficción no realista que no está basada en fenómenos sobrenaturales. Su nombre deriva de una traducción bastante literal del término en inglés, ya que la traducción apropiada siguiendo las reglas del castellano sería «ficción de/sobre la ciencia» (dos sustantivos, como el nombre original en inglés), y algunos lo llevan a traducir «ficción científica» (sustantivo más adjetivo) pero esto sería en inglés «scientific fiction». Si bien muchos expertos opinan que debería utilizarse este último, ficción científica, el término ya está arraigado a la cultura popular. El término original en inglés se escribe con un guion de unión cuando ocupa la función de un adjetivo o de un complemento. Por ejemplo: science-fiction novel (novela de ciencia ficción). Para tales casos, en inglés, puede usarse si se lo desea la abreviatura «sci-fi». Este uso anglosajón del guion ha dado lugar a nuevos malentendidos lingüísticos pues el guion en español aglutina sustantivos donde el segundo modifica al primero, es decir, al contrario que en inglés. Por tanto, el uso «ciencia-ficción» en castellano no solo es una falta de ortografía sino que se distancia aún más del significado original en inglés. En español la regla ortográfica del término «ciencia ficción», escrito correctamente siempre sin guion, no es otra que la de la adjetivación del segundo substantivo, como en los términos «hombre lobo» u «hombre rana», escritos siempre sin guion. En castellano también se utilizan las iniciales «CF» para referirse al género. Historia de la literatura de ciencia ficción Artículo principal: Historia de la ciencia ficción El término «ciencia ficción» fue acuñado en 1926 por Hugo Gernsback (por el cual se llaman así los Premios Hugo) cuando lo incorporó a la portada de una de las revistas de narrativa especulativa más conocidas de los años 1920 en Estados Unidos: Amazing Stories. El uso más temprano del mismo parece datar de 1851 y es atribuido a William Wilson, pero se trata de un uso aislado y el término no se generalizó con su acepción actual, hasta que Gernsback lo utilizó de forma consistente (después de hacer un intento previo con el término «scientifiction», el cual no llegó a cuajar). Es muy posible que hoy se usara la palabra «cientificción», pero Gernsback se vio obligado a vender su primera publicación, que tenía ese nombre. Sin darse cuenta, había vendido los derechos sobre el término y muy a pesar suyo se vio obligado a dejar de usarlo y utilizar en su lugar el término «ciencia ficción». De modo, que hasta el año 1926 la ciencia ficción no existía como tal. Hasta esa fecha las narraciones que hoy día no dudamos en calificar de ciencia ficción recibían diversos nombres, tales como «viajes fantásticos», «relatos de mundos perdidos», «utopías», o «novelas científicas». El canadiense John Clute denomina a esta época anterior a la eclosión del género proto ciencia ficción. Proto ciencia ficción y ciencia ficción primitiva (1818-1937) A pesar de la existencia de una proto ciencia ficción francesa que consta de Le voyageur philosophe dans un pays inconnu aux habitants de la Terre (1761) de Daniel Jost de Villeneuve y El año 2440 (1771) del prerromántico francés Louis-Sébastien Mercier, e incluso de una española integrada por el Viaje estático al mundo planetario, 1780, de Lorenzo Hervás y Panduro y el Viaje de un filósofo a Selenópolis (1804) de Antonio Marqués y Espejo, para muchos (para los anglosajones, sobre todo) la primera obra de ciencia ficción con contenidos similares a los del género, tal y como hoy se entiende, se remonta a 1818, año en que se publica Frankenstein o El moderno Prometeo de Mary Shelley. Aunque algunos ven elementos de ciencia ficción en leyendas y mitos muchos siglos antes. En la mitología griega, se cuenta que Dédalo, el padre de Ícaro y constructor del Laberinto de Creta, construyó estatuas de madera que eran capaces de moverse solas, lo que podría ser una idea primitiva del concepto moderno de robot. Y en el folclore judío también está presente el mito del Golem. Incluso el viaje a la Luna fue objeto de iniciativas literarias antes de 1818. Luciano de Samosata, siglo II, en una novela corta, Historia Verdadera, relata un viaje a la Luna en un barco arrastrado por una providencial tromba de agua. Sin embargo, las más conocidas primerizas historias de viajes a la Luna son la de Cyrano de Bergerac, en el siglo XVII, y la del Barón de Münchhausen, siglo XVIII. Sin embargo, Carl Sagan e Isaac Asimov coinciden en que Somnium (1634) de Johannes Kepler es el primer relato de ciencia ficción como tal. Somnium describe a un aventurero que viaja a la Luna y muestra la preocupación de Kepler por el tema de cómo se verían los movimientos de la Tierra desde la Luna. Un siglo antes, Juan Maldonado, publicaría una obra de idéntico nombre y partes de la trama que Kepler en 1541, posiblemente la obra de Kepler se publicará póstumamente por acusaciones de plagio. Ilustración de Lhistoire comique contenant les états et empires du soleil de Cyrano de Bergerac. Habrá algunos que cuestionen la calificación de estas obras como ciencia ficción (ni siquiera como proto ciencia ficción). El propio John Clute excluye la obra de Bergerac frente a otros que consideran que Otros Mundos es auténtica ciencia ficción, ya que a pesar de estar escrito en tono de comedia recurre a los términos científicos de la época. En cualquier caso, cualquiera de estos clásicos cuentos heredan gran parte del espíritu del racionalismo cartesiano del XVII que sentó las bases de la ciencia moderna. Resulta difícil establecer límites. Clute, en su enciclopedia ilustrada, pone en duda la existencia del género antes de finales del siglo XVII pero cita como precursor a Tomás Moro; que en su más famosa obra, Utopía (1516), describe en forma de narración una sociedad perfecta que reside felizmente en la isla Utopía. Sin embargo, como se comenta más arriba, casi todos los expertos reconocen que la obra que supuso un antes y un después en la concepción de la literatura de ficción científica fue la obra de Shelley. Los primeros años tras la aparición de Frankenstein dieron pocos frutos. Se puede destacar quizá otra de las obras de Shelley como El último hombre. En la década de 1830, el estadounidense Edgar Allan Poe anticipó igualmente la narrativa de ciencia ficción (o ficción científica) en relatos como La incomparable aventura de un tal Hans Pfaall, El poder de las palabras, Revelación mesmérica, La verdad sobre el caso del señor Valdemar, Un descenso al Maelström, Von Kempelen y su descubrimiento, etc. Dichos relatos reúnen algunos de los elementos primitivos de la ciencia ficción, como el mesmerismo y los viajes en globo ,muy en boga en aquella época, y la especulación cosmológica, también presente en su visionario ensayo Eureka, en el cual parecen describirse los agujeros negros y algo parecido al Big Crunch (op. cit. p. 11). Posteriormente, en la década de 1850 aparece el que probablemente pasa por ser uno de los autores más prolíficos del siglo XIX en el campo de las aventuras de corte científico: Julio Verne, quien en 1863 publicó su primera obra con contenido de ficción científica: Cinco semanas en globo. La aparición de esta obra supone un hito. A partir de su publicación, este género empieza a transformar sus intenciones. La ciencia subyacente pasa de ser un motivo de inquietud o de preocupación por lo desconocido, a ser un soporte de historias de aventuras y descubrimientos. Ciencia ficción primitiva Europa Ilustración de un trípode realizada por Alvim Corréa para la edición francesa de 1906 del clásico de H.G. Wells La guerra de los mundos. La rama europea de la ciencia ficción comenzó propiamente a finales del siglo XIX con las novelas científicas de Julio Verne (1828-1905), cuya ciencia se centraba más bien en invenciones, así como con las novelas de crítica social con orientación científica de H. G. Wells (1866-1946). Sin embargo, aunque Wells suele ser reconocido como el gran iniciador del género, Roger Luckhurst demuestra que solo fue el más influyente de una corriente que comenzó pocos años antes. Wells y Verne rivalizaron en la primitiva ciencia ficción. Los relatos y novelas cortas con temas fantásticos aparecieron en las publicaciones periódicas en los últimos años del siglo XIX, y muchos de ellos emplearon ideas científicas como una excusa para lanzarse a la imaginación. Aunque es más conocido por otros trabajos, Arthur Conan Doyle también escribió ciencia ficción. El único libro en el que Charles Dickens se aventura en el territorio de la especulación científica y los extraños misterios de la naturaleza (en contraposición a los claramente sobrenaturales fantasmas de Navidad) fue en su novela Bleak House (1852) en la que uno de sus personajes muere por «combustión humana espontánea». Dickens investigó casos registrados de tal efecto antes de escribir sobre el tema para ser capaz de contestar a los escépticos que se escandalizaran con su novela. El siguiente gran escritor británico de ciencia ficción tras H. G. Wells fue John Wyndham (1903-1969). A este autor le gustaba referirse a la ciencia ficción con el nombre de «fantasía lógica». Antes de la Segunda Guerra Mundial Wyndham escribió exclusivamente para las revistas pulp, pero tras la contienda se hizo famoso entre el público en general, más allá de la estrecha audiencia de los fanes de la ciencia ficción. La fama le vino de la mano de sus novelas El día de los trífidos (1951), El kraken acecha (1953), Las crisálidas (1955) y Los cuclillos de Midwich (1957). Fuera del ámbito anglosajón hay que destacar la figura de Karel Čapek, introductor del término robot en su obra teatral R.U.R. y creador del clásico de la ciencia ficción La guerra de las salamandras en 1937. Mucho tiempo antes de que la novela de H.G. Wells, La máquina del tiempo, viera la luz, el escritor Enrique Gaspar ya había publicado una novela sobre viajes temporales. De su imaginación nació El Anacronópete, que podría ser considerada como la primera novela en la que una máquina del tiempo aparece como elemento central. Editada en Barcelona a principios de 1887. No obstante, a finales del siglo XIX y principios del siglo XX, numerosos escritores de prestigio escriben relatos, novelas y obras de teatro de ciencia ficción, como por ejemplo Miguel de Unamuno, Azorín, Vicente Blasco Ibáñez, Agustín de Foxá, Ramiro de Maeztu o Jardiel Poncela. Muchos de estos relatos fueron publicados en una antología por Santiáñez-Tió e incluso se van editando poco a poco textos inéditos o de difícil acceso. América Artículos principales: Ciencia ficción en Venezuela, Ciencia ficción en Ecuador, Ciencia ficción en Costa Rica, Ciencia ficción en Colombia y Ciencia ficción en Perú. El primer antecedente de la ciencia ficción en América Latina (y en todo el continente americano) lo escribió el fraile franciscano Manuel Antonio de Rivas a finales del siglo XVII, en Mérida, Yucatán. Se trata de Sizigias y cuadraturas lunares, un librito de once folios que narra un viaje a la luna.  Luego, a mediados de siglo XIX, surgen autores ligados al romanticismo que incursionan en la ciencia ficción, como Sebastián Camacho y Zulueta en México, Juan Vicente Camacho y Nicanor Bolet Peraza en Venezuela, Eduardo Ladislao Holmberg y Leopoldo Lugones en Argentina, Francisco Campos Coello en Ecuador, o Máximo Soto Hall en Guatemala. Será a principios del siglo XX cuando el género conocerá un auge con obras como Guayaquil: novela fantástica (1901) de Manuel Gallegos Naranjo, Granja blanca (1904) de Clemente Palma, La tienda de muñecos (1927) de Julio Garmendia, Una triste aventura de catorce sabios (1928) José Félix Fuenmayor, La galera de Tiberio (1932) de Enrique Bernardo Núñez, El regreso de Eva (1933) de Pepe Alemán, o los cuentos de Pablo Palacio. Posteriormente, conocerá su consolidación a mediados del siglo XX con autores como Hugo Correa, Oscar Hurtado, Adolfo Bioy Casares, Angélica Gorodischer, José Balza, Luis Britto García.  En los Estados Unidos el género puede remontarnos a Mark Twain y su novela Un yanqui en la corte del rey Arturo, una novela que exploraba términos científicos aunque fueran enmarcados en una ficción caballeresca. Mediante el recurso a la «transmigración del alma» y la «transposición de épocas y cuerpos» el yankee de Twain es transportado hacia atrás en el tiempo y arrastra consigo todo el conocimiento de la tecnología del siglo XIX. Los resultados son catastróficos, ya que la caballeresca aristocracia del rey Arturo se ve pervertida por el notable poder de destrucción que ofrecen máquinas como las ametralladoras, los explosivos y el alambre de espino. Ciencia ficción «Narraciones en las que se habla de viajes interplanetarios». , Michel Butor. Otro autor que escribió algunas historias de este tipo es Jack London. El autor de las novelas de aventuras en el salvaje Yukon, Alaska, y el Klondike, también escribió historias sobre extraterrestres (The Red One), sobre el futuro (El talón de hierro) o sobre los conflictos del futuro (La invasión sin precedentes). También escribió una historia sobre la invisibilidad y otra sobre un arma de energía para la que no existía defensa alguna. Estas historias impactaron en el público estadounidense y comenzaron a perfilar algunos de los temas clásicos de la ciencia ficción. Pero el autor estadounidense que mejor simboliza el nacimiento en Estados Unidos de la ciencia ficción como género de masas es Edgar Rice Burroughs quien, poco antes de la Primera Guerra Mundial, publicó Bajo las lunas de Marte (1912) en varios números de una revista especializada en aventuras. Burroughs siguió publicando en este medio durante el resto de su vida, tanto fantasía científica como historias de otros géneros (misterio, horror, fantasía y, cómo no, su personaje más conocido: Tarzán); pero, las historias de John Carter (ciclo de Marte) y Carson Napier (ciclo de Venus), aparecidas en aquellas páginas, hoy día se consideran joyas de la ciencia ficción más temprana. Ciencia ficción «Ciencia ficción es todo lo que los editores publican bajo la etiqueta ciencia ficción». , Norman Spinrad. No obstante, el desarrollo de la ciencia ficción estadounidense como género literario específico hay que retrasarlo hasta 1926, año en el que Hugo Gernsback funda Amazing Stories, creándose la primera revista dedicada exclusivamente a las historias de ciencia ficción. Por otra parte, dado que como es bien conocido, fue él quien eligió el término scientifiction para describir a este género incipiente, el nombre de Gernsback y el vocablo al que dio origen han quedado unidos para la posteridad. Las historias que se publicaban en esta y otras exitosas revistas pulp (Weird Tales, Black Mask...), no gozaban del aval de la crítica seria, que en su mayoría las consideraban sensacionalismo literario, sin embargo fue en estas revistas, que mezclaban a partes iguales la fantasía científica con el terror, donde empezaron a brillar algunos de los grandes nombres del género, como Howard Phillips Lovecraft, Fritz Leiber, Robert Bloch, Robert E. Howard, etc. Todo ello atrajo a muchos lectores a las historias de especulación científica propiamente dicha. La Edad de Oro (1938-1950) Artículo principal: Edad de Oro de la ciencia ficción Con el surgir en 1938 del editor John W. Campbell y su actividad en la revista Astounding Science Fiction (fundada en 1930) y con la consagración de los nuevos maestros del género: Isaac Asimov, Arthur C. Clarke y Robert A. Heinlein, la ciencia ficción empezó a ganar estatus como género literario, especialmente con este último, que fue el primer autor que consiguió que se editaran historias del género en publicaciones más generales, y fue también el que le dio mayor madurez al género e influyó poderosamente en su desarrollo posterior. Las incursiones en el género de autores que no se dedicaban exclusivamente a la ciencia ficción también generaron un mayor respeto hacia el mismo; cabe destacar Karel Čapek, Aldous Huxley, C. S. Lewis y en castellano Adolfo Bioy Casares y Jorge Luis Borges. Después de la Segunda Guerra Mundial se produce una transición del género. Es la época en la que los cuentos empiezan a ser desplazados por las novelas y los argumentos ganan en complejidad. Las revistas mostraban llamativas portadas con monstruos de ojos de mosca y mujeres medio desnudas, dando una imagen atrayente para lo que era su público principal: los adolescentes. Se fundan nuevas revistas: hasta 15 nuevas publicaciones en un solo año; y alguna incluso atraviesa el océano Atlántico como la francesa Galaxie (prima hermana de la estadounidense Galaxy que empieza a publicarse el año 1950), pero ahora el género empieza a salir del terreno exclusivo del pulp. La Edad de Plata (1951-1965) Aviones despegando desde una plataforma en la estratosfera. Ilustración de 1953 para un periódico realizada por Helmuth Ellgaard. Posiblemente, el que puede tal vez considerarse como primer título notable de la posguerra no fue escrito por un autor habitualmente catalogado como escritor de ciencia ficción y, de hecho, el libro ni siquiera fue catalogado como tal por su editor; pero sin duda lo es, y le dio a su autor fama mundial; nos referimos a 1984 (1948) de George Orwell. Pero la mejor tarjeta de visita del período de los años 1950 es su interminable lista de escritores que han sido la columna vertebral del género hasta casi finales de siglo: Robert A. Heinlein, Isaac Asimov, Clifford D. Simak, Arthur C. Clarke, Poul Anderson, Philip K. Dick, Ray Bradbury, Frank Herbert, Stanislav Lem y muchos otros. En cuanto a los títulos, de esta época son libros que hoy son considerados clásicos: Crónicas marcianas o Fahrenheit 451 de Ray Bradbury, Mercaderes del espacio de Frederik Pohl y Cyril M. Kornbluth, Más que humano de Theodore Sturgeon; sin olvidar El fin de la Eternidad de Isaac Asimov, y Lotería solar o El hombre en el castillo de Philip K. Dick. Algunas de ellas serían adaptadas al cine o la televisión; La naranja mecánica de Anthony Burgess es un buen ejemplo de ello. También es en esta época cuando empiezan a otorgarse los premios Hugo, cuya primera edición fue en 1953. En realidad, pese a que desde el punto de vista académico se ha venido en calificar como «edad de oro» a la etapa comprendida entre los años 1938 y 1950, para muchos, esta época debería extenderse unos quince años. Otra novela importante de este período es Dune (1965) de Frank Herbert. La Nueva Ola Artículo principal: Nueva ola literaria Los años transcurridos entre 1965 y 1972 son el período de mayor experimentación literaria de la historia del género. En Reino Unido, se puede asociar con la llegada de Michael Moorcock a la dirección de la revista New Worlds. Moorcock, entonces un joven de 24 años, dio espacio a las nuevas técnicas ejemplificadas en la literatura de William Burroughs y J.G. Ballard. Los temas empezaron a distanciarse de los tan manidos robots e imperios galácticos de las edades de oro y plata de la ciencia ficción, centrándose en temas hasta entonces inexplorados: la consciencia, los mundos interiores, relativización de los valores morales, etcétera. En Estados Unidos, los ecos de los cambios experimentados en el panorama británico tuvieron su reflejo. Autores como Samuel Ray Delany, Judith Merril, Fritz Leiber, Roger Zelazny, Philip K. Dick, Ursula K. LeGuin, Philip José Farmer y Robert Silverberg, representan la esencia de las nuevas vías de este género literario. El ciberpunk Artículo principal: Ciberpunk En la década de 1980 las computadoras cada vez más ubicuas y la aparición de las primeras redes informáticas globales dispararon la imaginación de jóvenes autores, convencidos de que tales prodigios producirían profundas transformaciones en la sociedad. Este germen cristalizó principalmente a través del llamado movimiento ciberpunk, un término que aglutinaba una visión pesimista y desencantada de un futuro dominado por la tecnología y el capitalismo salvaje con un ideario «punk» rebelde y subversivo, frecuentemente anarquista. Una nueva generación de escritores surgió bajo esta etiqueta, encabezados por William Gibson, Bruce Sterling y Neal Stephenson. Postciberpunk Artículo principal: Postciberpunk A principios de la década de 1990 ocurrió un cambio significativo en la literatura de ciencia ficción. Autores antes plenamente ciberpunk o que nunca habían pertenecido a esa corriente, comenzaron a rechazar explícitamente los clichés de dicho género, y de paso, a considerar a la tecnología con una visión más positiva. Es notorio que esto ocurría casi al mismo tiempo que se daba la acelerada introducción de las computadoras e Internet en la vida cotidiana. Conforme los autores empezaron realmente a usar las computadoras y la red global, sus opiniones y obras empezaron a cambiar y a rechazar la rebeldía y exaltación de la marginalidad del ciberpunk. En las novelas postciberpunk, es mucho más frecuente que los protagonistas sean integrantes respetables de sus comunidades: científicos, militares, policías e incluso políticos. Aun en el caso de personajes más marginales, su interés suele residir en mantener o mejorar el statu quo, no en destruirlo, tal y como era lo típico en el ciberpunk; y cuando no lo hacen, suelen ser los antagonistas. La primera novela etiquetada como postciberpunk es Snow Crash (1992) de Neal Stephenson. Además de Stephenson, han sido etiquetados como postciberpunk autores tan dispares como Nancy Kress, Greg Egan, Tad Williams, Charles Stross o Richard Morgan. Subgéneros contemporáneos En épocas recientes, a la ciencia ficción se le han agregado varios subgéneros cuyos nombres usan también el postfijo «punk». Esto por analogía con el «ciberpunk», que es ciencia ficción centrada en la cibernética. Estos subgéneros responden en ocasiones a impulsos estilísticos de los autores, o a la demanda de los lectores y espectadores, pidiendo más obras con el mismo estilo de ciertas obras originales. Entre estos subgéneros están: El Steampunk, o ciencia ficción centrada en la presencia anacrónica de ciertas tecnologías avanzadas basadas en, o coexistiendo con el motor a vapor, y situadas durante la Revolución Industrial y la época victoriana. El Biopunk, donde la ficción se centra en el impacto de grandes avances de la biotecnología. Pudiendo situarse tanto en el futuro, presente o en un pasado anacrónico. Ejemplos de obras de este estilo son el filme Gattaca, o la saga de videojuegos Bioshock. El Retrofuturismo, que retoma en tono serio o irónico, el entusiasmo por el futuro y la imaginería optimista de los años 1930, 1940 y 1950, ejemplos de este género serían obras como Sky Captain y el mundo del mañana. El Solarpunk, es fundamentalmente un ciencia ficción climática, con una estética que enverdece las ciudades, y una organización social comunitaria. La ciencia ficción en otros medios En las revistas Portada de la revista Amazing Stories (1928). La ciencia ficción está ineludiblemente ligada a las revistas. La propia expresión ciencia ficción apareció en una de ellas. Probablemente, la primera revista periódica con algunos cuentos de este género (todavía sin nombre oficial) se podría considerar The Argosy 1896. No obstante, The Argosy no era una revista exclusivamente dedicada a las historias fantásticas con contenido científico. Otra revista temprana fue All Story, que comenzó a publicarse en 1911; en ella aparecieron la mayoría de los cuentos de Edgar R. Burroughs de fantasía científica. Sin embargo, las dos revistas precursoras más famosas no llegarían hasta la década de 1920; en 1923 empezó a publicarse Weird Tales (cuya versión española se llamó Narraciones Terroríficas), y 1926, año en el que Hugo Gernsback acuñó el término con el que definitivamente se conocería el género para la otra de las dos «precursoras oficiales»: Amazing Stories. Amazing fue la primera de todas ellas en dedicarse de forma exclusiva a la ficción de corte científico y tuvo una larga trayectoria. Sus primeras historias eran principalmente reimpresiones de obras de Poe, Wells y Verne; pero también se publicaron relatos inéditos de gente como Burroughs y Merrit. Amazing se puede considerar como la revista más influyente durante muchos años y un punto de referencia durante todo el curso de su existencia. En 1980, tras su última etapa bajo la edición de Kim Mohan, la revista dejó de publicarse y, aunque varios editores han intentado resucitarla desde entonces, actualmente se puede considerar fuera de circulación. En 1930 surgió otra de las revistas clásicas que todos los historiadores incluyen en su relación de publicaciones de la «edad de oro», Astounding Stories, la que más tarde sería reeditada por John W. Campbell como Astounding Science Fiction (1938) y que finalmente derivaría en la actual Analog Science Fiction and Fact (1960) y en la que escribieron los grandes escritores del género de aquellos días, entre los que se incluyen a Isaac Asimov, Robert A. Heinlein y Poul Anderson. Astounding/Analog (también conocida por sus siglas ASF) es considerada una revista de corte más «cientificista» que otras, siendo una de las publicaciones esenciales desde sus inicios hasta el presente. En 1971, tras la muerte de Campbell, Analog pasó a ser editada por Ben Bova, también conocido por ser el valedor de Orson Scott Card y aquel que lo lanzó a la fama. Desde 1978 la edita Stanley Schmidt. En 1949 empezó a publicarse otra revista que tiene en su haber la mayor serie de colaboraciones (en este caso ensayos científicos) de Isaac Asimov, un total de 399 colaboraciones mensuales a lo largo de 33 años. Se trata de The Magazine of Fantasy & Science Fiction. Esta revista fue primeramente editada por Antony Boucher, y su editor actual, Gordon van Gelder, mantiene una revista de gran calidad literaria. En sus páginas se han publicado clásicos como Flores para Algernon de Daniel Keyes. Otra de las revistas que no podíamos dejar de mencionar es Galaxy (1950). Inicialmente editada por Horace Leonard Gold tiene en su haber las mejores críticas literarias gracias a la aceptación del público de un género que empezaba a consagrarse fuera de los círculos del pulp. Con ver la lista de autores que publicaron en su primer número podemos hacernos una idea de su calidad y empuje: Clifford D. Simak, Theodore Sturgeon, Fritz Leiber o Isaac Asimov. Esta revista llegó a publicarse en Europa (en Francia y Alemania), tuvo cierto éxito durante casi treinta años hasta que en 1980 dejó de publicarse. A principio de los años 1990 el hijo de su fundador retomó la publicación de Galaxy, pero finalmente la empresa terminó de forma infructuosa en 1995. El género está en alza. Todos los años aparecen nuevas revistas. Algunas intentan aprovechar el tirón publicitario de un nombre conocido para entrar en un mercado muy competitivo. Es, por ejemplo, el caso de Asimovs Science Fiction que empezó a publicarse en 1977 bajo la dirección del propio Isaac Asimov y con George H. Scithers como editor. Este hecho, no obstante, no tiene porqué restar calidad a estas empresas y, por ejemplo, las historias publicadas en Asimovs han sido galardonadas con frecuencia con premios Hugo y Nébula. También en español, llegaron a publicarse algunas revistas clásicas, como la anteriormente mencionada Narraciones. Aunque también hubo iniciativas puramente autóctonas. De ellas, la más conocida comenzó su vida en 1968. Se trata de Nueva Dimensión (ND), fundada por Domingo Santos, y estuvo en circulación hasta 1983, habiendo obtenido durante esos años varios premios internacionales. Otra revista, esta mucho más moderna, con cierto renombre es Gigamesh, que empezó a publicarse en 1991; no obstante, nunca ha llegado a tener la repercusión literaria de ND. Tras varias publicaciones sin periodicidad alguna ha dejado también de publicarse. También la revista Galaxia, que bajo la dirección de León Arsenal, obtuvo en 2003 el premio a la mejor publicación de literatura fantástica, concedido por la Sociedad Europea de Ciencia ficción. Como vemos, muchas revistas han sufrido una trayectoria muy irregular, con sucesivas resucitaciones y desapariciones, hecho que ha impedido que lleguen a ser conocidas de forma extensa. Volvió a aparecer durante un tiempo una de estas últimas: Asimov Ciencia Ficción (versión española de su homónima estadounidense), pero cerró definitivamente al cabo de unos pocos años. Ninguna de las importaciones de la célebre revista estadounidense en España ha tenido éxito. Ya en los últimos años, ha aparecido el magazín en línea Scifiworld Magazine que dedicado principalmente al género fantástico en el medio audiovisual informa cada mes de las novedades del género junto a artículos de diversa índole. A partir de julio de 2006, la revista pasa a formar parte de la cadena de televisión Sci Fi y pasa a llamarse scifi.es. Otras publicaciones digitales han sido Revista Exégesis, surgida en el 2009 y especializada en el cómic de ciencia ficción; y en el género de cuento figura Axxón (una de las más antiguas revista digitales de ciencia ficción, originada en 1989). También en 2006, la A.C. Xatafi comienza la publicación digital de la revista Hélice: reflexiones críticas sobre ficción especulativa. Desde entonces ha mantenido una regularidad notable con un considerable éxito. Esta revista ha sido la primera en plantear una dignificación del género en España mediante estudios y críticas de mayor nivel, entre la difusión y el academicismo, con una consideración profesional de la figura del crítico. Ganó el premio Ignotus de la AEFCFT a la mejor revista publicada en 2007. En 2008 también la A.C. Xatafi publicó el primer número digital de Artifex, revista de cuentos de género que recoge el relevo de la edición en papel. Su precursora, tras pasar por varios formatos, se ha mantenido durante años como el referente para la publicación de relatos de ciencia ficción en España. 2008 es también el año en el que aparece la versión impresa de Scifiworld Magazine, independizados ya del canal SyFy, y que hasta el momento ha sobrepasado los 40 números convirtiéndose en la revista más longeva en España dedicada a la ciencia ficción, la fantasía y el terror en la cultura y el entretenimiento. En 2009 nace Sci-FdI, revista creada en la Facultad de Informática de la Universidad Complutense y dedicada única y exclusivamente al género de la ciencia ficción. La revista está centrada especialmente en relatos cortos, pero también incluye ensayos y algunas entrevistas, así como reseñas e introducciones de nuevos libros. En el cine Artículo principal: Cine de ciencia ficción El género de la ciencia ficción ha estado presente en el cine bien mediante la adaptación de cuentos y novelas, bien mediante la producción de películas con guiones especialmente creados para la pantalla. El cine de ciencia ficción se ha utilizado en ocasiones para la crítica de aspectos políticos o sociales y para la exploración de cuestiones filosóficas como la propia definición del ser humano. El género ha existido desde los comienzos del cine mudo, cuando el Viaje a la Luna (1902) de Georges Méliès asombró a su audiencia con sus efectos fotográficos. Desde la década de 1930 hasta la de 1950, el género consistía principalmente en películas de serie B de bajo presupuesto. Tras el hito de Stanley Kubrick de 2001: A Space Odyssey (1968), el cine de ciencia ficción fue tomado más en serio. A finales de la década de 1970, películas de presupuesto alto con efectos especiales se convirtieron en populares entre la audiencia. Películas como Star Wars o Close Encounters of the Third Kind allanaron el camino de éxitos de ventas en las siguientes décadas como Alien: el octavo pasajero (1979), E.T., el extraterrestre (1982), Blade Runner (1982), Volver al futuro (1985), Hombres de negro (1997) y El quinto elemento (1997). En la televisión Artículo principal: Ciencia ficción en la televisión Leonard Nimoy y William Shatner, protagonistas de la serie original de Star Trek La ciencia ficción apareció primeramente en televisión durante la época de oro de la ciencia ficción, primero en Reino Unido y después en los Estados Unidos. Los efectos especiales y otras técnicas de producción permiten que los creadores presenten una imagen viviente de un mundo imaginario que no se limita a la realidad; esto hace de la televisión un medio excelente para la ciencia ficción, que a su vez contribuye a su popularidad de esta forma. Debido a su modo de presentación visual, la televisión emplea mucha menos exposición que los libros para explicar los apuntalamientos de la puesta de ficción. Como resultado, la definición y los límites del género son observados de una forma menos estricta que en los medios impresos. Como el costo de crear un programa de televisión es relativamente alto en comparación con el costo de escribir e imprimir libros, los programas de televisión están obligados a atraer a una audiencia mucho mayor que la ficción impresa. Algunos escritores y lectores creen que un efecto de mínimo común denominador le resta calidad de la ciencia ficción en televisión, en relación con los libros. Al debilitarse los límites del género, los guionistas y espectadores deben utilizar estándares más inclusivos que los autores y lectores, de tal modo que en muchos contextos se considera que la categoría de ciencia ficción en televisión incluye a todos los géneros especulativos, entre ellos el de fantasía y el de terror. En Reino Unido, a este grupo se le llama «telefantasía». Los ejemplos más famosos y duraderos sobre trabajos en este campo son Doctor Who, Star Trek, Galáctica y Stargate, aunque muchas otras series han atraído audiencias grandes y pequeñas durante décadas. En la historieta Artículo principal: Historieta de ciencia ficción La historieta o cómic de ciencia ficción constituye uno de los géneros más importantes en los que puede dividirse la producción historietística. Los años 1970 y 1980 fue el momento de mayor auge de la ciencia ficción en este medio, que popularizó el género entre millones de lectores. Las historietas ofrecieron las escenas más acertadas de la navegación interestelar, de los alunizajes, de las bombas atómicas o de las sociedades hiperindustrializadas. En los juegos La ciencia ficción también está presente en numerosos videojuegos y juegos de rol. Premios Literarios Los tres premios más importantes del género son los premios Hugo, los premios Nébula y los premios Saturn. Los premios Hugo, llamados así en memoria del pionero de la ciencia ficción Hugo Gernsback, son concedidos en diversas categorías por la Sociedad mundial de ciencia ficción (WSFS) durante la celebración anual de la Worldcon. Durante la misma se entrega además el premio John W. Campbell al mejor autor novel del año. Los premios Nébula son concedidos anualmente también en varias categorías por la Asociación de escritores de ciencia ficción y fantasía de Estados Unidos (SFWA). Esta asociación además concede los cotizados premios Gran Maestro a los más importantes escritores del género por la labor de toda una vida. Algunos otros premios también tienen nombres de otros insignes autores y editores del ramo: John W. Campbell Memorial (no confundir con el del mismo nombre al mejor autor novel) y los premios Clarke, Sturgeon y Philip K. Dick. También las publicaciones especializadas otorgan algunos premios de relevancia como es el caso de la revista estadounidense Locus Magazine, que anualmente otorga los premios Locus. En Europa, la Sociedad Europea de Ciencia Ficción (ESFS) se creó en 1972 y reúne a diversos profesionales del sector. Inicialmente programaba una convención bianual que a partir de 1982 se convirtió en anual, durante la cual se otorgan los premios europeos de ciencia ficción en los que se nomina al mejor: autor, traductor, promotor, publicación periódica, editorial, artista y revista. En España, existen dos grandes premios. Los premios Ignotus, otorgados por la AEFCFT, que son votados por los socios y por los asistentes a la convención nacional anual Hispacón. Serían los equivalentes españoles a los Hugo. Han sido otorgados desde 1991 y cuentan con varias categorías. Por otra parte, el Premio Xatafi-Cyberdark es otorgado por la A.C. Xatafi y por la librería virtual Cyberdark. Los premiados son elegidos por un jurado rotativo compuesto por varios críticos de toda España que a lo largo de un año discuten en lista privada sobre todo lo publicado el año anterior. Se concede desde 2006 e incluye las categorías de Mejor libro español, Mejor libro extranjero, Mejor cuento español, Mejor cuento extranjero y Mejor iniciativa editorial en España. Desde 2012, la revista Scifiworld concede también un premio en el que incluye obras literarias y audiovisuales. Otros países también tienen sus premios nacionales: el premio Seiun en Japón, los BSFA británicos, los Ditmar australianos, etcétera. Cinematográficos En Estados Unidos, se otorgan los Saturno por la Academia de cine de ciencia ficción, fantasía y horror, siendo, probablemente, los premios más importantes del género. En Europa los premios están más relacionados con festivales concretos, en los que se exhiben diferentes películas. El Festival de Cine de Sitges junto con el Festival Internacional de Cine Fantástico de Bruselas son las dos citas europeas más importantes del género. En Latinoamérica existen pocos festivales especializados, uno de ellos es el Buenos Aires Rojo Sangre. Subgéneros Artículo principal: Subgéneros de ciencia ficción Ciencia ficción hard y soft Esta clasificación dicotómica, literalmente dura y blanda, se refiere a dos tendencias opuestas a la hora de elaborar los planteamientos científicos sobre los que se basa la obra. En el caso de la ciencia ficción hard los elementos científicos y técnicos están tratados con el máximo rigor, incluso cuando estos entran dentro de la pura especulación, y la narración se subordina a este rigor. La película de ciencia ficción hard por excelencia es 2001: Una odisea del espacio. Gran parte de la ciencia ficción soviética se inscribe en esta línea.Barceló (1990) dice con referencia al hard: Cuando la ciencia ficción retoma los temas más estrictamente científicos y se basa principalmente en el mundo de la ciencia, se habla de ciencia ficción «dura», comúnmente de ciencia ficción hard, utilizando directamente la palabra original inglesa ya que casi nadie usa su traducción literal del castellano. Por lo general, la física, la química de la biología, con sus derivaciones el ámbito de la tecnología, las ciencias que soportan la mayor parte de especulación temática de la ciencia ficción hard. Barceló (1990, p. 55) Respecto a la ciencia ficción soft escribe: En contraposición a la base científico-tecnológica de la ciencia ficción más clásica, los años sesenta contemplaron [...] los intentos [...] por incorporar las ciencias sociales como la antropología, la historia, la sociología y la psicología al ámbito de la ciencia ficción. [...] Sus autores suelen caracterizarse por una escasa o nula formación científica y un interés casi exclusivo por lo meramente literario. Gracias a ello [...] ha incorporado una mayor calidad literaria a la ciencia ficción y [...] ha provocado una evidente mejora del género. Barceló (1990, p. 59) Obviamente la distinción entre ambas vertientes es difusa y podemos encontrarnos obras que comparten ambos enfoques. Pero, por lo general, los autores de ciencia ficción se pueden englobar en una categoría u otra. Principales géneros Distopías Ucronías Historia contrafactual Cyberpunk Postciberpunk Ciencia ficción militar Space opera Romance planetario Espada y planeta Space western Steampunk Utopía Temas frecuentes Artículo principal: Recursos de la ciencia ficción En la ciencia ficción se tratan una gran cantidad de temas. Algunos de ellos son: Los posibles inventos o descubrimientos científicos y técnicos futuros (tecnología de ficción) y los avances en campos como la biotecnología, nanotecnología, biónica, etc. Ingeniería genética y clonación. Futuro utópico, distópico o apocalíptico. Ucronía. Viajes en el tiempo. Vida extraterrestre. Exploración y colonización del espacio exterior. Viaje por el espacio interestelar y el espacio intergaláctico. Inteligencia artificial y robótica. Ordenadores o computadoras y redes informáticas. Solipsismo. Aportes de la ciencia ficción a la ciencia Sello soviético, perteneciente a una serie de 1967 dedicada a la ciencia ficción. El texto dice: «En la luna. Ciencia ficción espacial». De igual manera que la ciencia ficción ha tomado muchos de sus argumentos y elementos de ambientación de conceptos o creaciones de la ciencia, esta ha tomado en ocasiones elementos de la literatura de ciencia ficción para convertirlos en conceptos reales o hipótesis de trabajo de cara al futuro científico o tecnológico. Los casos más conocidos de esta transferencia son los del término robot empleado por primera vez por el escritor checo Karel Čapek -el cual deriva de la palabra «robota», que en su idioma significa «trabajo duro y pesado»; dado que se entendía por estos como máquinas específicas para realizar estas funciones- en su obra R.U.R. (Robots Universales de Rossum), el término derivado robótica, creado en las novelas de robots de Isaac Asimov, el ascensor espacial, imaginado por Arthur C. Clarke y Charles Sheffield de manera independiente, o el concepto de órbita geoestacionaria, desarrollado por Herman Potočnik y posteriormente por Arthur C. Clarke. Es por ello que también se conoce como órbita de Clarke. Otros conceptos han sido profusamente desarrollados por la ciencia ficción incluso antes de ser tenidos en cuenta por la ciencia. Por ejemplo, Julio Verne en De la Tierra a la Luna (1865) describió cómo tres hombres son lanzados desde Florida hacia la Luna. De ese mismo punto partieron los astronautas del Apolo 11 cien años después. En The world set free (El mundo liberado, 1914), H.G. Wells predijo la energía nuclear y la utilización de la bomba atómica en una futura guerra con Alemania.  Y en la novela Ralph 124C 41+ (1911), Hugo Gernsback describió detalladamente el radar antes de haber sido inventado. La ciencia ficción también ha especulado sobre la antimateria, los agujeros de gusano o la nanotecnología antes que la propia ciencia. Algunos conceptos han tenido una notable influencia, a pesar de no ser en la actualidad más que meras invenciones de la imaginación. Por ejemplo, la psicohistoria de Asimov ha influido levemente en la forma de ver la sociología desde un punto de vista matemático. Finalmente, y de modo sorprendente, algunas invenciones de la ciencia ficción han inspirado alguna de las líneas de investigación actual, como la comunicación instantánea (ansible, taquiones). Terminología Artículo principal: Terminología de la ciencia ficción Dentro de la terminología de la ciencia ficción, existen palabras que resultan comunes para los lectores asiduos del género pero no para los nuevos lectores. Sin embargo, no están creadas como una forma de lenguaje identificativo, sino que la mayor parte de las veces son ideas y conceptos interesantes que se han convertido en dominio público, dentro del género e incluso fuera de él, en el mundo de la ciencia. Estos términos son muy usados dentro de los relatos y novelas de ciencia ficción. Como ejemplo tenemos el hiperespacio, que es una especie de «espacio alternativo» por el que se puede viajar de un punto a otro; las sociedades o mentes colmena, que son sistemas con inteligencia compuestos por la mente de muchos seres y no solo de uno, etcétera."
ksampletext_wikipedia_fict_literaturafantastica: str = "Literatura fantástica. El término novela fantástica alude a un género narrativo basado sobre todo en los elementos de fantasía, dentro del cual se pueden agrupar varios subgéneros, entre los que están la literatura de terror, ciencia ficción o la literatura gótica. El término es enormemente confuso, debido a la divergencia de criterios respecto a su aplicación. Se conoce como literatura fantástica a cualquier relato en que participan fenómenos sobrenaturales y extraordinarios, como la magia o la intervención de criaturas inexistentes. Esta definición resulta ineficaz, debido a que los elementos sobrenaturales están presentes en todos los relatos mitológicos y religiosos y su presencia tiene un carácter muy distinto del que posee en la civilización actual. Definición Según Maupassant Mary Shelley, autora de Frankenstein. Guy de Maupassant (1850-1893) realizó una suerte de esbozo de lo que luego sería la definición de Todorov. Maupassant distinguió lo fantástico de otras dos formas parecidas que son lo maravilloso y lo insólito, definiendo más bien las propiedades del primero por oposición al fantástico que las del segundo. La diferencia radicaría en que el cuento de hadas (prototipo de lo maravilloso para el escritor) permite racionalizar los elementos sobrenaturales mientras que el verdadero fantástico permanece en una zona de ambivalencia entre respuestas netamente racionales y respuestas sobrenaturales explicadas al lector. Maupassant también insistió en la importancia del temor en la identificación del relato fantástico, miedo que deviene de la inseguridad a la que es arrastrado el lector. Todorov, por el contrario considera que «El temor se relaciona a menudo con lo fantástico, pero no es una condición necesaria de su existencia». Según Todorov En la ya clásica Introducción a la literatura fantástica, Tzvetan Todorov definió lo fantástico como un momento de duda de un personaje de ficción y del lector implícito de un texto, compartido empáticamente. Los límites de la ficción fantástica estarían marcados, entonces, por el amplio espacio de lo maravilloso, en donde se descarta el funcionamiento racional del mundo y lo extraño o el fantástico explicado, en el que los elementos perturbadores son reducidos a meros eventos infrecuentes pero explicables. Contra la definición amplia de lo fantástico, esta definición presenta la debilidad de ser demasiado restrictiva. Se han propuesto diferentes reformulaciones teóricas que intentan rescatar el núcleo de esta definición con diversas salvedades. Otras definiciones Dracula, de Bram Stoker. Otra definición posible con criterios históricos sostiene que la literatura fantástica se define en el seno de una cultura laica, que no atribuye un origen divino y por tanto sobrenatural a los fenómenos conocidos, sino que persigue una explicación racional y científica. En esta situación, el relato fantástico introduce un elemento sobrenatural, discordante con el orden natural, que produce inquietud en el lector. El elemento sobrenatural no solo sorprende y atemoriza por ser desconocido, sino que abre una fisura en todo el sistema epistemológico de su mundo, susceptible de dar cabida a toda clase de sucesos insólitos y monstruosos. Por otro lado, la crítica literaria argentina Ana María Barrenechea sostiene que, la literatura fantástica ofrece acontecimientos que van de lo cotidiano hasta lo anormal. Estos son presentados en forma problemática para los personajes, para el narrador y para el lector. También menciona la aparición de criaturas y elementos de fantasía y extraordinarios. En ocasiones, este género nos ofrece un relato basado en hechos insólitos que al analizarlos se escapan de la realidad, sin embargo, más adelante de la historia, dichos sucesos tienen una explicación lógica o científica, pero esto no siempre sucede y algunas veces el relato concluye sin salirse de la irracionalidad. La literatura fantástica, puede también presentarnos un objeto o personaje tomado de la realidad, realizando acciones que en un entorno real serían descabelladas o imposibles. Subdivisiones del género fantástico Para entender las variedades de los relatos de este género Tzvetan Todorov nos aporta que la Literatura Fantástica puede situarse en el límite de otros géneros, como pueden ser los siguientes: Lo extraño puro, donde se relatan acontecimientos que pueden ser explicados por medio de las leyes de la razón y son, de una manera u otra, increíbles, extraordinarios, chocantes, singulares, inquietantes, insólitos y provocan en el lector real y en el personaje una reacción semejante a la inducida por el texto fantástico puro. La explicación racional no parte directamente del texto, sino que el lector real, por medio de indicios que este ofrece, la obtiene. La pura literatura de horror pertenece a este género y se relaciona con lo fantástico puro en el hecho de que posee descripciones que provocan horror, temor o terror. Ejemplo: Los diez negritos de Agatha Christie. Lo fantástico extraño, donde los acontecimientos que, a lo largo del relato parecen sobrenaturales, reciben, finalmente, una explicación racional. La explicación parte del mismo texto y no de suposiciones deducidas por el lector a través de indicios. Ejemplo: Manuscrito encontrado en Zaragoza de Jan Potocki. Lo fantástico maravilloso, que es la clase de relato es el que más se acerca a lo fantástico puro por el hecho de quedar inexplicado, no racionalizado, nos sugiere en efecto la existencia de lo sobrenatural; sin embargo, la presencia o ausencia de ciertos detalles permiten siempre tomar una decisión. El hecho fantástico tiene resolución por medio de otro hecho fantástico que en vez de resolver el misterio lo que hace es complicar más lo inexplicable. Ejemplo: La muerta enamorada de Théophile Gautier. Lo maravilloso puro, en este caso, los elementos sobrenaturales no provocan ninguna reacción particular en los personajes ni en el lector implícito, pero sí en el lector real. Lo característico de lo maravilloso no es una actitud hacia los acontecimientos relatados, sino la naturaleza misma de esos acontecimientos. Se acostumbra a relacionar el género de lo maravilloso con el cuento de hadas; en realidad, el cuento de hadas no es más que una de las variedades de lo maravilloso y los acontecimientos sobrenaturales no provocan en él sorpresa alguna. Por su parte, Italo Calvino ha propuesto una subdivisión del género fantástico en fantástico visionario, con elementos sobrenaturales como fantasmas y monstruos (que incluye como subgéneros a la ciencia ficción, el terror, o la narrativa gótica) y el fantástico mental (o cotidiano), donde lo sobrenatural se realiza todo en la dimensión interior (cabe pensar, por ejemplo, en La vuelta de tuerca de Henry James, o a Marcovaldo del propio Calvino). Historia J. R. R. Tolkien. Inicios En sentido amplio puede hablarse de literatura fantástica o de fantasía desde los comienzos del hombre, en que se recitaban versos propiciatorios de carácter sagrado o épico, para pedir la benevolencia de los dioses o celebrar las gestas de los guerreros. En la literatura moderna se considera que comenzó con los cuentos de hadas y la fábula, géneros nacidos para aumentar la fantasía de los adultos más que la de los niños, aunque ahora se asocien más a la infancia. Las obras El castillo de Otranto, escrita por Horace Walpole en 1764, y El diablo enamorado, escrita por Jacques Cazotte en 1772, están consideradas como las primeras novelas fantásticas. Algunos autores románticos, como E.T.A. Hoffmann y Edgar Allan Poe, cultivaron el género, otorgándoles a sus relatos fantásticos un cariz de terror psicológico que habría de presagiar en cierto grado el descubrimiento del inconsciente (Freud se inspira en un relato de Hoffmann para su definición de lo siniestro) y la concepción contemporánea de la mente como creadora de realidad, dotándola de elementos fantásticos. Otros hitos en la historia de la literatura fantástica son Frankenstein o el moderno Prometeo (Mary Shelley, 1818), Drácula (Bram Stoker, 1897) o El extraño caso del doctor Jekyll y míster Hyde (R. L. Stevenson, 1886) El agotamiento del género: lo neofantástico y lo maravilloso Arturo Uslar Pietri, quien acuñaría el término realismo mágico para referirse a la negación poética de la realidad, en la cuentística venezolana de principios del siglo XX. Durante la transición del siglo XIX al siglo XX, el paradigma epistemológico de Occidente sufre diversas sacudidas. Su inflexible orden racional se ve sacudido desde todos los campos del saber: las ciencias humanas (Marx), la filosofía (Nietzsche), la psicología (Freud) e incluso la física (Einstein). La revolución que supone la relativización de todo el conocimiento acumulado durante siglos es recogida desde el arte dinamitando todos los presupuestos históricos, incluido el propio concepto de realidad. De este modo, un suceso sobrenatural ya no puede amenazar un orden inconsistente. Los escritores reaccionan de dos maneras: regresando a la literatura mitológica (H.P. Lovecraft, Lord Dunsany) o introduciendo el fenómeno sobrenatural ya no como un inquietante misterio, sino como un elemento integrado con naturalidad en el mundo. Así, La Metamorfosis de Kafka empieza presentándonos a su protagonista como un insecto, sin que esto merezca ninguna explicación por parte del narrador ni haga tambalear la visión del mundo de ninguno de los personajes de la historia. Jorge Luis Borges fotografiado por Eduardo Comesaña en 1971. El surgimiento de las primeras vanguardias del siglo XX trae consigo un nuevo interés en lo fantástico y en particular en dos corrientes narrativas y estéticas: en primer lugar, aquella que se relaciona también con lo que Arturo Uslar Pietri denominó realismo mágico y Alejo Carpentier real maravilloso, que tiene que ver con un nuevo entendimiento de la realidad indígena, negra y mestiza de América Latina, en el que lo sobrenatural carece de elemento de asombro.  En esta época surgen tres obras precursoras del género: Leyendas de Guatemala (1930) de Miguel Ángel Asturias, Las lanzas coloradas (1931) de Uslar Pietri y ¡Ecué-Yamba-O! (1933) de Carpentier. Esta estética fue denominador común de muchos de los escritores del boom hispanoamericano como Gabriel García Márquez, Elena Garro o Carlos Fuentes. Y en segundo lugar, una literatura fantástica más ligada a lo raro, lo metaficcional, la ficción conceptual, la ciencia ficción que comienza con autores como Pablo Palacio, Julio Garmendia (La tienda de muñecos), Felisberto Hernández y posteriormente tendrá como referentes a autores como Macedonio Fernández, Julio Córtazar y por supuesto a Jorge Luis Borges con sus recopilaciones de cuentos conectados por temas comunes, como los sueños, los laberintos, la filosofía, las bibliotecas, los espejos, autores ficticios y mitología europea.  Por su parte, la literatura maravillosa ha creado un público y un sector editorial especializado, gracias al gran éxito de (además del mencionado Lovecraft) Robert E. Howard, J. R. R. Tolkien, C. S. Lewis, J. K. Rowling, Ursula K. LeGuin, Terry Pratchett (quien aborda el género desde la posmoderna perspectiva de la parodia y la metaficción) o George R. R. Martin. Esta literatura se conoce igualmente bajo el nombre de literatura fantástica, si bien, como hemos explicado, esta definición es imprecisa. Literatura fantástica en distintas tradiciones España En España, el género literatura fantástica es menos fuerte que en Latinoamérica debido a factores sociológicos. En el siglo XIX, España vivía en el antiguo régimen; a diferencia de otros países europeos, el capitalismo aún no se desarrollaba plenamente, la clase burguesa era una minoría y las editoriales no estaban consagradas a este género. El grupo social dominante no tenía motivos para cambiar su visión del mundo y abandonar el racionalismo. Entre los precursores peninsulares de este género se encuentra Agustín Pérez-Zaragoza Godínez que en 1831 publicó una colección de novelas góticas llamada Galería fúnebre de espectros y sombras ensangrentadas, o sea el historiador trágico de las catástrofes del linaje humano. Tanto para Agustín como para sus contemporáneos, el terror debe ser algo que provenga del tema y no de la estructura interna del texto. En la segunda mitad del siglo XIX se produce literatura inspirada en el goticismo y los autores reciben influencia del realismo y naturalismo. Algunas obras de este periodo son La Sombra (1870) de Benito Pérez Galdós, El monte de las ánimas (1864) de Gustavo Adolfo Bécquer o Vampiro (1901) de Emilia Pardo Bazán y Pedro Antonio de Alarcón con El amigo de la muerte (1852) y su cuento La mujer alta (1881). Algunos textos de la época realista y naturalista que figuran en el modelo de Todorov son La muerte de Capeto (Memorias de un patriota) de Vicente Blasco Ibáñez (1888) o La santa de Karnar de Emilia Pardo Bazán (1891). La característica principal de estos textos es que ante la incertidumbre responden con soluciones oníricas. En el siglo XX surge un sentimiento de sensibilidad como respuesta a la profunda crisis en la sociedad. Se rechaza a la razón y aumenta el interés por el inconsciente, los sueños y la imaginación. La teoría de la relatividad suscita una crisis en el mundo de las ciencias exactas. Estos factores, provocan un cambio en la estructura de la literatura fantástica. Una obra que caracteriza este periodo es Los caprichos de Ramón Gómez de la Serna. Las primeras décadas del siglo XX están marcadas por el realismo social pero, a partir de los años sesenta gracias a la literatura de Latinoamérica y a la traducción de obras como La metamorfosis de Kafka, el género fantástico sufre un nuevo impulso. Algunas obras de este periodo son El Hotel del Cisne de Pío Baroja e Industrias y andanzas de Alfanhuí de Rafael Sánchez Ferlosio. Otro exponente de los años sesenta fue Alfonso Sastre con Las noches lúgubres (1963) quien con su realismo crítico se acercó a la literatura fantástica. Alfonso Sastre rompe con el modelo de Todorov porque al final de sus relatos no queda incertidumbre y logra dar una explicación racional. En los años setenta gracias a la literatura experimentalista, a la metatextualidad y la narratividad de los textos surgen diversos textos fantásticos. En 1978, Carmen Martín Gaite publicó una novela llamada El cuarto de atrás. En dicho texto se discute la tesis de Todorov por medio de un metarrelato. En las últimas décadas las obras literarias cobran diversos matices, en ocasiones por la mezcla de elementos de otros géneros. Así surgen obras que van desde el relato fantástico, el cuento de terror y la fantasía épica hasta la ciencia-ﬁcción y el ciberpunk, sin dejar de lado el creciente movimiento fandom. Como parte de esta nueva corriente de literatura fantástica destacan Laura Gallego, María Zaragoza o Antonio Martín Morales, entre otros muchos. México En México, hay una gran tradición en este género. Las obras de Amparo Dávila, Salvador Elizondo, Emiliano González, Álvaro Uribe, Mario González Suárez, Pablo Soler Frost o Alberto Chimal son sólo algunos ejemplos de la riqueza y buena salud de la que goza la literatura fantástica. También es importante señalar que, a pesar del olvido, desde hace una década se ha venido rescatando la obra de Francisco Tario, quien, antes de Rulfo y Arreola, fue el precursor del género fantástico en México. Ecuador César Dávila Andrade En Ecuador, la literatura fantástica inicia en el siglo XIX con la publicación de varios cuentos. Entre ellos destaca Gaspar Blondín de Juan Montalvo, así como los cuentos de Juan León Mera, compilados en 1904 en el libro Tijeretazos y plumazos, en los que cuenta las aventuras del Dr. Moscorrofio, un científico con semejanzas al Doctor Víctor Frankenstein que, entre otras cosas, crea una máquina para escuchar hablar a las pulgas, hace un trasplante de cabeza y visita el infierno. Además, se publicaría Un viaje a pruebas, en 1894 por Alberto Arias Sánchez y que relata un viaje a la Luna en una nave en forma de cóndor. Sin embargo, los mayores exponentes de este género vendrían después y son Pablo Palacio y César Dávila Andrade. El primero destacaría durante la vanguardia en Ecuador durante las primeras décadas del siglo XX con cuentos como La doble y única mujer como parte del libro Un hombre muerto a puntapiés (1927), cuya protagonista es una mujer siamesa. Esta tradición sería continuada por César Dávila Andrade autor clave dentro de la nueva narrativa ecuatoriana y con importantes libros de cuentos como Abandonados en la tierra (1952), 13 relatos (1955) y Cabeza de gallo (1966). Sin embargo, el cuento más destacado en este género es En la rotación viviente del dodecaedro (1965), que sería publicado de manera separada. Este género sería continuado por su sobrino, Jorge Dávila Vásquez quien sería escritor de cuentos y microrrelatos donde dialoga con la mitología griega, explora la relación entre la imaginación, los relatos orales y la tradición literaria. En la actualidad, Mónica Ojeda y María Fernanda Ampuero exploran lo fantástico a través del horror, la violencia y lo grotesco. Venezuela Artículo principal: Literatura fantástica en Venezuela Julio Garmendia. Venezuela tiene una vasta tradición de literatura fantástica que se remonta a mediados del siglo XIX, con autores como Julio Calcaño, Juan Vicente Camacho, Nicanor Bolet Peraza y Pedro Emilio Coll. Ya entrado el siglo XX, el surgimiento de las vanguardias venezolanas trae consigo un nuevo interés en lo fantástico y lo maravilloso, y eso que luego Uslar Pietri denominó el realismo mágico, con obras como La tienda de muñecos (1927) de Julio Garmendia , Cubagua (1931) y La galera de Tiberio (1933) de Enrique Bernardo Núñez; los primeros cuentos de Arturo Uslar Pietri publicados en 1928: Barrabás y otros relatos y en 1936: Red, así como su primera novela (de corte fantasmagórico, onírico, una especie de trance alucinatorio), publicada en 1931: Las lanzas coloradas; y la poesía de José Antonio Ramos Sucre (publicada en su totalidad entre para 1929).   La literatura de Julio Garmendia sería particularmente importante, pues dejaría atrás la estética del fantástico dieciochesco, propio de la tradición romántica, y exploraría ideas como la paranoia, el doppelgänger, la ucronía, la distopía, la metaficción, el futurismo, la anticipación, la inmortalidad, el pacto demoníaco o la noción cristiana del infierno; poniendo bajo cuestión los avances técnico-científicos y las nociones de realidad y de moral.   Posteriormente, a mediados del siglo XX, irrumpe un interés por lo abstracto, lo experimental, que tendría como máximo ejemplo la narrativa fantástica de Guillermo Meneses (La mano junto al muro, El falso cuaderno de Narciso Espejo). Meneses cambiaría la literatura de Venezuela e influenciaría a autores también incursionarían en el género como José Balza, Oswaldo Trejo, Ida Gramcko y Alfredo Armas Alfonzo.   Algunos exponentes actuales del género son: Ednodio Quintero, Juan Carlos Chirinos (Los cielos de Curumo, Renancen las sombras), Israel Centeno (Criaturas de la Noche), Karina Sainz Brogo (La isla del Doctor Schubert), Michelle Roche Rodríguez (Malasangre), o Norberto José Olivar (Un vampiro en Maracaibo). Argentina Liliana Bodoc En Argentina, existe una vasta tradición de literatura fantástica, que en la actualidad encuentra una de sus vertientes más prolíficas en el género de la épica fantástica; aunque la influencia a nivel latinoamericano del subgénero fantastique es innegable gracias a la trascendencia de autores como Jorge Luis Borges, Silvina Ocampo, Julio Cortázar, Angélica Gorodischer y Adolfo Bioy Casares, por nombrar algunos. Actualmente, Liliana Bodoc, escritora santafecina, se nos ofrece como exponente de este género con su popular Saga de los Confines, compuesta por tres libros: Los días del Venado, Los días de la Sombra y Los días del Fuego. La saga narra hechos mágicos, fantásticos y también, colectivos, en la medida en que exceden el heroísmo individual. Su dimensión colectiva se entrama, a su vez, con una búsqueda de modificar la realidad del mundo, como plantea la literatura épica. Chile En Chile, el género fantástico nunca ha logrado desprenderse por completo del canon realista chileno y, por lo tanto, llegar a ser considerado un género per se dentro de su tradición narrativa; a diferencia de lo que ocurre en Argentina. No obstante, sí ha habido algunos exponentes de este, ya sea mediante algunas novelas o cuentos, que bien permiten plantear la presencia del género en cuestión, al mismo tiempo que se les reconoce como obras en contraste y diálogo permanente con el canon chileno, tal como recoge Andrés Rojas-Murphy en su Antología de cuentos chilenos de ciencia ficción y fantasía. De modo que, las obras de María Luisa Bombal, Elena Aldunate, Augusto D´Halmar, Braulio Arenas, Enrique Araya, Hugo Correa, Miguel Arteche y más recientes como Nefilim en Alhué de Omar Pérez Santiago, Aldo Astete Cuadra, José Baroja, Jesús Diamantino y Jorge Baradit, entre otros, sirven de ejemplo para demostrar la presencia del género en Chile. También se destacan editoriales independientes que han aportado en este género, tales como Áurea Ediciones con autores como Sofía Bertelsen (Ad Infinitum), Vincenzo Guazzini Guevara (Posapocalíptica) o Cindy Cruz (Freya). Ejemplos de literatura fantástica A lo largo de la historia de la literatura fantástica, se han desarrollado grandes obras pasando de lo más clásico hasta lo más actual, algunos ejemplos notables son: La Odisea. Es una fantástica narración épica en la que se refleja el desarrollo religioso, político y cultural de Grecia, además gracias al autor de la Odisea, la épica pasa de una forma de transmisión oral a una escrita. Con este hecho, la literatura de ese tiempo tomo otro rumbo, ya que empezó a transmitirse por medio de las letras. La Eneida. Obra maestra de la literatura latina, realizada por el poeta con la intención de glorificar a Roma, por pedido del emperador Augusto. El tema fundamental de la obra es el esfuerzo, pues en la obra se nota claramente el empeño o esfuerzo que ponía cada personaje para alcanzar las metas que se habían propuesto realizar de una forma u otra. El Señor de los Anillos. Esta obra ha influido de tal manera en toda la literatura fantástica posterior que podría considerarse la madre de todas las sagas de fantasía del siglo XX. El Señor de los Anillos no es solo una novela con personajes y lugares de fantasía, sino un universo entero con su geografía, lenguas, razas e historias propias. Tolkien desarrolló ese mundo mucho más de lo que se deja entrever en sus novelas, estableciendo las bases para la literatura fantástica de los años venideros. La Rueda del Tiempo. La historia de La Rueda del Tiempo está ambientada en un mundo fantástico ambientado a finales del siglo XVII. En las más de catorce novelas con las que cuenta la saga (veinte en la edición española) tienen lugar innumerables tramas diferentes y muchos personajes, que se basaron en elementos mitológicos europeos y asiáticos."
ksampletext_wikipedia_fict_shingekinokyojin: str = "Shingeki no Kyojin. Shingeki no Kyojin («Titán de ataque»), también conocida como Ataque a los titanes en España y Ataque de los titanes en Hispanoamérica, es una serie de manga japonesa escrita e ilustrada por Hajime Isayama. El manga se publicó en septiembre de 2009 en la revista Bessatsu Shōnen Magazine de la editorial Kōdansha y fue difundida de forma mensual hasta abril de 2021 con un total de 139 capítulos y después de doce años de publicación mensual. El manga es distribuido en España por Norma Editorial, en Argentina por Ovni Press y en el resto de Hispanoamérica por Panini. La historia gira en torno a Eren Jaeger, quien vive en un mundo ficticio donde la humanidad está al borde de la extinción a causa de unas criaturas humanoides llamadas «titanes», lo que obliga a los supervivientes a refugiarse en tres enormes murallas para protegerse. Eren, quien pierde a su madre a manos de los titanes, decide junto con sus amigos de la infancia unirse al ejército, con el objetivo de vengarse y destruir a dichos monstruos. Más adelante, descubre que posee la habilidad de transformarse en un titán con características especiales, lo que desencadenará toda una serie de acontecimientos que tendrán repercusión tanto en Eren como en quienes lo rodean. La obra fue adaptada a una serie de anime dirigida por Tetsurō Araki y producida por Wit Studio, en colaboración con Production I.G. Fue emitida por la cadena televisiva Mainichi Broadcasting System (MBS) [n. 2] desde abril a septiembre de 2013 con un total de veinticinco episodios. Cuatro años después, se transmitió la segunda temporada de doce episodios en 2017. La tercera temporada se emitió en dos partes: la primera parte entre junio y octubre de 2018 y la segunda entre mayo y junio de 2019. En 2020 se anunció la producción de la cuarta y última temporada, la cual estuvo a cargo del estudio MAPPA, dividida en tres partes: la primera parte se emitió entre diciembre de 2020 hasta marzo de 2021; la segunda desde enero hasta abril de 2022; finalmente la tercera y última parte se dividió en dos segmentos, a manera de especiales, los cuales se emitieron el 3 de marzo y el 4 de noviembre del 2023, respectivamente. Desde su publicación, Shingeki no Kyojin se convirtió en un éxito comercial tanto en Japón como en el resto del mundo. Alcanzó las cien millones de unidades vendidas a finales de 2019, mientras que su adaptación al anime obtuvo altos números de audiencia en todo el mundo. Tanto el manga como el anime recibieron diversos premios y reconocimientos. También se realizaron productos derivados como novelas ligeras, videojuegos, OVAS, dos películas de imagen real, parodias y una producción a cargo de Warner Bros aún sin confirmar fecha de lanzamiento. Argumento La historia está ambientada en la isla «Paradis», ubicada al noreste del país de «Marley», en donde Eren Jaeger vive con su familia; con su hermana adoptiva, Mikasa Ackerman; y con su mejor amigo, Armin Arlert. Su pueblo colinda con la «muralla María», la más externa del Reino de las «tres murallas». Estas enormes murallas fueron construidas cien años atrás, con el fin de protegerse de los «titanes», criaturas de entre tres y sesenta metros de alto que casi aniquilaron a la humanidad. Además de su gran tamaño, los titanes se caracterizan por su estructura corporal parecida a la de los humanos, a quienes devoran de forma instintiva, y la única manera conocida de matarlos es cortándoles la nuca. La población humana ha sido capaz de vivir en una paz incómoda dentro de las murallas hasta el día en que aparece un titán de sesenta metros, que puede asomar la cabeza por la muralla María y que de una patada logra crear una grieta en la misma. Esto permite la entrada de titanes más pequeños a la ciudad y provoca la evacuación masiva de toda la población superviviente hacia el interior de la muralla intermedia, «Rose». Durante dicho ataque, Eren observa aterrado cómo su madre, Carla, es devorada por un titán mientras que su padre, Grisha, ha desaparecido de forma extraña, lo que lo deja con Mikasa y Armin como sus únicos compañeros. Eren jura vengar la muerte de su madre y la pérdida de su ciudad, por lo que se enlista en el «Ejército de las Murallas» junto con sus amigos. Cinco años después, los tres jóvenes se gradúan de cadetes en el «Distrito de Trost» (トロスト区 Torosuto-ku?), situado a las afueras de la muralla Rose. Un día, frente a Eren y sus compañeros del ejército aparece el mismo titán gigantesco que devastó su hogar, ahora dispuesto a destruir la muralla Rose, y la pesadilla vuelve a repetirse. En medio del combate, Eren es devorado por un titán mientras rescata la vida de Armin. En otro lugar, Mikasa lucha por sobrevivir hasta que es salvada por un titán musculoso y atípico que ayuda a los humanos a ganar la batalla en Trost, solo para descubrir que es Eren quien se transformó en ese titán, para sorpresa de todos. Algunos, como los nobles, el clero y la Policía Militar, lo consideran una amenaza, mientras que la Legión de Exploración ven a esta habilidad como una oportunidad de salvar a la humanidad y vencer a los titanes. En consecuencia, Eren se somete a un juicio militar, en el que se determina que se una a la Legión de Exploración en el «escuadrón de operaciones especiales» bajo el mando y supervisión directa del capitán Levi. Conforme avanza la historia, se revelan muchos misterios respecto a la habilidad de transformación de Eren, cuyo poder obtuvo gracias a su padre; el motivo por el que lo adquirió; y el hecho de que no es el único humano que posee dicha capacidad. Se descubre también la procedencia y origen de los titanes ,mediante la historia de Ymir Fritz,, la revelación de sus verdaderos enemigos ,como la nación de Marley y el mundo entero, y de las decisiones que Eren deberá tomar respecto al futuro de la humanidad, el de sus amigos y el suyo. Trasfondo La historia de Shingeki no Kyojin transcurre en un mundo alternativo cuyo periodo tiene similitudes con la Edad Media ,aunque con elementos y tecnología de la Revolución Industrial―. Tiempo atrás, unas criaturas gigantes humanoides conocidas como «titanes» (巨人 Kyojin?) aparecieron y casi aniquilaron a toda la humanidad. Los sobrevivientes se refugiaron dentro de tres enormes murallas de más de cincuenta metros de altura. La pared externa se llama «muralla María» (ウォール・マリア Wōru Maria?); la intermedia, «Rose» (ウォール・ローゼ Wōru Rōze?); y la interna, «Sina» (ウォール・シーナ Wōru Shīna?). En el centro de este complejo, diseñado para impedir la exterminación de la civilización humana, se encuentra la capital del reino, «Mitras» (ミットラス王都 Mittorasu-Ōto?). Dentro de los muros, la humanidad vivió una paz incómoda hasta el comienzo de la historia, con la aparición de un titán de sesenta metros de altura que ataca parte de la muralla María, lo que permite la entrada de otros titanes menores, quienes destruyen la ciudad y matan a millares de personas ―incluyendo a la madre de Eren―. Los supervivientes son evacuados al interior de la segunda muralla. Por la pérdida de la Muralla María y parte de su territorio, la población sufre de una escasez de alimentos, hasta el punto en que el gobierno decide sacrificar una parte de los refugiados en una misión suicida contra los titanes. Cinco años después, la Legión de Exploración ―con la ayuda de los poderes de Eren convertido en titán― consiguen detener el avance de los titanes y, finalmente, recuperar la muralla María.  Titanes Diagrama de la organización de las tres murallas. Los titanes (巨人 Kyojin?) son criaturas que se asemejan mucho a los seres humanos, ya que son bípedas, con casi la misma característica de sus miembros y funciones del cuerpo, a excepción de las partes reproductivas. Desde el punto de vista humano, la mayoría cuenta con dimensiones deformes en mayor o menor grado, que van desde anormalidades leves hasta desproporciones notorias. Tienen una estatura de entre tres y sesenta metros de alto, con una estructura corporal parecida a la de los humanos. Aunque aparentan no necesitar comida, instintivamente atacan y comen humanos a primera vista. Funcionan con energía solar, están compuestos de vapor de agua y su único punto débil conocido es la nuca.  Existen dos tipos de titanes: los «normales» y los «cambiantes», estos últimos con poderes y habilidades que potencian su nivel de ataque y resistencia. Los primeros son personas de raza eldiana que fueron convertidos en titanes por la fuerza a través de un suero inyectado por el gobierno de Marley, y no tienen la capacidad de volver a ser humanos por voluntad propia. En cambio, los cambiantes son aquellos eldianos que adquieran uno de los nueve poderes, que les permite transformarse en titanes con capacidades únicas. A diferencia de los titanes normales, poseen una fuerza mayor así como diversas habilidades específicas, y otras de manera general como la capacidad de volver a ser humanos, la regeneración en su cuerpo y la herencia de memorias de los anteriores portadores.[n. 3] La obtención de dicho poder tiene un costo para el portador, ya que desde entonces está sometido a la «maldición de Ymir» (ユミルの呪い Yumiro no Noroi?), que consiste en reducir el tiempo de vida a trece años a partir de la adquisición. Todos cuentan con un único punto débil, ubicado en la nuca. Existen nueve titanes cambiantes: el «titán de ataque» (進撃の巨人 Shingeki no Kyojin?), el «titán acorazado» (鎧の巨人 Yoroi no Kyojin)?), el «titán colosal» (超大型巨人 Chō ōgata Kyojin?), la «titán femenina» (女型の巨人 Megata no Kyojin?), el «titán mandíbula» (顎の巨人 Agito no Kyojin?), el «titán carreta» (車力の巨人 Shariki no Kyojin?), el «titán bestia» (獣の巨人 Kemono no Kyojin?), el «titán martillo de guerra» (戦槌の巨人 Sentsui no Kyojin?) y el «titán fundador» (始祖の巨人 Shiso no Kyojin?). Este último es el más fuerte de todos, pero su poder solo puede ser utilizado por aquellos individuos que pertenezcan a la familia Fritz.  Para combatir a los monstruos, las Fuerzas Armadas de la humanidad se dividen en tres ramas. En primer lugar se encuentra el «Cuerpo de Exploración» (調査兵団 Chōsa Heidan?), también conocida como la «Legión de Reconocimiento», encargados de explorar más allá de las murallas con el objetivo de recuperar territorio. Dicho cuerpo es muy criticado y ridiculizado en la sociedad debido a la gran cantidad de pérdidas humanas que sufre y el poco progreso que logran. La segunda y más grande rama es el «Regimiento de guarnición» (駐屯兵団 Chūton Heidan?), también conocida como las «Tropas estacionarias», que protegen las murallas y a la población civil. La tercera rama es la «Policía militar» (憲兵団 Kenpeidan?), quienes protegen a la familia real y a los ciudadanos de la capital, aunque son vistos como corruptos e incompetentes. Como arma principal, los soldados usan un sistema de amarre y sujeción llamado «Equipo de maniobras tridimensionales» (立体機動装置 Rittai Kidō Sōchi?), que les permite alcanzar grandes alturas utilizando las paredes de las murallas, los árboles o edificios cercanos, con el fin de alcanzar la nuca de los titanes. Están armados con dos espadas en forma de cuchillas y usan gas como combustible. A pesar de ser el arma principal, tanto ofensiva y defensiva, es inútil en terrenos abiertos y planos, como el campo o las llanuras. Cosplayers disfrazados como el «titán de ataque» (izquierda), el «titán colosal» (centro) y el «titán femenino» (derecha), respectivamente. Eldia y Marley Mapa del mundo ficticio de Shingeki no Kyojin. En rojo se observa la isla Paradis, en donde se ubica el reino de las murallas, hogar de Eren y la mayoría de sus compañeros. En morado, se observa el imperio de Marley. Para dicho mundo, Isayama se inspiró en el mundo real, siendo la isla Paradis un símil de la isla de Madagascar, y Marley un símil de África y parte de Europa. Más adelante, se revela que los titanes fueron creados 1820 años antes del inicio de la historia, cuando una joven llamada Ymir Fritz ( ユミル・フリッツ Yumiru Furittsu?), quien en un principio fue una esclava de la tribu de los eldianos, encontró la «fuente de toda vida orgánica» y se convirtió en el primer titán de la historia al obtener el poder del «titán fundador». El jefe de aquella tribu, quien se convirtió en el primer rey Fritz, comenzó a utilizar a los poderes de Ymir ―a quien tomó como concubina― para hacer prosperar a su pueblo y conquistar a las otras naciones. Cuando Ymir murió, sus poderes se dividieron, primero entre sus tres hijas y sucesivamente en nueve partes, una de las cuales es el titán fundador de Ymir, el cual sería aprovechado para extender el territorio eldiano hasta formar un imperio.  La familia Fritz gobernó el mundo conocido hasta la época del 145.º Rey de Eldia llamado Karl Fritz (カール・フリッツ Kāru Furittsu?), quien después de una gran guerra decidió terminar con el imperio, llevando a gran parte de su gente a una isla llamada «Paradis» (パラディ島 Paradi-tō?). Sin embargo, no todos los eldianos pudieron ,o quisieron, irse con él, y muchos se quedaron en el continente, solo para ser sometidos por el recién creado imperio de «Marley» (マーレ Māre?). Los eldianos rezagados fueron enviados a vivir en guetos, al ser considerados personas de segunda clase, y luego utilizados como armas tras ser convertidos en titanes mediante inyecciones a su torrente sanguíneo. De estos eldianos descienden los personajes de Reiner Braun, Bertolt Hoover y Annie Leonhart, quienes se unieron a las Fuerzas Armadas de Marley en la división de los «guerreros» (戦士隊 Senshi-tai?)[n. 4] y heredaron el poder de los titanes cambiantes, o Ymir, quien adquirió su poder titán de otra manera. Una vez que llegó a Paradis, Karl Fritz utilizó el poder del titán fundador, también conocido como «la coordenada» (座標 Zahyō?), y levantó tres grandes murallas con la ayuda de titanes colosales, para después borrar la memoria en la mayoría de sus habitantes. Fritz fundó el «Reino de las Murallas» con el supuesto objetivo de «protegerlos de los males externos»[n. 5] y evitar que abandonaran las murallas al inventar la existencia de titanes en el exterior. Fritz permaneció como rey de las murallas y cambió el apellido de su familia a «Reiss», cuyos descendientes gobernaron desde las sombras con el poder del titán fundador. Durante la caída de Shiganshina, Grisha Jaeger ,un eldiano que escapó de Marley y vivió de incógnito dentro de las murallas, logró robar el poder del titán fundador de los Reiss, para más adelante ,y a costa de su vida, transferirlo a su hijo Eren.  Personajes principales Artículo principal: Lista de personajes de Shingeki no Kyojin Protagonistas Yūki Kaji, seiyū de Eren Jaeger. Yui Ishikawa, seiyū de Mikasa Ackerman. Shiori Mikami, seiyū de Historia Reiss. Ayane Sakura, seiyū de Gabi Braun. Eren Yeager (エレン・イェーガー Eren Yēgā?) Protagonista de la historia e hijo de Grisha y Carla Jaeger. Es una persona testaruda e impulsiva, pero con un fuerte sentido de lealtad y deseo de proteger y cuidar a sus seres queridos. Tras perder a su madre a manos de los titanes, se une al ejército con el único propósito de vengarse, pero cuando descubre sus poderes de titán, decide usarlos en defensa de la humanidad. Se graduó en quinto lugar de la promoción de la Tropa de Reclutas N.º 104 y posteriormente se unió a la Cuerpo de Exploración. Eren posee las habilidades del titán de ataque, el titán fundador ―ambos adquiridos de su padre― y el titán martillo de guerra, que obtuvo en el ataque a Marley. En la adaptación japonesa su seiyū es Yūki Kaji, mientras que en el doblaje español es Jaume Aguiló y en el hispanoamericano es Miguel Ángel Leal.  Mikasa Ackerman (ミカサ・アッカーマン Mikasa Akkāman?) Es la hermana adoptiva de Eren, con quien mantiene un fuerte vínculo desde que la salvó de unos secuestradores que mataron a sus padres y pretendían venderla como esclava, lo que contribuyó a un cambio radical en su personalidad, al volverse más reservada y menos alegre. Después de la caída de Shiganshina decidió, junto con Eren y Armin, unirse al ejército, donde se graduó primera en la Tropa de Reclutas N.º 104 debido a sus grandes habilidades. Posteriormente, se unió al Cuerpo de Exploración para combatir a los titanes con el objetivo de proteger a su hermano adoptivo. Más adelante, participó en el ataque a Marley, pero se muestra decepcionada por el cambio de actitud de Eren. En la adaptación japonesa su seiyū es Yui Ishikawa, en el doblaje español es Laura Prats, mientras que en Hispanoamérica es interpretada por Ana Lobo. Armin Arlert (アルミン・アルレルト Arumin Arureruto?) Es un amigo de la infancia de Eren y de Mikasa. Físicamente es más débil que el resto de sus compañeros, pero demuestra una gran inteligencia a través de su capacidad estratégica y soluciones rápidas cuando la situación lo amerita. Se graduó en la Tropa de Reclutas N.º 104 y más tarde se unió al Cuerpo de Exploración. Durante la batalla para retomar la muralla María, casi muere al ser quemado con el vapor emitido por Bertoldt Hoover en su forma de titán colosal. Para salvarlo, se le inyectó un suero que lo transformó en titán común y terminó por devorar con vida a Bertoldt, por lo que se convirtió en el nuevo titán colosal. Más adelante participó en el ataque a Marley, a partir de lo que empieza cuestionar las acciones de Eren. En la adaptación japonesa su seiyū es Marina Inoue, mientras que en el doblaje español es Marc Gómez y el Hispanoamericano es Héctor Ireta de Alba.  Christa Lenz (Historia Reiss) (クリスタ・レンズ (ヒストリア・レイス) Kurisuta Renzu (Hisutoria Reisu)?) Es la hija ilegítima del noble Rod Reiss y única miembro de la familia real Reiss que sigue con vida. Se graduó décima en la Tropa de Reclutas N.º 104 y posteriormente se unió a la Legión de Exploración. Es una chica amable, dispuesta a hacer una buena acción sin importar el precio, aunque demuestra ser tímida y no saber cómo reaccionar en ciertas situaciones. Con la ayuda de sus amigos, quienes orquestan un golpe de Estado en el reino, se convierte en la nueva gobernante de las murallas, en donde gobierna con justicia y en favor de los más necesitados. En la adaptación japonesa su seiyū es Shiori Mikami, mientras que en el doblaje español es Mireia Fontich y en Hispanoamérica es Cristina Hernández.  Levi Ackerman ( リーバイ　アッか－マン　Ribai Akkaman) Es el capitán de la Legión de Exploración y se lo considera «el soldado más fuerte de la humanidad», debido a sus grandes habilidades para matar titanes. Siente un gran respeto por la disciplina y tiene carácter firme, ideas claras y personalidad seria, además de gran fidelidad y respeto a su comandante, Erwin Smith. Antes de unirse a la Legión de Exploración, era un delincuente famoso en la zona subterránea de la capital, pero fue rescatado por su tío, Kenny, quien lo entrenó y le enseñó a sobrevivir en ese mundo tan cruel. En la adaptación japonesa su seiyū es Hiroshi Kamiya, mientras que en el doblaje español es Héctor García y en el Hispanoamericano es Alfredo Gabriel Basurto.  Hange Zoë (ハンジ・ゾエ Hanji Zoe?) Fue la 14.º comandante de la Legión de Exploración, quien sucedió a Erwin Smith después que este muriera en la batalla por recuperar la muralla María. Es una persona muy apasionada por su trabajo, el cual consiste en investigar a los titanes, además de tener una personalidad hiperactiva y curiosa, para irritación de Levi. También, es una experta combatiente capaz de vencer a un titán sin mayores problemas, además de poseer grandes habilidades de análisis durante situaciones difíciles que le valieron su puesto de líder de la Legión. Sin embargo, después del ataque a Marley, se considera incapaz de entender las acciones de Eren. En la adaptación japonesa su seiyū es Romi Park, en el doblaje español Pepa Pallares, mientras que en Hispanoamérica es Rossy Aguirre. Falco Grice (ファルコ・グライス Faruko Guraisu?) Es un eldiano que vivía en la zona de internamiento de Liberio quien, junto con Gabi Braun, protagoniza la temporada final del anime. Al igual que su hermano mayor, Colt, es un candidato a guerrero que busca adquirir el titán acorazado de Reiner. Es un joven amable y reflexivo que está enamorado de Gabi, a quien busca proteger de la maldición de Ymir. Fue indirectamente responsable del ataque de Eren a Marley, al ser engañado por este. En la adaptación japonesa su seiyū es Natsuki Hanae, y en el doblaje hispanoamericano es Diego Becerril. Gabi Braun (ガビ・ブラウン Gabi Buraun?) Es una eldiana que vivía en la zona de internamiento de Liberio en Marley, prima de Reiner Braun. Es protagonista de la temporada final del anime junto con Falco Grice, quien está enamorado de ella. Como candidata a guerrera, posee una personalidad explosiva, un fuerte sentido de patriotismo y, gracias a la propaganda de Marley, un odio virulento hacia los eldianos de la isla Paradis. Sobrevive al ataque de Liberio realizado por Eren y es capturada junto con Falco después de matar a Sasha Braus. Una vez que llega a Paradis, empieza a cuestionar su forma de pensar. En la adaptación japonesa su seiyū es Ayane Sakura, y en el doblaje hispanoamericano es Danann Galván. Antagonistas Reiner Braun (ライナー・ブラウン Rainā Buraun?) Nacido y criado en Liberio, ubicado en Marley, es hijo ilegítimo de una mujer eldiana y un padre marleyano. Reiner era un joven tranquilo pero al mismo tiempo muy impulsivo, y se unió al programa de guerreros con la esperanza de que su padre pudiese vivir con él y su madre. En el año 845, se infiltró en la isla Paradis junto con Annie Leonhart, Bertolt Hoover y Marcel Galliard, con el objetivo de recuperar al titán fundador y devolverlo a Marley. Se graduó segundo en la promoción de la Tropa de Reclutas N.º 104 y posteriormente se unió a la Legión de Reconocimiento. Durante el transcurso del manga, sufre una crisis de identidad debido a sus acciones pasadas. En la adaptación japonesa su seiyū es Yoshimasa Hosoya, mientras que en el doblaje al español Víctor Velascos y en el hispanoamericano es Alfonso Obregón.  Annie Leonhart (アニ・レオンハート Ani Reonhāto?) Nacida en Liberio, es una muchacha de baja estatura y buena condición física. Se le considera una chica solitaria con poco o nulo deseo de socializar con los demás. Se graduó en cuarto lugar en la Tropa de Reclutas N.º 104 y fue la única de su promoción que optó por unirse a la Policía Militar. Annie destaca por su destreza en el uso del equipo de maniobras tridimensionales y por su combate cuerpo a cuerpo, lo que deja fascinado a Eren. Al igual que Reiner y Bertholdt, es un titán cambiante que usa sus habilidades de lucha para vencer a todo oponente. En la adaptación japonesa su seiyū es Shimamura Yu, en el doblaje español es Marta Moreno, mientras que en Hispanoamérica, Georgina Sánchez le pone su voz. Bertholdt Hoover (ベルトルト・フーバー Berutoruto Fūbā?) Nació en el país Marley y era amigo de la infancia y compañero de Reiner, quien lo acompañaba en cada misión. Era tranquilo, tímido y reservado. Carente de iniciativa propia, se consideraba a sí mismo un «cobarde». Se graduó tercero de su promoción en la Tropa de Reclutas N.º 104 y posteriormente se unió a la Legión de Exploración. Junto con Reiner y Annie, poseía la habilidad de transformarse en un titán cambiante. Durante la batalla para recuperar la Muralla María, es vencido y devorado por Armin Arlet, quien adquiere su poder. En la adaptación japonesa su seiyū es Tomohisa Hashizume, mientras que en el doblaje español es Pau Ferrer y en el hispanoamericano es Yamil Atala.  Kenny Ackerman (ケニー・アッカーマン隊長 Kenii Akkaaman?) Antiguo asesino y tío de Levi, a quien cuidó durante su niñez después de que su madre muriera. Fue capitán del Escuadrón de Supresión Anti-Humanos de la división Central de la Policía militar y, junto con Rod Reiss, ejerció de antagonista en la tercera temporada. Fingió ser la mano derecha de Reiss cuando su verdadero objetivo era devorar a Eren y obtener sus poderes. En la adaptación japonesa su seiyū es Kazuhiro Yamaji, mientras que en el doblaje español es Josep Manel Casany y en el hispanoamericano es Gerardo Reyero.  Zeke Jaeger (ジーク・イェーガー Jiiku Yēgā?) Es el Jefe de Guerra (戦士帳 Senshi-chō?) de Marley y líder de los guerreros eldianos, que sirven al gobierno de ese país con el propósito de recuperar la «Coordenada» y eliminar a los eldianos que habitan dentro de las murallas. Durante la mayor parte de su vida pareció mostrar una completa lealtad a Marley, hasta el punto de entregar a sus padres a las autoridades de Seguridad Pública luego de que estos lo infiltraran en el programa de guerreros con el propósito de obtener al titán fundador para los restauradores de Eldia. Sin embargo, su verdadero objetivo es diferente y se muestra dispuesto a traicionar a Marley para cumplirlo. Tiene un carácter frío, cruel, despiadado y sarcástico, aunque no necesariamente sádico. En la adaptación japonesa su seiyū es Takehito Koyasu, en el doblaje español es Germán Guijón del Valle y en el hispanoamericano es Ricardo Brust.  Rod Reiss (ロッド・レイス Roddo Reisu?) Fue el padre de Historia Reiss, gobernante de facto del reino de las murallas ―en donde ejercía toda la influencia y poder político desde las sombras― y principal antagonista de la tercera temporada del anime. Su objetivo era recuperar el poder del titán fundador robado por Grisha Jaeger, el cual se encontraba dentro de Eren. Para ello, intentó convencer a su hija Historia para que devorara a Eren y recuperara dicho poder para su familia. Tras fallar en su objetivo, se transformó en un titán gigantesco de 120 metros, aunque finalmente fue vencido y asesinado por su hija. En la adaptación japonesa su seiyū es Yusaku Yara, en el doblaje español es Edu Borja y en el hispanoamericano es Pedro DAguillón Jr.  Producción Origen Hajime Isayama, creador de Shingeki no Kyojin. En 2006, Hajime Isayama escribió un one-shot de 65 páginas cuyo contenido difiere con la obra final en algunos aspectos, como el origen de los titanes y las características de los personajes. En una entrevista realizada con el medio japonés NHK en 2018, reveló que su fuente de inspiración provino de su ciudad natal, Hita (prefectura de Ōita), que está bordeada por murallas. En su niñez, el autor fantaseaba con la posibilidad de que hubiera monstruos más allá de las murallas. También, comentó que incluyó a los titanes porque los consideraba criaturas «repulsivas». En un principio presentó su obra a la Shūkan Shōnen Jump de Shueisha, pero estos consideraron que no se ajustaba a sus lineamientos editoriales y, para ser aprobada por la editorial, le sugirieron modificar la historia. El autor rechazó la propuesta y buscó otra editorial, hasta que logró publicarla en la revista Weekly Shōnen en Kodansha. Antes de que el manga comenzara a publicarse en 2009, ya había pensado los giros que se desarrollarían en el transcurso de la serie. En una entrevista a la revista The Asahi Shinbun, Isayama declaró que, mientras trabajaba en un Cibercafé, se encontró con un cliente borracho que lo agarró por el cuello. Este incidente le mostró al mangaka sobre «el miedo de conocer a una persona con la que no me puedo comunicar», misma sensación que trasmiten los titanes. Isayama también mencionó que calculaba su línea de tiempo mensual: una semana para el guion gráfico y tres semanas para ilustrar el capítulo. Planifica la historia de antemano e incluso marca en cuál de los volúmenes recopilados se revelará alguna «verdad». En septiembre de 2013, señaló que su objetivo era terminar la serie en 20 volúmenes. Inicialmente planteó dar a la historia una conclusión trágica similar a la adaptación cinematográfica de La niebla, donde cada personaje muere, pero descartó la posibilidad debido a la respuesta positiva de los seguidores y decidió cambiar el final para no decepcionar a la audiencia. Influencias En la imagen se observa al luchador profesional Brock Lesnar, en el cual Isayama se inspiró para diseñar al «titán acorazado». La ciudad Nördlingen, ubicada en Alemania, inspiró a Hajime Isayama para crear la ambientación del manga. Isayama comentó que, al momento de diseñar la apariencia física de los titanes, se basó en la constitución física de algunas personas. Utilizó varios modelos, como el artista marcial Yushin Okami para la forma de titán de Eren Jeager y en Brock Lesnar para el titán blindado de Reiner Braun. George Wada, productor del anime, declaró que el «muro del miedo» (en referencia a los tres muros) estaba influenciado por «la naturaleza aislada y cerrada de la cultura japonesa». El autor mencionó que el mundo de Shingeki no Kyojin está inspirado en parte de la novela visual Muv-Luv Alternative, enfocada en seres alienígenas que invaden la Tierra, y en Project ARMS, por el formato y estilo del manga. Para el desarrollo de los personajes, Isayama tomó referencias de películas como Watchmen, Jurassic Park y Saving Private Ryan, en la saga de novelas ligeras de Zero no Tsukaima y en el manga Berserk. También involucró videojuegos como Mega Man Legends y Monster Hunter y películas de monstruos protagonizadas por Godzilla, Mothra y Gamera. Algunos analistas observaron que la estética de la obra podría haberse visto influenciada por el pintor Francisco de Goya.  En un episodio de Discovery of the Worlds Mysteries, programa documental de la cadena televisiva TBS, se mostró que la arquitectura de Nördlingen es muy similar a los muros que protegen a la humanidad en la serie, porque esta ciudad alemana se encuentra rodeada por una muralla y conserva un aspecto medieval. También se puede mencionar al Midgard de la mitología nórdica y al Tártaro (una prisión rodeada por una triple pared en la que los titanes están encarcelados) de la mitología griega como posibles fuentes de inspiración. Para la geografía del mundo de Shingeki no Kyojin, Isayama se basó en territorios del mundo real y utilizó un mapamundi invertido para las localizaciones; por ejemplo, representó a «Marley» con el continente de África y parte de Europa, mientras que para la isla «Paradis» se guio en Madagascar. Análisis y controversias (Izquierda) Imagen de la estrella de nueve puntas, la cual debe llevar cada eldiano en el brazo para identificarse en Marley. (Derecha) la insignia amarilla, usada por los nazis para identificar a lo judíos que vivían dentro de los territorios del Reich. Algunos analistas comparan la discriminación que sufren los eldianos en el mundo de Shingeki no Kyojin con la que sufrieron los judíos en la Alemania nazi. Desde su publicación, y enmarcado por su popularidad, el manga ha sido objeto de estudio sobre las influencias políticas, sociales y culturales que parece contener. Los sitios webs Women write about comics y Comicbook realizaron algunas comparaciones a la política de segregación de «Marley» contra los eldianos con la campaña antisemita de la Alemania nazi, principalmente por el hecho de que las comunidades judías eran confinadas por el Tercer Reich en guetos al igual que los eldianos. Desde la aparición de «Marley» y el concepto del pueblo eldiano, la obra ha sido señalada de tener un mensaje fascista y antisemita por diferentes analistas y lectores.  Tom Speelman del sitio web Polygon criticó que el manga hacía apología al militarismo, fascismo y el antisemitismo por la forma en que representa a los eldianos y la similitud que estos tienen con los judíos. Por el contrario, John F. Trent, de Bounding in to Comics, publicó un artículo en el que defendió a la obra, al igual que lo hizo Dave Trumbore de Collider. Ambos analistas criticaron la postura de Polygon; el primero declaró que Speelman no tenía pruebas en sus afirmaciones y el segundo lo acusó de malinterpretar el mensaje del manga. Faiyaz Chowdhury de CBR.com declaró que el trasfondo político de Shingeki no Kyojin está «lejos de ser sutil», pero no lo consideró una apología sino un crítica hacia el fascismo y el racismo, y aludió que los protagonistas y los eldianos son víctimas de los regímenes totalitarios que gobiernan a Marley y ,hasta el ascenso de Historia Reiss al trono, la isla Paradis. Rafael Motamayor de la página web Observer resaltó el cambio de paradigma de las primeras tres temporadas a la temporada final, así como los debates que se han llevado respecto a la serie. El analista concluyó que Shingeki no Kyojin «parece estar insinuando una última guerra, una guerra no contra las personas, sino contra los sistemas de creencias e idolatría que solo se preocupan por el poder y enfrentan a las personas entre sí». El manga también fue analizado desde el punto de vista de diversidad e inclusión. Isayama diseño al personaje de Hange Zóe de forma tan ambigua para que los lectores no supieran si era hombre o mujer. Kofi Outlaw de Comicbook resaltó la relación platónica entre a Ymir e Historia Reiss, con la primera enamorada de la segunda, pero sin poder Historia corresponder sus sentimientos. Mientras que Margarida Bastos de Collider elogió el papel de los personajes femeninos al darles arcos de desarrollo sin necesidad de ser sexualizadas como objeto de fanservice. Por el lado de Asia, la cobertura del anime apareció en la portada del periódico gratuito el 27 de mayo de 2013 en Hong Kong am730, relacionado con su popularidad en Hong Kong, China continental y Taiwán. La serie también fue objeto de críticas. La revista surcoreana Electronic Times acusó de contener un «mensaje militarista» que sirve a las tendencias políticas del entonces primer ministro japonés, Shinzō Abe, ,como mencionaba Polygon,; el manga también resonó en los jóvenes de Hong Kong quienes vieron a los titanes invasores como una metáfora de la China comunista. El comentarista hongkonés Wong Yeung-Tat alabó el estilo y la versatilidad de la serie, al permitir a los lectores realizar diversas interpretaciones. En 2013, diversos medios periodísticos vincularon una publicación del autor donde mencionaba que el personaje Dot Pixis, comandante en jefe de las Tropas estacionarias, estaba influenciado en el general imperial japonés Akiyama Yoshifuru, quien participó en la primera guerra sino-japonesa que causó la muerte de miles de personas. Se produjeron una serie de Flame en su blog, entre amenazas de muerte y desprecio al mangaka, por causa de los crímenes de guerra del general Yoshifuru y el caso de la masacre de Port Arthur. Debido a que muchas de las amenazas escritas en japonés tenían errores gramaticales, se presumió que fueron escritas por personas extranjeras. El 12 de junio de 2015, el Ministerio de Cultura de China incluyó a Shingeki no Kyojin junto con otros 38 títulos de anime y manga prohibidos por el propio país, debido a que «alienta la delincuencia juvenil, glorifican la violencia e incluyen contenido sexual». En Rusia, la exhibición en cines de la adaptación en imagen real de la obra de Isayama ,junto a otras obras japonesas, fue prohibida por el gobierno ruso con el argumento de velar «por el bienestar de la juventud del país». En Malasia, el manga fue editado al incluir pantalones y sujetadores de ropa a los titanes para cubrir la desnudez de estos, en razón a que existe una ley en el país asiático en contra de mostrar desnudos. Dicha censura fue motivo de burlas en internet. En 2021, la promoción de la temporada final del anime no estuvo exenta de polémica. En febrero, a Yuki Kaji ―seiyū de Eren, se le reprochó por algunas publicaciones en su cuenta oficial de Twitter, ya que varios usuarios japoneses mencionaron que la actitud del personaje de Gabi Braun ,quien fue comparada con los coreanos, es un reflejo del «resentimiento» de ciertos países de Asia contra Japón por los crímenes que cometió durante el expansionismo del Imperio Japonés. En noviembre, salieron a la venta unas bandas para brazo que incluyen la estrella eldiana de nueve puntas en el anverso, de la misma manera en que algunos personajes del anime la utilizan. Dicho producto fue objeto de críticas por parte de algunos usuarios de internet quienes los compararon con los brazaletes usados por los judíos durante el Holocausto. La compañía suspendió las reservas y emitió un mensaje de disculpa mediante su cuenta oficial de Twitter. Contenido de la obra Manga Artículo principal: Lista de volúmenes de Shingeki no Kyojin Tomos del manga de Shingeki no Kyojin publicados por Norma Editorial en la esquina de una librería. El manga comenzó su publicación mensual a partir del primer número, el 9 de septiembre de 2009 en la revista mensual Bessatsu Shōnen Magazine, de la editorial japonesa Kōdansha. El primer volumen comenzó el 17 de marzo de 2010 y terminó en abril de 2021, con un total de 34 volúmenes y 139 capítulos. En 2014, Shintaro Kawakubo había anunciado que finalizaría la serie entre 2017 o 2018, pues consideraba que era el tiempo necesario para que el autor la terminara. Debido al éxito obtenido y al deseo de la editorial de darle más libertad creativa al creador, decidieron extender la trama. Asimismo manifestó no tener una fecha definida para concluirlo. En noviembre de 2018, a través de la cuenta de Twitter oficial del programa documental Jōnetsu Tairiku de la MBS, confirmó que el manga entraba en el arco final de su historia, y en la que Isayama mostraba una viñeta del capítulo final del manga. En diciembre de 2019, el autor publicó una imagen en la cuenta oficial de Twitter de la revista Bessatsu Shonen Magazine, su intención de terminarla en 2020. Finalmente el 9 de abril de 2021, el autor concluyó el desarrollo de la historia con el capítulo 139 publicado ese mismo día, después de casi doce años de su debut. En un comunicado, pronunció algunas palabras de agradecimiento a los seguidores de su obra: «¡gracias por once años y medio! ¡Continúen disfrutando de la revista Bessatsu Shōnen!».  Por otro lado, se realizaron adaptaciones al manga de algunas novelas ligeras de la obra. El dibujante Satoshi Shiki y el escritor Ryō Suzukaze adaptaron la novela Before the Fall, bajo la supervisión de Isayama. Attack on Titan: Before the Fall se publicó en la revista Gekkan Shōnen Sirius de Kōdansha desde agosto de 2013 hasta marzo de 2019, con diecisiete volúmenes publicados. Se realizó un segundo manga spin-off, escrito por Gun Snark e ilustrado por Hikaru Suruga, el cual está enfocado en la novela visual No Regrets y se publicó en la revista de manga shōjo Aria. Dicha novela se centra en los orígenes del capitán Levi ,uno de los personajes principales, y su motivación de unirse a la Legión de Exploración. La novela Lost Girls también recibió una adaptación manga a cargo de Ryōsuke Fuji, que comenzó a publicarse en la revista Bessatsu Shōnen entre agosto de 2015 y mayo de 2016. También salió el manga de estilo comedia Shingeki! Kyojin Chūgakkō, creado por Saki Nakagawa, publicado en Bessatsu Shōnen Magazine en la edición de mayo de 2012 y cuya trama se orienta en los personajes principales que luchan contra titanes en la escuela secundaria. Otra parodia llamada Spoof on Titan ( 寸 劇 の 巨人 lit. Titán de parodia?) dibujada por Hounori, sería lanzada en Kōdansha por medio de una aplicación para teléfonos inteligentes y tabletas desde diciembre de 2013 hasta el 30 de diciembre de 2014 en inglés y japonés.  El manga de Shingeki no Kyojin comenzó a publicarse traducido al español, tanto en España e Hispanoamérica. La Editorial Panini adquirió los derechos de publicación y distribución en México como Ataque de los titanes. En España, Norma Editorial obtuvo los derechos de distribución del manga al que renombró con el nombre Ataque a los titanes. En Argentina, el manga fue publicado por la editorial Ovni Press en mayo del 2016, bajo el nombre de Attack on Titan. Novelas ligeras y visuales Una serie de novelas ligeras titulada Shingeki no Kyojin: Before the Fall (進撃の巨人: Before The Fall lit. Shingeki no Kyojin: Antes de la caída?), escrita por Ryō Suzukaze e ilustrada por Thores Shibamoto, comenzó a publicarse el 1 de abril de 2011 por Kodansha en tres volúmenes totales. Mientras que la primera novela cuenta la historia de Angel Aaltonen, un herrero que desarrolla los primeros prototipos del Equipo de maniobras tridimensionales, las siguientes se enfocan en un bebé que encontraron dentro del estómago de un titán. Una segunda serie de novelas ligeras, escrita por Gun Snark e ilustrada por Hikaru Suruga, bajo el nombre Shingeki no Kyojin: No Regrets (進撃の巨人悔いなき選択 Shingeki no Kyojin Kui Naki Sentaku ?), salió a la venta en septiembre de 2013. Levi es el protagonista, enfocándose en su pasado y reclutamiento en la Legión de exploración. La tercera serie de novelas, Shingeki no Kyojin: Harsh Mistress of the City (進撃の巨人: 隔絶都市の女王 Shingeki no Kyojin Kakuzetsu Toshi no Joō?), escrita por Ryō Kawakami e ilustrada por Gama Murata, comenzó a publicarse el 1 de agosto de 2014 y su trama se orienta en dos miembros del regimiento de Guarnición. Una tercera titulada Shingeki no Kyojin: Lost Girls (巨人 撃 の 巨人: Lost Girls?) ,escrita por Hiroshi Seko,, y puesta a la venta el 9 de diciembre de 2014, y parte en tres historias cortas Adiós Muralla Sina, Perdidos en el mundo cruel y Chicas perdidas. La novela profundiza en distintos momentos de las vidas de Mikasa Ackerman y Annie Leonhart, que ayuda a comprender mejor a ambos personajes. Su popularidad hizo que se adaptara a manga (en dos volúmenes) y a anime (en formato OVA), publicado por Norma Editorial en la versión español en 2017 y posteriores volúmenes en febrero de 2018.  En Norteamérica, la editorial estadounidense Vertical lanzó una novela original en inglés llamada Garrison Girl: An Attack on Titan Novel, escrita por la novelista Rachel Aaronfue. Fue publicada por Quirk Books el 7 de agosto de 2018. La trama se centra en Rosalie Dumarque, quien desafía a su familia a unirse a la Guarnición militar. En España, Norma Editorial ha publicado y traducido al español las novelas Before The Fall, No Regrets, y Lost Girls, junto con la mayoría de la serie de mangas relacionado con Shingeki no Kyojin. Anime Artículo principal: Lista de episodios de Shingeki no Kyojin La adaptación al anime estuvo producida por Wit Studio ,una subsidiaria de IG Port,, en colaboración con Production I.G, bajo la dirección de Tetsurō Araki mientras que el diseño de personajes corrió a cargo de Kyōji Asano. Hasta inicios de septiembre de 2013, contó con tres guionistas: Hiroshi Seko, Yasuko Kobayashi y Noboru Takagi, quienes escribieron nueve, siete y seis episodios, respectivamente. Se estrenó en Japón el 6 de abril de 2013 por la cadena televisiva Mainichi Broadcasting System (MBS), en donde emitieron los 25 episodios de unos veinticuatro minutos cada uno. El último episodio se exhibió en algunas salas de cine en Japón. La segunda temporada se emitió desde el 1 de abril hasta el 17 de junio de 2017, que contó con doce episodios, y fue anunciado previamente en enero del mismo año en la revista Bessatsu Shōnen. Imagen de Wikipe-tan con un estilo de dibujo Super deformed. Dicho estilo fue utilizado en el anime/parodia de Shingeki! Kyojin Chūgakkō. Una tercera temporada de veinticinco episodios se anunció a finales de 2017, fijándose su debut en julio de 2018, y transmitida en la cadena televisiva NHK. Sin embargo, el episodio cuarenta y ocho se retrasó por una semana durante su estreno a causa de un tifón. Además, por cuestiones de fuerza mayor, solo se transmitieron los primeros doce episodios de la tercera temporada, aquellos que abarcaron el «arco de la insurrección» en el manga,[n. 6] mientras que los episodios relacionados con el «arco de la reconquista de Shiganshina»[n. 7] fueron retrasados para estrenarse el siguiente año. En febrero de 2019, salió el avance de la segunda parte de la tercera temporada y confirmó su estreno para abril del mismo año. En junio de 2019, se difundió el rumor sobre los planes de crear una cuarta temporada, así como el hecho en que su producción ya no estaría a cargo de Wit Studio. Tetsuro Araki declaró en una entrevista que la cuarta temporada será producida pero sin mencionar el estudio que lo realizaría. Finalmente, NHK confirmó la producción de la cuarta y última temporada, la cual pasó a manos del estudio de animación MAPPA y que inicialmente se emitiría en octubre de 2020, aunque se postergó hasta diciembre del mismo año debido a la pandemia de COVID-19.  La primera parte de dieciséis episodios se emitió desde el 7 de diciembre de 2020 hasta el 29 de marzo de 2021, mientras que la segunda parte se emitió desde el 9 de enero hasta el 3 de abril de 2022. Una tercera parte fue anunciada para 2023 dividida en dos segmentos que serían transmitidos a manera de especiales. El primer segmento se emitió el 3 de marzo de 2023 y el segundo el 4 de noviembre del mismo año, los cuales marcaron el final definitivo del anime. En cuanto a su emisión fuera de Japón, la primera temporada se distribuyó en España por Selecta Visión y su primer volumen en DVD y Blu-ray se puso a la venta el 18 de diciembre de 2013. Los derechos para la segunda temporada de la serie los adquirió Funimation Entertainment en España y su distribución en DVD y Blu-ray por Selecta Visión, cuyo primer volumen salió a la venta el 25 de abril de 2018. Los primeros episodios de la tercera temporada fueron emitidos en Amazon Prime Video, Netflix, Movistar Xtra y YouTube, mediante el canal oficial de Selecta Visión. En Latinoamérica, ambas temporadas fueron emitidas por Crunchyroll y Amazon Prime Video en su día y distribuidas por Funimation ,con doblaje realizado en México,. La primera parte de la cuarta temporada también fue emitida por Funimation ―con doblaje de México incluido― y Crunchyroll ,solo con subtítulos al español―. Por otro lado, se realizó una adaptación a anime del manga parodia con el nombre Shingeki! Kyojin Chūgakkō, que contó con doce episodios los cuales se emitieron desde el 4 de octubre al 20 de diciembre de 2015. Mientras que en los contenidos de Blu-ray de la primera temporada, se añadieron unos cortos hechos con Flash que formaron una miniserie de veinticinco cortos ,divididos en nueve partes de seis a once minutos,, la cual se llamó «Shingeki no Kyojin: Chimi Kyara Gekijo - Tondeke Kunren Heidan». En ella, presentaron a personajes del manga ilustrados en estilos chibi, parodiado en forma alegre y humorística. Otros nuevos cortos se crearon para DVD y Blu-ray de la tercera temporada. OVAS y Películas recopilatorias Sala de cine en Japón adornada con material promocional de la primera película recopilatoria de Shingeki no Kyojin. En agosto de 2013, se planteaba lanzar una animación original sobre el capítulo especial del quinto volumen, y otra junto con la edición especial del undécimo volumen, pero sería pospuesto para incluirlo con la edición especial del volumen doce. La primera de ellas, «Iruze no techō» (イルゼの手帳 lit. el diario de Ilse?), narra la historia de Ilse Langnar (イルゼ・ラングナー Iruze Rangunā?), una miembro de la Legión de reconocimiento que registra en su diario un acontecimiento importante relacionado con los titanes. La segunda, «Totsuzen no Raihō-sha: Sainamareru Seishun no Noroi» (突然の来訪者: 苛まれる青春の呪い lit. Un visitante inesperado: La tortuosa maldición de la juventud?), se publicaría a la venta con la edición limitada del tomo trece; el cual trata sobre un concurso de cocina entre Sasha, Connie y Reiner contra Jean, Armin y Annie. Mientras que la tercera titulada «Konnan» (困難 lit. angustia?), se enfoca en el entrenamiento de los miembros de la Tropa de Reclutas N.º 104 en una misión de rescate. Dos OVAS nuevas se lanzaron a la venta en diciembre de 2014 y abril de 2015 junto con los volúmenes 15 y 16 respectivamente; el argumento de estas se basaron en la novela visual No Regrets, que tuvo como protagonista a Levi en sus juventud, e introdujo a nuevos personajes como Furlan Church e Isabel Magnolia. Otra tanda de tres nuevas OVAS salieron a la venta el 8 de diciembre de 2017, luego el 9 de abril y 9 de agosto de 2018, junto a los volúmenes 24, 25, y 26 respectivamente; en ellas, adaptaron la novela Lost Girls. Las dos primeras OVAS fueron protagonizadas por Annie Leonhart, mientras que la tercera tuvo como protagonista a Mikasa Ackerman. Se realizaron tres películas recopilatorias del anime, todas ellas incluyeron escenas nuevas no vistas en la serie de TV, así como un redoblaje y adaptación de sonido al 5.1. La primera película «Shingeki no Kyojin Zenpen ~Guren no Yumiya~» (「進撃の巨人」前編～紅蓮の弓矢～ lit. Shingeki no Kyojin: El arco y la flecha escarlata?), se enfoca a los acontecimientos de los episodios 1 al 13. La segunda película «Shingeki no Kyojin Kōhen ~Jiyū no Tsubasa~» (「進撃の巨人」後編～自由の翼～ lit. Shingeki no Kyojin: Alas de la Libertad?), cubrió los acontecimientos de los episodios 14 al 25. La segunda temporada estrenada en 2017, produjo una tercera película recopilatoria, «Shingeki no Kyojin ~Kakusei no Hōkō~» (「進撃の巨人」 ～覚醒の咆哮～ lit. Shingeki no Kyojin: El rugido del despertar?) que incluye escenas nuevas con un extra después de los créditos, y un redoblaje a 5.1. El 17 de julio de 2021, se estrenó una cuarta película recopilatoria bajo el nombre «Shingeki no Kyojin: Chronicle» (「進撃の巨人」〜クロニクル〜 lit. Shingeki no Kyojin: Crónica?), la cual resume las primeras tres temporadas. En España, Selecta Visión adquirió los derechos de distribución de las primeras tres películas, que publicó a la venta las primeras dos en DVD y Blu-ray, la tercera película se estrenó en diciembre de 2018 en cines españoles. En marzo de 2022, HBO Max adquirió los derechos de las películas para su plataforma de streaming con doblaje hispanoamericano. Banda sonora Artículo principal: Banda sonora de Shingeki no Kyojin Logo de la banda musical Sound Horizon quienes se encargaron de componer e interpretar cuatro de los temas de apertura y uno de cierre de Shingeki no Kyojin. La banda sonora del anime, los OVAS y las películas fue compuesta por Hiroyuki Sawano y lanzada en dos CD distribuidos por Pony Canyon. El primer álbum, Shingeki no Kyojin Original Soundtrack (「進撃の巨人」オリジナルサウンドトラック?), se lanzó el 28 de junio de 2013 y contiene dieciséis pistas. Un segundo CD, Shingeki no Kyojin Original Soundtrack II (「進撃の巨人」オリジナルサウンドトラック2?), se puso en venta el 16 de octubre del mismo año, junto con la edición limitada del cuarto volumen del DVD/Blu-ray del anime, con un total de once pistas. La primera temporada de la serie de televisión contiene dos canciones del tema de apertura y dos de cierre. Para los episodios 1 a 13,5, el tema de apertura fue «Guren no Yumiya» (紅蓮の弓矢 lit. Arco y flecha escarlata?), compuesto e interpretado por la banda Sound Horizon ,entonces conocida como Linked Horizon,, y el tema de cierre fue «Utsukushiki Zankoku na Sekai» (美しき残酷な世界 lit. Este cruel y hermoso mundo?), interpretado por Yoko Hikasa. El disco que contenía el primer tema de cierre fue lanzado el 8 de mayo de 2013. Para los episodios 14-25, el tema de apertura cambió a «Jiyuu no Tsubasa» (自由の翼 lit. Alas de la Libertad?), también de Sound Horizon, y el tema de cierre cambió a «Great Escape», de Cinema Staff, cuyo CD se puso a la venta el 21 de agosto del mismo año. Para la segunda temporada, el tema de apertura fue «Shinzou wo Sasageyo!» (心臓を捧げよう! lit. Dediquen sus corazones?), de Linked Horizon, cuyo CD salió a la venta el 17 de mayo de 2017; mientras que el tema de cierre fue «Yuugure no Tori» (夕暮れの鳥 lit. Pájaros al atardecer?), de Shinsei Kamattechan, salió a la venta una semana después. Sawano volvió a componer la banda sonora, Attack on Titan Season 2 Original Soundtrack (「進撃の巨人」Season 2 オリジナルサウンドトラック?), y se lanzó el 7 de junio por Pony Canyon. Esta contiene dos CD de dieciséis y diecisiete pistas, respectivamente. Imágenes de los músicos Yoshiki de la banda X Japan (izquierda) y Hyde de LArc~en~Ciel (derecha). Ambos trabajaron juntos en la canción «Red Swan», cuarto opening de Shingeki no Kyojin. Para la tercera temporada, se anunció un nuevo tema de apertura llamado «Red Swan», interpretado por el cantante Hyde de LArc~en~Ciel, junto con la banda X Japan. Para el tema de cierre, se escogió «Requiem der Morgenröte» (暁の鎮魂歌 Akatsuki no chinkonka, lit. Descansa en el amanecer?), de Linked Horizon, quienes por primera vez no interpretaron el tema de apertura del anime. Para la segunda parte de la tercera temporada, Sound Horizon volvió a interpretar el tema de apertura, «Shoukei to Shikabane no Mich» (憧憬 と 屍 の 道 lit. El camino sangriento y de los cadáveres?), y el tema de cierre ―«Name of love»― por Cinema Staff. Sawano participó de nuevo en la banda sonora de la tercera temporada en sus dos partes. Respecto a la temporada final, la banda sonora nuevamente fue compuesta por Hiroyuki Sawano, esta vez en cooperación con Kohta Yamamoto y bajo la dirección de Masafumi Mima. Se escogieron dos temas de apertura y cierre, debido a que la temporada fue dividida en dos partes. Para la primera, el tema de apertura fue «My War» (僕 の 戦 争 Boku no Sensō, lit. Mi guerra?), interpretado por Shinsei Kamattechan, y el tema de cierre, «Shock» ( 衝 撃 Shōgeki?), por Yūko Andō. Para la segunda parte, el tema escogido fue «The Rumbling» (ランブリング Ranburingu lit. El Retumbar?), interpretado por la banda de Metal alternativo SiM, y el tema de cierre, «Akuma no Ko» ( 悪の子 lit. un hijo del demonio?), fue interpretado por la cantante Ai Higuchi. Videojuegos Al igual que otras series, se crearon numerosos videojuegos de Shingeki no Kyojin y estos aparecieron en diferentes tipos de plataformas de consolas. Los juegos se centraron en el universo ficticio de la obra y se basaron principalmente en su mismo argumento. En mayo de 2013 se anunció que cuatro novelas visuales desarrolladas por el personal de Nitroplus en colaboración con Production I.G serían lanzadas a la venta. Nitroplus comentó que «como compañía no se encuentra implicada en la producción de estos videojuegos, pero su personal de forma independiente, sí lo está». Los juegos se incluyeron en los primeros ejemplares del tercer y sexto volumen de Blu-ray del anime, y consisten en historias spin off sobres los personajes de la obra. Un juego de acción, Humanity in Chains, desarrollado por Spike Chunsoft para Nintendo 3DS se distribuyó en Japón el 5 de diciembre de 2013, en Norteamérica el 12 de mayo de 2015 y en Europa el 2 de julio de ese año, posteriormente salió «Shingeki no Kyojin: Hangeki no Tsubasa» (進撃の巨人 ~反撃の翼~ lit. Shingeki no Kyojin: Las alas del contraataque?), desarrollado por la misma compañía y plataforma en 2013. Otro juego, Attack on Titan 2: Future Coordinates se lanzó el 30 de noviembre de 2017 en Japón, que incluye nuevas opciones como la personalización de personajes y una misión extra, que al ganar, se obtiene un disfraz especial. Se creó el juego móvil «Shingeki no Kyojin: Jiyu e no Hoko» ( 進撃の巨人-自由への咆哮- lit. Shingeki no Kyojin: El rugido de libertad?) desarrollado por el estudio Mobage para iOS y Android, en el que los jugadores interpretan a un personaje original que fue exiliado de la muralla Rose, para construir y fortificar una ciudad fuera de la muralla y expandirse mediante la fabricación y obtención de artículos, el uso de titanes y la explotación de los recursos de otros jugadores. En la publicación del volumen 17 del manga, se anunció que Omega Force, estudio de Koei Tecmo, desarrollaba una adaptación del manga como videojuego llamado «Attack on Titan» El juego fue presentado en la Gamescom 2015, y anunciado para su venta en las consolas PlayStation 4, PlayStation 3 y PlayStation Vita. Fue lanzado a la venta el 18 de febrero de 2016 en Japón. Más adelante, se confirmó su lanzamiento en todo el mundo junto con las versiones para PC y Xbox One. En 2017, se anunció la producción de su secuela «Attack on Titan 2», puesto a la venta en marzo de 2018 para las mismas plataformas que su predecesora, aunque más adelante, se incluyeron para PC y Nintendo Switch. La expansión de esta, «Attack on Titan 2: Final Battle» salió a la venta en Japón el 4 de julio de 2019, y en Norteamérica y Europa el 5 de julio, estuvo disponible para PlayStation 4, Nintendo Switch, Xbox One (con soporte para Xbox One X) y PC a través de Steam. Un conjunto de trajes de la serie, se agregó a Dead or Alive 5 Last Round en julio de 2016, junto con un escenario jugable basado en la muralla Rose durante un ataque del titán colosal. Además, el juego y la mercancía de Shingeki no Kyojin se presentó en un evento cruzado con Nexon MMORPG MapleStory en sus versiones en japonés y GMS. En Japan Amusement Expo del 2016, Capcom anunció el desarrollo de un juego de arcade con el nombre «Shingeki no Kyojin: Team Battle», que permitiría a los jugadores poder volar por el aire utilizando el equipo de maniobra omnidireccional y contaría con una «inmersión sin igual» parecido en el manga. El juego albergaría desde uno hasta ocho jugadores en línea, pero en diciembre de 2018, la compañía anunció su cancelación. Películas en imagen real Vista aérea de la isla de Hashima, lugar de rodaje de la película en imagen real de Shingeki no Kyojin. Se anunció la producción de una película de imagen real en octubre de 2011 con Tetsuya Nakashima como director del mismo. En diciembre de 2012, se informó que Nakashima dejó su puesto de director ,anunciado por Toho, por tener diferencias creativas en la escritura del guion y otros asuntos. En diciembre de 2013, se reveló que Shinji Higuchi será el director y responsable con los efectos especiales. Se anunció que el escritor Yūsuke Watanabe y el crítico/experto en subcultura Tomohiro Machiyama escribirían la película junto con el propio Isayama. En julio de 2014, se reveló que se lanzaría una película dividida en dos partes en 2015, protagonizada por Miura Haruma como Eren Jaeger. También se reveló que ambas películas tendrían muchos cambios argumentales respecto a la obra original, como la omisión de algunos personajes importantes, notablemente el personaje Levi. En marzo de 2015 se publicó un avance de la primera película, y al mes siguiente, Toho lanzó el segundo avance y anunció que la segunda entrega bajo el nombre de «End of the World». Para junio, salió un tercer avance, que reveló el equipo de maniobra tridimensional, y ratificó su lanzamiento que llegaría a cines IMAX de Japón. El estreno de la primera parte se realizó el 1 de agosto de 2015 y la segunda parte el 19 de septiembre del mismo año. Se realizó una miniserie de imagen real bajo el nombre «Shingeki no Kyojin: Hangeki no Noroshi» (進撃の巨人 反撃の狼煙 lit. Shingeki no Kyojin: Contraataque?), empleó los mismos actores que las películas, que comenzó a transmitirse en video en línea de NTT DoCoMo servicio dTV el 15 de agosto de 2015. Se conformó en tres episodios, cuya trama se centra en el personaje de Hange Zoë y su investigación de los titanes, y de como se creó el equipo de maniobras tridimensionales. Las dos películas se estrenaron en los cines en España y después distribuidos en DVD y Blu-ray, con disponibilidad en Filmin, y en Latinoamérica por Funimation. En febrero del 2018, Sato Company anunció que distribuiría las dos películas y se estrenarían a través de la cadena de cines Cinemark, Brasil sería el primero en ofrecerlo al público. La revista Deadline Hollywood informó que Warner Bros. estaba en negociaciones para asegurar los derechos cinematográficos de la franquicia de Shingeki no Kyojin. El productor David Heyman, habría mostrado interés en trabajar un proyecto de dos películas, aunque un día después, los representantes de Kodansha negaron que existieran negociaciones con Warner. En octubre de 2018, se reveló que Warner Bros y Kodansha llegaron a un acuerdo para producir una nueva adaptación del manga en imagen real, cuya realización estaría a cargo del director Andy Muschietti. Otros medios Cafetería de Amway Animate cafe en Taipéi la cual se encuentra en el primer piso del edificio Sugar Tengyun Building, promocionando productos de Shingeki no Kyojin. Entre el 9 de abril y el 9 de septiembre de 2013, se publicaron dos guías del manga tituladas «Inside» y «Outside», que incluyeron arte conceptual en escenas de anime y manga, perfiles de personajes y entrevistas exclusivas con el autor. Posteriormente, ambos se lanzaron en Norteamérica el 16 de septiembre de 2014 por Kodansha USA. Se produjo un disco compacto de 16 minutos, interpretado por el elenco del anime y se incluyó en el número de enero de 2014 de Bessatsu Shōnen Magazine. El 3 de noviembre de 2014, el escritor estadounidense C.B. Cebulski, reveló que se estaba en preparación un crossover entre Shingeki no Kyojin y Marvel Comics. Cebulski se encargó de la ilustración, mientras que la escritura estuvo a cargo del propio autor. El crossover de un solo capítulo incluyó a Spider-Man, Los Vengadores y los Los Guardianes de la Galaxia enfrentándose contra varios monstruos, como el titán colosal, el titán blindado y la titán femenina en las calles de la ciudad de Nueva York. Durante el Día del cómic gratuito 2015, Secret Wars de Marvel incluyó una presentación de ocho páginas Attack on Avengers con arte de Gerardo Sandoval. En la ComicCon de Nueva York 2015, se anunció la publicación de un cómic estadounidense titulado Attack on Titan Anthology. De igual forma se realizaron pequeños crossovers de la obra con franquicias como Pokémon, y The Seven Deadly Sins, así como homenajes realizados por los mangakas Kanae Hazuki y Arina Tanemura. Durante enero y mayo de 2015, Universal Studios Japan abrió varias atracciones orientadas en el manga, nombrado «The Real Attack on Titan Experience». En el lugar, aparece una figura de quince metros de estatura del titán de Eren, que lucha contra otro titán. También se incluye un titán a nivel del suelo, en donde los visitantes pueden posarse y tomarse fotos. En 2017, se anunció la obra de teatro LIVE IMPACT con la participación de más de 150 actores en el elenco y que se llevaría a cabo entre julio y septiembre.  Sin embargo, se canceló después de que uno de los miembros del equipo falleció en un accidente durante los ensayos. En junio de 2019, se anunció una exhibición en celebración de los diez años de publicación del manga, en el que se expusieron bocetos del manga, contenido inédito y una muestra del formato sonoro de lo que sería el final del mismo. Mercadotecnia Estuche de lapiceros con el emblema de la Legión de Exploración, un ejemplo de todos los productos que se han creado en relación con la franquicia de Shingeki no Kyojin. En Japón se creó una campaña de mercadeo sobre el manga. Las marcas Pizza Hut y Lotteria realizaron publicidad relacionada con Shingeki no Kyojin en 2013. Surgieron numerosos memes publicados en las redes sociales basados en cosplays o escenas que imitan el anime. Entre los productos que se comercializaron, se encontraban almohadas dakimakura de Eren y Levi, así como equipos de maniobras tridimensionales de papel. También se vendieron figuras de diferentes personajes, que se adquirían en Banpresto, Good Smile Company, o Max Factory. La editorial francesa «Pika» lanzó su línea de abanicos con imágenes de los mangas que tiene adquiridos, incluyéndose la serie. En 2013, la marca japonesa de perfume Koubutsu-ya anunció la venta de perfumes inspiradas en la serie. El personaje de Levi, apareció en la portada de la revista femenina Frau en su edición de agosto de 2014. En 2015, Adidas lanzó una línea deportiva relacionada con Attack on Titan. En ese mismo año, se puso a la venta una revista especial de doce números, Monthly Attack on Titan y que incluía figuras de los personajes del manga y un diorama. En 2021, con motivo de promocionar la temporada final del anime, se realizó una intensa campaña de mercadotecnia. El 12 de marzo, el juego para móviles Garena Free Fire anunció una colaboración con el anime que se extendió hasta abril del mismo año. Según el comunicado, los jugadores tendrían acceso a nuevos aspectos basados en la serie como el uniforme de la Legión de Exploración, skins del titán acorazado y titán de ataque. En abril, la compañía SHISEIDO Uno publicó una serie de comerciales especiales relacionadas con el anime, esto con el propósito de promocionar una línea de crema de afeitar. De igual manera, la marca de Ramen Myojo Food se unió a la campaña de promoción de la temporada final del anime, en donde anunciaron que quienes participen en su edición especial del Ramen Charumera, recibirán premiós relacionados con la serie de anime; además, se vendió una edición limitada del ramen, con consomé y salsa de soya, así como con los ñames de Shingeki no Kyojin como ingrediente principal. Se realizó una exhibición en las instalaciones de la Yokohama Landmark Tower ,el segundo edificio más alto de Japón, en donde celebraron el final del manga, dicha exhibición duró desde el 29 de abril hasta el 30 de junio del 2021. En agosto, el sitio oficial del parque Forest Adventure Okuhita, también anunció una colaboración con la franquicia animada de Shingeki no Kyojin, la cual dio inicio en el mismo mes, en las instalaciones del parque ubicado en la Prefectura de Ōita ,lugar de nacimiento de Hajime Isayama,. En diciembre, el servicio y aplicación de taxis, S.RIDE anunciaron una campaña promoción del anime que se llevó a cabo en el periodo comprendido del 27 de diciembre de 2021 al 9 de enero de 2022 en Japón. Recepción Ventas y audiencias Saturno devorando a su hijo y El coloso, ambas pinturas de Francisco de Goya. Según The Daily Dot, la apariencia de los titanes en Shingeki no Kyojin podrían estar influenciadas por el arte de Goya. El manga tuvo un éxito inmediato en Japón y se convirtió en uno de los más exitosos de 2011. Logró el undécimo lugar en el ranking de los más vendidos de Oricon por serie con casi 3,8 millones de copias vendidas en dicho año, y al año siguiente llegó a la decimoquinta posición de dicha lista con más de 2,6 millones de copias vendidas. Posteriormente en diciembre de 2012, la circulación total de los primeros nueve volúmenes era de diez millones de copias. La adaptación al anime aumentó significativamente el número de ventas en la serie; los diez volúmenes del manga estuvieron presentes en el top 50 de Oricon durante la semana del 8 al 14 de abril de 2013. En mayo de 2013, los cinco volúmenes superaron más de un millón de copias cada uno, y elevó la circulación total a 19,5 millones de copias a mediados de aquel año. Las ventas de las ediciones francesa del manga se ubicaron en el cuarto lugar en el ranking de Oricon con casi 4,3 millones de copias vendidas. En junio de 2013, se vendieron 8,7 millones de copias desde el estreno de la serie anime. En septiembre del mismo año, el número de volúmenes vendidos superó los veinte millones de copias, convirtiéndose en la séptima serie que supera este encabezado desde la creación del Oricon en 2008. Durante el año fiscal 2013, obtuvo el segundo puesto de mangas más vendidos con casi dieciséis millones de copias, y con los volúmenes completos finalizó en el top 15 de Oricon. La editorial del manga de Kōdansha registró un aumento de su facturación por primera vez en dieciocho años. En abril de 2014, más de tres millones de copias fueron vendidas en Japón, y la circulación total en todo el mundo superó los treinta y ocho millones de copias en el primer semestre de 2014, el manga terminó en el primer lugar del ranking de Oricon con 8,34 millones de copias vendidas en Japón, incluso llegó a superar a One Piece por primera vez en los últimos cinco años. En el año fiscal 2014, el manga ocupó el segundo lugar en el ranking de Oricon con once millones de copias vendidas, representó más de 7,3 millones de yenes en efectivo. En Norteamérica, la circulación total de la serie era de 500 000 en octubre de 2013. 660 000 en marzo de 2014 y los 2,5 millones de copias hasta julio de 2015. En septiembre de 2016, las ventas del manga ascendieron a más de 60 millones de copias. En diciembre de 2017, la circulación superaba los 71 millones de copias. En noviembre de 2018, la circulación mundial superaba los 86 millones de copias, con 76 millones publicados en Japón y diez millones en el resto del mundo. Desde enero hasta mayo de 2019, se vendieron 2,8 millones de volúmenes más en Japón. En noviembre de 2019, alcanzó más de cuatro millones de unidades vendidas, y llegó a posicionarse en el 7.º lugar entre los mangas más vendidos en Japón, un mes después, la Bessatsu Shonen anunció por medio de su cuenta en Twitter que logró alcanzar los cien millones de unidades vendidas en todo el mundo. La adaptación al anime también gozó de aceptación en ventas de DVD y Blu-ray en Japón, el primer volumen del DVD llegaría al puesto uno en ventas a mediados de 2013, así como los volúmenes posteriores.  En los Estados Unidos, se transmitió por Toonami en Adult Swim, donde obtuvo una audiencia de más de un millón de espectadores por episodio durante mayo de 2014, mientras que sus ventas en formato doméstico sobrepasaron las 200 000 unidades. Asimismo, alcanzó el primer lugar de preferencias y se consideró el anime favorito de los fanáticos de Funimation en 2014. En Francia, la primera temporada alcanzó una audiencia promedio de 100 000 espectadores durante su transmisión en Wakanim. En España, Selecta Visión realizó un simulcast de la segunda temporada, que alcanzó más de 300 000 espectadores. Durante el día del estreno de la segunda parte de la temporada final en México y América Latina, cientos de usuarios reportaron en sus redes sociales que experimentaban fallas y dificultades para acceder a las plataformas Crunchyroll y Funimation que emitían la serie en streaming, debido a lo saturado que estaban ambos sitios web. Algunos temas de apertura del anime tuvieron millones de reproducciones en YouTube. El tema de apertura de la segunda temporada «Shinzou wo Sasageyo!» alcanzó las cien millones de reproducciones. En la cuenta oficial de Twitter del anime, se publicó un mensaje especial de reconocimiento en que el tema de apertura del anime «The Rumbling» consiguió superar las diez millones de reproducciones acumuladas en solo tres días desde su publicación oficial, y el 18 de enero, SiM anunció que su tema musical alcanzó las veinte millones de vistas. Crítica Yoshiyuki Tomino ,creador de la franquicia Gundam, elogió a Isayama por la forma en que refleja sus experiencias personales en la obra, aunque criticó el estilo de dibujo, al que definió como «tosco y grosero». Desde su publicación, el manga obtuvo críticas positivas, centradas en la trama, los personajes y los temas que trata; en contraposición, se cuestionó la calidad del dibujo. Analistas de Asahi Shinbun dijeron que Shingeki no Kyojin ilustraba «la desesperación que sienten los jóvenes en la sociedad actual». El sitio web Manga-News destacó la densidad y la riqueza de los volúmenes, e indicó que la puesta en escena de los titanes y las secuencias de acción tenían una «gran intensidad». Sébastien Kimbergt, de AnimeLand, señaló «el golpe de genio del autor» que logró adaptar dos conceptos muy populares, y consideró que los titanes no son más que «una mezcla entre zombis gigantes y mechas impulsadas». El escritor Mao Yamawaki la calificó como «una historia de la mayoría de edad de los niños y niñas en su núcleo, con un nuevo misterio en cada capítulo». Yoshiyuki Tomino ,creador de la franquicia Gundam, alegó que el hecho de que el universo del manga provenga tanto de la experiencia personal del autor como de varias investigaciones sobre los titanes debe ser reconocido. Sucesivamente, consideró que el dibujo es «demasiado tosco y grosero». El estilo artístico del manga ha sido criticado por algunos analistas, quienes lo definieron como «crudo» y con un diseño «amateur». Tomofusa Kure comentó que el diseño de los personajes mejoró con el paso del tiempo, y declaró que si las ilustraciones se hubieran «refinado», «no transmitirían el carácter misterioso de la obra». Jason Thompson de Crunchyroll notó cómo los personajes obtienen «power-ups» para crear giros en la trama, pero luego dijo que «eran necesarios» por el estilo apocalíptico de la obra. La adaptación al anime fue bien recibida por la mayoría de críticos; en particular fue elogiado el diseño de los personajes de Kyoji Asano que los hacía más reconocibles que en el manga, y las escenas de acción se beneficiaban de «técnicas de animación controladas» con los movimientos tridimensionales.  Sobre la trama, los analistas de Anime News Network proporcionaron críticas variadas; Carl Kimlinger señaló que «[traer] de vuelta el terror del grupo fee-fi-fo-fum [...] no hace un buen show». No obstante, Rebecca Silverman comentó que la serie es «tanto hermosa como horrible a la vista», y «una excelente mezcla de lo que la novelista gótica del siglo XVIII Ann Radcliffe definió como horror versus terror: uno es físico, que te hace querer mirar hacia otro lado, y el otro es intelectual, que te hace querer saber qué pasará luego». Aunque existen varias series de acción apocalíptica, Carlo Santos dijo que «pocas se acercan tanto a la perfección como Shingeki no Kyojin». Asimismo, la describió como «una obra maestra de muerte y destrucción», si bien solo vio el primer episodio. Theron Martin agradeció el «intenso e impactante primer episodio» junto con su banda sonora, pero consideró que tiene una «animación limitada». En su reseña, mencionó que el ambiente y la estética visual del manga es comparable con la de Claymore. Sin embargo, con el estreno de la cuarta temporada del anime, producida por MAPPA, recibió críticas negativas por parte de la audiencia quienes aludieron a un declive de calidad en la animación mostrada, mientras que otros defendieron el trabajo del estudio y agradecieron el esfuerzo invertido de los miembros del estudio. Cosplayers interpretando a los personajes de Shingeki no Kyojin. En las reseñas de Latinoamérica y España, el sitio Ramen para Dos comentó de forma positiva varios aspectos del manga, como el universo creado por el autor, las escenas de acción, los personajes, y la edición al español publicada por Norma, pero criticó su estilo de dibujo al que calificó como «tosco, casi abocetado, y totalmente plano» y añadió que «los cuerpos resultan totalmente rígidos y artificiales, muchas veces desproporcionadas [...] pueden llegar a confundirse con facilidad dado que mantienen una serie de rasgos semejantes». Por el contrario, el anime fue aclamado por «su calidad de animación, su banda sonora y su fidelidad a la hora de adaptar el manga». La página web En tu pantalla, describió a la serie anime de «adictiva y llena de giros» y como «un anime muy recomendable, tanto si te gustan los animes o si quieres empezar a ver uno por primera vez. Tiene una historia que engancha y una factura impecable». El periódico El País la reconoció como una parte del género shōnen, y elogió sus giros de la trama el cual comparaba con la serie Games of Thrones. Por otro lado, las películas en acción real recibieron críticas negativas,  Kotaku culpó de forma directa los cambios argumentales realizados en relación con el material original que estaban presentes en la película. Pablo López de SDP Noticias criticó la película por «dejar de lado la historia original, su desarrollo plano y excesivamente rápido, sus efectos especiales paupérrimos», concluyó que «no es recomendable ni para fanáticos, ni para el público en general». Homenajes Una versión alternativa de Lisa Simpson apareció con la interpretación de Mikasa Ackerman en el especial de Halloween XXV de la serie animada Los Simpson. En dicho especial, se rindió homenaje a la animación japonesa, con disfraces llevados por la familia amarilla en alusión a personajes del anime. El presidente de Francia, Emmanuel Macron, se reunió con distintos autores reconocidos de manga después de la apertura de los Juegos Olímpicos de Tokio 2020. Macron había pedido ,sin éxito, reunirse específicamente con los autores de Demon Slayer ,Koyoharu Gotouge, y Hajime Isayama de Shingeki no Kyojin. El 6 de abril, Weekly Shonen Magazine compartió dos postales en la edición semanal de su revista publicada ese día. Las ilustraciones correspondían a las portadas de los volúmenes 4 y 23 del manga, Hisayama agradeció la publicación con un mensaje en su Twitter. Días antes de la finalización del manga, un grupo de aficionados homenajearon la serie y al estudio Ghibli donde pintaron en murales a varios personajes en las calles de Monterrey (México), y en la capital de ese país. Se plasmaron personajes como Levi Ackerman, Mikasa Ackerman y Eren Jaeger, y otros, orientados en la película El viaje de Chihiro.  El 9 de abril de 2021, el creador de la serie Kanojo, Okarishimasu Reiji Miyajima, reveló una ilustración del personaje Chizuru Mizuhara con el uniforme de la Legión de exploración como forma de homenaje y reconocimiento hacia la serie. Asimismo, el creador de Tokyo Ghoul Sui Ishida, publicó una ilustración sobre el protagonista Eren con un diseño oscuro, particularmente enfocado al estilo de su serie. Premios y reconocimientos Cosplayer disfrazada como Mikasa Ackerman, uno de los personajes más elogiados por la crítica. El manga ganó el premio Kōdansha Manga Shō de 2011 en la categoría mejor shōnen. Ese mismo año, fue nominado al Premio Manga Taishō. En 2012, fue nominado en la 16.º Premio Cultural Osamu Tezuka, para luego aparecer también en la edición de 2014. Además, fue elegido Manga del Año en 2013 y 2014 por la revista Da Vinci. En los NewType Anime Awards de 2013 logró cinco premios, como mejor serie animada, mejor estudio para Wit Studio, mejor director (Tetsuro Araki), mejor guionista, mejor música (Sawano) y mejor personaje femenino para Mikasa Ackerman. De igual manera, ganó en la categoría de mejor animación en los 18.ª Premios de Animación Kobe en 2013, y en el Premio Tokio Anime de 2014 se adjudicó el galardón a la mejor serie de TV de 2013. El editor de la revista Weekly Playboy de Shūeisha lo reconoció como el anime que marcó época en 2013. En Estados Unidos, la división Young Adult de la Asociación de Bibliotecas la incluyó en el primer lugar de su lista de las mejores novelas gráficas para adolescentes de 2013. En 2014, el manga ganó el Premio Harvey al mejor trabajo extranjero. En Francia, los lectores de la revista Animeland lo eligieron como el manga más esperado en la 20.ª edición del Anime Manga Grand Prix de 2012. En la siguiente edición, ganó en las categorías el mejor manga, mejor anime y mejor tema de apertura. En 2013, los lectores de Manga-news lo eligieron como el mejor shōnen del año. En 2014, el primer volumen del manga fue seleccionado en el 41.ª festival internacional de cómics Angoulême. En Italia, el manga recibió el premio Micheluzzi a la mejor obra extranjera en 2014. En mayo de 2019, el episodio 54 titulado «Héroe», perteneciente a la tercera temporada de la serie, alcanzó en su momento la calificación más alta de la historia de la televisión en IMDb, al punto de superar a series como Game of Thrones, Breaking Bad y Lucifer. Como parte de la campaña para promocionar la temporada final del anime, Kodansha puso a la venta una edición de gran tamaño del primer volumen del manga de Shingeki no Kyojin, que fue reconocida por el Libro Guinness de los récords como «el cómic de mayor tamaño jamás publicado». En la encuesta realizada por la cadena de televisión Japonesa TV Asahi para elegir los 100 Mejores mangas de la historia, Shingeki no Kyojin alcanzó el puesto número 6."
ksampletext_wikipedia_fict_starwars: str = "Star Wars. Star Wars, conocida también en español como La guerra de las galaxias, es una franquicia de medios de fantasía compuesta primordialmente de una serie de películas concebidas por el cineasta estadounidense George Lucas en la década de 1970, y producidas y distribuidas inicialmente por 20th Century Fox y posteriormente por The Walt Disney Company a partir de 2012. Su trama describe las vivencias de un grupo de personajes que habitan en una galaxia ficticia e interactúan con elementos como «la Fuerza», un campo de energía metafísico y omnipresente que posee un «lado luminoso» impulsado por la sabiduría, la nobleza y la justicia y utilizado por los Jedi, y un «lado oscuro» usado por los Sith y provocado por la ira, el miedo, el odio y la desesperación. La primera película de la serie, Star Wars: Episodio IV - Una nueva esperanza que fue producida por 20th Century Fox (1977), contó con actores como Mark Hamill, Harrison Ford, Carrie Fisher, Peter Cushing, Alec Guinness, Anthony Daniels, Kenny Baker, Peter Mayhew y David Prowse. Aunque tuvo numerosas dificultades durante la producción, 20th Century Fox se encargó de su distribución y fue estrenada el 25 de mayo de 1977. Se convirtió en un fenómeno de la cultura popular y su influencia ha sido reconocida por numerosos cineastas. Su éxito hizo que Lucas financiara dos secuelas más, Star Wars: Episodio V - El Imperio contraataca (1980) y Star Wars: Episode VI - Retorno del Jedi (1983), que completaron la denominada «trilogía original» y a las que se incorporaron actores como Billy Dee Williams, Frank Oz e Ian McDiarmid. Casi dos décadas después se estrenó Star Wars: Episodio I - La amenaza fantasma (1999), la primera cinta de una nueva trilogía de precuelas, a la que siguieron Star Wars: Episodio II - El ataque de los clones (2002) y Star Wars: Episodio III - La venganza de los Sith (2005). Esta saga se centra en la historia antes de la primera trilogía. Liam Neeson, Ewan McGregor, Natalie Portman, Hayden Christensen, Samuel L. Jackson y Christopher Lee fueron algunos de los actores que se sumaron al reparto, en el que también participaron algunos actores de la trilogía original. A finales de 2012 Disney adquirió Lucasfilm, productora de las seis películas anteriores, y anunció la realización de una tercera trilogía cinematográfica integrada por Star Wars: Episodio VII - El despertar de la Fuerza (2015), Star Wars: Episodio VIII - Los últimos Jedi (2017) y Star Wars: Episodio IX - El ascenso de Skywalker (2019), en la que nuevamente participó parte del elenco original junto con actores como Daisy Ridley, John Boyega, Oscar Isaac y Adam Driver. El estudio es responsable también de la producción de un par de películas derivadas que abordan elementos del canon y se encargan de retratar el universo extendido de la franquicia como son Rogue One (2016) y Han Solo (2018). Además del ámbito cinematográfico, la franquicia incluye una amplia variedad de productos tales como novelas, series de televisión, videojuegos, historietas, atracciones de parques temáticos, juegos de rol, de guerra o de miniaturas y juguetes, que componen una parte importante del conocido como «universo expandido» de Star Wars. Cada año la marca genera unos ingresos por la venta de sus productos en todo el mundo que ascienden a más de 24 000 millones USD, lo que la convierte en una de las más exitosas de todos los tiempos, mientras que la recaudación de sus películas la posicionan como una de las series más taquilleras en la historia del cine. Argumento Sinopsis La trama descrita en las nueve películas que componen la serie principal de Star Wars relata las vivencias de la familia Skywalker, «hace mucho tiempo en una galaxia muy muy lejana», cuyos integrantes son capaces de percibir y utilizar «La Fuerza», lo cual les permite desarrollar habilidades como la telequinesis, la clarividencia y el control mental, entre otras. Un par de sables de luz cruzados entre sí; un sable de luz es el arma principal utilizada por los Jedi y sus enemigos, los Sith, y consiste en una empuñadura de metal pulido que lleva un cristal que proyecta una hoja de energía de aproximadamente un metro de longitud. En la «trilogía de precuelas» se describe el entrenamiento del joven Anakin Skywalker, bajo la guía del maestro Jedi Obi-Wan Kenobi, durante una época de inestabilidad política derivada de una disputa entre la República Galáctica y la Federación de Comercio. El ataque por parte de un ejército de droides motiva al Senado Galáctico a conceder poderes extraordinarios al canciller Palpatine, que mantiene en secreto su identidad como el Sith Darth Sidious con el objetivo de incitar las Guerras Clon, derrocar el sistema republicano y fundar el Imperio Galáctico bajo su mando. Sus planes se ven facilitados cuando descubre que Anakin manifiesta debilidad por el «lado oscuro de la Fuerza», debido a sus sentimientos de ira, miedo y odio, reforzados por el asesinato de su madre, la desconfianza de la orden Jedi en sus capacidades y las visiones de su esposa Padmé muriendo durante el parto. Finalmente, Anakin se convierte en el aprendiz de Sidious, bajo el nombre de Darth Vader, y le ayuda a poner en marcha la Orden 66, una directriz que conduce a la exterminio de los Jedi. Tras un enfrentamiento con Obi-Wan en el que Anakin resulta severamente herido, este último adopta un traje robótico negro gracias al cual puede mantenerse con vida. La «trilogía original» parte del exilio de los pocos supervivientes del exterminio Jedi y la formación de un grupo de rebeldes que buscan acabar con el dominio del imperio, a la vez que ven amenazados por la construcción de un arma imperial masiva conocida como «Estrella de la Muerte», que posee la capacidad de destruir planetas en poco tiempo. Los mellizos Luke y Leia, que pasaron su infancia en distintos lugares de la galaxia tras la muerte de Padmé, desconocen su parentesco entre sí y con Vader hasta que coinciden en la rebelión, cuando ella consigue los planos robados de la Estrella de la Muerte y los deposita en el interior del droide R2-D2, con la esperanza de que ayuden a identificar la vulnerabilidad del arma imperial. Con la ayuda de Han Solo, Chewbacca, R2-D2 y C-3PO, los Skywalker consiguen destruir la Estrella de la Muerte. Tras ser entrenado por el Jedi Yoda, y asistido por el mismo equipo al que se integra Lando Calrissian, Luke se enfrenta sin éxito a Vader y este le revela que es su padre. Finalmente, en otro encuentro con Sidious y Vader, este último rescata a Luke y sacrifica su vida para acabar con Sidious. La «trilogía de secuelas» lidia con los acontecimientos derivados del establecimiento de la Primera Orden, una organización que comparte ciertos rasgos de la ideología imperial y que es encabezada por el Líder Supremo Snoke y Kylo Ren ,hijo de Solo y Leia,, y en la que se ven involucrados nuevos personajes como Rey, Finn y Poe Dameron, quienes forman parte de la denominada resistencia. Cronología El universo de Star Wars abarca varios períodos históricos ficticios, algunos de los cuales han sido explorados en las distintas producciones cinematográficas y televisivas. Estas eras fueron definidas en 2021 y se perfeccionaron y ampliaron en abril de 2023. A continuación aparecen listadas, en orden cronológico, las distintas eras en las que se desarrollan los sucesos de la franquicia: Área temática Star Wars Galaxys Edge en Disneyland. La Alta República (en idioma original, The High Republic): Está ambientada en la República Galáctica, 200 años antes de la trilogía de precuelas. Un par de producciones que cubren este período son la colección literaria Star Wars: The High Republic, la serie animada Young Jedi Adventures y The Acolyte. Caída de los Jedi (en idioma original, Fall of the Jedi): Son los sucesos descritos en la trilogía de precuelas y las series The Clone Wars y Tales of the Jedi. El sistema republicano es corrompido por el canciller supremo Palpatine, quien es en secreto el lord Sith Darth Sidius. Es responsable de las Guerras Clon con la confederación separatista, que llega a su final con el exterminio de la orden Jedi y el establecimiento del Imperio Galáctico.  Reinado del Imperio (en idioma original, Reign of the Empire): Etapa inmediata a la trilogía de precuelas que explora la consolidación del sistema imperial. Sus eventos son descritos en The Bad Batch, Han Solo: Una historia de Star Wars, Obi-Wan Kenobi, Star Wars Rebels, Andor y en Rogue One: una historia de Star Wars. También incluye a los videojuegos Jedi: Fallen Order, Jedi: Survivor y Vader Immortal: A Star Wars VR.  Era de la Rebelión (en idioma original, Age of Rebellion): Son los sucesos descritos en la trilogía original, y describe el enfrentamiento del Imperio con la Alianza Rebelde en una guerra civil que se extiende durante varios años y culmina con la caída del Imperio. También se incluyen los videojuegos Star Wars Battlefront II (2017) y Star Wars: Squadrons.  La Nueva República (en idioma original, The New Republic): Ambientada en los primeros años de la Nueva República y cuyos eventos son explorados en las producciones The Mandalorian, The Book of Boba Fett, Ahsoka y en Skeleton Crew. Ascenso de la Primera Orden (en idioma original, Rise of the First Order): Son los sucesos descritos en la trilogía de secuelas y en la serie Star Wars Resistencia, tras el surgimiento de la Primera Orden con los remanentes del extinto Imperio. Véase también: Cronología de Star Wars Personajes Además de la raza humana, se describen múltiples tipos de especies extraterrestres procedentes de los numerosos planetas y satélites que forman dicha galaxia y que pertenecen a la alianza de planetas de la República Galáctica. Darth Vader. Existen dos grupos de humanos que son sensibles a la Fuerza pero que poseen ideologías diferentes entre sí: los Jedi y los Sith. La estructura jerárquica de la orden Jedi incluye a los maestros, como Obi-Wan Kenobi, Luke Skywalker y Yoda; sus aprendices o también conocidos como padawan; los iniciados o younglings que suelen ser menores de edad; y los caballeros, cuyo estatus progresa al de un maestro una vez que concluyen el entrenamiento de su respectivo aprendiz. Además de los anteriores, existen otros tipos de Jedi que persiguen fines específicos: por ejemplo, los guardianes, los centinelas y los consulares. Los Sith poseen una estructura similar, en la que un maestro puede tener aprendices bajo su tutela, aunque también incluyen a otros miembros como los Jedi oscuros y los Inquisidores. Los primeros comprenden aquellos que «contravienen el código Jedi», mientras que los Inquisidores constituyen «una organización de agentes sensibles a la Fuerza que trabajaban para el Imperio». Algunos ejemplos de Sith, Jedi oscuros e Inquisidores son Darth Sidious y Darth Vader, Asajj Ventress, y el Gran Inquisidor, respectivamente. A su vez, entre los funcionarios imperiales se pueden mencionar Orson Krennic, Wilhuff Tarkin y Moff Gideon. Otros grupos de humanos son los mandalorianos, cuyo principal rasgo es el uso de cascos y armaduras fabricados con acero beskar, y entre los cuales se incluyen Din Djarin y Boba Fett; los clone trooper al servicio de la república como es el caso del Capitán Rex y que en su mayoría pasaron a ser stormtrooper tras el establecimiento del imperio galáctico; y los rebeldes e integrantes de la Resistencia que se oponen al dominio imperial como Lando Calrissian y Cassian Andor, y Poe Dameron y Finn, entre otros. Otros personajes recurrentes son los robots y los androides, creados generalmente para servir a un propósito concreto, por lo que figuran así droides astromecánicos, médicos, de protocolo y de combate, entre otros. Algunas otras especies de criaturas de Star Wars son los Chiss, caracterizados por su piel azul y ojos rojos como Thrawn; los Ewok, una comunidad de guerreros peludos y de baja estatura que habitan en la Luna de Endor; los Gungan que poseen una apariencia de anfibios como Jar Jar Binks; los Hutt de aspecto grotesco y voluminoso, entre los cuales se encuentra Jabba; los Kaleesh como Grievous que presentan rasgos tanto humanoides como de anfibios; y los Wookiee provenientes de Kashyyyk, con características similares a los Ewok pero más altos. Chewbacca pertenece a esta última raza. Véase también: Personajes de Star Wars Escenarios Mapa de la galaxia ficticia donde tiene lugar la saga de Star Wars. La galaxia ficticia en la que tienen lugar los acontecimientos de Star Wars está formada por varias regiones, que a su vez se subdividen en sectores y sistemas, con numerosos planetas y satélites como Bespin, Dagobah, Utapau o Yavin, y entre los que destacan principalmente: Coruscant: Capital de la República Galáctica y del posterior imperio. Se encuentra situado en el centro de la galaxia y es uno de sus planetas más poblados, hasta el punto que casi todo él es una gran ciudad muy industrializada. Allí se encuentran el Senado Galáctico y el Templo Jedi, sede de la orden y donde se reúne su consejo. Endor: Una luna montañosa caracterizada por sus grandes bosques y situada en el sistema homónimo. Sus habitantes más conocidos son los ewoks, unos seres de pequeño tamaño cuya forma corporal se asemeja a la de los osos. Geonosis: de aspecto rocoso y árido. Sus principales pobladores, los geonosianos, tienen rasgos insectoides y sus construcciones son similares a los nidos de termitas. Hoth: Un inhóspito planeta helado cuya temperatura se mantiene durante todo el día por debajo de cero grados centígrados. Entre su fauna autóctona se encuentran el depredador wampa y el tauntaun. Jakku: Un planeta desértico habitado por ladrones, forajidos y refugiados que sirvió anteriormente como base secreta de investigación imperial. Kamino: Azotado por fuertes lluvias, es un planeta acuático cuyos habitantes dominan la tecnología de la clonación. Kashyyyk: Un planeta selvático. Está poblado por los wookiees, quienes viven en ciudades construidas sobre los grandes árboles del planeta y que se caracterizan por la gran cantidad de vello que cubre su cuerpo y por su altura, alcanzando en su forma adulta más de dos metros. Naboo: Con capital en Theed, es un pacífico planeta colonizado por los humanos y colmado de verdes praderas y colinas. Sus habitantes nativos son conocidos como gungan, tienen forma de anfibio y su mayor ciudad, Otoh Gunga, se encuentra sumergida bajo el lago Paonga. Lugar de nacimiento y gobierno de Amidala Padmé, esposa de Anakin Skywalker. Tatooine: Un planeta desértico iluminado por dos estrellas y con un duro clima. Poblado escasamente, la mayoría de sus habitantes trabajan en granjas de humedad, donde emplean vaporizadores con el fin de obtener agua para su propio consumo y venta. Está bajo el control de los hutts y es un lugar de tránsito para piratas, cazarrecompensas y traficantes. Entre sus pobladores autóctonos se encuentran los jawas, unos pequeños vendedores de chatarra que se cubren por completo con una túnica, y los incursores tusken o moradores de las arenas, una agresiva raza muy territorial y xenófoba que rechaza la convivencia con cualquier otra en el planeta. Lugar de nacimiento de Anakin Skywalker. Véase también: Planetas y satélites de Star Wars Temáticas Los emblemas de la República Galáctica y la Confederación de Sistemas Independientes, cuyo enfrentamiento es uno de los temas principales en la segunda trilogía (o trilogía precuela) de la saga. Los acontecimientos tienen lugar en una galaxia ficticia de nombre desconocido y en un tiempo no especificado ,solo se dice que fue «hace mucho tiempo, en una galaxia muy, muy lejana»,. Los viajes espaciales son comunes y la mayoría de los planetas que aparecen en la saga están afiliados a la República Galáctica, la unión democrática que rige la galaxia y cuyo gobierno, presidido por un canciller supremo, está formado por representantes de toda ella, elegidos o designados y agrupados en el llamado Senado Galáctico, ubicado en el planeta Coruscant. En oposición a la República se encuentra la Confederación de Sistemas Independientes (Separatistas), siendo el enfrentamiento de ambas uno de los temas más importantes en la trama de las tres primeras películas de Star Wars. Uno de los elementos principales en la saga es «la Fuerza», un campo de energía metafísico y omnipresente creado por las cosas que existen, que impregna el universo y todo lo que hay en él. La Orden Jedi es una organización de caballeros unidos por su creencia y percepción de la Fuerza, que luchan por la paz y la justicia en la República Galáctica. Se entrenan en el uso del sable de luz o espada láser (un arma similar a una espada tradicional salvo por el hecho que su hoja es un haz de energía). Los Jedi son capaces de manejar la Fuerza y lograr así habilidades como la telequinesis, la clarividencia, el control mental o una amplificación de los reflejos, la velocidad y otras capacidades físicas mentales. No obstante y aunque dicho grupo la utiliza con fines positivos, la Fuerza tiene un lado oscuro provocado por la ira, el miedo y el odio. Este lado es usado por los Sith para exterminar a los Jedi y tomar el control de la galaxia. La Primera Orden es una organización que juega un rol antagónico importante en la trilogía producida por Disney, cuyos rasgos y funcionamiento están alineados con los del Imperio Galáctico, en contraposición a la Nueva República y los integrantes de la Resistencia. Otros temas abordados por Star Wars, y que inclusive están presentes en su tema musical principal, son la emoción, el heroísmo, la aventura y la victoria. Influencias George Lucas admitió haberse inspirado en varios de sus géneros literarios y cinematográficos favoritos para Star Wars, tales como el western y los seriales de cine, específicamente Flash Gordon y las películas La fortaleza escondida y Los siete samuráis, ambas de Akira Kurosawa: Conceptos como la caballería medieval y la sociedad feudal sirvieron de inspiración para crear a varios personajes y conceptos de Star Wars. Confirmó que las temáticas de la caballería medieval y los caballeros, así como la figura del paladín y demás miembros de la sociedad feudal inspiraron a varios personajes y conceptos de la primera película. Asimismo la obra El héroe de las mil caras, del escritor Joseph Campbell influyó de manera importante en la serie.  Una nueva esperanza está inspirada mayoritariamente en los filmes de Kurosawa. La similitud más evidente es la de los campesinos y los droides C-3PO y R2-D2, pues la película es vista desde la perspectiva de dichos personajes. Otras fuentes fílmicas de inspiración fueron The Searchers y Lawrence de Arabia. La escena de la batalla final en Una nueva esperanza se basa en los combates aéreos de la Segunda Guerra Mundial, los cuales pueden apreciarse en las películas de guerra de los años 1950 y 1960. El personaje de Jabba el Hutt estaba basado originalmente en el aspecto físico de Sydney Greenstreet, tal y como aparece este en Casablanca. En La venganza de los Sith, la escena de la batalla ambientada en Kashyyyk es un tributo a las escenas iniciales de Saving Private Ryan y Touch of Evil. Las grandes batallas entre los buques y las maniobras de flanqueo evocan a películas del género de la piratería. La escena de Anakin Skywalker aceptando ser el aprendiz de Palpatine para salvar a la persona que más ama es una metáfora de Fausto. Considerado un apasionado del cine épico e histórico, Lucas realizó la escena de los podracers inspirándose en el famoso segmento de cuadrigas de Ben-Hur. Para la escena del traslado en falanges de las tropas robóticas de la Federación de comercio se inspiraron en Espartaco. Para el segmento final de Una nueva esperanza, en donde el protagonista es recompensado con una medalla al valor, se inspiraron en Los tres mosqueteros. Películas Artículo principal: Películas de Star Wars Saga Skywalker Película Fecha de estreno (en EE. UU.) Director Guionista(s) Responsable de la historia Productor(s) Refs Trilogía original Star Wars: Episodio IV - Una nueva esperanza 25 de mayo de 1977 George Lucas Gary Kurtz  Star Wars: Episodio V - El Imperio contraataca 21 de mayo de 1980 Irvin Kershner Leigh Brackett & Lawrence Kasdan George Lucas  Star Wars: Episodio VI - El retorno del Jedi 25 de mayo de 1983 Richard Marquand Lawrence Kasdan & George Lucas Howard Kazanjian  Trilogía de precuelas Star Wars: Episodio I - La amenaza fantasma 19 de mayo de 1999 George Lucas Rick McCallum  Star Wars: Episodio II - El ataque de los clones 16 de mayo de 2002 George Lucas George Lucas & Jonathan Hales George Lucas  Star Wars: Episodio III - La venganza de los Sith 19 de mayo de 2005 George Lucas  Trilogía de secuelas Star Wars: Episodio VII - El despertar de la Fuerza 18 de diciembre de 2015 J. J. Abrams J. J. Abrams & Lawrence Kasdan y Michael Arndt Kathleen Kennedy, J. J. Abrams and Bryan Burk  Star Wars: Episodio VIII - Los últimos Jedi 15 de diciembre de 2017 Rian Johnson Kathleen Kennedy y Ram Bergman  Star Wars: Episodio IX - El ascenso de Skywalker 20 de diciembre de 2019 J. J. Abrams Chris Terrio & J. J. Abrams Derek Connolly & Colin Trevorrow y Chris Terrio & J. J. Abrams Kathleen Kennedy, J. J. Abrams y Michelle Rejwan  Antologías Película Fecha de estreno (en EE. UU.) Director Guionista(s) Responsable de la historia Productor(s) Refs Rogue One: una historia de Star Wars 16 de diciembre de 2016 Gareth Edwards Chris Weitz & Tony Gilroy John Knoll & Gary Whitta Kathleen Kennedy, Allison Shearmur y Simon Emanuel  Han Solo: una historia de Star Wars 25 de mayo de 2018 Ron Howard Jonathan Kasdan & Lawrence Kasdan  Películas futuras  Película Fecha de estreno (en EE. UU.) Director Guionista(s) Responsable de la historia Productor(s) Estado Refs The Mandalorian & Grogu 22 de mayo de 2026 Jon Favreau Jon Favreau & Dave Filoni Jon Favreau, Dave Filoni & Kathleen Kennedy Posproducción  Star Wars: Starfighter 28 de mayo de 2027 Shawn Levy Jonathan Tropper Kathleen Kennedy & Shawn Levy En rodaje Película de Sharmeen Obaid-Chinoy sin título TBA Sharmeen Obaid-Chinoy George Nolfi Kathleen Kennedy En desarrollo  Película de Dave Filoni sin título Dave Filoni Jon Favreau, Dave Filoni & Kathleen Kennedy  Película de James Mangold sin título James Mangold James Mangold & Beau Wilimon Kathleen Kennedy  Película de Taika Waititi sin título Taika Waititi  Lucas dirigió la primera película en imagen real de Star Wars cuyo estreno en los cines de Estados Unidos ocurrió en mayo de 1977. A esta le siguieron El Imperio contraataca (1980) y Return of the Jedi (1983), secuelas dirigidas por Irvin Kershner y Richard Marquand respectivamente, y que aparecen numeradas en su escena de apertura como «Episodio V» y «Episodio VI» aunque originalmente se las promocionó solamente con sus subtítulos. La cinta original de la serie llevaba por título simplemente Star Wars (La guerra de las galaxias), pero más tarde adoptó el subtítulo Episodio IV - Una nueva esperanza, con el fin de diferenciarla de sus secuelas y posteriores precuelas. En 1997, con motivo del vigésimo aniversario del estreno del Episodio IV, Lucas coordinó la remasterización de la trilogía original con efectos generados por computadora para su relanzamiento en cines. Años después volvió a realizar otros cambios para el lanzamiento de las tres primeras películas en formato DVD, por ejemplo, en septiembre de 2004. Más de dos décadas después del estreno de Una nueva esperanza, el universo de Star Wars continuó con una trilogía de precuelas formada por La amenaza fantasma (1999), El ataque de los clones (2002) y La venganza de los Sith (2005), que explican los acontecimientos previos a la trama de la trilogía original y que fueron dirigidas por Lucas. En 2008 Lucasfilm produjo The Clone Wars, una cinta animada por ordenador y estrenada también en cines, cuya trama se sitúa cronológicamente entre los Episodios II y III. Aunque Lucas tenía la intención de remasterizar en formato 3D la trilogía de precuelas para un nuevo lanzamiento en cines, al final Industrial Light and Magic solo se encargó de convertir el Episodio I a dicho formato, y su estreno ocurrió en febrero de 2012.  Tras la adquisición de Lucasfilm por The Walt Disney Company en octubre de 2012, se anunció la producción de una tercera trilogía formada por los Episodios VII, VIII y IX, estrenados entre 2015 y 2019 y dirigidos por J. J. Abrams y Rian Johnson. De manera intercalada a las fechas de estreno de estos episodios, se produjo otro par de cintas derivadas del canon principal, las cuales llevan el subtítulo de Una historia de Star Wars: Rogue One (2016) y Han Solo (2018). Desde 2017 Disney ha dejado entrever el desarrollo de futuras películas sobre el universo de Star Wars, cuya trama habría de apartarse de la temática Skywalker que sirvió de hilo conductor de las anteriores trilogías de la serie. En el evento Disney Investor Day de 2020 se confirmaron dos de ellas: Rogue Squadron, dirigida por Patty Jenkins y con un estreno programado para diciembre de 2023, y otra a cargo del cineasta Taika Waititi a partir de un guion escrito por él en colaboración con Krysty Wilson-Cairns. Asimismo, una nueva trilogía escrita y dirigida por Johnson se encuentra en etapa de producción, así como un par de largometrajes por el director J. D. Dillard, y por los productores Kathleen Kennedy y Kevin Feige. Actualmente, los tres últimos proyectos y Rogue Squadron están puestos en pausa. En abril de 2023, durante la Star Wars Celebration, se anunciaron tres nuevas películas con historias que abarcan diferentes épocas dentro de la franquicia. James Mangold escribirá y dirigirá una entrega titulada Dawn of the Jedi, que tendrá lugar antes de The Phantom Menace y explorará a los primeros Jedi. Dave Filoni dirigirá una película sin título, descrita como una película de eventos que culmina las diversas series de televisión en Disney+ y servirá como el final de The Mandolorian. Sharmeen Obaid-Chinoy dirigirá una tercera película, actualmente sin título pero ambientada en una época que el estudio denominó como la Nueva Orden Jedi. Steven Knight escribe el guion, inicialmente desarrollado por Damon Linderof, que tendrá lugar quince años después de The Rise of Skywalker, y explora las experiencias de Rey como una nueva Maestra Jedi que restablece la Orden Jedi. Daisy Ridley regresara a interpretar al personaje. Trilogía original Artículo principal: Primera trilogía de Star Wars En 1971 Universal Studios aprobó la producción de dos largometrajes, American Graffiti y Star Wars, como parte de un contrato que hizo con George Lucas. El rodaje de American Graffiti finalizó en 1973 y, pocos meses después, Lucas escribió un boceto al que tituló «The Journal of the Whills» y en el que relataba el entrenamiento del aprendiz C. J. Thorpe como comando espacial «Jedi-Bendu» a manos del legendario Mace Windu. Al pensar que esta historia sería demasiado difícil de comprender, redactó un tratamiento de trece páginas al que llamó «The Star Wars» y que consistía en una especie de remake de la película La fortaleza escondida, dirigida por Akira Kurosawa. Para 1974 ya había extendido dicho tratamiento a un guion en el que incorporaba elementos como los Sith, la Estrella de la Muerte y un joven llamado Annikin Starkiller como personaje central. La segunda versión fue muy simplificada e introdujo al joven héroe, llamado ahora Luke, en una granja; además, convirtió a Anakin en el padre del protagonista y en un sabio caballero Jedi, e incorporó la Fuerza como un poder sobrenatural. En la siguiente versión, Lucas eliminó al personaje paterno y lo reemplazó por un sustituto llamado Ben Kenobi. Una cuarta versión fue terminada en 1976, siendo la definitiva para comenzar la etapa de rodaje. Al principio, el filme llevaba por título Adventures of Luke Starkiller, as taken from the Journal of the Whills, Saga I: The Star Wars. Durante la producción, Lucas cambió el apellido de Luke a Skywalker, simplificando asimismo el título a Star Wars. Hasta ese punto, Lucas no tenía en mente realizar una serie de cintas sobre Star Wars. De hecho, el guion definitivo pasó por cambios que lo volvieron más concluyente y sustancial al mismo tiempo, concluyendo precisamente con la aniquilación del Imperio a partir de la destrucción de la Estrella de la Muerte, Sin embargo, poco antes Lucas determinó que sería la primera entrega en una serie de aventuras, confirmando a la vez que Star Wars no habría de ser la primera en la cronología, sino la inaugural de una segunda trilogía en la saga completa. Lo anterior se menciona de forma explícita en el prólogo de la reedición de la novela de Alan Dean Foster El ojo de la mente, reedición de 1994: La antigua ciudad de Tikal (Guatemala) sirvió como escenario para la base rebelde en la luna Yavin. El Hotel Sidi Driss, en Matmata (Túnez), representó la granja de Owen y Beru Lars y su sobrino Luke. El parque nacional Redwood, en California (Estados Unidos), donde se rodaron escenas de la luna Endor. No mucho tiempo después de que comenzara a escribir Star Wars, concluí que la historia daba para más de lo que una simple película podía dar cabida. Mientras completaba la saga de los Skywalker y los caballeros Jedi, empecé a visualizarlo como un relato que tomaría lugar en, por lo menos, nueve películas ,tres trilogías, y decidí continuar justo entre los hechos precedentes y los sucesivos, partiendo entonces con la historia intermedia. George Lucas George Lucas, creador de Star Wars El segundo borrador contenía el adelanto de una continuación inédita: La Princesa de Ondos. Para cuando concluyó la redacción del tercer escrito, meses después Lucas negoció un contrato para obtener los derechos correspondientes para la realización de dos secuelas más. Más o menos en ese mismo período, se encontró con el escritor Alan Dean Foster, a quien contrató para adaptar dichas secuelas a manera de novelas. La intención de todo ello era que si Star Wars resultaba exitosa, el director podría adaptar esas novelas en guiones. Asimismo, poco después desarrolló un esquema general de toda la historia de la saga para apoyarse durante el proceso de redacción de cada secuela. Una vez que Star Wars alcanzó el éxito tanto en recaudaciones como en críticas, Lucas decidió usar el filme como base para un serial más elaborado, sin embargo, en algún momento optó por rechazar dicho proyecto. A pesar de lo anterior, ciertamente su idea era instituir un centro independiente de filmación ,hoy en día, el Rancho Skywalker,. Para ello, vio en la serie una manera viable de financiación. Por otra parte, Foster ya había empezado a escribir la primera novela a manera de continuación de Star Wars, así que Lucas optó por adaptar la obra de Foster en ese mismo instante; el libro fue publicado al año siguiente bajo el título Splinter of the Minds Eye. Al principio, el cineasta no veía una serie con un determinado número de filmes, sino que la concebía simplemente como una franquicia tipo James Bond. En una entrevista concedida a Rolling Stone en agosto de 1977, mencionó que su intención era que cada uno de sus amigos dirigiera una película a la vez, con tal de ofrecer su propio estilo a la serie. Asimismo, dijo que las escenas donde Darth Vader se convierte al lado oscuro, mata al padre de Luke y pelea con Ben Kenobi en un volcán mientras la República cae bajo el dominio imperial, eran buenas ideas para realizar la secuela. Brackett terminó su primer borrador a principios de 1978; Lucas dijo luego que se sentía decepcionado del escrito, aunque antes de que pudiera discutirlo con la guionista, ella murió de cáncer. Sin otro escritor disponible en esos momentos, el cineasta tuvo que escribir el siguiente borrador por cuenta propia. De hecho, fue en este donde utilizó por primera vez el término «Episodio» para enumerar cada una de sus cintas; así, en ese entonces, designó a El Imperio contraataca como Episodio II. Al respecto, Michael Kaminski, en The Secret History of Star Wars, menciona que la desilusión en torno al guion de Brackett, pudo haber sido la causa de que Lucas tomara un rumbo distinto en cuanto a la historia. Así, hizo uso de un nuevo giro argumental: Darth Vader revela que es el padre de Luke. Según el propio George Lucas, haber escrito ese borrador fue una experiencia agradable, en comparación al año pasado, el cual fue problemático en cuanto a la redacción de la primera parte. Poco después, redactó un par adicional de escritos, ambos terminados para abril de 1978. Asimismo, dio un toque más oscuro a la historia, al incorporar la escena donde Han Solo queda aprisionado en carbonita, dejando su vida a la suerte. Con esta nueva orientación, Lucas decidió que la serie pasaría a ser una trilogía, cambiando en el siguiente borrador a El Imperio contraataca de «Episodio II» a «Episodio V». Lawrence Kasdan, quien acababa de terminar el guion de Raiders of the Lost Ark, fue contratado para escribir los siguientes bocetos, obteniendo incluso más detalles de la historia a partir del director Irvin Kershner. Kasdan, Kershner y el productor Gary Kurtz concibieron a esta parte en particular como más seria y adulta, siendo ayudada en gran parte por la nueva historia más oscura que, prácticamente, redefinió los orígenes con los que había surgido la primera película. Para el momento en que empezó a escribir el «Episodio VI» en 1981 (en ese entonces, conocido como La venganza de los Jedi), muchas cosas habían cambiado. La producción de El Imperio contraataca se tornó estresante y costosa, pasando Lucas por momentos muy delicados en su vida personal. Desanimado por esta situación, decidió que no haría más películas de Star Wars, mencionando incluso públicamente que la serie había concluido en una entrevista ofrecida a la revista Time en mayo de 1983. Los primeros guiones de Lucas para este nuevo episodio, que se remontan a 1981, muestran a un Darth Vader compitiendo con el emperador por la posesión de Luke. En el segundo guion, consistente en una versión mejorada del original, Vader se convierte en un personaje simpático. Kasdan fue contratado nuevamente para encargarse del libreto final, siendo el responsable de que el villano principal se redima y quede desenmascarado en las últimas escenas. Adicionalmente, este cambio en el personaje habría de proporcionar un precedente para la historia trágica de Darth Vader, la cual se convertiría en el eje argumental de las precuelas de Star Wars, producidas décadas después. Trilogía de precuelas Artículo principal: Segunda trilogía de Star Wars La Plaza de España, en Sevilla (España), sirvió como escenario para representar el planeta Naboo. Restos del escenario de Mos Espa (Tatooine), construido en el desierto de Túnez. Villa del Balbianello, junto al lago Como (Italia), otra de las localizaciones de Naboo. Después de perder gran parte de su fortuna en un acuerdo de divorcio en 1987, Lucas no tenía intenciones de continuar Star Wars, por lo que de manera no oficial anunció la cancelación de las tentativas de continuación para cuando se estrenó Return of the Jedi. Sin embargo, la idea de las precuelas, que ya estaban en una fase avanzada de desarrollo, continuaban interesándole. Una vez que Star Wars se volvió nuevamente popular con la publicación de la línea de historietas homónima de Dark House y la trilogía de novelas de Timothy Zahn, comúnmente referida como Heir to Empire o «Trilogía Thrawn», Lucas se percató de que todavía existía una considerable audiencia atenta a la serie de películas. Asimismo, sus hijos ya comenzaban a crecer, y ante la llegada masiva de la tecnología CGI, decidió finalmente regresar a la dirección de Star Wars. De esta forma, para 1993 se anunció en publicaciones como Variety que él se encargaría de realizar las precuelas. Jake Lloyd y Hayden Christensen interpretaron a Anakin Skywalker en la trilogía de precuelas, cuya trama principal se enfoca en la conversión de este al lado oscuro de la Fuerza. El proceso de redacción de la historia ahora se enfocó en un tono trágico a nivel general, pues se examina la transformación de Anakin Skywalker en un ser cruel y despiadado. Un aspecto importante en todo esto fue la concepción de cómo las precuelas se podían adaptar adecuadamente a las historias de las películas originales; al principio, Lucas supuso que simplemente iría añadiendo detalles al argumento que podrían ser paralelos o tangenciales a los originales. No obstante, poco después consideró que debía comenzar con una larga historia que empezaría con la infancia de Anakin, concluyendo con su trágica muerte. De hecho, este fue el paso definitivo para, por fin, convertir a Star Wars en una serie, más allá de una sola trilogía. En 1994, Lucas comenzó a escribir el primer guion titulado Episodio I: El comienzo. Tras el debut en cines de dicha película, anunció que consideraría si sería el responsable de dirigir también la segunda parte, en la cual ya había comenzado incluso a trabajar. El borrador original del Episodio II estuvo terminado tan solo semanas antes de que comenzara el rodaje de Attack of the Clones. Para revisar el escrito, Lucas contrató a Jonathan Hales, quien previamente había colaborado con él en la serie de televisión Las aventuras del joven Indiana Jones. Debido a que para entonces todavía no había pensado en el subtítulo oficial de la película, a manera de broma Lucas lo llamó «La gran aventura de Jar Jar». Cabe señalar que el concepto de El ataque de los clones surgió cuando Lucas se hallaba escribiendo el guion de El Imperio contraataca. En un inicio, el personaje de Lando Calrissian era un clon que provenía de un planeta habitado solamente por clones, lo cual originó la «Guerra de los Clones» mencionada brevemente por Obi-Wan en Una nueva esperanza; a partir de esto, Lucas desarrolló una estructura alternativa en torno a un ejército de milicia clon proveniente de un planeta distante, el cual atacó a la República pero fue exitosamente vencido por los caballeros Jedi. Así, los elementos básicos de la trama original se convirtieron en el eje del Episodio II, con la nueva adición de que todo el acontecimiento relatado había sido enteramente manipulado por el propio Palpatine. Lucas empezó a trabajar en el Episodio III incluso antes de que se estrenara El ataque de los clones; en el proceso, les mencionó a los guionistas auxiliares que el filme abriría con un montaje de siete batallas acontecidas en las Guerras de los Clones. A pesar de ello, mientras realizaba una revisión al libreto, realizó varios cambios de estructuración de la historia. Kaminski, en The Secret History of Star Wars, ofrece evidencia de que los problemas en torno a la caída de Anakin al lado oscuro hicieron que Lucas realizara numerosas modificaciones al argumento, primero rehaciendo el montaje de la escena inicial para que Palpatine apareciera como secuestrado, y Dooku siendo asesinado por Anakin en el siguiente segmento. Tras concluir el rodaje en 2003, Lucas realizó todavía más cambios al personaje de Anakin, reescribiendo la escena de su conversión al lado oscuro; con los nuevos cambios, el héroe, antes de elegir el lado oscuro, intentaría salvar, sin éxito, a Padme de la muerte. Al no poder lograrlo, decide convertirse para tratar de salvarla con los poderes de un sith. Anteriormente, Anakin se volvía al lado oscuro simplemente ante la suposición de que los Jedi se estaban volviendo malos, y por ende querían apoderarse de la República. Esta modificación se hizo justo en el proceso de edición del material grabado, así como en la etapa de grabación de nuevas escenas adicionales en 2004. En declaraciones recientes, Lucas ha exagerado en cuanto a la cantidad de material que escribió en un inicio sobre Star Wars; como se ha mencionado anteriormente, gran parte de los escritos surgieron a partir de 1978, justo cuando la serie comenzaba a volverse cada vez más popular. Kaminski explicó que dichas exageraciones consistían realmente en una medida tanto publicitaria como de seguridad; según él, desde que la historia de Star Wars cambió radicalmente con el paso de los años, siempre fue la intención de su creador que la historia original cambiara de forma retroactiva, pues el público solamente habría de mirar el material desde la perspectiva de él.  Trilogía de secuelas Tras el éxito de Una nueva esperanza aumentó a la presión sobre Lucas en cuanto a la producción de una secuela inmediata. En 1978, un artículo publicado por la revista Time reveló que la recién creada Star Wars Corp. se encargaría de producir Star Wars II, «y además, contando a las anteriores, otras diez continuaciones tentativas». Una vez que se estrenó El Imperio contraataca, Nicholas Wapshott detalló que «al menos, seis películas más estaban siendo planeadas». Para entonces, este notó que las historias eran diferentes a las de los seriales típicos de Hollywood, puesto que tendrían relatos individuales más allá de seguir un mismo arco argumental: «Star Wars podría ser el primer serial de cintas conformado de historias consecutivas». En 1983, otro artículo de Time confirmó la saga de nueve películas, al igual que otras publicaciones como The Times (en una reseña de Return of the Jedi), The Washington Post siendo el mismo tema referido a finales del siglo XX y comienzos del siglo XXI.  En el libro Icons: Intimate Portraits (1989), escrito por Denise Worrell, se menciona que Lucas tenía para entonces una noción vaga de lo que ocurriría en las tres películas de la trilogía fallida. Ahí mismo, se cita la declaración del cineasta al respecto: «Si la primera trilogía aborda temas sociales y políticos, tratando sobre cómo una sociedad evoluciona, Star Wars es más sobre el crecimiento personal y la autorrealización, mientras que la tercera trilogía lidiaría más con problemas de tipo moral y filosófico. Las continuaciones tratan sobre la comunidad Jedi, la justicia, la confrontación y sobre poner en práctica lo que has aprendido». En una entrevista con TheForce.net, el productor Gary Kurtz describió una línea argumental para una serie de hasta nueve películas; en ese entonces, declaró que el plan original era que en Return of the Jedi, Han Solo muere, mientras que Leia se convierte en la «Reina» de su pueblo. Incluso Leia no era la hermana de Luke. Kurtz añadió que, bajo este concepto, el Episodio VII se centraba en la vida de Luke como un Jedi, mientras que el Episodio VIII estaría marcado primordialmente por la aparición de la hermana de Luke (otro personaje que no era Leia), para que así el Episodio IX tuviera como antagonista principal al emperador. En una entrevista posterior ofrecida a Film Threat, él dijo que la idea de una tercera trilogía de Star Wars «era muy vaga. Trataba sobre el viaje de Luke para convertirse en una especie de caballero Jedi bajo el entrenamiento de Obi-Wan Kenobi, así como su enfrentamiento final con el emperador. De eso se trataba en sí toda la trama». Pese a que en 1999 Lucas negó haber considerado una serie de nueve películas para Star Wars, casi una década después comentó que la idea de continuar con los Episodios VII, VIII y IX le parecía «interesante», sin embargo no era «nada realista» en ese entonces. El 30 de octubre de 2012 The Walt Disney Company adquirió Lucasfilm y anunció la producción de una tercera trilogía que comenzaría en 2015 con el Episodio VII de la saga, en la cual Lucas habría de participar como consultor creativo mientras que la productora Kathleen Kennedy quedó a cargo de la presidencia de Lucasfilm así como de la producción ejecutiva de las nuevas películas de Star Wars. Aunque al principio se había anunciado a Michael Arndt como guionista para la séptima entrega de la serie, en octubre de 2013 se reveló que esta labor recaería en Lawrence Kasdan mientras que J. J. Abrams fue contratado como director. Kasdan, junto con Simon Kinberg, estuvieron a cargo también de los libretos de los Episodios VIII y IX, así como de una serie de películas adicionales derivadas de las tres trilogías principales de Star Wars. En marzo de 2015 Lucasfilm anunció que Rian Johnson habría de dirigir el Episodio VIII, mientras que la dirección del Episodio IX corrió a cargo nuevamente de J. J. Abrams. Antología En febrero de 2013, Iger dio a conocer el desarrollo de un par de películas derivadas del canon principal y que habrían de llevar el subtítulo «Una historia de Star Wars» para diferenciarlas de las trilogías cinematográficas. La primera de ellas, Rogue One, surgió en 2016 a partir de una idea concebida por John Knoll, supervisor de efectos visuales de las precuelas, para la serie Star Wars: Underworld que nunca llegó a concretarse. Su dirección y libreto recayeron en Gareth Edwards y Chris Weitz, respectivamente. El título hace alusión al Rogue Squadron de Una nueva esperanza además de ser la primera cinta de Star Wars que no forma parte de la saga principal. De igual manera describe la personalidad de su coprotagonista Jyn Erso. El filme, cuyo estreno ocurrió en diciembre de 2016 y contó con un reparto conformado por Felicity Jones, Diego Luna, Ben Mendelsohn, Donnie Yen, Mads Mikkelsen, Alan Tudyk, Riz Ahmed, Jiang Wen y Forest Whitaker, se centra en la hazaña de los rebeldes para obtener los planos de la Estrella de la Muerte antes de los sucesos del episodio IV. La segunda producción de la antología, Han Solo, llegó a los cines en mayo de 2018 y sigue las andanzas del personaje homónimo durante su juventud al lado de Chewbacca y Lando. Cabe mencionar que su desarrollo se remonta a la producción de La venganza de los Sith, ya que se pretendió incluir en esa película una versión infantil de Han Solo que habría de participar en la batalla de Kashyyyk. Años después, Lucas quiso hacer algo similar en la serie Star Wars: Underworld, para mostrar el primer encuentro del cazarrecompensas con Chewbacca y la forma en la que se apropió del Halcón Milenario de manos de Lando. Tras la adquisición de Lucasfilm por Disney en 2012, Jonathan Kasdan se hizo cargo de redactar el guion iniciado previamente por Lawrence Kasdan, a petición del propio Lucas. El elenco estuvo conformado por Alden Ehrenreich, Woody Harrelson, Emilia Clarke, Donald Glover, Thandiwe Newton, Phoebe Waller-Bridge, Joonas Suotamo y Paul Bettany, mientras que Ron Howard se ocupó de la dirección. Series de televisión Varios telefilmes y series de televisión basados en el universo de Star Wars han sido producidos desde el estreno de la primera entrega de la saga en 1977. La cadena CBS estrenó el fin de semana anterior a Acción de Gracias el primer producto derivado o spin-off oficial titulado Star Wars Holiday Special, un especial televisivo[Nota 1] dirigido por Steve Binder, escrito por Pat Proft, Leonard Ripps, Bruce Vilanch y Rod Warren, y con un limitado apoyo de George Lucas en la producción. Con una duración de unas dos horas, incluyendo los anuncios publicitarios, se trataba de un programa contenedor en el que los diferentes espectáculos y secuencias están hilvanados por una trama argumental situada unos dos años después de los sucesos acontecidos en Una nueva esperanza. Dicha trama se centra en la visita de Chewbacca y Han Solo al planeta natal del primero, Kashyyyk, para celebrar el «Día de la vida» ,Life Day,, una fiesta análoga a la Navidad. En este programa aparecen también Luke Skywalker, C-3PO, R2-D2, Darth Vader y Leia Organa, todos ellos interpretados por los mismos actores que en la trilogía, así como tres miembros de la familia de Chewbacca: su padre Attichitcuk, su esposa Mallatobuck y su hijo Lumpawarrump. El programa no fue bien recibido ni por el público ni por la crítica, y no fue puesta a la venta ninguna copia de forma oficial; el propio Lucas afirmó no estar contento con los resultados obtenidos y a partir de entonces supervisó personalmente el resto de proyectos, interviniendo en el guion o desempeñándose como productor. El éxito del que gozaron los nuevos personajes introducidos en Return of the Jedi entre los más jóvenes dio lugar a la creación de otras dos películas para televisión, emitidas en Acción de Gracias por la cadena ABC: Caravan of Courage: An Ewok Adventure (1984) y Ewoks: The Battle for Endor (1985), cuyas tramas tienen lugar entre El Imperio contraataca y Return of the Jedi. The Ewok Adventures se centra en el accidente de una familia humana en la luna de Endor y en el rescate que los dos hijos, Cindel y Mace, tienen que llevar a cabo junto a los nativos ewoks tras el secuestro de sus padres a manos de la criatura Gorax. En Ewoks: The Battle for Endor, un ejército de sanyassan y la bruja Charal atacan el poblado ewok, asesinando a Mace y sus padres; Cindel y el ewok Wicket W. Warrick consiguen escapar y se encuentran con el anciano Noa, quien también se estrelló en la luna, y juntos logran rescatar a los ewoks cautivos, vencer a los invasores y reparar una nave dañada para marcharse de Endor. Ambas películas ganaron un premio Primetime Emmy a los mejores efectos visuales, que estuvieron a cargo de Industrial Light and Magic, y una nominación al mejor programa infantil. Dos meses antes de la emisión de Ewoks: The Battle for Endor, en septiembre de 1985, ABC estrenó dos series de animación producidas por Lucasfilm y Nelvana Limited: Star Wars: Droids, con R2-D2 y C-3PO como protagonistas y emitida hasta junio de 1986, y Ewoks, con los ewoks como protagonistas y emitida hasta diciembre de 1986. Cartoon Network Studios produjo Star Wars: Clone Wars entre 2003 y 2005, una serie de dibujos animados centrada en el período de las Guerras Clon acontecido entre las películas El ataque de los clones y La venganza de los Sith. Con un total de tres temporadas, los veinte primeros episodios tienen una duración aproximada de tres minutos, mientras que los cinco últimos, diseñados para enlazar directamente con La venganza de los Sith, duran entre doce y quince minutos. Tres años después, y tras el estreno en cines de Star Wars: The Clone Wars, Cartoon Network inició la emisión de una serie homóloga que continúa la historia comenzada en el largometraje. La serie fue emitida en la cadena por cinco temporadas, hasta 2012. Tras la adquisición de Lucasfilm por Disney, la producción fue cancelada y se emitió una sexta temporada extra en el canal de pago por visión Netflix. Paralelamente, se dio luz verde a una nueva serie de animación por computadora, Star Wars Rebels, ambientada entre los episodios III y IV y que comenzó a emitirse a partir de 2014 en el canal Disney XD. A continuación aparecen listadas las series, películas y especiales televisivos de Star Wars: Series Serie N.º Temp. N.º de Episodios Fechas de emisión (en EE. UU.) Plataforma Primera emisión Última emisión Series animadas Star Wars: Droids 1 14 7 de septiembre de 1985 7 de junio de 1986 ABC Star Wars: Ewoks 2 26 7 de septiembre de 1985 13 de diciembre de 1986 Star Wars: The Clone Wars Película 15 de agosto de 2008 Estreno en cines 7 133 3 de octubre de 2008 4 de mayo de 2020 Cartoon Network, Netflix, Disney+ Star Wars Rebels Cortos 4 11 de agosto de 2014 1 de septiembre de 2014 Disney XD 4 75 3 de octubre de 2014 5 de marzo de 2018 Star Wars Resistencia Cortos 12 10 de diciembre de 2018 31 de diciembre de 2018 Disney Channel 2 40 7 de octubre de 2018 26 de enero de 2020 Star Wars: The Bad Batch 3 47 4 de mayo de 2021 1 de mayo de 2024 Disney+ Star Wars: Visions 2 18 22 de septiembre de 2021 Presente Star Wars: Tales 2 12 26 de octubre de 2022 Presente Young Jedi Adventures 1 25 4 de mayo de 2022 Presente Disney +, Disney Junior Miniseries animadas Star Wars: Guerras Clon 3 25 7 de noviembre de 2003 25 de marzo de 2005 Cartoon Network Star Wars: Blips 1 8 3 de mayo de 2017 4 de septiembre de 2017 Youtube Star Wars: Forces of Destiny 2 32 3 de julio de 2017 25 de mayo de 2018 Star Wars: Galaxy of Adventures 2 55 30 de noviembre de 2018 27 de agosto de 2020 Star Wars Roll Out 1 16 9 de agosto de 2019 1 de abril de 2020 Series en imagen real The Mandalorian 3 24 12 de noviembre de 2019 Presente Disney+ El Libro de Boba Fett 1 7 29 de diciembre de 2021 9 de febrero de 2022 Obi-Wan Kenobi 1 6 27 de mayo de 2022 22 de junio de 2022 Andor 2 24 21 de septiembre de 2022 13 de mayo de 2025 Ahsoka 1 8 23 de agosto de 2023 Presente The Acolyte 1 8 4 de junio de 2024 16 de julio de 2024 Skeleton Crew 1 8 3 de diciembre de 2024 Programa de concursos Jedi Temple Challenge 1 10 10 de junio de 2020 5 de agosto de 2020 StarWarsKids.com Otros programas Película Fecha de estreno (en EE. UU) Director Guionista(s) Responsable de la historia Productor(s) Refs Especial Star Wars Holiday Special 17 de noviembre de 1978 Steve Binder Pat Proft, Leonard Ripps, Bruce Vilanch, Rod Warren y Mitzie Welch Joe Layton, Jeff Starsh, Ken Welch y Mitzie Welch Películas Caravan of Courage: An Ewok Adventure 25 de noviembre de 1984 John Korty Lucas Guzmán, Chano Godoy y Mandingo Rey. George Lucas Thomas G. Smith y Patricia Rose Duignan Ewoks: The Battle for Endor 24 de noviembre de 1985 Jim Wheat y Ken Wheat Thomas G. Smith y Ian Bryce Novelas e historietas Artículo principal: Historietas de Star Wars Anthony Daniels, actor que interpreta a C-3PO en las películas. Debido al poco apoyo que recibió por parte de Fox, más allá de la concesión de licencias para la comercialización de camisetas y pósteres, la productora Lucasfilm contrató a Charles Lippincott como director de marketing de Una nueva esperanza y este consiguió un trato con la editorial Marvel Comics para adaptar la película a una historieta. El universo ficticio de Star Wars comenzó a ampliarse incluso antes de que se estrenará la primera película de la serie, a partir de la novelización de Una nueva esperanza, en 1976 ,escrito por Alan Dean Foster y acreditado a Lucas,. La novela de Foster, El ojo de la mente (1978), se convirtió en la primera obra del universo expandido en ser comercializada. Su trama se sitúa entre Una nueva esperanza y El Imperio contraataca, y expande considerablemente la cronología de la serie antes y después de los acontecimientos presentados en ambas películas. Ciertamente, se puede decir que el material alternativo comenzó a producirse durante la época en que se estrenó la trilogía original (1977-1983), sin embargo su desarrollo decayó en los años venideros. Pese a lo anterior, en 1992 se puso a la venta Thrawn Trilogy, de Timothy Zahn, la cual despertó nuevamente el interés en el universo de Star Wars. Desde entonces, varios cientos de novelas tipo tie-in han sido publicadas por Bantam y Del Rey Books. Un resurgimiento parecido ocurrió en el universo expandido en 1996, tras la aparición de la novela Shadows of the Empire de Steve Perry, cuya trama se sitúa entre El Imperio contraataca y Return of the Jedi. LucasBooks modificó de manera radical el universo de Star Wars con la introducción de la serie New Jedi Order, la cual tiene lugar aproximadamente veinte años después de Return of the Jedi, siendo protagonizada por un elenco integrado por personajes inéditos. Para las audiencias más jóvenes, se publicaron tres series: Jedi Apprentice, que sigue las aventuras de Qui-Gon Jinn y su aprendiz Obi-Wan antes de La amenaza fantasma; Jedi Quest, que relata las experiencias vividas por Obi-Wan y su aprendiz Anakin entre La amenaza fantasma y El ataque de los clones; y finalmente The Last of the Jedi, que retoma las aventuras de Obi-Wan y otro Jedi superviviente casi inmediatamente después de La venganza de los Sith. Marvel Comics publicó algunas series de cómics basadas en Star Wars entre 1977 y 1986. En su producción trabajaron varios escritores y dibujantes, entre los cuales pueden mencionarse Roy Thomas, Archie Goodwin, Howard Chaykin, Al Williamson, Carmine Infantino, Gene Day, Walt Simonson, Michael Golden, Chris Claremont, Whilce Portacio, Jo Duffy y Ron Frenz. Asimismo, Marvel lanzó una tira de prensa creada por Russ Manning, Steve Gerber, y Goodwin ,este último aparece en los créditos bajo un seudónimo,. A finales de los años 1980, Marvel anunció que publicaría una nueva historieta de Star Wars a cargo de Tom Veitch y Cam Kennedy. No obstante, en diciembre de 1991, Dark Horse Comics adquirió la licencia correspondiente para usar los derechos de Star Wars e incorporarlos así en un número considerable de secuelas literarias a la trilogía original, incluyendo la popular serie Dark Empire. A partir de entonces, Dark Horse ha publicado un gran número de escritos relativos al universo de Star Wars, aunado al lanzamiento de algunas historietas cómicas como Tag and Bink. Videojuegos Artículo principal: Videojuegos de Star Wars Desde 1982, se ha lanzado una amplia variedad de videojuegos basados en Star Wars, comenzando con Star Wars: El Imperio Contraataca, para la consola Atari 2600 por la compañía Parker Brothers. Desde entonces Star Wars ha sido trasladado a una multitud de juegos de acción, relacionados con varios juegos de simulación de vuelo espacial, disparos en primera persona, juegos de rol, juegos de ETR, y similares. Dos juegos de mesa oficiales diferentes, de estilo RPG, han sido desarrollados para el universo de Star Wars: una versión de West End Games en los años 1980 y 1990, y uno por Wizards of the Coast en la década de 2000. Algunos de los juegos más vendidos son los Lego Star Wars y la serie Battlefront, conformada principalmente de Battlefront I y Battlefront II, con 12 y 10 millones de unidades, respectivamente. Otro juego que goza de amplia popularidad en el género es Star Wars: Caballeros de la Antigua República. Si bien The Complete Saga se centra en los seis episodios de la serie, The Force Unleashed ,cuyo nombre es el mismo que el del proyecto multimedia del cual forma parte, toma lugar en el período inexplorado en las películas y situado entre La venganza de los Sith y Una nueva esperanza, y hace que los jugadores asuman el papel del «aprendiz secreto» de Darth Vader cuya misión es la de cazar a los Jedi supervivientes. El juego presenta un nuevo motor de juego en el aspecto gráfico, y fue lanzado el 16 de septiembre de 2008 en Estados Unidos. Hay dos títulos más basados en las Guerras Clon que fueron puestos a la venta a partir de noviembre de 2008 para Nintendo DS ,Star Wars Las Guerras Clon: La alianza de los jedi, y para Wii ,Star Wars Las Guerras Clon: Duelos con espadas de luz,. Además, existe una serie de videojuegos de rol titulada Star Wars: Caballeros de la Antigua República, la cual está conformada por Star Wars: Caballeros de la Antigua República ,2003; Xbox y PC,, Star Wars: Caballeros de la Antigua República 2: Los Señores Sith ,2005; Xbox y PC, y Star Wars: The Old Republic ,2011,. Otro videojuego ambientado en las Guerras Clon que gozó de gran popularidad, lanzado en Xbox y PC fue Star Wars: Republic Commando (2005). En dicho juego el jugador controla a un comando clon de fuerzas especiales de la República. Los últimos juegos lanzados a la venta a partir de los años 2010 son Star Wars: The Force Unleashed II (2010), Star Wars: The Old Republic (2011), Lego Star Wars III: The Clone Wars (2011), Kinect Star Wars (2012), Star Wars: Battlefront (2015), Star Wars Battlefront II (2017), Star Wars Jedi: Fallen Order (2019) y Star Wars: Squadrons (2020). Canon oficial Orden cronológico Star Wars: The High Republic Star Wars: Young Jedi Adventures Star Wars: The Acolyte Star Wars: La Amenaza Fantasma Star Wars: El Ataque de los Clones Star Wars: The Clone Wars Star Wars: La Venganza de los Sith Star Wars: The Bad Batch Star Wars Jedi: Fallen Order Star Wars Jedi: Survivor Star Wars: Tales (Diferentes épocas) Han Solo: Una Historia de Star Wars Obi-Wan Kenobi Star Wars Rebels Andor: Una Historia de Star Wars Rogue One: Una Historia de Star Wars Star Wars: Una Nueva Esperanza Star Wars: El Imperio Contraataca Star Wars: El Retorno del Jedi Star Wars: Squadrons Star Wars Battlefront II The Mandalorian El Libro de Boba Fett Ahsoka Star Wars: Skeleton Crew The Mandalorian & Grogu Star Wars Resistencia Star Wars: El Despertar de la Fuerza Star Wars: Los Últimos Jedi Star Wars: El Ascenso de Skywalker Producción de las películas Un equipo de rodaje con una cámara de alta definición CineAlta, creada por Sony y modificada por Panavision. Dos modelos de este tipo de cámaras, el HDW-F900 y el HDC-F950, se usaron para filmar los episodios II y III de Star Wars.  Las películas de Star Wars fueron rodadas en una relación de aspecto de 2.39:1.[Nota 2] Para los episodios IV y V, se utilizaron cámaras y lentes anamórficas de Panavision, mientras que para los episodios VI y I se usaron cámaras Arriflex, el primero con lentes Joe Dunton Camera (JDC) y el segundo con lentes Hawk, ambas también anamórficas. Por otro lado, los episodios II y III se filmaron con cámaras digitales de alta definición CineAlta, de la compañía Sony. Para la trilogía de secuelas estrenadas entre 2015 y 2019 primordialmente se utilizaron distintos modelos de cámaras de Panavision, que incluyen los Panaflex Millennium XL2. En los episodios VII y VIII también se usaron cámaras de Arri Alexa y lentes Hasselblad. Cabe señalarse que estas cintas estuvieron también disponibles en formato IMAX durante su estreno en cines.  George Lucas contrató a Benjamin Burtt, Jr. para que diseñara los efectos sonoros de la saga. Su contribución en Una nueva esperanza fue tal que la Academia de Artes y Ciencias Cinematográficas le condecoró con un premio especial a la trayectoria, debido a que no había reconocimiento alguno en esa época para el trabajo que había realizado. Además, Lucasfilm desarrolló el estándar de reproducción de sonido THX para Return of the Jedi. John Williams se encargó de componer la música de la saga. Desde un principio concibió piezas musicales majestuosas para Star Wars, con motivos únicos para cada personaje y cada concepto importante de la trama. El tema musical de Star Wars, compuesto también por Williams, está considerado desde entonces como uno de los más famosos y conocidos en la historia de la música contemporánea.. La coreografía técnica de los sables de luz para la trilogía original fue desarrollada por el especialista en esgrima de Hollywood Bob Anderson. Este fue quien entrenó al actor Mark Hamill (Luke Skywalker) y ejecutó todas las escenas de combate con sables de luz de Darth Vader en El Imperio contraataca y Return of the Jedi, vistiendo para ello la indumentaria del personaje. La participación de Anderson en la trilogía original de Star Wars fue destacada en el documental Reclaiming The Blade, donde él comparte sus experiencias como coreógrafo de luchas y principal responsable del manejo de los sables de luz en la saga. Reparto principal Personaje Películas La amenaza fantasma (1999) El ataque de los clones (2002) La venganza de los Sith (2005) Una nueva esperanza (1977) El Imperio contraataca (1980) Return of The Jedi (1983) El despertar de la Fuerza (2015) Los últimos Jedi (2017) El ascenso de Skywalker (2019) Darth Vader Hayden Christensen James Earl Jones (voz) David Prowse James Earl Jones (voz) James Earl Jones (voz) Anakin Skywalker Jake Lloyd Hayden Christensen Sebastian Shaw Hayden Christensen (edición en DVD de 2004) Hayden Christensen (voz) Obi-Wan Kenobi Ewan McGregor Alec Guinness Ewan McGregor (voz) Alec Guinness (archivo de voz) Ewan McGregor (voz) Alec Guinness (archivo de voz) Padmé Amidala Natalie Portman C-3PO Anthony Daniels R2-D2 Kenny Baker Jimmy Vee Lee Towersey Yoda Frank Oz (voz) Frank Oz (voz) Frank Oz (archivo de voz) Frank Oz (voz) Palpatine Darth Sidious Ian McDiarmid Clive Revill Ian McDiarmid (edición en DVD de 2004) Ian McDiarmid Ian McDiarmid Mace Windu Samuel L. Jackson Samuel L. Jackson (voz) Nute Gunray Silas Carson Jar Jar Binks Ahmed Best (voz) General Grievous Matthew Wood (voz) Bail Organa Jimmy Smits Conde Dooku Lord Tyranus Christopher Lee Jango Fett Temuera Morrison Shmi Skywalker Pernilla August Qui-Gon Jinn Liam Neeson Liam Neeson (voz) Liam Neeson (voz) Sabé Keira Knightley Watto Andy Secombe (voz) Jabba el Hutt Larry Ward (voz) (actor sin acreditar, voz) (Edición Especial 1997) Larry Ward (voz) Darth Maul Ray Park Peter Serafinowicz (voz) Sebulba Lewis MacLeod (voz) Luke Skywalker Aiden Barton (bebé, cameo) Mark Hamill Leia Organa Aiden Barton (bebé, cameo) Carrie Fisher Carrie Fisher (archivo de escenas eliminadas) Han Solo Harrison Ford Harrison Ford Chewbacca Peter Mayhew Joonas Suotamo Boba Fett Daniel Logan Jeremy Bulloch Jason Wingreen (voz) Temuera Morrison (voz) (edición en DVD de 2004) Wedge Antilles Denis Lawson Denis Lawson Lando Calrissian Billy Dee Williams Billy Dee Williams Wicket W. Warrick Warwick Davis Warwick Davis Maximilian Veers Julian Glover Owen Lars Joel Edgerton Phil Brown Beru Lars Bonnie Piesse Shelagh Fraser Wilhuff Tarkin Wayne Pygram Peter Cushing Lor San Tekka Max Von Sydow Rey Daisy Ridley Finn John Boyega Poe Dameron Oscar Isaac Snoke Andy Serkis Andy Serkis (voz) Kylo Ren Adam Driver Hux Domhnall Gleeson Maz Kanata Lupita Nyongo Phasma Gwendoline Christie Rose Tico Kelly Marie Tran Amilyn Holdo Laura Dern DJ Benicio del Toro Jannah Naomi Ackie Zorii Bliss Keri Russell Música Artículo principal: Música de Star Wars Las películas de Star Wars poseen un conjunto de temas musicales con motivos orquestados y un toque de calidad consistente que difícilmente pueden hallarse en conjunto en otras series cinematográficas. Su estructura, conformada por varios motivos individuales, y su composición únicas constituyen una herramienta de estudio, de forma equiparable a la banda sonora de El Señor de los Anillos, de Howard Shore. Spielberg le sugirió a Lucas contratar al compositor John Williams, con quien había colaborado previamente en Tiburón y por el cual se hizo acreedor a un primer premio Óscar. Lucas sentía que la película debía describir mundos inéditos en el plano visual, aunque en el aspecto musical su enfoque era brindar a la audiencia una conexión emocional con la historia. En marzo de 1977, Williams comenzó a dirigir a la Orquesta Sinfónica de Londres para grabar la música de Star Wars durante un período de veinte días y ocho sesiones en los estudios Anvil, en Denham, Inglaterra. La orquestación recayó en Herbert W. Spencer, frecuente socio de Williams, que también se encargó de la misma función en los siguientes dos filmes de Star Wars. En 2005, el American Film Institute reconoció a la banda sonora de Episodio IV como la música cinematográfica más memorable de todos los tiempos. Asimismo, Williams obtuvo tres premios Grammy en febrero de 1978,   además de su tercer premio Óscar en su trayectoria como compositor tan solo un mes después, el 3 de abril, ambos por la creación del material musical de la película. Cabe señalar que el tema principal fue inspirado en la composición de Kings Row (1942), creada por Erich Wolfgang Korngold, mientras que la pista «Dune Sea of Tatooine» fue trazada a partir del movimiento llamado Le Sacrifice del ballet La consagración de la primavera escrito por el compositor ruso Igor Stravinsky en 1913. Para Filmtracks, «Ninguna composición orquestal ha tenido mayor influencia en la historia del cine y en sus elementos musicales que la original Star Wars». Para El Imperio contraataca, Williams fue nuevamente convocado para grabar la banda sonora, y este recurrió una vez más a la Orquesta Sinfónica de Londres. La banda sonora del Episodio II quedó concluida después de un total de 18 sesiones de grabación en los mismos estudios Anvil, un proceso que duró únicamente tres días de diciembre de 1979, aunado a seis días más en enero de 1980. Finalmente, al igual que en sus predecesoras, Williams recurrió a la Sinfónica de Londres para grabar el material musical de Return of the Jedi, el cual se grabó en los estudios Abbey Road, y fue distribuida por la discográfica RSO Records. Cabe señalar que en 2004 la empresa Sony Classical compró los derechos de distribución de las bandas sonoras de la trilogía original de Star Wars ,lo anterior se debió, principalmente, a que ya contaba para entonces con los derechos de comercialización de la música relativa a la trilogía de precuelas,. De acuerdo al sitio web Filmtracks: «Williams mantuvo un nivel de calidad sobresaliente para El Retorno del Jedi que muchos considerarían incluso mejor que las bandas sonoras de cualquiera de las precuelas que le siguen […] logró desafiar los avances tecnológicos de su época». John Williams fue el responsable de todos los temas musicales de Star Wars. La segunda trilogía definió un nuevo estilo de redacción de las letras, que se tornó extraordinariamente complejo para Williams. Las bandas sonoras de las siguientes dos películas tomarían varios elementos prestados de las composiciones incorporadas en el conjunto de La Amenaza Fantasma. Según Amazon.com, Episodio I «la película más anticipada de los años 1990, regresa a los orígenes de Star Wars […] La sorpresa más ambiciosa del compositor es la bienvenida adición de elementos corales fuertes, los cuales usa tanto de forma majestuosa como amenazadora». Siendo considerada como una de las bandas sonoras con mayor expectativa en la historia de Hollywood, fue lanzada al mercado incluso dos semanas antes de que se estrenará la película en salas de cine, ocasionando una fuerte demanda en Internet, que acabó con las descargas ilícitas de algunos fragmentos del álbum. Para Filmtracks, «uno de los pocos defectos en este recopilatorio es que su música para la invasión del ejército droide, así como algunas de las piezas de cuerda más bajas, se asemeja bastante a la de Indiana Jones y la última cruzada». Por otra parte, la banda sonora de El ataque de los clones «continuó con las altas expectativas impuestas por sus predecesoras, logrando con éxito incorporar algunos nuevos temas musicales al mismo tiempo que lleva la fábula galáctica al punto de partida». En las propias palabras de Williams: «La Amenaza Fantasma sirvió como una reintroducción, o un regreso, a Star Wars después de 22 años […] Pensé que sería una transición complicada, pero en realidad fue más bien como ir en bicicleta. Parte de esa banda sonora, y de esta también [la de Episodio II], es musicalmente incestuosa, en comparación con los temas anteriores, y eso nos ayuda a volver de vuelta a la imaginación de Lucas». La banda sonora de La venganza de los Sith se grabó en un plazo de cinco días en los estudios Abbey Road, y contó de nuevo con la participación de la Orquesta Sinfónica de Londres y el conjunto London Voices. El proceso de grabación comenzó el 3 de febrero de 2005; una mañana entera fue dedicada específicamente a la grabación vocal. De acuerdo a Amazon.com, «debido a que esta música acompaña a la más emocionante película de Star Wars, esta es más divertida, maligna, así como más malévola y desigual». Mientras tanto, Filmtracks opinó que, para John Williams, con el lanzamiento de la banda sonora de Episodio III, «su carrera se elevó al estrellato, pues sus temas para las películas persisten en las vidas incluso de aquellos que no han visto una cinta de Star Wars en los cines en veinte años». En términos generales, de acuerdo al mismo sitio web: «Una vez que La Amenaza Fantasma revivió la saga en 1999, Williams hizo un esfuerzo concertado para continuar con la exposición de sus ideas […] a pesar de que hubo fuertes críticas en su época contra La Amenaza Fantasma por su aparente debilidad [… Fue] entonces cuando apareció El Ataque de los Clones, donde más allá de crear tres o cuatro temas principales, motivos o ritmos, condensó todos ellos en el tema Across the Stars, una pieza extremadamente poderosa y efectiva». Finalmente, en palabras del propio Williams: «En La venganza de los Sith, hay tres o cuatro piezas de nuevo material […] Un par de ellos consisten en lamentaciones, y acompañan precisamente algunas de las escenas más oscuras del filme. Y hay también una especie de pieza divertida, que incluye mucha música de percusión, para el personaje de Grievous». De acuerdo al sitio web oficial, «al final, solamente tres de los cuatro previos lanzamientos de Star Wars pueden ser asociados con la evolución de la película original de Lucas: la primera banda sonora de la 20th Century Fox de 1977; el compilatorio musical de la primera trilogía de Star Wars lanzado en 1993 y el lanzamiento de BMG en 1997, con las versiones remasterizadas y expandidas de la primera película de la serie». Efectos especiales El sector de los efectos especiales era casi inexistente en Hollywood cuando George Lucas empezó a rodar Star Wars: Episodio IV - Una nueva esperanza. Por este motivo, Lucas decidió crear su propia compañía de efectos especiales. La compañía ,bautizada como Industrial Light & Magic (ILM), alquiló una gran almacén en el valle de San Fernando, Los Ángeles, y se puso a trabajar en el diseño de las maquetas, las tomas con efectos y el equipo. La tecnología necesaria para conseguir los efectos que Lucas quería no existía, así que hizo falta mucho trabajo de investigación y desarrollo antes de poder rodar un solo fotograma. Esta larga fase de preproducción se convirtió en un tema controvertido en los meses previos al rodaje principal de La guerra de las galaxias, ya que se retrasaron cientos de tomas mientras se perfeccionaba el equipo. El primer dispositivo que creó la compañía fue la cámara de control de movimiento o cámara Dykstraflex. Se trataba de la primera cámara totalmente controlada por ordenador que permitía siete ejes de movimiento y podía reproducir los mismos movimientos en tomas múltiples. Tal innovación resultó crucial para producir las tomas del Halcón Milenario huyendo de la Estrella de la Muerte o del desfile aéreo del Destructor Estelar con el que comienza la película. También fue fundamental conseguir el aspecto gastado y realista que Lucas quería que tuviera aquella lejana galaxia. Los maquetistas de ILM y el departamento artístico británico, dirigidos por el diseñador de producción John Barry, diseñaron las maquetas, los decorados y los vehículos de tamaño real con quemaduras y manchas de polución, lo que les daba credibilidad sobre el telón de fondo irreal de astros, planetas y una estación espacial del tamaño de la Luna. A principios de 1977, cuando solo faltaban unos meses para el estreno de la película, las tomas de los efectos especiales aún iban con retraso. Sin embargo, ILM entregó el género justo a tiempo y, finalmente, tanto el público como la crítica recibieron con entusiasmo los pioneros efectos de La guerra de las galaxias. La primera película de la saga Star Wars combinó modernos efectos especiales de la época con lo tradicional: las miniaturas que se movían manualmente. En este largometraje de Lucas había maquetas a tamaño real como, por ejemplo, una de parte del Halcón Milenario cuyo movimiento simulaban varios operarios agitándola. Además, también se construyeron maquetas en miniatura y las máscaras que llevaban algunos personajes, en especial los que aparecían en la escena de la cantina de Mos Eisley. El personaje de Yoda, el Maestro Jedi, es un buen ejemplo para analizar la evolución de los efectos especiales en Star Wars –y en la propia historia del cine- puesto que aparece en cinco de las seis primeras películas de la saga. Su primera aparición fue en El Imperio contraataca (1980) y fue también la más primitiva en cuanto a los efectos. En esta película, Yoda es una marioneta movida por hilos, creada por Stuart Freeborn y accionada por Frank Oz, quien puso voz al Maestro Jedi en todas las películas. Los escenarios de Dagobah estaban situados metro y medio por encima del plató para que Oz, escondido, pudiera manejar al pequeño Yoda, que no realizaba movimientos demasiado complejos en esa película ni en la siguiente, El retorno del Jedi. El caso de Yoda, en la trilogía original, fue similar al de otros personajes como Jabba el Hutt, que también era una marioneta gigante. En La amenaza fantasma (1999), Yoda pasó a ser un personaje creado por ordenador, igual que muchos otros como Jar Jar Binks. Los cambios en su aspecto físico son notables porque se supone que el personaje es un poco más joven y no lleva 20 años exiliado en un planeta pantanoso. No obstante, sin tener en cuenta el factor más puramente estético, los cambios más importantes de Yoda fueron a nivel de movimientos. De ser un personaje más bien parado, pasó a ser un personaje mucho más activo en Star Wars Episodio II: El ataques de los Clones. En su duelo contra el conde Dooku, su antiguo padawan, descubrimos que Yoda es capaz de moverse muy rápido y dar increíbles saltos –pese a su baja estatura- mientras empuña el sable láser. Para El Imperio contraataca se creó un Halcón Milenario a escala real. El decorado pesaba 23 toneladas y medía casi 5 metros de altura y 20 de diámetro. En los episodios I, II y III la mayor parte de los decorados se realizaron con ordenador, y cuando estos tenían una base real, como la plaza de España de Sevilla, que simula ser un edificio del planeta Naboo en El ataque de los clones, las imágenes eran retocadas digitalmente. A diferencia del Episodio IV, que tan solo tenía 365 tomas con efectos especiales, el Episodio III: La venganza de los Sith, cuenta con 2200 tomas con efectos especiales. Es más, en dicha película, todos los planos fueron retocados digitalmente, independientemente de que fueran combates de naves o escenas de diálogo. La Dykstraflex no fue la única creación tecnológica creada a propósito de la saga Star Wars. En la trilogía original, para animar a las criaturas, ILM mejoró la técnica del Stop-motion y nació así el “Go motion”, donde se seguían utilizando los modelos usados en la animación tradicional pero con las articulaciones motorizadas para que realizasen los movimientos en el tiempo en el que la cámara rodaba un fotograma de la película. Más tarde, se difuminaron los contornos para dar una mayor sensación de velocidad. La técnica empleada para los vehículos siguió siendo la del Stop motion, con el movimiento de los vehículos realizado de forma manual fotograma a fotograma. En el ataque de los AT-AT en el planeta nevado Hoth, por ejemplo, cada movimiento de una pata del vehículo implicaba muchos movimientos manuales ya que un segundo de grabación comprende 24 fotogramas. En 2012, Disney compró Lucasfilm y los derechos de las películas a Lucas por 3125 millones de euros y se puso manos a la obra para relanzar la saga Star Wars. A pesar de que el director del Episodio VII: El despertar de la Fuerza, J. J. Abrams, dijo que quería volver al espíritu de la trilogía clásica, ni Abrams ni Disney renunciaron a las nuevas tecnologías a la hora de crear efectos. La actriz Lupita Nyongo, por ejemplo, dio vida a un personaje llamado Maz Kanata, hecho por medio de la técnica del “motion capture” o captura del movimiento. Dicha técnica consiste en recubrir la cara y el traje del actor de sensores para captar todos sus gestos y movimiento, que luego se trasladan a un personaje creado por ordenador. Otro de los efectos especiales utilizado en la primera trilogía de Star Wars fue el matte painting, una técnica artística muy utilizada para crear escenarios. Consiste en aplicar capas de pintura opaca (generalmente óleo) sobre un cristal o panel de plexiglás para después iluminar el resultado desde atrás. Las zonas que se deseen más iluminadas se cubren con tonos más claros o con menos cantidad de pintura y, las más oscuras, se saturan de pigmento para que no pase la luz. Si el escenario tiene focos o puntos de luz como estrellas, se dejan pequeños puntos sin pintura para que dejen pasar la luz. En el caso de Star Wars, los recónditos interiores de la Estrella de la Muerte o algunas de las escenas en Tatooine y Endor se rodaron utilizando esta técnica. Recepción Recaudaciones Hasta 2019 la recaudación generada por las tres trilogías de Star Wars es de 9.243.321.766 dólares, lo que la convierte en la segunda saga cinematográfica más exitosa de todos los tiempos. Según un estudio realizado por la revista Forbes en 2005, se estima que el universo entero de Star Wars (incluyendo todo tipo de productos licenciados a terceras empresas, entre los cuales se encuentran novelas, libros, juegos de tablero, muñecos, etc.) ha ganado más de 20 000 millones de dólares desde el debut de Una nueva esperanza. En cuanto a la recaudación lograda por cada una de las seis películas, puede recurrirse a la siguiente tabla para mayor información: Película Estreno Recaudación (en dólares) Ref(s) Estados Unidos Resto del mundo Total Presupuesto Star Wars: Episode IV - A New Hope 25 de mayo de 1977 460.998.507$ 314.513.557$ 775.512.064$ 13.000.000$  Star Wars: Episode V - The Empire Strikes Back 21 de mayo de 1980 290.271.960$ 257.607.494$ 538.375.067$ 18.000.000$  Star Wars: Episode VI - Return of the Jedi 25 de mayo de 1983 309.306.177$ 166.040.934$ 482.365.284$ 32.5.000.000$  Star Wars: Episode I - The Phantom Menace 19 de mayo de 1999 474.544.677$ 552.538.030$ 1.027.082.707$ 115.000.000$  Star Wars: Episode II - Attack of the Clones 16 de mayo de 2002 310.676.740$ 343.103.230$ 653.779.970$ 115.000.000$  Star Wars: Episode III - Revenge of the Sith 19 de mayo de 2005 380.270.588$ 488.765.058$ 868.035.635$ 113.000.000$  Star Wars: The Clone Wars 15 de agosto de 2008 35.161.554$ 33.121.290$ 68.282.844$ 8.5.000.000$  Star Wars: Episode VII - The Force Awakens 18 de diciembre de 2015 936.662.225$ 1.131.561.399$ 2.068.223.624$ 306.000.000$  Rogue One: A Star Wars Story 10 de diciembre de 2016 532.177.324$ 523.879.949$ 1.056.057.278$ 200–265.000.000$  Star Wars: Episode VIII - The Last Jedi 15 de diciembre de 2017 620.181.382$ 714.358.507$ 1.334.539.889$ 317.000.000$  Solo: A Star Wars Story 25 de mayo de 2018 213.767.512$ 179.157.295$ 393.924.807$ 275–300.000.000$  Star Wars: Episode IX - Rise of Skywalker 20 de diciembre de 2019 515.202.543$ 561.941.706$ 1.077.232.589$ 275.000.000$  Total de la triple trilogía 5.082.065.844 5.038.322.809 10.340.762.580 2.092–2.157.000.000$  Véase también: Anexo:Películas con las mayores recaudaciones Crítica La huella de Darth Vader impresa en el Graumans Chinese Theatre, apenas unos meses después del debut de Una nueva esperanza. Star Wars logró redefinir el género cinematográfico de la ciencia ficción gracias a sus efectos especiales innovadores, iniciando asimismo el concepto de «película taquillera del verano» y, a rasgos generales, el enfoque y la atención que las producciones fílmicas posteriores dirigieron hacía las audiencias más jóvenes. Descritas como «innovadoras» por gran parte de la crítica contemporánea, la producción de cada película fue recibida posteriormente con opiniones opuestas: se reconoció la sofisticación técnica, en todo caso, sin embargo se criticó el argumento débil, los diálogos y los personajes estereotipados. En una reseña de 1977, Roger Ebert describió Una nueva esperanza como «una experiencia extra-corporal» y comparó sus efectos visuales con los de 2001: A Space Odyssey aunque para Pauline Kael de la revista The New Yorker, la película carecía de «aliento, poesía y adherencia emocional». A su vez, Jonathan Rosenbaum de Chicago Reader apuntó: «Ninguno de los personajes tiene profundidad, y se usan como elementos secundarios», mientras que para Stanley Kauffmann, de la publicación The New Republic: «El trabajo de Lucas en Star Wars resulta más inventivo que el de THX 1138». Algunos consideran que uno de los principales logros técnicos de esta producción fue la mejora de la tecnología del sonido en las salas de cine, pues «popularizó el sistema Dolby de reducción de ruido y dio lugar a la creación del sistema THX y a la introducción del sonido digital». Por otra parte, la crítica elogió El Imperio contraataca ,considerada como la mejor película de Star Wars,, aunque algunos detalles como la estructura de la historia fueron evaluados de forma negativa, señalando que esta no presenta ni un principio ni un final, aunque The Return of the Jedi, de forma contraria, fue recibida con críticas variadas, quejándose algunos especialistas de que el último episodio de la serie es el menos original y el más antiestético de la trilogía original. Chris Gore comentó al respecto: «Desafortunadamente, los problemas de los Jedi no pueden arreglarse ni siquiera con el mejor software digital en la galaxia: la historia débil (otro asalto a una nueva Estrella de la muerte, una visita más a Dagobah, el exótico planeta de los árboles, y osos de peluche fastidiosos), las malas actuaciones (Carrie Fisher admitió que en realidad ella estaba como «perdida» durante el rodaje de la cinta), las bromas con los eructos (tres en la primera media hora. Creo que perdí el sentido del humor en los primeros dos) y el extraño peinado de Luke tipo trapeador. Es lamentable». A pesar del gran éxito de recaudaciones de la trilogía de precuelas ,La amenaza fantasma batió varios récords de taquilla, incluyendo el de mayores ganancias en su primer fin de semana de proyección,, las críticas no fueron tan benévolas: aunque las primeras dos películas de esta nueva trilogía fueron aclamadas por sus efectos especiales, hubo descontento general respecto a las malas actuaciones y un guion mediocre. Poco después, Hayden Christensen fue nominado a un premio Razzie como «Peor actor de reparto» por su actuación en La venganza de los Sith, mientras que las dos primeras cintas resultaron nominadas igualmente en la categoría de «Peor película».  La amenaza fantasma fue criticada particularmente por el personaje de Jar Jar Binks, el torpe aliado de Qui-Gon y Obi-Wan diseñado con un aspecto caricaturesco. Tanto la audiencia como la crítica mostraron su desprecio hacia ese personaje, que fue considerado como un pretexto para utilizar bromas infantiles, y calificado como un estereotipo afroamericano o jamaiquino de tipo racista. Respecto a esta última acusación, Lucas negó cualquier intención discriminatoria en el personaje, diciendo: «¿Cómo podrías comparar a un anfibio naranja y decir que es un jamaiquino? Es completamente absurdo. Créanme, Jar Jar no fue concebido a partir de un jamaiquino, ni siquiera me pasó por mi imaginación […] Hay un grupo de cinéfilos que no les agradan los compañeros cómicos del personaje estelar. Quieren que las películas sean serias como The Terminator, y se decepcionan bastante, y se muestran obstinados con cualquier cosa que tenga que ver con algo infantil». Aunque los críticos consideraron que El ataque de los clones presenta graves defectos e incongruencias en el libreto, ciertamente es mejor que su predecesora al ser un producto más equilibrado y mejor estructurado. A diferencia de las anteriores, La venganza de los Sith obtuvo las mejores evaluaciones de la trilogía: si bien se criticó la actuación de Christensen, los especialistas elogiaron las participaciones de Ewan McGregor como Obi-Wan, y Ian McDiarmid como Palpatine, además de resaltar la calidad del guion, en el cual colaboró Tom Stoppard. En lo referente al guion, se criticaron fuertemente las escenas de diálogos románticos entre Anakin y Padmé. Ebert señaló en su crítica: «Anakin tiene una cita con Padmé; en la película anterior, se casaron en secreto, y ahora ella le revela que está embarazada. La reacción de este es la de un niño en una comedía adolescente, tratando de parecer contento, mientras se pregunta cómo afectará eso a las cosas que él suele hacer. Decir simplemente que George Lucas no sabe escribir una escena romántica es un eufemismo; incluso las tarjetas de felicitación son más apasionadas». Finalmente, algunos críticos señalaron que la película incorpora una fuerte crítica contra George W. Bush y la guerra de Irak. Por otro lado, para los escritores y críticos culturales Peter Biskind y John Wight, Star Wars es una vez más el retorno de los «valores tan oxidados como el heroísmo y el individualismo» que, mediante un «fundamentalismo moral maniqueo», reafirman la política estadounidense anterior a la época de la crítica y la reflexión social que va desde el fin de la guerra de Vietnam a la gestión de Jimmy Carter.  A pesar de que la trilogía original se considera, en la actualidad, superior a las precuelas, el sitio web Rotten Tomatoes, en un análisis de ambas trilogías, concluye que la superioridad de la nueva trilogía se debe a que la mayoría de los comentarios de las películas originales, aplicándose lo mismo a las opiniones de los fanáticos, fue emitida en 1997 (debido a las reediciones de las películas), por críticos que eran niños para cuando debutó la serie original, y por lo tanto mantenían sus recuerdos de la infancia. De esta forma, sus evaluaciones pueden verse más o menos afectadas por ese detalle. Además, algunas reseñas de esa época entraban en conflicto entre sí, en comparación con las críticas de las precuelas, donde se presentaron opiniones más similares. La película Star Wars: Episodio VII - El despertar de la Fuerza no fue muy bien recibida por parte de la crítica y el público en general, considerada una copia descarada de la primera película de la saga, lo que motivo a modificar el rumbo de la franquicia. de la franquicia tras las reseñas dispares que obtuvo la trilogía precuela. El mismo George Lucas criticó el enfoque que presentaba la cinta, alegando que era una versión retro de la saga, además arremetió contra Disney (la nueva dueña de la franquicia) llamándolos esclavistas blancos aunque retiró sus palabras poco después. Los críticos elogiaron particularmente las actuaciones de Daisy Ridley y John Boyega como los nuevos protagonistas de la saga, en los papeles de Rey y Finn respectivamente.[cita requerida] Sin embargo, debido la pobre escritura de sus secuelas, disney optó por centrarse en lanzar precuelas en lugar de continuar la saga. Película Rotten Tomatoes Metacritic IMDb FilmAffinity CinemaScore General Top Critics Audiencia A New Hope 93 % (113 reseñas) 90 % (39 reseñas) 96 % 92 (24 reseñas) 8,6 (1 044 805 votos) 7,9 (130 463 votos) - Empire Strikes Back 94 % (90 reseñas) 88 % (24 reseñas) 97 % 82 (25 reseñas) 8,8 (972 828 votos) 8,1 (130 252 votos) - Return of the Jedi 80 % (86 reseñas) 76 % (25 reseñas) 94 % 58 (24 reseñas) 8,3 (799 788 votos) 7,9 (116 317 votos) - The Phantom Menace 55 % (216 reseñas) 41 % (58 reseñas) 59 % 51 (36 reseñas) 6,5 (611 856 votos) 6,2 (96 707 votos) A- Attack of the Clones 66 % (245 reseñas) 40 % (50 reseñas) 57 % 54 (39 reseñas) 6,6 (535 444 votos) 6,3 (100 672 votos) A- Revenge of the Sith 80 % (289 reseñas) 67 % (52 reseñas) 66 % 68 (40 reseñas) 7,6 (597 057 votos) 7,1 (96 652 votos) A- The Force Awakens 93 % (383 reseñas) 89 % (54 reseñas) 88 % 81 (54 reseñas) 7,9 (842 647 votos) 6,9 (56 476 votos) A Rogue One 84 % (446 reseñas) 76 % (67 reseñas) 86 % 65 (51 reseñas) 7,8 (535 899 votos) 6,9 (42 782 votos) A The Last Jedi 90 % (471 reseñas) 95 % (62 reseñas) 43 % 84 (56 reseñas) 7,0 (544 887 votos) 6,1 (39 134 votos) A Solo 70 % (471 reseñas) 61 % (59 reseñas) 63 % 62 (54 reseñas) 6,9 (275 279 votos) 6,0 (21 483 votos) A- The Rise of Skywalker 51 % (491 reseñas) 48 % (56 reseñas) 86 % 53 (61 reseñas) 6,6 (341 406 votos) 5,8 (24 881 votos) B+ Promedio 81 % 72 % 75 % 71 7,7 7,1 A- Premios Las dos primeras trilogías de Star Wars recibieron un total de veintidós nominaciones a los premios Óscar de la Academia de Artes y Ciencias Cinematográficas, de los que recibió finalmente siete y tres premios especiales al mérito, uno para Benjamin Burtt Jr. por la creación de las voces de alienígenas, criaturas y robots en A New Hope y dos para el equipo de efectos visuales en The Empire Strikes Back y Return of the Jedi. Por otra parte, la serie obtuvo un total de quince nominaciones a los premios BAFTA, logrando triunfar en cuatro categorías, entre las cuales se incluyen dos reconocimientos Anthony Asquith para mejor música de película (para A New Hope y The Empire Strikes Back, ambos para John Williams). En cuanto a los premios Globo de Oro, únicamente fueron nominados los episodios IV y V; el primero recibió en total cuatro nominaciones, de las cuales obtuvo un único galardón en la categoría de «Mejor banda sonora». Finalmente, The Empire Strikes Back se hizo acreedor a una sola nominación en la categoría de «Mejor banda sonora». A continuación, se señala el número de premios y nominaciones que obtuvo en total cada una de las películas de Star Wars (en la tabla posterior, se muestran únicamente los resultados obtenidos en los premios Óscar, BAFTA y Globo de Oro, los cuales son los más populares en la industria del cine): Una nueva esperanza: 33 premios y 21 nominaciones. El Imperio contraataca: 11 premios y 12 nominaciones. Return of the Jedi: 11 premios y 14 nominaciones. La amenaza fantasma: 11 premios y 52 nominaciones. El ataque de los clones: 10 premios y 36 nominaciones. La venganza de los Sith: 13 premios y 27 nominaciones. El despertar de la Fuerza: 14 premios y 74 nominaciones. Rogue One: 7 premios y 36 nominaciones. Los últimos Jedi: 9 premios y 36 nominaciones. Han Solo: 2 premios y 10 nominaciones. El ascenso de Skywalker: 7 premios y 21 nominaciones. Mercadotecnia Artículo principal: Universo expandido de Star Wars Star Wars también marcó una importante innovación en el dominio del merchandising que hasta entonces era considerado como una parte esencial para la promoción de una película. De hecho en ese entonces solo había una productora o estudio cinematográfico orientado a esta actividad: Universal. En cualquier caso, casi era seguro que únicamente The Walt Disney Company podía obtener buenas ganancias con dicha práctica. No obstante, Lucas en vez de pedir un aumento de sueldo por la dirección de Una nueva esperanza, le pidió a Fox los derechos de licencia de la mayor parte del merchandising y los ingresos derivados de la película. Estos últimos aceptaron, independientemente del potencial comercial que tenía la película. Sin embargo, nadie quería producir productos basados en la película; unos cuantos pósteres eran el único tipo de producto que se podía adquirir respecto a las películas. Kenner solo trató de producir una línea de figuras de acción, y hubo una alta demanda por parte de los coleccionistas en cuanto comenzaron a ser producidos en 1977. Sin embargo, no eran suficientes los ejemplares que salían a la venta para entonces, pues a partir del estreno de Una nueva esperanza, la demanda se incrementó. Por lo tanto, se adoptó una medida especial por parte de la empresa: poner en el mercado una serie de envases denominados «Early Bird», los cuales contenían cada uno un boleto, con el cual podrían adquirir el producto, comprometiéndose entonces a enviar al cliente la figura de acción, tan pronto como lo produjeran. Desde entonces, el merchandising relacionado con la saga se ha vuelto cada vez más en un asunto de interés para los productores, llegando a expandirse en una amplia variedad de productos: desde bustos, estatuas y réplicas de utilería, hasta videojuegos, cómics, libros y carteles, los cuales son solo una fracción del vasto imperio de productos vinculados con Star Wars. En enero de 2010, la empresa Adidas lanzó al mercado una colección de zapatos y ropa dedicados a la saga. Hoy en día, la venta de productos derivados de la saga ha sido fructífera; tan solo en Estados Unidos, se llegan a consumir cerca de 100 millones de dólares al año, y desde 1977 se calculan ganancias por un total de nueve mil millones de dólares en todo el mundo. Star Wars: Legends El término «universo expandido», del inglés Expanded Universe, es usado para referirse a todos los elementos de ficción, historias, personajes, vehículos, planetas, etcétera, que no provienen de las películas mismas de la saga. El universo ficticio en el que transcurren las películas oficiales de la saga se ve de este modo oficialmente enriquecido y aumentado con todos aquellos productos derivados autorizados por Lucasfilm para formar parte del universo de ficción de Star Wars. Entre estos productos surgidos del merchandising de la franquicia pueden contarse, por ejemplo, series o programas de televisión, otras películas, novelas, historietas, suplementos de ambientación para juegos de rol, videojuegos, etc. Todos estos elementos, oficializados por Lucasfilm como parte integrante de Star Wars como universo ficticio, extienden la historia de Star Wars más allá de las dos trilogías cinematográficas. El tiempo en el que se sitúan estas historias retrocede hasta unos 25 000 años antes de los acontecimientos narrados en Una nueva esperanza (año 0 ABY) y avanza hasta 140 años después. El primer producto del universo expandido se lanzó a la venta en enero de 1978 y consistió en la séptima entrega de una serie de historietas sobre Star Wars ,los seis ejemplares previos eran más bien una adaptación literaria de Una nueva esperanza,, siendo publicada por Marvel Comics, y la cual continuaba la historia de Luke Skywalker tras los acontecimientos de la primera cinta estrenada de la serie. Dicha historia llevó por título «New Planets, New Perils». No obstante, se tiene conocimiento del cómic a manera de serie titulado «The Keepers World», publicado en octubre de 1977, como parte de la revista Pizzazz. De esta forma, este último es considerado como el primer material inédito que extendió los relatos presentados en las películas. Apenas un mes después del lanzamiento de «New Planets, New Perils», apareció la novela Splinter of the Minds Eye, de Alan Dean Foster, la cual fue seguida directamente por el programa especial de televisión Star Wars Holiday Special que, a pesar de sus desfavorables críticas, incorporó algunos elementos e ideas que luego serían usados en las siguientes películas de Star Wars ,como por ejemplo el cazarrecompensas Boba Fett, que aparecería más tarde en El Imperio contraataca o el planeta boscoso de los wookiees, Kashyyyk, que aparecería en La venganza de los Sith,. Antes de la adquisición de Lucasfilms por Disney en 2012, Lucas conservaba el control creativo sobre el universo expandido, siendo algunos de sus elementos usados después en las películas. Así, por ejemplo, el planeta capital Coruscant apareció por primera vez en la novela Heir to the Empire (1991), de Timothy Zahn, antes de ser utilizado en La amenaza fantasma, mientras que la jedi Aayla Secura apareció en la serie de historietas de Dark Horse Comics, Star Wars: Republic, antes de que Lucas decidiera incorporarla como personaje secundario en El ataque de los clones. Ciertos aspectos como la muerte de algún personaje principal debía ser aprobada por Lucas antes de que fueran incorporadas en cualquier tipo de material alternativo. La función de Lucasfilm era asegurar una especie de continuidad en los diversos trabajos creados por varios autores con el paso del tiempo. Como máximo responsable de las decisiones tomadas en torno a Star Wars, Lucasfilm está penediente de que cualquier material relacionado con la space opera no se reproduzca sin su autorización. Tal es el caso de la empresa tecnológica Jedi Mind, la cual fue demandada en agosto de 2010 por «infringir los derechos de autor y promocionar sus productos bajo un concepto creado originalmente por Lucas». La demanda surgió ante el uso del concepto de la Fuerza y la filosofía Jedi en programas software. Lucas tuvo un rol significativo en la producción de varios proyectos televisivos, apareciendo usualmente como guionista o productor ejecutivo de ellos. Además, Star Wars dio lugar a varias adaptaciones de radio; la primera de ellas, basada en Una nueva esperanza, se transmitió por vez primera en National Public Radio en 1981. Esta fue creada por el escritor de ciencia ficción Brian Daley y dirigida por John Madden. Poco después, fue seguida por las adaptaciones de El Imperio contraataca (1983) y Return of the Jedi (1996). Peculiarmente todas ellas incluyeron material original de Lucas que no fue incluido en las películas. Mark Hamill, Anthony Daniels y Billy Dee Williams aparecieron en esas adaptaciones, interpretando sus personajes de Luke Skywalker, C-3PO y Lando Calrissian, respectivamente, con excepción de Return of the Jedi, en donde Luke fue interpretado por Joshua Fardon, mientras que Lando fue personificado por Arye Gross. El 25 de abril de 2014, Lucasfilm Ltd. anunció que, en preparación de la nueva trilogía de secuelas, la continuidad del universo expandido existente hasta la fecha no sería respetado en los futuros lanzamientos de la saga; el material del universo expandido existente continuaría siendo producido bajo la marca Star Wars Legends y una nueva continuidad canónica sería producida partiendo de la base de la historia de las seis películas existentes hasta la fecha y la serie Star Wars: The Clone Wars, añadiéndose a partir de dicho momento todo el material producido en el futuro. Aunque el universo expandido existente fuese declarado no canónico en su totalidad, continuaría siendo usado como fuente para futuro material de Star Wars. Star Wars Celebration El 30 de abril de 1999, con motivo del estreno de La amenaza fantasma, Lucasfilm llevó a cabo en Denver, Colorado una convención inspirada en el universo de Star Wars. Tuvo una duración de tres días (finalizó el 2 de mayo de ese año), y contó con una asistencia de 12 000 seguidores de la franquicia. Desde entonces, se han celebrado cinco acontecimientos Celebration más: Celebration II, acto al que acudieron 27 000 asistentes, aconteció en mayo de 2002, en Indianápolis, y se realizó con motivo del estreno de El ataque de los clones; Celebration III se celebró en abril de 2005 nuevamente en Indianápolis, tuvo una asistencia de 29 000 aficionados y fue realizado con motivo del debut en cines de La venganza de los Sith. Mientras tanto, el Celebration IV (mayo de 2007; tuvo un total de 35 000 asistentes, y se situó en Los Ángeles), Celebration Europe (julio de 2007; en el Centro de Exposiciones ExCeL, en Londres, con una asistencia de 29 000 seguidores de Star Wars) y Celebration Japan (julio de 2008; en el Makuhari Messe, en Tokio, con un total de 17 000 asistentes) se llevaron a cabo para festejar el 30 aniversario del lanzamiento de Una nueva esperanza. En agosto de 2010, con motivo del 30 aniversario del estreno de El Imperio contraataca y para festejar la continuidad de la serie animada Star Wars: The Clone Wars, se llevó a cabo Celebration V en Orlando, Florida. Este último tuvo una concurrencia estimada en 32 000 asistentes. De acuerdo con el sitio web oficial, Star Wars Celebration se define como «el acto oficial de Lucasfilm que celebra todas las cosas relacionadas con Star Wars, producidas por y para fanáticos». Nombre Fecha Lugar Ciudad Celebration I 30 de abril-2 de mayo de 1999 Wings Over the Rockies Air and Space Museum Denver, Colorado Celebration II 3-5 de mayo de 2002 Centro de Convenciones de Indiana Indianápolis, Indiana Celebration III 21-24 de abril de 2005 Indiana Convention Center Indianápolis, Indiana Celebration IV 24-28 de mayo de 2007 Centro de Convenciones de Los Ángeles Los Ángeles, California Celebration Europe I 13-15 de julio de 2007 Centro de Exposiciones ExCeL Londres, Reino Unido Celebration Japan 19-21 de julio de 2008 Makuhari Messe Chiba, Japón Celebration V 12-15 de agosto de 2010 Orange County Convention Center Orlando, Florida Celebration VI 16-19 de agosto de 2012 Orange County Convention Center Orlando, Florida Celebration Europe II 26-28 de julio de 2013 Messe Essen Essen, Alemania Celebration Anaheim 16-19 de abril de 2015 Centro de Convenciones de Anaheim Anaheim, California Celebration Europe III 15-17 de julio de 2016 Centro de Exposiciones ExCeL Londres, Reino Unido Celebration VII 13-16 de abril de 2017 Orange County Convention Center Orlando, Florida Celebration VIII 11-15 de abril de 2019 McCormick Place Chicago, Illinois Celebration Anaheim II 18-21 de agosto de 2022 Centro de Convenciones de Anaheim Anaheim, California Star Wars: In Concert En 2009 se estrenó Star Wars: In Concert, una producción de Lucasfilm realizada en asociación con Another Planet Touring, diseñada por Steve Cohen y dirigida por Dirk Brosse. Es una actuación musical de dos horas de duración en la que la Orquesta Filarmónica Real y un coro interpretan los temas musicales compuestos por John Williams, mientras se proyecta de forma sincronizada el montaje de toda una serie de escenas provenientes de las películas de Star Wars en una gran pantalla led de alta definición de hasta «tres pisos de altura». Además de la ejecución musical, Star Wars: In Concert está acompañado por una narración declamada por el actor Anthony Daniels, intérprete del personaje C-3PO en las películas. Originalmente el espectáculo era denominado Star Wars: A Musical Journey durante su gira por Europa, y además de las interpretaciones orquestales consistió en una exposición de artefactos inéditos usados en las películas, tales como vestuarios, bocetos y una variedad de utilería. Dicho acto se realizó por primera vez en abril de 2009, en el O2, en Londres. Su gira por el continente americano comenzó el 1 de octubre de 2009, cuando se presentó en el Honda Center, de Anaheim, California, y continuó durante 2010 recorriendo varias ciudades de los estados de Florida, Tennessee, Los Ángeles, Kansas, Texas, Arizona, California y Nevada, en Estados Unidos. Asimismo, el 9 de junio el espectáculo llegó a Monterrey, México, donde se presentó en la Arena Monterrey, mientras que el 12 del mismo mes se produjo en el Auditorio Nacional del Distrito Federal. Hasta 2010 se habían realizado 140 funciones en un centenar de ciudades de 11 países distintos. Según George Lucas: He visto algunas presentaciones de orquestas en vivo con clips de video de Star Wars y ese tipo de cosas. Pero esto [Star Wars: In Concert] es mucho más que eso: es mucho más emocional, puesto que lo que ellos han hecho aquí es tomar el contenido emocional de las bandas sonoras … uno [de los temas] es obviamente la Marcha imperial, otro es obviamente romántico … y han hecho un montaje por su cuenta con todas las imágenes de las películas, así que lo que ven ahí es en verdad una amplia gama de escenas que van acorde con la música […] A lo que voy es que esto realmente funciona, pues fue montado específicamente para hacer resaltar las emociones. Tras su primeriza presentación en Londres, la revista The Hollywood Reporter detalló en su reseña: «Si creen que el mundo no necesita otra variación de Star Wars aparte de las seis películas, las animaciones, videojuegos y juguetes, están olvidando la extraordinaria fuerza que posee la música de John Williams, escrita específicamente para esta épica de ciencia-ficción». Juegos Juegos de rol de mesa Artículo principal: Star Wars (juegos de rol) Hasta ahora ha habido tres juegos de rol de mesa ambientados en el universo de Star Wars: El primero fue editado entre 1987 y 1999 por la editorial West End Games con el título original en inglés Star Wars: The Roleplaying Game. Fue traducido al castellano en 1990 por la editorial barcelonesa Joc Internacional. El segundo fue editado entre 2000 y 2010 por Wizards of the Coast, con el título original en inglés Star Wars Roleplaying Game. En 2002 se publicó en inglés una revisión del primer manual de dos años antes. Esta revisión es el así llamado «manual básico revisado» ,Revised Core Rulebook en la edición original,. Es esta revisión de la primera edición la que ha sido traducida y publicada en castellano en 2003 por la editorial barcelonesa Devir Iberia. La segunda edición del juego ,titulada Saga Edition en Estados Unidos, «edición saga», no fue nunca traducida al castellano. El tercero, titulado en inglés igual que el precedente, Star Wars Roleplaying Game, empezó a ser publicado a partir de agosto de 2012 por Fantasy Flight Games, quien había adquirido la licencia de juegos de cartas, de miniaturas y de rol de Star Wars en 2011 después de que Wizards of the Coast la hubiese abandonado en 2010. El juego de rol de Star Wars de Fantasy Flight Games está dividido en tres standalone games ,tres manuales básicos independientes que no necesitan de los otros dos,, y cada uno de ellos está especializado en la interpretación de un tipo particular de personaje. Los tres manuales son los siguientes: Star Wars: Edge of the Empire, para jugar con personajes contrabandistas, cazarrecompensas, piratas, etc., Star Wars: Age of Rebellion, para jugar con personajes de la Alianza Rebelde, y Star Wars: Force and Destiny, para jugar con caballeros jedi que se ocultan del Imperio. Los tres manuales, junto a sus cajas de inicio, se encuentran, desde el año 2013, en curso de traducción al castellano por parte de la editorial sevillana Edge Entertainment. El primero de estos juegos, el publicado por West End Games entre 1987 y 1999, ha tenido una gran importancia e influencia en lo que se refiere a la elaboración del universo expandido. Aparte de las películas mismas y de unos pocos libros que habían tratado el tema, como la guía del universo Star Wars, publicada por Ballantine Books en 1984, se sabía todavía relativamente poco sobre la Galaxia de Star Wars en el momento de la publicación del juego de rol por parte de West End Games, en 1987. Ya entonces George Lucas, el autor de la saga cinematográfica, había establecido tanto las líneas principales como un gran número de detalles de su universo de ficción, pero no había entrado en todos los detalles posibles e imaginables que pudieran ir apareciendo en las innumerables partidas de un juego de rol. Consecuentemente, fue la editorial misma del juego, West End Games, la que tuvo la iniciativa de ir publicando suplementos de ambientación que describieran con detalle el universo en el que los jugadores del juego de rol interpretarían a sus personajes. Hasta el momento en que West End Games abandonó la licencia de explotación, en 1999, el volumen de suplementos publicados acabó siendo literalmente del orden de las decenas, influyendo además de manera permanente en la constitución canónica y oficial del universo de George Lucas. La contribución de los autores de West End Games a la construcción de este universo de ficción ,como Bill Slavicsek, Curtis Smith, Ken Rolston, Grant Butcher, Greg Gorden o Paul Murphy, fue tal que, a título de ejemplo, cuando el escritor Timothy Zahn llevaba casi un año escribiendo su primera novela ambientada en el universo de Star Wars, Heredero del Imperio, un encargo de Lucasfilm, la misma productora decidió enviarle por correo, en julio de 1990, una caja llena con los suplementos que West End Games había publicado hasta entonces. La voluntad de Lucasfilm no era otra que hacer que la literatura de Zahn no entrara en contradicción con los aportes de West End Games. La primera reacción de Zahn fue colérica, principalmente por el tiempo que él mismo había invertido hasta ese momento llevando a cabo sus propias investigaciones, pero en cuanto leyó los suplementos de West End Games, se convenció de la calidad de los mismos, como él mismo reconoció en el prólogo del libro The Thrawn Trilogy Sourcebook ,La guía de la trilogía de Thrawn,. Juguetes y figuras de acción La saga Star Wars ha dado lugar a numerosos juguetes de colección desde el estreno de su primera película. Las primeras colecciones de figuras de acción, a las que hoy en día se denomina con el anglicismo vintage ,«añejas», fueron comercializadas de 1977 a 1985. Posteriormente salieron a la venta otras colecciones, como Power of the Force, Power of the Jedi, Original Trilogy Collection, Saga Collection, 30th Anniversary, Unleashed, entre otras. Juegos de construcción (Lego) El universo de Star Wars ha sido también reproducido en una serie de juegos de construcción de la famosa marca LEGO. En 2019, Lego Star Wars celebrará su 20 aniversario y en 2022 la está programado que la licencia de Star Wars para Lego expire. Juegos de tablero El primer juego de tablero derivado de las películas de la saga fue Star Wars: Escape from the Death Star ,«Star Wars, evasión de la Estrella de la Muerte»,, editado y distribuido en el año mismo del estreno del episodio IV: Star Wars: Escape from the Death Star (1977) Con la salida al mercado en 1987 de su juego de rol, West End Games también editó, sucesivamente, los siguientes juegos de tablero: Star Wars: Star Warriors (1987) Star Wars: Assault on Hoth (1988) Star Wars: Battle for Endor (1989) Star Wars: Escape from the Death Star (1990; no confundir con el juego homónimo de 1977) Personas disfrazadas como Chewbacca y una criatura jawa, en 2007 Otros juegos de tablero de otros editores y también relacionados con la temática Star Wars son los siguientes: De la colección Batalla naval: Battleship: Star Wars Advanced Mission (2002) De la colección Monopoly: Monopoly Star Wars Limited Collectors Edition (1996) Monopoly Star Wars (1997) Monopoly Star Wars Episode 1 (1999) Monopoly: Star Wars Episode II (2002) Monopoly Star Wars Saga Edition (2005) Monopoly Clone Wars (2008) De la colección Risk Star Wars: Risk Star Wars The Clone Wars Edition (2005) Risk Star Wars Original Trilogy Edition (2006) De la colección Stratego: Stratego: Star Wars (2002) Stratego: Star Wars Saga Edition (2005) De la colección Trivial Pursuit: Trivial Pursuit: Star Wars Classic Trilogy Collectors Edition (1998) Trivial Pursuit Star Wars - Episode I (1999) Trivial Pursuit - Star Wars, Bite Size (2005) Trivial Pursuit DVD: Star Wars Saga Edition (2005) Trivial Pursuit DVD: Star Wars Saga Edition (2006) Otros: Star Wars : Destroy the Death Star (1977; Kenner) Star Wars: Return of the Jedi: Battle at Sarlaccs Pit (1983; Hasbro) Star Wars: The Queens Gambit (2000; Hasbro) Star Wars: Silent Death Starfighter Combat Game (2001; Wizards of the Coast) Star Wars: Epic Duels (2002; Hasbro) Star Wars: Jedi Unleashed (2002; Hasbro) Juegos de miniaturas En 1991 West End Games editó un juego de miniaturas titulado Star Wars Miniatures Battles. En 2004 Wizards of the Coast editó un juego de miniaturas coleccionables titulado Star Wars Miniatures. En 2012 Fantasy Flight Games editó Star Wars: X-Wing Miniatures Game, un juego de miniaturas coleccionables en el que se representan combates de naves espaciales y en el que las miniaturas son precisamente versiones a escala reducida de las naves de combate de la saga cinematográfica. En 2014 Fantasy Flight Games editó Star Wars: Imperial Assault, un juego de miniaturas que reúne características propias de los juegos de estrategia y de los juegos de escaramuzas. En 2015 Fantasy Flight Games editó Star Wars: Armada, un juego de miniaturas sobre combates entre cruceros (flotas de grandes naves espaciales que se enfrentan la una a la otra, a saber la Armada Imperial y la Flota de la Alianza Rebelde), a imagen y semejanza del combate naval real, tal y como se ve en las películas de la saga. Cromos Las tarjetas coleccionables de Star Wars han sido publicadas desde la primera serie «azul», realizada por Topps, en 1977. Desde entonces, decenas de series han sido producidas, con Topps siendo el creador autorizado en territorio estadounidense. Algunas series de tarjetas son de fotogramas de las películas, mientras que otras son obras artísticas originales. Además, muchas de estas tarjetas se han convertido en piezas altamente coleccionables referentes a la saga, con algunas «promos» muy extrañas, como por ejemplo la tarjeta enumerada como «P3» de un «Yoda flotante» que acompañó a la Galaxy Series II de 1993. Por lo general dichas cartas se llegaban a vender en 1000 USD o más. Cabe señalar que los sets de «cartas base» ,también conocidas como «cartas comunes», eran abundantes, aunque muchas «cromos» tipo insert card[Nota 3] eran muy difíciles de adquirir en el mercado. Star Wars: Frames A principios de 2011, Lucas dio a conocer una colección de fotografías titulada Star Wars: Frames la cual consiste de varias imágenes tomadas directamente de cada una de las escenas de las películas de Star Wars y seleccionadas específicamente por Lucas. Estas ediciones impresas ,una por cada película, fueron puestas a la venta tras el lanzamiento de la saga en formato de video Blu-ray, en septiembre de 2011, en un compilatorio protegido con una cubierta de madera y tuvieron un tiraje limitado a 1138 copias, con un precio de adquisición de 3000 USD.  Formatos de vídeo casero La serie de películas de Star Wars se ha distribuido en varios formatos de video, desde los años 1980. La mayoría de estos productos presenta varios cambios implementados por el propio Lucas con el fin de acercar cada película más a su visión original. Aunque Star Wars se lanzó inicialmente en video casero de formato VHS, en 1977 Ken Films sacó a la venta en Estados Unidos una serie de clips de video con escenas selectas de Una nueva esperanza en formato Súper 8, mismos que podían verse a través de un cañón proyector. Al inicio, se distribuían fragmentos de nueve minutos de duración de la cinta (los cuales podían incluir o no sonido durante su reproducción), para luego dar paso a escenas de casi 20 minutos. Sin embargo, Fox consideró en su momento a estos productos como una técnica de marketing para promocionar Star Wars, en vez de catalogarlos meramente como un producto de consumo. En cuanto a la nueva trilogía, si bien The Phantom Menace y El ataque de los clones se lanzaron tanto en formato VHS como en DVD, a partir de La venganza de los Sith únicamente se distribuyeron los DVD de ambas entregas ,la versión VHS solo se puso a la venta en Australia y Reino Unido,. De acuerdo al sitio web Counting Down: «20th Century Fox y Lucasfilm lanzarán Star Wars: Episodio III - La venganza de los Sith, la cual se espera que se convierta en uno de los filmes más exitosos de la temporada, exclusivamente en DVD. Star Wars será la primera gran producción en despedirse del VHS, aunque algunos dicen que esto no resulta sorprendente, pues existe una gran comunidad de seguidores del DVD». Ambas trilogías fueron lanzadas nuevamente a la venta en dos paquetes diferentes de DVD, uno por cada trilogía, disponibles en territorio estadounidense a partir del 4 de noviembre de 2008. Cabe añadir que los discos en su interior no contienen ningún bonus o material adicional inédito. A finales de 2011, las seis películas fueron distribuidas en formato Blu-ray. VHS, CED, Betamax y LaserDisc A finales de los años 1970, el VHS no resultó ser un fenómeno generalizado: varias películas se distribuyeron incluso mucho tiempo después de su proyección en cines. Por lo tanto, el VHS de Star Wars comenzó a comercializarse a partir de 1980, cuando el formato ya gozaba de una gran popularidad. La primera vez que la saga estuvo disponible oficialmente a la venta en formato de video fue el 1 de septiembre de 1982; tres meses antes, Fox les envió un comunicado a todos los distribuidores de video confirmándoles que los casetes VHS disponibles a partir de mayo de ese año, solamente se ofrecerían en alquiler, pues las copias en sí empezarían a venderse a partir de septiembre. Lo anterior ocurrió debido a que posiblemente Fox era consciente de que el lanzamiento de los videos en plena temporada veraniega podría competir con el estreno en cines de Star Wars. No obstante, varios distribuidores se quejaron de esta determinación, pues sabían que la demanda de los videos de la trilogía era muy alta, así que optaron en cambio por ofrecer a sus clientes lo que catalogaron como «un alquiler de por vida», en la que cada consumidor en Estados Unidos pagaba 120 USD por adquirir el VHS. En cuanto a posibles problemas legales, debido a que eran varias las distribuidoras que realizaron esta estrategia, Fox no pudo tomar cartas en el asunto. En la temporada decembrina de 1984, se estrenó El Imperio contraataca en VHS y Betamax; se estima que en su primer año de debut, se vendieron hasta 375 000 unidades tan solo en territorio estadounidense. Al año siguiente, en 1985, se lanzaron las dos primeras cintas de la trilogía original en formato LaserDisc, mientras que al año siguiente debutó Return of the Jedi en formato de video casero. Con el paso del tiempo, se lanzaron hasta tres distintas versiones de la trilogía original en VHS: una a finales de 1990 (siendo la primera vez que la trilogía aparecía en compilatorio, incorporando algunas características adicionales), otra en 1994 ,la cual contó con una remasterización del audio, y otra más en 1995 ,la última previo a la aparición de las ediciones especiales; además, para esta edición, se utilizó marketing viral para promocionar el compilatorio al crearse un sitio web exclusivo por parte de Fox,. Precedidas por el lema «Ahora o Nunca», refiriéndose al período limitado durante el cual se vendieron, aparecieron las versiones VHS de los LaserDisc distribuidos unos años antes. Al mismo tiempo, se comercializaban ediciones de la trilogía en formato CED, cuya única diferencia, respecto al VHS, era su costo reducido. En 1992, la trilogía debutó en formato de pantalla ancha en VHS, en el paquete Star Wars Trilogy Special Letterbox Collectors Edition, conteniendo el video titulado From Star Wars to Jedi: The Making of a Saga a manera de material adicional.. En 1993, se lanzó el compilatorio de LaserDiscs titulado Star Wars Trilogy: The Definitive Collection, la cual contenía la trilogía original en formato de video de pantalla ancha y remasterizada con tecnología THX, comentarios de audio, así como varias características especiales, junto con una copia del libro George Lucas: The Creative Impulse, el cual contiene las vivencias del director en sus primeros veinte años de trayectoria cinematográfica. Meses después, en octubre de 1995, apareció nuevamente el mismo paquete, solo que esta vez no incorporaba el libro, y contrariamente venía acompañado de un par de discos adicionales, los cuales abordaban la producción de la trilogía. Debido a limitaciones técnicas, la versión Laserdisc de Star Wars: Episodio V - El Imperio contraataca fue comprimida, por lo que su duración fue limitada a solamente dos horas. Hasta la fecha, no existen versiones VHS de la saga disponibles en el mercado. En 2000, se estrenó La amenaza fantasma en formato VHS, siendo el único formato de video disponible de la película en Estados Unidos. Remasterizado con tecnología THX, el casete se hizo acompañar de un documental especial sobre el proceso de filmación de la cinta, además de un libro de 48 páginas con ilustraciones, dibujos y bocetos usados originalmente en la producción. Edición especial de 1997 En 1997, la trilogía fue remasterizada y se redistribuyó en las salas de cine ,a unas cuantas semanas de diferencia entre cada película, a partir de enero,, y más tarde en VHS bajo el nombre de «Ediciones Especiales». Además de una limpieza hecha a los negativos, Lucas agregó y cambió la mayoría de las escenas para convertir cada película en lo que originalmente había planeado. Según las propias palabras de Lucas: Habrá solo una. No va a ser un corte «en bruto», sino que será la edición definitiva. La otra versión [la versión original proyectada en cines] será una especie de artefacto interesante que la gente mirará y dirá: Había un boceto antes de esto. Lo mismo ocurre con las obras de teatro y los borradores de los libros. En esencia, las películas nunca se terminan, sólo son abandonadas. […] Pero no es así como debería funcionar. Ocasionalmente, se puede volver atrás y hacer la versión que había esperado, lo cual hice con American Graffiti y THX 1138; ese es el lugar en donde vivirán por siempre. Entonces, lo qué es finalmente importante para mí, es cómo será la versión en DVD, porque será algo que todos recordarán. Las otras versiones desaparecerán. Incluso las 35 millones de copias de vídeo de Star Wars que están por ahí no van a durar más de treinta o cuarenta años. En un centenar de años, la única versión de la película que todos recordarán será la versión en DVD [de la Edición Especial], y serán capaces de proyectarla en una pantalla de cine con la calidad óptima. Creo que es la prerrogativa de un director, y no la de un estudio, volver atrás y reinventar una película. George Lucas. Muchos de los cambios consistieron en adiciones digitales, como la ampliación de los ambientes, el incremento de los personajes adicionales o de limpieza de fondos. Sin embargo, esto despertó cierto descontento entre muchos fanes; la más famosa, que engendró a varios sitios de protesta, fue una escena en Una nueva esperanza, que presentaba a Han Solo siendo perseguido por un cazarrecompensas llamado Greedo, en la que este negocia su libertad. En la versión original, Han le dispara al cazarrecompensas, causándole así la muerte, mientras que en la modificación, poco antes de que Han disparara, Greedo trata de matarlo, pero fracasa. La escena cambia, en cierto modo, el carácter de Han Solo, puesto que si en la primera versión el personaje es mostrado con una actitud casi mala y ambigua, en la segunda versión, él parece defenderse del enemigo de una manera legítima. Incluso la escena en la que Han Solo se encuentra con Jabba the Hutt en el puerto espacial, pisoteando su cola sin que este se moleste, no coincide con la caracterización de Jabba, la cual es descrita como alguien que se encuentra poco dispuesto a aceptar cualquier tipo de insulto. Este tipo de cambios no fue bien recibido por los fanáticos, y tiempo después estos mostraron su preferencia por las versiones originales de las películas. DVD El 16 de octubre de 2001, Star Wars debutó en formato DVD con el lanzamiento de La amenaza fantasma en un set de dos discos, conteniendo varias características adicionales. Además, fue la primera vez que se incluyeron escenas eliminadas de Star Wars en formato de video casero. Al año siguiente, se lanzó el Episodio II en el mismo formato, conteniendo una serie de materiales extra al igual que su predecesor. Edición de 2004 Ian McDiarmid, quien interpretó a Palpatine en Return of the Jedi y en la trilogía de precuelas, grabó una escena para la edición de El Imperio contraataca lanzada en 2004 que sustituyó a la introducción original de su personaje en la saga. La empresa Lowry Digital Images realizó un largo trabajo para mejorar la trilogía original de Star Wars, tanto en el ámbito del audio como del video, y el 24 de septiembre de 2004 fue distribuida en formato DVD. El paquete incluyó cuatro discos: los tres primeros con las películas y uno adicional con varias características especiales. Uno de los cambios más visibles es la escena introductoria de Palpatine en El Imperio contraataca: el actor Ian McDiarmid, quien interpretó al personaje en Return of the Jedi y en la trilogía de precuelas, grabó dicha escena durante la producción de La venganza de los Sith y fue añadida en sustitución de la original. Asimismo, Lucas modificó parte del diálogo entre Vader y Palpatine para que fuera más acorde con los acontecimientos de las precuelas. De forma similar, se volvió a montar la escena final de The Return of the Jedi en la que aparecen los espíritus de Anakin, Yoda y Obi-Wan: el actor original que interpretó a Anakin para esa única escena, Sebastian Shaw, fue reemplazado con un archivo de video de Hayden Christensen, intérprete de Anakin en la trilogía de precuelas, causando descontento entre los seguidores de la saga. Otras modificaciones menores pueden evidenciarse en el DVD de Una nueva esperanza: la escena del enfrentamiento entre Han Solo y Greedo fue montada nuevamente, solo que en esta versión los dos disparan prácticamente de forma simultánea (Greedo sigue siendo el primero que dispara). Mientras tanto, en El Imperio contraataca, el actor Temuera Morrison grabó algunos diálogos de Boba Fett y se eliminó prácticamente la escena en la que Luke grita una vez que salta para escapar de Vader en Ciudad Nube. De acuerdo a declaraciones del propio Lucas, la edición en DVD de 2004 constituye la versión canónica de la trilogía original. Edición de 2006 El 12 de septiembre de 2006, se lanzaron en DVD las «versiones inalteradas» de las películas originales de Star Wars, conteniendo la edición original proyectada en cines de cada filme, junto con la edición remasterizada de 2004. Cabe añadir que esta edición estuvo disponible a la venta por un tiempo limitado, concluyendo su distribución el 31 de diciembre de 2006. No obstante, poco después se aclaró que el término «versión inalterada» no se refiere en sí a los negativos originales de cada cinta, sino más bien a la edición LaserDisc de 1993, de acuerdo a un documento expedido por Lucasfilms. Blu-Ray A principios de 2010, Lucasfilm confirmó que se ha estado trabajando por años en la edición de alta definición Blu-ray de Star Wars. En agosto de 2010, durante el evento Celebration V, George Lucas anunció el lanzamiento de las seis películas de la franquicia en formato Blu-Ray en un box set que salió a la venta en septiembre de 2011. Asimismo, se incluyó material inédito adicional, como por ejemplo una escena eliminada de Return of the Jedi, donde Luke usa por primera vez su nuevo sable de luz de hoja color verde, para infiltrarse al palacio de Jabba the Hutt. Tras terminar de ensamblar su sable, Luke lo esconde en la cúpula de R2-D2. A principios de 2011, StarWars.com dio a conocer que habría tres ediciones de Star Wars para Blu-ray: una que tendrá 9 discos con las seis películas de la serie, otra con la trilogía de precuelas y con 3 discos en su haber, y una más con las tres secuelas en 3 discos Blu-ray. También se encuentran disponibles las seis películas por separado en este formato. Finalmente, la edición Blu-ray debutó el 13 de septiembre de 2011 en Londres, Inglaterra, y al día siguiente llegó a Hispanoamérica y a España. El 16 de septiembre comenzó a distribuirse en EE. UU. Apenas en su primera semana de lanzamiento, se vendió un millón de copias de las ediciones Blu-ray de Star Wars, convirtiéndose en la franquicia que más ingresos generó en dicho formato desde el debut del Blu-ray en el mercado. No obstante, algunas ediciones realizadas por Lucas en esta nueva versión provocaron reacciones variadas de los seguidores de la serie. Una de las escenas modificadas que fueron objeto de crítica incluye la adición de un diálogo de Darth Vader en el Episodio VI, cuando este se redime y mata a Palpatine. Legado e impacto cultural La galaxia NGC 936, que se asemeja al TIE Fighter, un caza de combate espacial muy usado en el universo de Star Wars. Cada una de las películas de Star Wars inicia con el popular prólogo «Hace mucho tiempo, en una galaxia muy, muy lejana […]». La saga de Star Wars ha tenido un impacto significativo en la cultura popular moderna. La saga ha influido a menudo en otras obras de ciencia ficción y las técnicas e innovaciones que introdujo han mejorado la realización cinematográfica. A ella se atribuye también el resurgimiento del interés popular en la ciencia ficción y su establecimiento como un género que puede resultar en un éxito de taquilla ,esto último logrado junto con la cinta Tiburón, de 1975,. De acuerdo al sitio web Cine y Letras: «Hay muchas películas de éxito. Hay muchas películas que son enormemente populares y han sido vistas por millones de espectadores en todo el mundo. Pero son muy pocas las que se han hecho un lugar en el imaginario colectivo como el que ha conquistado la saga Star Wars». Asimismo, Star Wars cambió de forma significativa las películas que se producían en Hollywood en ese entonces, haciendo que los productores pasaran de abordar historias profundas, serias y dramáticas para enfocarse en éxitos de taquilla en los cuales los efectos especiales cobrarían especial importancia en el desarrollo de la trama. Se puede decir entonces que el mundo de los efectos especiales se revolucionó con el estreno de la primera entrega, un aspecto que no había progresado mucho desde los años 1950, para la cual George Lucas creó la empresa Industrial Light & Magic y que desde entonces no ha dejado de aportar novedades en este campo. Igualmente, Lucasfilm introdujo innovaciones en el sonido y el montaje con la creación de EditDroid, que cambió la forma de montar las películas y que precedió a los sistemas que se usan en la actualidad. Cabe añadir que también creó el modelo de producción de trilogías cinematográficas, mostrándolo como un sistema que, gracias a sus derechos de merchandising, puede generar mayores ganancias que una sola película. En cuanto a 20th Century Fox ,productora de las películas,, tras el éxito de la primera película que obtuvo hasta 100 millones de dólares en solamente tres meses, las acciones de la empresa pasaron de 6 a 25 USD por acción, generando ganancias de 1,2 millones de dólares por día para el estudio. Con esto, Fox pudo adquirir las corporaciones Aspen skiing y Pebble Beach Golf, y todavía gozar de un considerable excedente en 1977. A partir de los relanzamientos, continuaciones y productos comerciales, el estudio continuó enriqueciéndose a tal grado que pasó de ser una productora al borde de la quiebra a un conglomerado estable en la industria. Star Wars ha influido en varias personalidades del cine, como por ejemplo James Cameron (director de filmes como Aliens y Avatar). Varios cineastas han reconocido que Star Wars les impulsó a dedicarse al cine, como James Cameron que dejó su trabajo de camionero tras ver la primera entrega en 1977, y otros han citado su influencia en algunas de sus películas, como John Singleton, Roland Emmerich, Ridley Scott y Peter Jackson. Estos dos últimos destacaron especialmente un concepto que usaron, respectivamente, en Alien y en la trilogía cinematográfica de El Señor de los Anillos: el «futuro usado», llamado así por el propio George Lucas y consistente en mostrar todo con un aspecto sucio, como si llevara años siendo usado, frente a la limpieza y perfección que anteriormente mostraban todas las películas de ciencia ficción. Scott lo calificó como «un toque maravilloso» y Jackson afirmó que el realismo que dicho concepto aportaba a las películas ayudaba además a que el espectador se acercara a los personajes. Según las propias palabras de John Lasseter, director de filmes como Toy Story y Bichos, una aventura en miniatura: «Hay pocos momentos en mi vida que nunca olvidaré y uno de ellos es haber visto Star Wars en el Teatro Chino –llevaba solo dos días en cartelera–. Recuerdo que mientras la veía no podía creer que una película pudiera emocionar tanto a alguien. Al final, me dio una sacudida. Miré hacia la audiencia y todos estaban gritando». Las películas y los personajes han sido parodiados en televisión, cine y música en numerosas ocasiones, y algunos términos como «Imperio maligno» y frases como «Que la Fuerza te acompañe» se han convertido en parte del léxico popular y audiovisual, hasta tal grado que esta última fue enlistada como la octava mejor frase en la historia del cine estadounidense por el American Film Institute. Por otra parte, cuando Ronald Reagan propuso la Iniciativa de Defensa Estratégica (SDI), un sistema basado en láseres y misiles para interceptar a misiles balísticos intercontinentales (ICBM) enemigos entrantes hacia Estados Unidos, el plan fue etiquetado con rapidez bajo el nombre de «Star Wars», implicando que dicha hazaña era cuestión de ciencia ficción y que ello también se relacionaba con la carrera de actuación de Reagan. De acuerdo con Frances FitzGerald, Reagan se molestó por esto, pero el subsecretario de Defensa, Richard Perle le dijo a sus colegas que «el nombre no era tan malo»; «¿Por qué no?», respondió. «Es una buena película. Además, los buenos ganaron». Esto ganó más resonancia cuando Reagan describió a la Unión Soviética como un «Imperio maligno». El 31 de octubre de 1997, se estrenó una exposición en el Museo Nacional del Aire y el Espacio titulada Star Wars: The Magic of Myth, donde se mostraban modelos de producción originales, así como accesorios y vestuarios usados en las primeras tres películas de la serie. Dicha instalación fue clausurada el 31 de enero de 1999, tras lo cual realizó una gira por territorio estadounidense a través de Smithsonian Institution Traveling Exhibition Services. Desde entonces, la única forma en que puede accederse a ella es por medio de la página web oficial del instituto, realizando un «recorrido virtual». Cabe señalar que existe una exposición similar, creada por el Boston Museum of Science, que no tiene vínculo alguno con Star Wars: The Magic of Myth. En octubre de 2007, la NASA lanzó un transbordador espacial que llevaba en su interior el sable de luz originalmente usado por Luke Skywalker en The Return of the Jedi, con el propósito de ponerlo en órbita; el accesorio pasó dos semanas en el espacio exterior, tras las cuales fue regresado a la Tierra el 7 de noviembre de 2007. Además, el efecto de video holográfico asociado con Star Wars sirvió a manera de herramienta tecnológica para la cadena CNN durante la cobertura especial de las elecciones presidenciales estadounidenses de 2008. En él, la reportera Jessica Yellin y el cantante will.i.am parecían estar en los estudios de CNN ,radicados en Nueva York,, hablando en persona con los conductores Anderson Cooper y Wolf Blitzer, cuando en realidad se hallaban en Chicago, específicamente en el mitin de Barack Obama. El efecto se logró a partir de la grabación de Yellin y will.i.am con cámaras 35 HD, frente a un panel azul especial ,blue screen,. Es importante añadir que dicho efecto holográfico inspiró a un grupo de científicos de la Universidad de Arizona para desarrollar una tecnología capaz de visualizar a una persona distante en tiempo real, tal y como se ve en las películas de Star Wars. Inicialmente, se contempló usar dicha tecnología para desarrollar un nuevo tipo de videoconferencias, sin embargo todavía se estudia su aplicación para fines más consumistas así como en cirugías y manufactura de medios de transporte. Tiempo después del estreno de su película Avatar, James Cameron mencionó su deseo de que esta se convierta en un fenómeno de la cultura popular, como Star Wars, Star Trek y las obras de J. R. R. Tolkien. En sus propias palabras: «Tienes que competir mano a mano con estos otros trabajos épicos de fantasía y ficción, los Tolkiens, Star Wars y Star Trek. La gente quiere una realidad alternativa que sea constante para sumergirse en ella y quieren los detalles que la enriquecen y hacen que merezca la pena invertir su tiempo». Tras el éxito de Avatar en formato 3-D, George Lucas reconsideró la posibilidad de adaptar las seis películas a ese mismo formato, y dijo: «El éxito que obtuvo el monstruo de James Cameron me ha convencido de que la tecnología ha evolucionado a tal punto que ahora estoy dispuesto a reconfigurar la serie que otorga esa atmósfera extraterrestre para que pueda ser vista en tres dimensiones». En cuanto a la terminología usada en las películas, la filosofía de los caballeros Jedi ha servido de inspiración para abrir incluso escuelas dedicadas exclusivamente a la impartición de estas técnicas de meditación, y en algunos casos los instructores responsables deciden vestirse como los personajes de Star Wars con tal de resultar más distinguibles para sus alumnos. Hasta 2011 existían tres escuelas de este tipo: una en Nueva York, otra en Rumania y una más en Chile. Parodias Las películas y los personajes de Star Wars han sido parodiados en numerosas ocasiones, tanto en televisión como en cine. Entre las parodias fílmicas destaca principalmente Hardware Wars (1977), un cortometraje dirigido y escrito por Ernie Fosselius y al que el propio George Lucas dio su beneplácito. Con un presupuesto de 8000 USD, fue filmado en bares, playas y garajes de San Francisco en tan solo cuatro días y logró recaudar medio millón de dólares, situándose como el cortometraje con mayor éxito de todos los tiempos. Lucasfilm realizó dos falsos documentales dedicados a la saga, ambos convertidos en obras de culto: Return of the Ewok (1982) y R2-D2: Beneath the Dome (2001). El primero fue dirigido y escrito por David Tomblin, asistente del director Richard Marquand, y cuenta una historia ficticia sobre como el actor Warwick Davis consiguió el papel del ewok Wicket W. Warrick en Return of the Jedi. El segundo fue dirigido por Don Bies y Spencer Susser durante el rodaje de El ataque de los clones y cuenta con la participación de parte del equipo de la saga como George Lucas y Francis Ford Coppola o los actores Carrie Fisher, Harrison Ford, Ewan McGregor, Hayden Christensen y Natalie Portman. Su trama se centra en una historia ficticia sobre la vida de R2-D2 como un droide real. Asimismo, existe una parodia oficial del filme The Clone Wars estrenada en 2009 y denominada Lego Star Wars: The Quest for R2-D2, la cual se transmitió en el canal Cartoon Network «con motivo de la celebración de los primeros diez años de LEGO Star Wars». En 1982 fue estrenada en Turquía la película del director Çetin İnanç, Dünyayı Kurtaran Adam, también conocida como la Star Wars turca o por su traducción literal al inglés The Man Who Saves the World («El hombre que salva el mundo»). Su guion, escrito por Cüneyt Arkın, hacía uso de material tomado ilegalmente de las películas de Star Wars e incluso fueron incluidas algunas de sus propias escenas, pero por su tono cómico fue considerada una parodia de la saga y fue catalogada como una película de culto tiempo después de su estreno. Spaceballs (1987), una película de Mel Brooks, es otra de las parodias realizadas sobre la saga, aunque también incluye referencias a películas como El planeta de los simios o Star Trek. En 2007 y 2008 respectivamente, se estrenaron dos parodias que contaron con la colaboración de Lucasfilm, Robot Chicken: Star Wars y su secuela Robot Chicken: Star Wars - Episode II, ambas con una duración de media hora, dirigidas por Seth Green y con actores de la saga que prestaron su voz a los personajes, como Fisher, Billy Dee Williams, Mark Hamill y el propio Lucas. A principios de 2010, se anunció la producción de una nueva serie animada de comedia, la cual fue escrita por Brendan Hay ,responsable de las noticias cómicas de The Daily Show,, y cuenta con la asistencia de los creadores de Robot Chicken, Seth Green y Matthew Senreich. Dicha serie toma lugar durante la trilogía original. Los guiones de películas como Toy Story 2, Back to the Future, Clerks, Hot Shots! 2, Looney Tunes: Back in Action, E.T., el extraterrestre y gran parte de las obras de Kevin Smith incluyen alusiones a la saga, así como numerosas series de televisión, entre ellos Lost, Doctor Who, The Big Bang Theory, 30 Rock, Tiny Toons, Muppet Babies, Los Simpson, Futurama, y South Park, de la que se originó el término Defensa Chewbacca. La serie de animación Padre de familia le dedicó tres episodios a la saga: Blue Harvest (2007), basado en Una nueva esperanza; Something, Something, Something, Dark Side (2010), basado en El Imperio contraataca e Its a Trap (2010), basado en Return of the Jedi. La serie de animación de 1995 Pinky y Cerebro destinó su último episodio a ser una parodia de Una nueva esperanza y Return of the Jedi donde además aparecían la mayoría de los personajes de la serie Animaniacs, todos representando a algún personaje de las películas originales (este episodio se llamó Star Warners, de 1998). Temas musicales Varios músicos han rendido homenaje al universo de Star Wars y han incluido referencias a la saga en sus canciones. El músico y productor discográfico Meco contactó en 1977 con Casablanca Records, la compañía propietaria de los derechos de autor de la banda sonora, para crear un álbum de versiones de esta que finalmente lanzó bajo el título Star Wars and Other Galactic Funk y en el que contó con la ayuda del arreglista Harold Wheeler. Posteriormente lanzó otros trabajos inspirados en la saga como Star Wars, Meco Plays Music from the Empire Strikes Back y Music Inspired by Star Wars. El humorista y cantante Weird Al Yankovic creó dos parodias sobre Star Wars: «Yoda», basada en el tema «Lola» de The Kinks, y «The Saga Begins», basada en «American Pie» de Don McLean y en la que vuelve a contar la historia de La amenaza fantasma desde la perspectiva de Obi-Wan Kenobi. En 2000, Jason Brannon y Chris Crawford crearon un tema basado en la trilogía original, «Star Wars Gangsta Rap», que se popularizó a través de un video musical realizado íntegramente en tecnología Flash y que fue listado por la revista Time como la mejor comedia en línea de 2001.  Asimismo, el dúo estadounidense The Carpenters, mencionó que la canción de «Space Encounter» tenía cierta alusiones a Star Wars, pues de acuerdo a las palabras de Richard Carpenter, líder del dúo, comentó que dicho trabajo representa «una verdadera pieza bombástica en honor a John Williams».  En 2015, la banda surcoreana EXO, sacó la canción titulada Lightsaber de la película Star Wars: Episodio VII - El despertar de la Fuerza, en colaboración con SM Entertainment, que fue sacada el 11 de noviembre, quien también la canción la acompañó en su álbum especial de invierno 2015 Sing For You en versión coreana y china y en ese mismo año la canción fue sacada en versión japonesa el 19 de diciembre. Política El paralelismo que existe entre la política estadounidense y Star Wars tiene su origen en la primera película de la serie, Una nueva esperanza. Estos ecos políticos de la serie han demostrado el hecho de que Lucas transportó parte de la idea de Apocalypse Now, a la saga de ciencia ficción, tal y como afirma Walter Murch: George Lucas tenía previsto dirigir Apocalypse Now. Tras el éxito de American Graffiti, resucitó la película, pero el tema seguía caliente y nadie quería invertir en un proyecto como este. Así que George revalorizó la cuestión, preguntándose lo que quería decir con Apocalypse Now y redujo el mensaje de la película en la capacidad de un grupo de personas para derrotar a un poderoso gigante solo con la fuerza de sus ideas. Esto influyó en la esencia de la historia en un contexto relacionado con la política y el ambiente de la película hace mucho tiempo, en un lugar muy lejano. Así dio origen a Star Wars. Los vietnamitas se convirtieron en los rebeldes y los Estados Unidos se convirtieron en el Imperio. Star Wars es la versión de Lucas de Apocalypse Now. Logotipo de la Iniciativa de Defensa Estratégica (SDI), conocida también como Star Wars La premisa de The Clone Wars ha sido, a menudo, yuxtapuesta con la de la Segunda Guerra Mundial. Hay varias similitudes históricas que existen entre Palpatine y Hitler, puesto que ambos usaron la guerra y varios chivos expiatorios con el fin de manipular las emociones y los sentimientos de la sociedad, y ambos se caracterizan por ser líderes fuertes y carismáticos. En cualquier caso, Lucas fue citado en varias entrevistas, y la principal fuente de complot político detrás de The Clone Wars y otras intrigas de la saga es la guerra de Vietnam y la era de Watergate, durante el cual los líderes políticos optaron por la corrupción y la trampa. Además, puede decirse que hay con certeza una serie de parecidos históricos entre The Clone Wars y la guerra de Irak.  Desde el 11 de septiembre, la administración de Bush ha usado nuestro miedo colectivo hacia el terrorismo tal y como el Emperador Palpatine usó las Guerras Clon: para incrementar a nivel interno el poder ejecutivo y acrecentar el poder de Estados Unidos en el exterior. Brian Fanelli. A pesar de sus afirmaciones de que la historia de Una nueva esperanza tiene ecos de la política estadounidense, Lucas argumentó que había escrito la película antes de la guerra en Irak: «Las semejanzas entre Vietnam y lo que estamos haciendo ahora en Iraq son increíbles. Escribí la historia en tiempos de Vietnam, no de Iraq». En varias entrevistas, sin embargo, dice que Bush no influyó en ningún aspecto en la historia de las películas, sino que fue el propio presidente Richard Nixon quien inspiró la escritura de los guiones. Lucas dijo que había leído algo de historia y se cuestionó «por qué, tras haber asesinado a Julio César, el Senado romano le dejó las cosas a su sobrino también sediento de poder, César Augusto, o por qué después de afrontar una revolución, Francia le dio sus dominios a Napoleón, un dictador».  Incluso en la política moderna, la saga ha dejado su huella. Un ejemplo es cuando Ronald Reagan propuso la Iniciativa de Defensa Estratégica (SDI) para utilizar los sistemas de armas con base en tierra y el espacio para proteger a los Estados Unidos de los ataques de misiles balísticos con ojivas nucleares; dicho proyecto se denominó Star Wars, generando una serie de críticas. Películas de aficionados El universo de Star Wars también ha dado lugar a la producción de películas de bajo presupuesto realizadas por los propios fanáticos. Star Wars ha servido de inspiración para muchos fanáticos de la serie que han optado por producir sus propios proyectos sin autorización de Lucasfilm, y que en su mayoría consisten en producciones caseras de bajo presupuesto. En 2002 Lucasfilm produjo la primera ceremonia de premiación denominada The Official Star Wars Fan Film Awards, en la que reconoció a los directores involucrados en las distintas películas de fanáticos así como al género naciente ,«películas de fanes»,. Debido a cuestiones de problemas legales relacionados con los derechos de autor y marcas registradas, el evento inicialmente estaba destinado a abordar parodias, falsos documentales y programas de tipo documental. Las películas de fanes situadas en el universo de Star Wars no podían resultar electas, sin embargo en 2007 Lucasfilm modificó los estándares de participación para permitir que participaran todos aquellos proyectos de esta temática. Aunque varias películas de fanes han incorporado elementos del universo expandido para abordar sus tramas, ninguna de estas es considerada como parte oficial del canon de Star Wars. No obstante, el personaje estelar de la serie Pink Five fue incorporado en la novela Allegiance (2007) de Timothy Zahn, siendo la primera vez que un personaje creado por un aficionado era usado en material oficial de la franquicia. Si bien la mayoría de las ocasiones Lucasfilm ha permitido la creación de material creado por fanes, ese material jamás ha sido aprobado como parte de Star Wars."


###############################################################################################


ksampletext_wikipedia_bacilo: str = "Bacilo. En bacteriología: la palabra bacilo se usa para describir cualquier bacteria con forma de barra o vara, y pueden encontrarse en muchos grupos taxonómicos diferentes tipos de bacterias. Sin embargo el nombre Bacillus, se refiere a un género específico de bacteria. El otro nombre Bacilli; hace referencia a una clase de bacilos que incluyen dos órdenes, uno de los cuales contiene al género Bacillus. Los bacilos son bacterias que se encuentran en diferentes ambientes y solo se pueden observar con un microscopio. Los bacilos suelen dividirse en el mismo plano y son solitarios, pero pueden combinarse para formar diplobacilos, estreptobacilos y cocobacilos: Diplobacilos: Dos bacilos dispuestos uno al lado del otro. Estreptobacilos: Bacilos dispuestos en cadenas. Cocobacilos: Ovalados y en forma de bastoncillo. Por tipo de bacteria los bacilos pueden ser: Bacilos Gram positivos: fijan el cristal violeta (tinción de Gram) en la pared celular porque tienen una gruesa capa de peptidoglucano. Bacilos Gram negativos: no fijan el cristal violeta y se tiñen con el colorante de contraste usado en la tinción de Gram que es la safranina, debido a que tienen una fina capa de péptidoglucano en medio de dos bicapas lipídicas en la cual se encuentran los lipopolisacáridos o también llamados endotoxinas (principalmente en la membrana externa). Aunque muchos bacilos son patógenos para el ser humano, algunos no hacen daño, pues producen algunos productos lácteos como el yogur (lactobacilos). A lo largo de la historia de la medicina y de la microbiología, varias de estas bacterias han producido enfermedad en los humanos y por lo general se han adoptado el nombre del científico que los descubría, por ejemplo: Bacilo de Aertrycke: Salmonela. Bacilo de Bang: Brucella abortus. Bacilo de Ducrey: Haemophilus ducreyi. Bacilo de Eberth: Salmonella typhi. Bacilo de Nicolaier: Tétano. Bacilo de Hansen: Mycobacterium leprae. Bacilo de Klebs-Löffler: Corynebacterium diphtheriae. Bacilo de Koch: Mycobacterium tuberculosis. Bacilo de Morex: Género Moraxella. Bacilo de Yersin: Yersinia pestis."


###############################################################################################

